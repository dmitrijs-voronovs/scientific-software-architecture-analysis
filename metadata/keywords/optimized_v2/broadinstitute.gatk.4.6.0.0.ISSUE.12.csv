quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,FuncotationMap refactoring and VCF/MAF concordance for protein changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4838:15,refactor,refactoring,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4838,1,['refactor'],['refactoring']
Modifiability,Funcotator - Need to create funcotator config file as an alternative to CLI options,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4581:39,config,config,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4581,1,['config'],['config']
Modifiability,Funcotator - Refactor Funcotation class to use a HashMap for each field,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3919:13,Refactor,Refactor,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3919,1,['Refactor'],['Refactor']
Modifiability,Funcotator - turn config file names into enum,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5465:18,config,config,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5465,1,['config'],['config']
Modifiability,Funcotator datasource configs should not honor spaces (and some other special characters) in the datasource name nor version.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5937:22,config,configs,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937,1,['config'],['configs']
Modifiability,Funcotator hg38 data source not working; configuration files contains reference to files not present,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521:41,config,configuration,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521,1,['config'],['configuration']
Modifiability,"Funcotator needs to be able to handle variants in the upstream and downstream flanks of a gene. Right now because we walk over variants that match genes in Gencode, there will be no gene matches for upstream and downstream variants. . To do this we will need to update our features to match +/- the maximum of the padding for each en(5' or 3' padding). This will also affect IGR processing. We will need to update the caching scheme in `FeatureCache` to cache around a locus rather than just in front of it (in a configurable manner). 5' should default to 5000 and 3' should default to zero.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4771:513,config,configurable,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4771,1,['config'],['configurable']
Modifiability,"Funcotator requires all GENCODE datasources (in the config) to be named ""Gencode""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4791:52,config,config,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4791,1,['config'],['config']
Modifiability,Funcotator throwing error more than one config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:40,config,config,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['config'],['config']
Modifiability,Funcotator: Add user input for default annotation values on command-line/config file.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3783:73,config,config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3783,1,['config'],['config']
Modifiability,Funcotator: get the NCBI build version from the datasource config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522:59,config,config,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522,1,['config'],['config']
Modifiability,GATK Reduced Docker Layers for ACR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:20,Layers,Layers,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,1,['Layers'],['Layers']
Modifiability,"G_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5355,Config,ConfigFactory,5355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,Gatk leaving behind config files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6771:20,config,config,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6771,1,['config'],['config']
Modifiability,GencodeFuncotationFactory refactoring for better separation of concerns between CNV vs small mutation annotation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5932:26,refactor,refactoring,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5932,1,['refactor'],['refactoring']
Modifiability,GenomicsDB: max # of alleles should be configurable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687:39,config,configurable,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687,1,['config'],['configurable']
Modifiability,"GenomicsDBImport lets users writes variants to GenomicsDB. The inputs are a loader JSON configuration file, callsets JSON file containing sample names and corresponding stream names and a stream JSON file containing files names of the streams. Note: This code uses GenomicsDB v0.4.0. Please check whether Maven central has the updated version first. @kgururaj , please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389:88,config,configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389,1,['config'],['configuration']
Modifiability,"Given that this helps you out, but doesn't change the behavior of our tools, it's pretty much a refactor from our end and I'm happy to give it a ðŸ‘",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-334156894:96,refactor,refactor,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2534#issuecomment-334156894,1,['refactor'],['refactor']
Modifiability,"Glad you were able to resolve your issue. Not sure if this is specific to the CNV tool or if the exception caused by the Spark configuration is more general. Tagging engine team @droazen, but closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686#issuecomment-467510488:127,config,configuration,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686#issuecomment-467510488,1,['config'],['configuration']
Modifiability,"Google is deprecating and removing their implementation of the old style GA4GH read and reference API's. . > ; > Reads API functionality is now replaced by the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global ge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:364,flexible,flexible,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,1,['flexible'],['flexible']
Modifiability,"Google made an incompatible change to the dataproc api which is causing all builds to fail. We're seeing errors like this:; ```; ERROR: (gcloud.beta.dataproc.clusters.create) The required property [region] is not currently set.; It can be set on a per-command basis by re-running your command with the [--region] flag. You may set it for your current workspace by running:. $ gcloud config set dataproc/region VALUE. or it can be set temporarily by the environment variable [CLOUDSDK_DATAPROC_REGION]; ```. It's mentioned in gcloud release notes here:; ```; 260.0.0 (2019-08-27); Breaking Changes; (Cloud Dataproc) Modified --region flag to be mandatory.; To use Cloud Dataproc commands, pass the --region flag on every invocation, or set the dataproc/region configuration variable via gcloud config set dataproc/region.; For gcloud beta dataproc commands, this flag/config value is required.; For gcloud dataproc commands, the default will remain global until January 2020.; ```. I'm going to set the environment variable in our travis config right now, and then open a separate PR to specify region in all the commands.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6129:383,config,config,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6129,8,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1578:153,config,configuration,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578,1,['config'],['configuration']
Modifiability,"Gradle is gradually transitioning to a new more flexible software model. We use this new style model for our native code, but not for java. We should investigate switching our java build to the new model. Some details are here https://docs.gradle.org/current/userguide/java_software.html. It includes some interesting new features like a mechanism for explicitly declaring a public api for a project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1579:48,flexible,flexible,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1579,1,['flexible'],['flexible']
Modifiability,"HI @lbergelson - ; I'm working on a bug/warning in the variant calling workflow where it's complaining about not finding a logger:. `21:04:59.525 INFO ProgressMeter - Starting traversal; 21:04:59.526 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (io.grpc.netty.shaded.io.netty.util.internal.logging.InternalLoggerFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:05:10.018 INFO ProgressMeter - chr1:4642050 0.2 205000 1172992.6`. I found that if I had gatk's build.gradle NOT exclude the log4j.properties file I get rid of that warning, so I'm trying to understand the issue [here](https://github.com/broadinstitute/gatk/blob/33bda5e08b6a09b40a729ee525d2e3083e0ecdf8/build.gradle#L441): (where you found log4j.properties clashed with log4j2.xml) . James Emery is on the git blame for that, but he thinks that's because of the refactoring he did. Thanks in advance - I'm not sure if there's something else I should be doing with the xml version of that file to avoid this warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722:1023,refactor,refactoring,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722,1,['refactor'],['refactoring']
Modifiability,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBU",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5129,Config,ConfigFactory,5129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5768,Config,ConfigFactory,5768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3617,Config,ConfigFactory,3617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5612,Config,ConfigFactory,5612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6251,Config,ConfigFactory,6251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5422,Config,ConfigFactory,5422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"Ha, this PR was much tinier than I expected -- feet are barely damp. My ""wishlist"" would include a much bigger refactor because I made a mess in Java7 and didn't have the time to clean up (especially with generics) after we switched in Java8. I'm still tinkering with the rank sum tests though, so it's not worth tackling the refactor until those are good to go. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2657#issuecomment-299206702:111,refactor,refactor,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2657#issuecomment-299206702,2,['refactor'],['refactor']
Modifiability,HaplotypeCallerSpark lose a lot of variable sites and the result jitter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4488:35,variab,variable,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488,1,['variab'],['variable']
Modifiability,"Hello - I'm not sure if you want me using issues for feedback, but i thought I'd pass this along. In another thread we discussed how to possibly do scatter/gather processing of GenomicsDB workspaces. The general idea is that we want to have long-lived workspaces to which we will repeatedly add more samples. Executing this append would be a lot more convenient to do scattered over intervals. Since GenomicsDB already has the data organized into folders by interval, I figured we might be back to manually split one workspace apart by copying each contig's folder out to make a new workspace, execute the merge over that interval, and then copy them back together. So far as I can tell this works. It seems like it will significantly speed the process of creating new workspaces and also adding samples. As I said on the other thread, since your docs recommend making a backup of a workspace before trying to append samples anyway, copying it out into new working folders to execute that append step isnt all that different. I realize we're doing a non-supported thing here. I post simply to mention that this scheme seems like it will be quite useful and I hope you might keep it in mind as GenomicsDB evolves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620:1204,evolve,evolves,1204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620,1,['evolve'],['evolves']
Modifiability,Hello - we're interested in creating a custom extension of Funcotator with different output formats. This PR should be quite low risk - it just converts a handful of privates fields/methods to protected to make it easier to extend this tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8124:224,extend,extend,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8124,1,['extend'],['extend']
Modifiability,"Hello - we're trying to run Funcotator with a custom data source, where that source is a locatableXsv (i.e. simple tab-delimited file with columns for contig, start, and end). I believe I understand how to make this TSV and the config file. The issue is that I dont see a way to create the index (i.e. tsv.idx), and GATK fails when I try to run against a data source without the index. Not that surprisingly, IndexFeatureFile errors when trying to index a TSV saying ""no suitable codecs found"". Is there another tool that's able to make indexes on simple TSVs?. FWIW, the only example LocatableXsv source I could find in the default data sources is Oreganno. The majority of TSV-based sources are simpleXSV and just map using Gene symbol (so apparently no index is required). When I try to index the existing oreganno.tsv file, I get the same problem. I dont know how that original index was created. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7986:228,config,config,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7986,1,['config'],['config']
Modifiability,"Hello @SZLux,. This looks suspiciously like #3050. I suspect this isn't a PathSeq issue, but to be sure can you please try to run another GATK Spark tool such as CountReadsSpark? If that does not work, it's likely an issue with your configuration or Spark/Java versions being incompatible. . What kind of environment are you running in? My suspicion is you are running on a cluster and have the correct Spark/Java version on the driver (master node) but perhaps not on the workers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768:233,config,configuration,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768,1,['config'],['configuration']
Modifiability,"Hello @cmnbroad. My current solution satisfy all the constraints and it's not too complicated, although is not as simple as a common generic class that just need to be extended. Have a look and if you like it I can implement some tests for `CountingVariantFilter`; if not, I could come back to a separate `CountingVariantFilter` with its own and/or/negate inner classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490:168,extend,extended,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2195#issuecomment-272482490,2,['extend'],['extended']
Modifiability,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didnâ€™t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:595,variab,variable,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,1,['variab'],['variable']
Modifiability,"Hello again, I have not been able to solve the problem with the launch ReadsPipelineSparkMulticore.wdl. :(; ![qoKs8pEt_-0](https://user-images.githubusercontent.com/55628707/161521914-49b7b10f-3264-43d5-8f5c-739d924656a1.jpg); I use this command:; `java -jar ../cromwell-77.jar run ReadsPipelineSparkMulticore.wdl -i exome/ReadsPiplineSpark_exome.json`; This is my json file:; ![qo2S-PSVs_xNXb5ZysWT8g](https://user-images.githubusercontent.com/55628707/164080985-ae983a19-104a-4742-9401-82ea938ef69e.jpeg); Here is my configuration (CPU and RAM):; ![PEP_IbcPaXyZtql0oefE_A](https://user-images.githubusercontent.com/55628707/164081131-58958f86-5aaf-450f-b985-d7885d4a8de4.jpeg); ![FDmF-6wr4RelT11qqByfGA](https://user-images.githubusercontent.com/55628707/164081137-2d07fe83-845e-463e-ab17-7a5cecb415e0.jpeg). Found this error in my stderr file:; 22/04/06 15:36:17 ERROR Executor: Exception in task 10.0 in stage 11.0 (TID 1596); java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateFractionalErrorArray(BaseRecalibrationEngine.java:440); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:141); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$null$0(BaseRecalibratorSparkFn.java:33); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn$$Lambda$705/136574652.accept(Unknown Source); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at org.broadinstitute.hellbender.utils.iterators.CloseAtEndIterator.forEachRemaining(CloseAtEndIterator.java:47); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$6ed74b3e$1(BaseRecalibratorSparkFn.java:33); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796:519,config,configuration,519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796,1,['config'],['configuration']
Modifiability,"Hello, . I have the same problem, . path_input_file=/work/gr-fe/archive/sample_repository/all_exome_gvcfs_hg38/FVH/exomes #patient GVCF; path_name_individu=/work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_name.txt; path_output=/work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_data.vcf.gz; tmp_folder=/scratch/sboutry/logs/combine_gvcf_file; nbr_groups=2. #Path to database and programs; REF=/work/gr-fe/saadat/Reference_Genome/GRCH38_no_alt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa.gz; BCFTOOLS=/work/gr-fe/sboutry/tools/bcftools/install/bin/bcftools; export BCFTOOLS_PLUGINS=/work/gr-fe/sboutry/tools/bcftools/bcftools/plugins; TABIX=/work/gr-fe/sboutry/tools/tabix/tabix-0.2.6/tabix; GATK=/work/gr-fe/sboutry/tools/gatk/gatk-4.2.2.0/gatk. cd ${path_input_file}. ${GATK} --java-options ""-Xmx180G -XX:ParallelGCThreads=36"" CombineGVCFs -R ${REF} --variant ${path_name_individu} -O ${path_output}/patient_data.g.vcf.gz. where all my files are like this . JL0015.g.vcf.gz; JL0016.g.vcf.gz; JL0017.g.vcf.gz; JL0018.g.vcf.gz; JL0019.g.vcf.gz; JL0020.g.vcf.gz; JL0182.g.vcf.gz; JL0183.g.vcf.gz; JL0184.g.vcf.gz; JL0185.g.vcf.gz; JL0186.g.vcf.gz; JL0234.g.vcf.gz; JL0278.g.vcf.gz; JL0412.g.vcf.gz; JL0417.g.vcf.gz; JL0515.g.vcf.gz. A USER ERROR has occurred: Cannot read file:///work/gr-fe/sboutry/excalibur/input/11_12_23_gvcf_FVH/patient_name.txt because no suitable codecs found. Thanks a lot for any help . Best, . Simon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8255#issuecomment-1918989841:652,plugin,plugins,652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8255#issuecomment-1918989841,1,['plugin'],['plugins']
Modifiability,"Hello, I made a PoN with my samples and created an hdf5 PoN. This was made using . ##Preprocess; ``; gatk PreprocessIntervals -R ref/hs37d5.fa --bin-length 10000 --padding 0 -O preprocessed_intervals.interval_list; ``. ##annotate; ``; gatk AnnotateIntervals -R ref/hs37d5.fa -L preprocessed_intervals.interval_list --interval-merging-rule OVERLAPPING_ONLY -O annotated_intervals.tsv; ``. ##PoN; ``; gatk --java-options ""-Xmx6500m"" CreateReadCountPanelOfNormals -I MD0078B1.counts.hdf5 -I MD1341B1.counts.hdf5 --minimum-interval-median-percentile 5.0 -O sandbox/cnvponC.pon.hdf5; ``. When I use DenoiseReadCounts on the .counts.hdf5 for the tumour samples, I get an error.; This is the command I used: ; ``; gatk DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; ``. I know that some of these errors are expected but I don't see any other errors and I'm not sure why it stopped running. Any help would be appreciated thank you!. ##Affected Version: gatk/4.0.1.2. ##Bug Report. Using GATK jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar DenoiseReadCounts -I BT1813.counts.hdf5 --count-panel-of-normals cnvponC2.pon.hdf5 --standardized-copy-ratios BT1813.standardizedCR.tsv --denoised-copy-ratios BT1813.denoisedCR.tsv; 20:08:44.839 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/hpf/tools/centos6/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:08:45.222 INFO DenoiseReadCounts - ------------------------------------------------------------; 20:08:45.222 INFO DenoiseReadCounts - The Genome Analysis Toolkit (GATK) v4.0.1.2; 20:08:45.222 INFO D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7258:553,sandbox,sandbox,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7258,1,['sandbox'],['sandbox']
Modifiability,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5948:1170,extend,extended,1170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948,1,['extend'],['extended']
Modifiability,"Hello, I would like to ask for the implementation of a SlidingWindowWalker (both for reads and variants), that could be very interesting for other tools. I was thinking that it could be similar to IntervalWalker, but generating the intervals internally using a window size and step size as parameters (provided by the user and without merging overlapping intervals, like suggested on #302). I think that this issue is different from issue #10 and the pull request #890 because it is a more general Walker that could be abstract and extendable by other tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1198:532,extend,extendable,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1198,1,['extend'],['extendable']
Modifiability,"Hello, more information on the parameters and runtime can be found here: #7492 . the stacktrace is now:; ```; ...; 22:14:59.985 INFO ProgressMeter - chrUn_JTFH01001653v1_decoy:301 116.6 2161460 18530.7; 22:15:11.142 INFO ProgressMeter - chrUn_JTFH01001673v1_decoy:301 116.8 2161540 18501.9; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-02waxZ.hg38.bam -tumor TUHR14TKB --germline-resource gs://depmapomicsdata/gnomad.genomes.r3.0.sites.vcf.bgz -pon gs://depmapomicsdata/1000g_pon.hg38.vcf.gz -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/cec2a1a6-ffc3-4f1b-ba94-27ae918c56e9/Mutect2/b389d86b-8b0b-4d77-8224-a5a3e3a0b4e5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0004-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ln: failed to access '/cromwell_root/*normal-pileups.table': No such file or directory; ln: failed to access '/cromwell_root/*tumor-pileups.table': No such file or directory; 2021/10/05 22:15:24 Starting delocalization.; ...; ```. I run mutect2 in tumor only mode. ; Interestingly, this error always only happen at the last shard only (every other shard runs to completion). Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494:344,variab,variable,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494,1,['variab'],['variable']
Modifiability,"Hello,. I am running GermlineCNVCaller and PostprocessGermlineCNVCalls (GATK v4.2.5) for CNV analysis on our targeted capture. . My output segment vcfs have no SVLEN or SVTYPE values although those are described in their headers. . Info from header includes:; ```; ##INFO=<ID=AC_Orig,Number=A,Type=Integer,Description=""Original AC"">; ##INFO=<ID=AF_Orig,Number=A,Type=Float,Description=""Original AF"">; ##INFO=<ID=AN_Orig,Number=1,Type=Integer,Description=""Original AN"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End coordinate of the variant"">; ##INFO=<ID=SVLEN,Number=.,Type=Integer,Description=""Difference in length between REF and ALT alleles"">; ##INFO=<ID=SVTYPE,Number=1,Type=String,Description=""Type of structural variant"">; ```; But the actual vcf output only has the END variable. Example output:. ```; 13	32839931	CNV_13_32839931_32945267	N	.	3076.53	.	END=32945267	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:63:169:3077:523:342; 13	32950659	CNV_13_32950659_32954345	N	<DEL>	3076.53	.	END=32954345	GT:CN:NP:QA:QS:QSE:QSS	0/1:1:7:709:3077:709:831; 13	32968699	CNV_13_32968699_73961012	N	.	3076.53	.	END=73961012	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:14:210:3077:295:630; 14	24883828	CNV_14_24883828_94854954	N	.	3076.53	.	END=94854954	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:72:100:3077:287:299; 15	32992921	CNV_15_32992921_91535389	N	.	3076.53	.	END=91535389	GT:CN:NP:QA:QS:QSE:QSS	0/0:2:35:102:3077:198:331; ```. Commands running are below:. ```; docker run -v /home/dnanexus/inputs:/data $GATK_image gatk GermlineCNVCaller \; -L /data/beds/filtered.interval_list -imr OVERLAPPING_ONLY \; --annotated-intervals /data/beds/annotated_intervals.tsv \; --run-mode COHORT \; $batch_input \; --contig-ploidy-calls /data/ploidy-dir/ploidy-calls/ \; --output-prefix CNV \; -O /data/gCNV-dir. parallel --jobs 8 '/usr/bin/time -v docker run -v /home/dnanexus/inputs:/data $GATK_image \; gatk PostprocessGermlineCNVCalls \; --sample-index {} \; --autosomal-ref-copy-number 2 \; --allosomal-contig X \; --allosomal-contig Y \; --",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7964:787,variab,variable,787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7964,1,['variab'],['variable']
Modifiability,"Hello,. I am trying to set up a python environment to use gatk DetermineGermlineContigPloidy module. I cannot use conda. I have tried to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:628,config,configparser,628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,2,['config'],['configparser']
Modifiability,"Hello,. I'd like to see if your team would potentially be willing to accept a PR to add a feature to GenotypeGVCFs. The general problem is this:. 1) When running GenotypeGVCFs, the default is to output variant sites, and this will therefore vary based on the set of samples. While there is an argument to include every site, calling against every position of the genome takes a very long time.; 2) As you know, a VCF file generally only includes variable sites in the current samples. Therefore, this doesnt differentiate between the situation where all samples have no data and when all samples are wild-type.; 3) We want to merge VCFs with data from different cohorts, including WGS and WES. It's just not practical to call 1000s of samples as one unit through GenotypeGVCFs (we're constantly adding new data and would need to keep re-calling). When merging these VCFs, we see a problem that is especially acute at sites with relatively rare variants. If the variant is only present in one or a few input VCFs, the other VCFs frequently lack that site (they are all wild-type). On merge, this is interpreted as no-data, which can be misleading. . The best solution I can devise is to force the input VCFs to output at a whitelist of sites, including if all samples are non-variant. While GenotypeGVCFs can be made to output at every genomic position, outputting everything is a huge leap in computational time. . I would propose to make a PR to augment GenotypeGVCFs to support an ""--always-output-calls-whitelist"" argument. The user can provide a FeatureInput. If provided, GenotypeGVCFs would output all variable sites (existing behavior), and also output any position spanning the intervals of this file, even if all samples at wild-type. . I only just started to look at how to implement this - i can some back with a more specific proposal. However, my initial thought is that we need to hook into drivingVariants or LocusWalker.traverse. Does your team have any thoughts on this, before we spe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239:446,variab,variable,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239,1,['variab'],['variable']
Modifiability,"Hello,; I was trying to hard-filter the vcf files outputed by GATK HaplotypeCaller, and I want to keep variants that meet the following condition: depth (QD) < 2.0 || FisherStrand (FS) > 60.0 || root mean square mapping quality (MQ) < 40.0 || mapping quality rank sum test(MQRankSum) <âˆ’12.5 || ReadPosRankSum <âˆ’8.0. Here is my code:. `; for i in *.vcf.gz; do samplename=${i##*/}; sample=${samplename%%.*}; $gatk VariantFiltration -V $i --filter-expression ""QD < 2.0"" --filter-name ""QD2"" --filter-expression ""FS > 60.0"" --filter-name ""FS60"" --filter-expression ""MQ < 40.0"" --filter-name ""MQ40"" --filter-expression ""MQRankSum < -12.5"" --filter-name ""MQRankSum-12.5"" --filter-expression ""ReadPosRankSum < -8.0"" --filter-name ""ReadPosRankSum-8"" -O ../../hardFilter/snp/${sample}.hardfil.snp.vcf.gz ; done; `; But I met some warings:; `; 17:37:07.563 WARN JexlEngine - ![0,9]: 'MQRankSum < -12.5;' undefined variable MQRankSum; 17:37:07.564 WARN JexlEngine - ![0,14]: 'ReadPosRankSum < -8.0;' undefined variable ReadPosRankSum; 17:37:07.564 WARN JexlEngine - ![0,9]: 'MQRankSum < -12.5;' undefined variable MQRankSum; 17:37:07.564 WARN JexlEngine - ![0,14]: 'ReadPosRankSum < -8.0;' undefined variable ReadPosRankSum; 17:37:07.564 WARN JexlEngine - ![0,9]: 'MQRankSum < -12.5;' undefined variable MQRankSum; 17:37:07.564 WARN JexlEngine - ![0,14]: 'ReadPosRankSum < -8.0;' undefined variable ReadPosRankSum; `; It seems that the ""MQRankSum"" and ""ReadPosRankSum"" were not defined.; Then I checked the filtered vcf, and the filter is seemed not working, many variations that do not satisfy the expression are also determined as pass:; ![å±å¹•æˆªå›¾ 2024-08-27 181728](https://github.com/user-attachments/assets/8478ec45-ea1f-4f81-8cf2-f49b8ca5369d); I hope to receive your help.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8964:903,variab,variable,903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8964,6,['variab'],['variable']
Modifiability,"Hello,; When I use GenomicsDBImport and GenotypeGVCFs , I get the following error: Couldn't create GenomicsDBFeatureReader, I have no problem with running CombineGVCFs with CombineGVCFs. I reference #6616 , but I think we have different errors, And I checked my environment variable, the parameter is displayed as ' declare -x TILEDB_DISABLE_FILE_LOCKING=""1"" '. Hope you can give me some help, thanks in advance. Exact GATK commands used : gatk GenotypeGVCFs -R path/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf. > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home//miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar GenotypeGVCFs -R /home//workdir/data_single_cell/sperm/hg38ncbi.fa -V gendb://mydatabase -O rawvariants.vcf; > 21:14:29.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/miniconda3/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > May 25, 2020 9:14:29 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 21:14:29.494 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.7.0; > 21:14:29.495 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:14:29.495 INFO GenotypeGVCFs - Executing as lbjiang@mu01 on Linux v3.10.0-327.el7.x86_64 amd64; > 21:14:29.495 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; > 21:14:29.495 INFO GenotypeGVCFs - Start Date/Time: May 25, 2020 9:14:29 PM CST; > 21:14:29.495 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:14:29.495 INFO GenotypeGVCFs - ----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6627:274,variab,variable,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6627,1,['variab'],['variable']
Modifiability,"Hello. I am Adam Yongxin Ye, a PhD candidate in Peking University, supervised by Prof Liping Wei. We have developed MosaicHunter, a bioinformatic tool that can identify postzygotic single-nucleotide mosaicisms (with allele fraction deviated from homozygous 0, 1 and heterozygous 0.5) in bulk sequencing data of a single sample without matched control. After I had the recent lectures on GATK4 tutorial in Beijing, I thought it might be great to merge MosaicHunter into GATK framework, especially for the local assembly function in HaplotypeCaller and Mutect2, to increase the sensitivity & specificity and even extend for mosaic indels, as well as to make MosaicHunter easy for more users to use. MosaicHunter utilized GATK preprocessing, distinguished mosaicisms from germline homozygous and heterozygous sites by a Bayesian genotyper, and applied several stringent hard filters. MosaicHunter has been published (https://academic.oup.com/nar/article/45/10/e76/2962179 and http://www.nature.com/articles/cr2014131) and is publicly available (http://mosaichunter.cbi.pku.edu.cn/ and https://github.com/zzhang526/MosaicHunter (with source code in java)). So I wonder if GATK team is interested in this suggestion. Could someone or may we contribute it into GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632:611,extend,extend,611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632,1,['extend'],['extend']
Modifiability,Help messages for missing environment variables are cruel cruel lies,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/571:38,variab,variables,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/571,1,['variab'],['variables']
Modifiability,"Here is an issue ticket for adding the diagnosetarget feature to DepthofCoverage. This request was created from a contribution made by Benoit Dewitte on February 09, 2022 13:15 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4418326549531-Whats-the-equivalent-of-gatk3-depthofcoverage-and-diagnosetarget-). \--. Hi! ; ; I'm currentlly refactoring ours pipeline which use depthofcoverage and diagnosetarget from GATK 3.8. DespiteÂ  google researchs I can not found the equivalent of diagnosetarget for gatk 4 and the depthofcoverage tool that i found is still in beta. I found this [github post](https://github.com/broadinstitute/gatk/pull/5913) which talk about pushing the diagnosetarget feature in depthofcoverage but I was not able to find any more informations. Should I stay on the gatk 3.8 version or upgrade our pipe to gatk 4?. I appreciate very much if someone could enlighten me.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270769'>Zendesk ticket #270769</a>)<br> gz#270769</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7702:494,refactor,refactoring,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7702,1,['refactor'],['refactoring']
Modifiability,"Here it is. An overview of what's been added:; - metrics package; - a few general metrics classes (e.g. MultiLevelMetrics); - may want to push these down into HTSJDK later; - added some utils; - utils.gene: gene annotation; - utils.illumina: general Illumina-related utils (adapters, etc); - utils.text.parsers: text parsing; - utils.variant: added dbSNP stuff; - MathUtils: added a few basic things (mean, stddev, etc) with unit tests; - tools; - three major packages:; - analysis: metrics + analyses (including necessary Rscripts); - illumina: Illumina parsing + validation; - vcf: VCF manipulation + GenotypeConcordance; - also two smaller packages, fastq and intervals, containing a few tools each; - tests; - all existing tests were ported; still, overall test coverage goes down by ~6%; - all CLP integration tests have been ported to the new argument system; - test data has also been carried over, and is neatly organized; - there are no huge files, and very few above 100KB (just a few VCFs I think); - however, the Illumina test data is pretty big - ~6MB spread over ~1700 files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/347:274,adapt,adapters,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/347,1,['adapt'],['adapters']
Modifiability,Here's another one with our exact problem (solved largely by putting the config onto HDFS). http://progexc.blogspot.com/2014/12/spark-configuration-mess-solved.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322554798:73,config,config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322554798,2,['config'],"['config', 'configuration-mess-solved']"
Modifiability,"Here's some old code that uses SamLocusIterator (from tfennel) that AllelicCapseg can adapt for now. From Tim:; ""They key to making this nice and simple is the SamLocusIterator class, which given a BAM file and a list of intervals, will give you pileups at each position in the intervals, filtered how you want them, and even provide convenience methods to access the exact base per read that is piled up at the site etc. The really nice things about doing it this way is that the constructor to SamLocusIterator takes a simple parameter to tell it whether to use an index/query mechanism (similar to what you're doing now) or to just read the BAM serially up until the last interval is reached and output the loci of interest. Running the below with ~100k sites on a standard exome (15GB or so) without using the index took only about 15 minutes."". ```; public void pileup(final File bam, final IntervalList intervals, final int minQ, final File outputFile) {; final int MAX_INTERVALS_FOR_INDEX = 25000; // just a guess, not sure what the right number is. final SamLocusIterator iterator = new SamLocusIterator(new SAMFileReader(bam), intervals, intervals.size() < MAX_INTERVALS_FOR_INDEX);; iterator.setEmitUncoveredLoci(false);; iterator.setQualityScoreCutoff(minQ);. final BufferedWriter out = IoUtil.openFileForBufferedWriting(outputFile); // will automatically gzip if filename ends with .gz; try {; while (iterator.hasNext()) {; final SamLocusIterator.LocusInfo locus = iterator.next();; int a=0, c=0, g=0, t=0;. for (final SamLocusIterator.RecordAndOffset rec : locus.getRecordAndPositions()) {; switch (rec.getReadBase()) {; case 'A' : ++a; break;; case 'C' : ++c; break;; case 'G' : ++g; break;; case 'T' : ++t; break;; }; }. out.append(locus.getSequenceName() + ""\t"" + locus.getPosition() + ""\t"" + a + ""\t"" + c + ""\t"" + g + ""\t"" + t + ""\t"");; }. out.close(); ; }; catch (IOException ioe) { throw new RuntimeIOException(ioe); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420:86,adapt,adapt,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/335#issuecomment-88102420,1,['adapt'],['adapt']
Modifiability,"Hey @bbimber I will have to think on this. The most simple solution might be to add a feature context side input for the annotation in question but looking at how that code is threaded in the variant callers it would take a little bit of work to add it to those tools and probably introduce some complicated questions, (like for example: what is the correct featurecontext to send to annotate a variant that only covers one base of the site in question where the feature context object exists?). Its possible to do something like that for variant annotator a little bit more easily but i guess the question comes down to this: How generalized do you think this annotation will be? Does it need to be annotatable with variant annotator or could you write a separate tool that does the variant -> variant association and calculates the annotation without using the plugin framework? If it needs to be generalizable I would agree with @droazen that the easiest approach would be to add the side input as an argument and make the annotation object responsible for querying the feature context. This is inelegant but might be preferable to putting the entire walker context into the `annotate()` function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851:863,plugin,plugin,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754249851,2,['plugin'],['plugin']
Modifiability,"Hey @magicDGS, since I'm on the forum documentation side of things, I'm not familiar with a plugin. Is this something @cmnbroad typically takes care of?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342820936:92,plugin,plugin,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342820936,1,['plugin'],['plugin']
Modifiability,Hey @valleema. We don't think this is a bug given that the argument `--max-mnp-distance` is intended to remove Multi-Nucliotide polymorphism which generally are adjacent SNPs(i.e. sites with the pattern ref: AA alt: GT for example). Your example site here doesn't have an MNP but rather is a multi-allelic site. Consequently the flag `--max-mnp-distance` is not doing anything wrong in this case. I would direct future questions about your use case (namely how to un-collapse multi-allelic sites) to our forums: https://gatk.broadinstitute.org/hc/en-us/community/topics. Thank you;,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7782#issuecomment-1115218078:128,polymorphi,polymorphism,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7782#issuecomment-1115218078,1,['polymorphi'],['polymorphism']
Modifiability,"Hey all, I'm still interested in supporting this. We don't really have a ""plugin API"", I am in fact the API, but if you give me something usable I'll plug it in. As this is marked ""QuixoticDream"" I don't think that's likely. I'm closing the corresponding IGV issue, too many open issues, but it doesn't mean I've lost interest.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3286#issuecomment-433201230,2,['plugin'],['plugin']
Modifiability,"Hi - @kaixinxiaonvwa-hub . I have a couple of questions:. - Can you post your full stack trace with the errors?; - Did you attempt to enable `gnomAD` data sources or is it doing this without any changes to the data sources directory? Did you do any other configuration steps after downloading the datasources and before running funcotator?. If you enable `gnomAD`, the datasources are hosted on google cloud. If you don't have an internet connection or google cloud is blocked, Funcotator will not be able to connect to read the gnomAD data and will show the error in your `1` case above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773:255,config,configuration,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1494703773,1,['config'],['configuration']
Modifiability,"Hi @MattMcL4475 - if you mean the images I used to test these, they are: `terrapublic.azurecr.io/gatk:4.5-squashed` and `terrapublic.azurecr.io/gatk:4.5-min-layers`. Note that these are manually built and pushed for this task and didn't go through any automated tests that are in this repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2159093348:157,layers,layers,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2159093348,1,['layers'],['layers']
Modifiability,"Hi @MattMcL4475 - so sorry I didn't see your reply until now (likely due to my email filters). Anyway, I don't think we have a squashed version of this in the official GATK image. From the conversation above, I think we decided to go with just the reduced layers version of the image which is what this PR is for. I do, however, have a sample squashed version here: `terrapublic.azurecr.io/gatk:4.5-squashed` - but then again, it's not in the official Docker hub repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2231781482:256,layers,layers,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808#issuecomment-2231781482,1,['layers'],['layers']
Modifiability,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:331,enhance,enhancement,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973,2,"['adapt', 'enhance']","['adapt', 'enhancement']"
Modifiability,"Hi @bhanugandham . I wasn't able to reproduce your problem on my laptop. Based on the error message, it looks like a network config Spark issue. What environment are you running this in? If it's MacOS, can you post the contents of `/etc/hosts`? Also the error message looks like it's truncated. You might be able to get the full stack trace by adding `--verbosity DEBUG`, which would be helpful as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380:125,config,config,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802#issuecomment-473419380,1,['config'],['config']
Modifiability,"Hi @cwhelan , I've expanded this PR to do more than what it originally was trying to fix, and separated the patches by commits as usual:. * the originally proposed fix, which brings back the annotation that are available to simple variants but go missing due to a careless bug, is now done in commit 50f1b640a31ddb528dc763b83b26a9d98dce8556; this commit also accordingly refactors the giant class `CpxVariantDetector` into three new classes; * in the 2nd commit 734516383fb665a79796de76535560fc03cb754b, I did more refactoring on how we group the descriptions for the annotation keys, and updated the test VCF files accordingly.; * because of the refactoring, the review comments were gone, so I added them back in the 3rd commit b7619c45a949dfba21d65a5ed876bc72e832aa77, which contains the comments and my replies. They come in as TODO's but are going to be removed ultimately; * in the following commits, I added tests for the CPX code path, selecting three representative cases (there's no limit how complex the scenario can go). One particular commit 224c97c7b736e94ed6b4d8b067ec830a9f8f2403 is large but most of it is for adding a flat file that contains the chromosome names in hg38 and their lengths for building a bare bone sequence dictionary used in building test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525:371,refactor,refactors,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4330#issuecomment-372761525,6,['refactor'],"['refactoring', 'refactors']"
Modifiability,"Hi @georgiiprovisor ,. Sorry this fell though the cracks. Did you find a workaround? I suspect some of our I/O refactoring caused this warning to be triggered incorrectly. Happy to take a look if that's still useful. -Laura",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8564#issuecomment-2435404906:111,refactor,refactoring,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8564#issuecomment-2435404906,1,['refactor'],['refactoring']
Modifiability,"Hi @hh1985 . Memory tuning is pretty tricky and can depend on a lot of things. How is your cluster configured? ; Are you using YARN? Are you running in client or cluster mode? . I'm assuming you're running with YARN. Mesos should also work but I don't have any experience configuring it. . BQSR should run safely with 4g of memory per core. (It should really work with much less I think, but 4 should definitely be sufficient.) There are a few different parameters that can help you adjust the memory ratios.; A good tuning might be something like; ; ```; --num-executors 5 ; --executor-cores 8 ; --executor-memory 32g ; ```. if you're not running with gatk-launch you'll need to set; ```; --conf spark.yarn.executor.memoryOverhead=600; ```; Without setting a higher than default yarn memory overhead like this we see consistent crashes, it's included in the settings gatk-launch applies already. That should run 5 separate executors with 8 cores each and give each one 32g, so 4g / core. . If you're running in cluster mode you'll have to carve out some memory and cores for the driver. You can set the driver settings with ; ```; --driver-cores 2; --driver-memory 4g; ``` ; or something along those lines. The driver doesn't need much memory or computer for BQSR. In general we've had better luck using the entire cluster for one job and running jobs in sequence rather than trying to run two jobs simultaneously using a subset of the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738:99,config,configured,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738,2,['config'],"['configured', 'configuring']"
Modifiability,Hi @icemduru ; Looks like your slurm workload manager was configured to have a limit of 48GBs of maximum process memory size per execution. Your java instance is set with -Xmx45G which will cover most of this limit and leaves only a handful of memory space for the native GenomicsDB library. Native libraries work above the heapsize so it is better for you to set your -Xmx to a more sensible size of 8~12GB and leave rest of the memory space to the native library to use. . Keep in mind that this memory limit on slurm could be set per user not per task therefore you may need to run a single contig at a time or maybe 2 of them simultaneously. Otherwise slurm may interefere with all the tasks and cancel all your jobs. . One final reminder. We strongly recommend users to set the temporary directory to somewhere else other than /tmp. Slurm workload manager interferes with this preference and sometimes results in premature termination of the gatk processes due to deletion of extracted native library and accessory files. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332:58,config,configured,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8918#issuecomment-2283694332,1,['config'],['configured']
Modifiability,"Hi @jean-philippe-martin ,. A `Feature` in our codebase has a specific meaning that is different from ""interval"": it is a record that 1. has a location on the genome plus (typically) some metadata information about that location and 2. is in one of the formats supported by our file-parsing framework tribble and is the product of a tribble codec. A VCF record is an example of a `Feature`. . The common interface between `Feature` and `SimpleInterval` is called `Locatable`. I recommend (for now) that you simply alter your uprooted version of BQSR to take a `List<? extends Locatable>` instead of a `List<? extends Feature>` in `apply()`. This should require no code changes beyond changing method parameter types, and it will allow you to feed BQSR `SimpleIntervals` for the known sites for now, and `Features` like VCF records later on when we're ready for that. Please do return `ArtificialTestFeature` to the `FeatureDataSourceUnitTest` from which it came -- this is a very incomplete class meant only for testing purposes and not for external use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247:568,extend,extends,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/511#issuecomment-100393247,2,['extend'],['extends']
Modifiability,Hi @jonn-smith I had some success getting outputs with this. However ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/ is no longer accessible. The most recent version of I accessed on 3/19/2018 still contained at least one error. I'm trying to correct them myself as I go using a more recent build of GATK but it would be helpful if the data files required by this program were available. The one I found is in `gencode_xrefseq.config` where it references a source that doesn't exist and I fixed that. After that I was able to get outputs with hg38. Thanks for your work on this!. I'd also point out there are a lot of fields in the MAF with `__UNKNOWN__` as the entry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645:447,config,config,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4521#issuecomment-383387645,1,['config'],['config']
Modifiability,Hi @lvzenglei ; This is not an issue that needs any fix in fact this is the default behavior that Mutect2 and HaplotypeCaller will have. Extending MNP distance does not change any of the Smith Waterman or PairHMM parameters that will eventually decide on the model used to genotype the region. If you are interested in getting larger complex events genotyped you may need to get longer reads (preferably 2x300) and use read backed phasing before you try genotyping any such events. In that case you may need to experiment with Smith Waterman and PairHMM parameters to prefer mismatches over SNPs but this still may not result in what you are actually looking for. One better approach would be to use read backed phasing along with post processing your phased variants to convert individual phased SNPs and INDELs into COMPLEX calls by other tools. . I hope this helps. Regards.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8310#issuecomment-2421513998:137,Extend,Extending,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8310#issuecomment-2421513998,1,['Extend'],['Extending']
Modifiability,Hi @potter-s ; Our docker image is already built with root account only however PATH is set to be usable by all users so if you wish to keep user priviledges after execution you may add ` -u $UID:$GID` parameter to docker command line therefore the container will run using your user permissions. . This has a catch of course. Temporary folders must be set where your user has RWX permissions therefore we want users to pay attention to that. There is a writing that we posted a while ago which you may refer to for setting up your temporary files for GATK workflows. . [How to setup temporary folder for GATK local executtion](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). For some of the tools such as gCNV or CNN you may need to setup additional environment variables to locate python compilation directory to a place where you have read and write permissions. . I hope this helps.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965:845,variab,variables,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965,2,['variab'],['variables']
Modifiability,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:73,inherit,inherited,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,1,['inherit'],['inherited']
Modifiability,Hi @yurivict I tested this on my machine and it works for me. Do you see a message saying the version was overridden?; ```; $ ./gradlew printVersion -Drelease=true -DversionOverride=myVersion1.2. > Configure project :; Version number overridden as myVersion1.2. > Task :printVersion; myVersion1.2; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-857796023:198,Config,Configure,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7143#issuecomment-857796023,1,['Config'],['Configure']
Modifiability,"Hi DarioS. FastaAlternateReferenceMaker is a really simple tool. It actually just looks at the alternate alleles at each site and uses the first non-symbolic one to make the fasta. It doesn't even look at the genotypes. So it should work fine with a multisample vcf but it will give you a mush of samples together as a single fasta. I could be extended to be smarter but it's not a high priority for us right now. . We should improve the documentation, I had to go look in the code to see what it was doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729:344,extend,extended,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7557#issuecomment-969237729,2,['extend'],['extended']
Modifiability,"Hi Stefan,. If there is no obvious error (e.g., is `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsvx` the correct filename, rather than `/media/Berechnungen/GATKTest/CN_transition_matrix_autosomal.tsv`? Does the file exist and is it correctly formatted?), then I would guess that this is likely an error with your nd4j configuration. Just to let you know, we have significantly revamped the both somatic and germline CNV pipelines for the release in January. If you would like a preview of the germline tool, you may want to look at this branch: https://github.com/broadinstitute/gatk/tree/sl_gcnv_ploidy_cli However, be aware that it is still under development.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467:335,config,configuration,335,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3996#issuecomment-352760467,1,['config'],['configuration']
Modifiability,"Hi all. Apologies for writing in an old issue. ; has this been fixed?; With java 1.8 and GATK 4.1.3.0 I think I'm getting the same error (in this case with HaplotypeCallerSpark). Any idea on how to extend the size?. The errors are:. `org.broadinstitute.hellbender.exceptions.UserException: Max size of locatable exceeded. Max size is 5000, but locatable size is 8638. Try increasing shard size and/or padding. Locatable: Contig1:65711-74348; 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:293); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder$5.computeNext(SparkSharder.java:281); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.TransformedIterator.hasNext(TransformedIterator.java:43); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$5.hasNext(Iterators.java:547); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994:198,extend,extend,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994,1,['extend'],['extend']
Modifiability,"Hi gatk team,. I'm working with generating CNV for somatic with wdl, using this command:. `java -Xmx75G -Dconfig.file=gatk.conf -jar cromwell-46.1.jar run cnv_somatic_panel_workflow.wdl -i parameters.json `. But I got this error in which I don't know the exact reason for it:. ```; [2019-10-01 02:52:52,49] [info] Running with database db.url = jdbc:hsqldb:mem:e98d186c-96db-46ae-92e5-c326e7aa05d9;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,19] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:901,config,configuration,901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configuration']
Modifiability,"Hi, ; This is what i got when i run the command gatk --help ; (base) ameni@ameni-Aspire-A315-55G:~/Documents/pharmacogenomics$ gatk --help. Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster ; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after -- ; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated ; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the ; java JVM at runtime. ; Java options MUST be passed inside a single string with space-separated values. --debug-port <number> sets up a Java VM debug agent to listen to debugger connections on a; particular port number. This in turn will add the necessary java VM arguments; so that you don't need to explicitly indicate these using --java-options.; --debug-suspend sets the Java VM debug agent up so that the run get immediatelly suspended; waiting for a debugger to connect. By default the port number is 5005 but; can be customized using --debug-port.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8280:506,Config,Configuration,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8280,2,"['Config', 'config']","['Configuration', 'config-file']"
Modifiability,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:562,config,configuration,562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,1,['config'],['configuration']
Modifiability,"Hi, I also meet this issue. . The Specificity in `HaplotypeCallerSpark` was a bitter less than local mode in my test. Do you known which configuration can improve the Specificity in `HaplotypeCallerSpark` ? @Atahualkpa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5323#issuecomment-433815315:137,config,configuration,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323#issuecomment-433815315,1,['config'],['configuration']
Modifiability,"Hi, I think this can be closed now. . I just found that this issue might due to the inconsistency between gCNV version and PostProcessGermlineCNVCalls version. ; I updated my python configuration from 4.1.8.0 to 4.2.2.0 and the issue is gone. . Sorry for the troubles here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-908550017:182,config,configuration,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-908550017,1,['config'],['configuration']
Modifiability,"Hi, I'm trying to generate a VCF with Mitochondrial mode of Mutect2 based on MT amplicon (PCR) sequencing. But I encountered two problems:; 1. The base site (Pos: MT:16320) has high depth and good quality, but the result of AF is low. In IGV, AF~=0.5; 2. The total depth(DP) is also quite different from the depth in bam; I tried changing various parameters but nothing seems to make a difference? and even tried turn on --disable-tool-default-read-filters. Is there a parameter I'm missing? What are the filtering criteria? How to adapt to PCR data?. version: GATK4.1.4.1; java -Xmx16g -Djava.io.tmpdir=./tmp -jar gatk-package-4.1.4.1-local.jar Mutect2 -I tmp.bam -R hs37d5.fa -L MT.bed -O raw.vcf --min-pruning 5 --mitochondria-mode --max-reads-per-alignment-start 10000. MT 16182 . A AC,ACC . . DP=262;ECNT=7;MBQ=31,26,29;MFRL=0,0,0;MMQ=60,60,60;MPOS=44,44;OCM=0;POPAF=2.40,2.40;TLOD=84.81,46.04 GT:AD:AF:DP:F1R2:F2R1:SB 0/1/2:157,72,31:0.261,0.118:260:122,48,21:0,0,0:0,157,0,103; MT 16183 . A C . . DP=262;ECNT=7;MBQ=30,34;MFRL=0,0;MMQ=60,60;MPOS=45;OCM=0;POPAF=2.40;TLOD=531.07 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:58,204:0.780:262:42,190:0,0:0,58,0,204; MT 16188 . CT C . . DP=262;ECNT=7;MBQ=34,29;MFRL=0,0;MMQ=60,60;MPOS=50;OCM=0;POPAF=2.40;TLOD=19.25 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:243,19:0.070:262:207,15:0,0:0,243,0,19; MT 16189 . T C . . DP=262;ECNT=7;MBQ=35,34;MFRL=0,0;MMQ=60,60;MPOS=51;OCM=0;POPAF=2.40;TLOD=931.43 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:2,260:0.989:262:2,237:0,0:0,2,0,260; MT 16266 . C A . . DP=1073;ECNT=4;MBQ=7,32;MFRL=0,0;MMQ=60,60;MPOS=50;OCM=0;POPAF=2.40;TLOD=3575.47 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:14,1039:0.996:1053:4,469:0,446:0,14,481,558; MT 16274 . G A . . DP=1073;ECNT=4;MBQ=33,12;MFRL=0,0;MMQ=60,60;MPOS=53;OCM=0;POPAF=2.40;TLOD=1.89 GT:AD:AF:DP:F1R2:F2R1:SB 0/1:1057,11:5.214e-03:1068:571,1:337,4:467,590,10,1; MT 16320 . C T . . DP=643;ECNT=4;MBQ=27,36;MFRL=0,0;MMQ=60,60;MPOS=21;OCM=0;POPAF=2.40;TLOD=11.29 GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB 0|1:564,60:0.017:624:48,60:358,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-575001931:532,adapt,adapt,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-575001931,1,['adapt'],['adapt']
Modifiability,"Hi, i can't install gatk via conda/mamba. couldyou pls help; pls see steps that i took. ```; $conda config --add channels conda-forge; $conda config --add channels bioconda; $conda config --add channels defaults; $conda config --set channel_priority strict; ```. install command; ```; bash:iscxf001:/data1/greenbab/users/ahunos/apps/gatk-4.5.0.0 1023 $ conda env create -n gatk -f gatkcondaenv.yml; ```. ```; Channels:; - conda-forge; - defaults; - bioconda; Platform: linux-64; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - conda-forge::typing_extensions==4.1.1; - conda-forge::theano==1.0.4; - pkgs/main::tensorflow==1.15.0; - conda-forge::scipy==1.0.0; - conda-forge::scikit-learn==0.23.1; - conda-forge::python==3.6.10; - bioconda::pysam==0.15.3; - conda-forge::pymc3==3.1; - conda-forge::pip==21.3.1; - conda-forge::pandas==1.0.3; - conda-forge::numpy==1.17.5; - conda-forge::mkl-service==2.3.0; - conda-forge::mkl==2019.5; - conda-forge::matplotlib==3.2.1; - conda-forge::keras==2.2.4; - conda-forge::joblib==1.1.1; - pkgs/main::intel-openmp==2019.4; - conda-forge::h5py==2.10.0; - conda-forge::dill==0.3.4. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/r/linux-64; - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https://conda.anaconda.org/conda-forge; - https:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8838:100,config,config,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8838,4,['config'],['config']
Modifiability,"Hi, recently I was trying expand the annotation data source when using funcotator, however, the document didn't give much information or example. Now I was trying to add CADD to the data source folder. After running the funcotator, I got the error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path file: funcotator_dataSources.v1.6.20190124s/cadd/hg19/cadd.config; at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:353); at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:305); at org.broadinstitute.hellbender.engine.FeatureDataSource.&lt;init&gt;(FeatureDataSource.java:256); at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:234); at org.broadinstitute.hellbender.engine.GATKTool.addFeatureInputsAfterInitialization(GATKTool.java:957); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createAndRegisterFeatureInputs(DataSourceUtils.java:328); at org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils.createDataSourceFuncotationFactoriesForDataSources(DataSourceUtils.java:277); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.onTraversalStart(Funcotator.java:774); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1037); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:413,config,config,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"Hi,. Thanks for the response. Running with -u isnâ€™t ideal as we canâ€™t control; how the user runs this (unless they do this on their own hardware or say a; cloud instance). However, I managed to convert the docker image into a singularity one and; that runs â€˜out of the boxâ€™ in user space. Simon. On 3 Jun 2024, at 18:43, GÃ¶kalp Ã‡elik ***@***.***> wrote:. Hi @potter-s <https://github.com/potter-s>; Our docker image is already built with root account only however PATH is; set to be usable by all users so if you wish to keep user priviledges after; execution you may add -u $UID:$GID parameter to docker command line; therefore the container will run using your user permissions. This has a catch of course. Temporary folders must be set where your user; has RWX permissions therefore we want users to pay attention to that. There; is a writing that we posted a while ago which you may refer to for setting; up your temporary files for GATK workflows. How to setup temporary folder for GATK local executtion; <https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution>. For some of the tools such as gCNV or CNN you may need to setup additional; environment variables to locate python compilation directory to a place; where you have read and write permissions. I hope this helps. â€”; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2145780965>,; or unsubscribe; <https://github.com/notifications/unsubscribe-auth/ABU3SAWISO2HSCUNHK3SGIDZFSTK5AVCNFSM6AAAAABIWRNXGKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNBVG44DAOJWGU>; .; You are receiving this because you were mentioned.Message ID:; ***@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154:1229,variab,variables,1229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8856#issuecomment-2155884154,2,['variab'],['variables']
Modifiability,"Hi,; Glad to know that you have tested GATK4 with Amazon S3 using NIO file system plugin. ; I have been stuck on this process for long...I would really appreciate if you could share the work around procedure detail for this.; Thanks in advance !; Senthil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-354368839:82,plugin,plugin,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708#issuecomment-354368839,1,['plugin'],['plugin']
Modifiability,"Hi,; I am trying to build from gatk-4 master sources. I received this error code `2` admitedly when I had no git-lfs installed. Now it is installed and in my PATH, but the error still occurs. Can't you capture the real error message?. ```; 22:05:55.883 [QUIET] [system.out] Executing: git lfs pull --include src/main/resources/large; 22:05:55.943 [DEBUG] [org.gradle.configuration.project.BuildScriptProcessor] Timing: Running the build script took 12.879 secs; 22:05:55.952 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:367,config,configuration,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"Hi,; I tried your commands (and many adaptions / changements) but I always get the same problem:; If the command line includes `--`, I get the JNI linkage error as if the spark related parameters were not parsed.; I tried many things, as:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass -- --sparkRunner SPARK --sparkMaster yarn --deploy-mode cluster; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --conf spark.driver.extraJavaOptions='-Dmapr.library.flatclass' --conf spark.executor.extraJavaOptions='-Dmapr.library.flatclass'. > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input hdfs://spark01:7222/user/axverdier/data/710-PE-G1.bam --output hdfs://spark01:7222/user/axverdier/testOutGATK_CountReadsSpark --javaOptions -Dmapr.library.flatclass --sparkRunner SPARK --sparkMaster yarn -- --master yarn --deploy-mode cluster --driver-java-options '-Dmapr.library.flatclass'. It's a non-exhaustive list, I tried a lot of configurations similar to these ones.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061:37,adapt,adaptions,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350227061,2,"['adapt', 'config']","['adaptions', 'configurations']"
Modifiability,"Hi,; I use your software with docker swarm where is deploy spark and hadoop the configuration for docker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:80,config,configuration,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['config'],['configuration']
Modifiability,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6071:168,config,configured,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071,1,['config'],['configured']
Modifiability,"Hi,; during compilation of 3.8 sources I get. ```; [INFO] --- exec-maven-plugin:1.2.1:exec (delete-mavens-links) @ gatk-aggregator ---; rm: missing operand; Try 'rm --help' for more information.; rm: missing operand; Try 'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:73,plugin,plugin,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,2,['plugin'],['plugin']
Modifiability,"Hi. I failed to build GATK4. . I am a very beginner of bioinformatics and data science. ; I am using google VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1001,Config,ConfigurationContainerInternal,1001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Config'],['ConfigurationContainerInternal']
Modifiability,"Hmm, I am also getting intermittent build errors like the following much more often:. ```; Could not determine the dependencies of task ':sparkJar'.; > Could not resolve all files for configuration ':sparkConfiguration'.; > Could not download gson.jar (com.google.code.gson:gson:2.2.2); > Could not get resource 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/google/code/gson/gson/2.2.2/gson-2.2.2.jar'. Received status code 403 from server: Forbidden; > Could not download core.jar (com.github.fommil.netlib:core:1.1); > Could not get resource 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'.; > Could not GET 'https://repo.maven.apache.org/maven2/com/github/fommil/netlib/core/1.1/core-1.1.jar'. Received status code 403 from server: Forbidden; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142:184,config,configuration,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601832142,1,['config'],['configuration']
Modifiability,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:469,refactor,refactor,469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,1,['refactor'],['refactor']
Modifiability,Hook arguments from SelectVariants/GenotypeGVCFs/GnarlyGenotyper to GenomicsDB Export Configuration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6456:86,Config,Configuration,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6456,1,['Config'],['Configuration']
Modifiability,Hooked the annotations barclay plugin into the tools that use vcf annotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4674:31,plugin,plugin,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674,1,['plugin'],['plugin']
Modifiability,How did you install the GATK Conda environment? Looks like a problem with the conda environment configuration. One possible reason could be that conda environment is not the version 4.3.0.0 is requesting.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952#issuecomment-2287925550:96,config,configuration,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952#issuecomment-2287925550,1,['config'],['configuration']
Modifiability,"I _believe_ your issue is that you are assigning 600GB to execution of cromwell, but the error is with the call to **VariantRecalibrator** in one of the tasks not having enough memory. A few tasks call **VariantRecalibrator**, do you know which task failed? Can you post the java call from the STDERR file? For me, it was task **SNPsVariantRecalibrator** which was assigned only 3.5GB of memory by default. In [joint-discovery-gatk4.wdl](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.wdl), the memory assigned for each task can be set via ""machine_mem_gb"", but it looks like the current [input.json](https://github.com/gatk-workflows/gatk4-germline-snps-indels/blob/master/joint-discovery-gatk4.hg38.wgs.inputs.json) does not have that variable, but instead ""mem_size"" for each task. . A simple solution would be to replace ${java_mem} with a static value in calls to **VariantRecalibrator** (lines 564 & 684). For example, replace:. `${gatk_path} --java-options ""-Xmx${java_mem}g -Xms${java_mem}g""`. with. `${gatk_path} --java-options ""-Xmx100g -Xms100g""`. I'm not certain this will help, but I think it's a step in the right direction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381:785,variab,variable,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165#issuecomment-571396381,2,['variab'],['variable']
Modifiability,"I added -m to `gsutil cp` in a previous PR but missed the `gsutil mv` step post-`bq load` - so here that is. tested it from the command line, works well. also confirmed that it will throw an error if one (or more) files has an error:. ```; $ cat test_files_bucket.txt | gsutil -m mv -I gs://dsp-fieldeng-dev/test_mv/. If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. Copying gs://dsp-fieldeng-dev/test_cp/test1.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test2.txt [Content-Type=text/plain]...; CommandException: No URLs matched: gs://dsp-fieldeng-dev/test_cp/test4.txt; Copying gs://dsp-fieldeng-dev/test_cp/test3.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test5.txt [Content-Type=text/plain]...; Copying gs://dsp-fieldeng-dev/test_cp/test6.txt [Content-Type=text/plain]...; Removing gs://dsp-fieldeng-dev/test_cp/test1.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test2.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test3.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test5.txt...; Removing gs://dsp-fieldeng-dev/test_cp/test6.txt...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7129:491,config,config,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7129,1,['config'],['config']
Modifiability,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:238,flexible,flexible,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,8,"['extend', 'flexible', 'plugin']","['extending', 'flexible', 'plugin']"
Modifiability,"I addressed some of your comments, @droazen. If you would like to have a properties file for the configuration, I will need some help on setting it up (although I will try by my own too). Although I still set up some of the environment in `Main`, now the `CommandLineProgram` class have the same instance passed by `Main`. Looking forward for your comments on the updates.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-271845715:97,config,configuration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-271845715,1,['config'],['configuration']
Modifiability,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:226,adapt,adapter,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314,2,['adapt'],"['adapter', 'adaptor-sequence']"
Modifiability,"I also just noticed that tests are failing on the branch because they still reference the old constants in a number of places:. ```; symbol: variable READ_NAME_LONG_NAME; location: class ReadNameReadFilter; /gatk/src/test/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptorTest.java:117: error: cannot find symbol; { PlatformReadFilter.class.getSimpleName(), ""--"" + PlatformReadFilter.PL_FILTER_NAME_LONG_NAME, ""fakePlatform"" }, ; ```. You'll need to update these references in order to get tests passing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542:141,variab,variable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4103#issuecomment-360806542,1,['variab'],['variable']
Modifiability,"I also removed the obsolete errorProbability variable line of code in the SomaticGenotypingEngine.java and noted this argument is deprecated in the M2ArgumentCollection. Somewhat relatedly, see request in #3123.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3124:45,variab,variable,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3124,1,['variab'],['variable']
Modifiability,"I also tried the following approach which did not generate an error:. 1. imported the 10 not-reblocked gvcfs from chr16 into genomicsdb ; 2. GenotypeGVCFs with the same command line as number 3 above. . So the error appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:455,variab,variable,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['variab'],['variable']
Modifiability,I am looking to call single nucleotide polymorphism (SNP) and INDEL from my genome mapping results generated from HISAT pipeline. Please suggest process and command for that.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6592:39,polymorphi,polymorphism,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6592,1,['polymorphi'],['polymorphism']
Modifiability,I ask that because for htsjdk defaults they must be system properties and they're final and set statically on load so mucking about resetting system properties after the JVM started already is going to be a bit of a fiddly ordering nightmare. . [Stack overflow](http://stackoverflow.com/questions/6736235/set-java-system-properties-with-a-configuration-file) doesn't seem to think that it's possible to initialize them from a file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704:339,config,configuration-file,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704,1,['config'],['configuration-file']
Modifiability,"I attended a journal club some months ago where a paper stated most researchers use default settings of tools. The paper benchmarked tool with default settings and with tweaked parameters. I can dig up the paper if anyone is interested. So there is some expectation from the user community that the default parameters reflect some sweet spot parameterization for running the tool. . I highly support @cmnbroad's suggestion for making argument sets callable by one flag. For exomes, please do not label flag as `WES`. We want to refer instead to _targeted exomes_, so `EXOMES` or variation is preferable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104:342,parameteriz,parameterization,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104,1,['parameteriz'],['parameterization']
Modifiability,I believe I've had this issue before but with different tools as well.; If you are on nextflow below is a config for scope docker. ```; docker.fixOwnership; Fix ownership of files created by the docker container.; ```. There is also another scope that could be set if there is only a single user. ```; docker.runOptions; This attribute can be used to provide any extra command line options supported by the docker run command. See the [Docker documentation](https://docs.docker.com/engine/reference/run/) for details.; ```; This one enables passing -u parameter to docker directly. . If none of them are set in the nextflow config then I would first suggest these options. If not we can escalate this with the team.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078156593:106,config,config,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078156593,2,['config'],['config']
Modifiability,"I believe that the problem is that HaplotypeCallerGenotypingEngine doesn't use the version of StandardCallerArgumentCollection.getSampleContamination() with the sampleID, so the sampleContamination variable is never initialized and we get a NPE. The -contamination argument is standard in our production GATK pipeline... and we're about to start telling everyone to use it! This is an important one to fix. Here's a stacktrace:. java.lang.NullPointerException; 	at java.util.Collections$UnmodifiableMap.<init>(Collections.java:1446); 	at java.util.Collections.unmodifiableMap(Collections.java:1433); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.StandardCallerArgumentCollection.getSampleContamination(StandardCallerArgumentCollection.java:89); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:566); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:218); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:295)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4312:198,variab,variable,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4312,1,['variab'],['variable']
Modifiability,"I can definitely appreciate that in some cases downstream analysis might be made easier if the original representations of GGA mode alleles were preserved. . Internally, HaplotypeCaller has to convert all variants at a position to share the same reference context so that read alignments can be compared to all possible alternate alleles simultaneously, and it would be a complex and error-prone process to re-map the unified alleles back to their original representations, and would also pose problems in terms of computing the correct values for INFO field annotations such as DP if the output VCF had to be split across multiple lines according to the how things were specified in the input files. I'm going to close this for now unless you strongly object or have an additional case that shows an error in GGA mode output. It's possible that in the future we could implement an enhancement in the form of a mode that preserves the GGA input allele representations, possibly under some stricter conditions upon the input, but that would likely be a tricky implementation. Pinging @ldgauthier to make sure she agrees.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950:882,enhance,enhancement,882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5385#issuecomment-435902950,1,['enhance'],['enhancement']
Modifiability,"I can't use it because of that, and it have lots of variables that I'm not using. I'm doing my own base test class, but I'd love to have a more general base test class in GATK to extend, without that many variables specific for this repository. Thanks a lot for your interest!. Should I do something for this PR? . So should I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242839085:52,variab,variables,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242839085,3,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"I changed the `--mask` and `-mask-name` arguments to be lists so it's possible to supply multiple mask files. There are still some questions to discuss that may warrant changes:. 1. Should `-filter-not-in-mask` also be a list, so the user specifies whether to do a mask or reverse mask for each file?; a. My inclination is no, since that would make things kind of complicated and probably you just want to filter variants that appear in none of the mask files; 2. What should `maskName` default to now?; a. Previously, it defaulted to ""Mask"".; b. I changed it to default to ""Mask"" for the first mask, and then ""Mask2"", ""Mask3"", etc. Not sure if this is ideal?; 3. Should the variable names be changed?; a. i.e. `mask` -> `masks` and `maskName` -> `maskNames`; b. Obviously the arguments would keep the same names; 4. When using `-filter-not-in-mask`, what should we list for filters?; a. All the mask names? (this is what I'm doing now, but it could obviously get very long and maybe be misleading?); b. Should we just allow one `-maskName` if `-filter-not-in-mask` is specified?; 5. Is my implementation likely to cause a prohibitive performance reduction?. Closes #8119",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8237:675,variab,variable,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8237,1,['variab'],['variable']
Modifiability,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:660,config,configuring,660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,2,['config'],"['configuration', 'configuring']"
Modifiability,"I concur, what it looks like we have here is code that switched (accidentally?) from the GCS adapter to GCS-NIO instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265014643:93,adapt,adapter,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265014643,1,['adapt'],['adapter']
Modifiability,"I definitely like the idea of moving in this direction, and it will become more compelling as we extend the GATKSparkTool hierarchy, which is currently pretty flat and doesn't mirror the GATKTool hierarchy. I think we should also consider using some of the concepts from the metrics refactoring, which introduces a layer that separates the implementation of the processing logic from the containing tool/driver, and allows a single implementation (i.e. metrics collector, but could be FlagStats, CountReads, SelectVariants, whatever) to be independent of the source and/or destination of the data. It adds more moving parts, but has the advantage of allowing a single implementation to be used from any of Spark tool, standalone tool, Spark pipeline, standalone pipeline, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254216118:97,extend,extend,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254216118,2,"['extend', 'refactor']","['extend', 'refactoring']"
Modifiability,"I did not use the official download of the Funcotator datasources (1.2), but I did find some extraneous files that should be removed. Complete list is below:. ```; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 achilles/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cancer_gene_census/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 clinvar/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_fusion/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 cosmic_tissue/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dbsnp/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 dna_repair_genes/; -rwxrwx--- 1 root vboxsf 6148 Apr 30 15:38 .DS_Store*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 familial/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xhgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 gencode_xrefseq/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:39 hgnc/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:44 .idea/; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 oreganno/; -rwxrwx--- 1 root vboxsf 5274 Apr 30 15:38 README.txt*; drwxrwx--- 1 root vboxsf 0 Apr 30 15:38 simple_uniprot/; -rwxrwx--- 1 root vboxsf 1557 Apr 30 15:38 template.config*. ```; `.DS_Store` and `.idea` should not be in the official download.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4722:1175,config,config,1175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4722,1,['config'],['config']
Modifiability,I did this PR because of the GenomeLocParser; what you suggested was the first thing that I tried. I did this instead of refactoring the BaseTest because I didn't want to cause major changes in the code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242854061:121,refactor,refactoring,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242854061,1,['refactor'],['refactoring']
Modifiability,"I discovered that one of the 345 input gvcfs failed VCF validation. When I removed that file and reran with no other changes, I did not get the ""terminate called without an active exception"" error. However, ImportGvcfs still fails; the failure seems to occur immediately after GenomicsDBImport logs success in importing all batches, in each shard. From all the Cromwell logs it looks like everything is working, but the top level workflow execution fails. I've been trying various configurations of memory, scatter count, and #nodes, so I don't have those log files around still. I can rerun with -DGATK_STACKTRACE_ON_USER_EXCEPTION=true and see if I get anything useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651:481,config,configurations,481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076#issuecomment-1295310651,1,['config'],['configurations']
Modifiability,"I do not think you should have two versions. Here is a task example from; another workflow:. ```; output {; File cnv_acs_conversion_skew = ""${output_skew_filename}""; Float cnv_acs_conversion_skew_float =; read_float(output_skew_filename); String cnv_acs_conversion_skew_string =; read_string(output_skew_filename); }; ```. While this adds some clutter, at least the task (and workflow) produces a; file, float, or string. Then you can decide which you actually want to; attach to the data model via the method configuration output. Clutter vs. fork? I say ""clutter"". Also, you may only need one alternate; type. On Tue, Jul 2, 2019 at 9:52 AM ldgauthier <notifications@github.com> wrote:. > *@ldgauthier* requested changes on this pull request.; > ------------------------------; >; > In scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl; > <https://github.com/broadinstitute/gatk/pull/6017#discussion_r299492696>:; >; > > @@ -242,6 +250,7 @@ workflow CNVGermlineCaseWorkflow {; > Array[File] gcnv_tracking_tars = GermlineCNVCallerCaseMode.gcnv_tracking_tar; > Array[File] genotyped_intervals_vcf = PostprocessGermlineCNVCalls.genotyped_intervals_vcf; > Array[File] genotyped_segments_vcf = PostprocessGermlineCNVCalls.genotyped_segments_vcf; > + Array[File] qc_status_files = CollectSampleQualityMetrics.qc_status_files; >; > Ideally I'd want to be able to flag failing samples in an obvious way in; > the workspace, like having new fields in the data model called; > ""sample_quality"" and ""model_quality"" with the QC status reported there. Are; > we violently opposed to having a Cromwell version and a Firecloud version; > of this WDL? (@LeeTL1220 <https://github.com/LeeTL1220>); > ------------------------------; >; > In scripts/cnv_wdl/cnv_common_tasks.wdl; > <https://github.com/broadinstitute/gatk/pull/6017#discussion_r299493991>:; >; > > @@ -453,3 +453,98 @@ task PostprocessGermlineCNVCalls {; > File genotyped_segments_vcf = genotyped_segments_vcf_filename; > }; > }; > +; > +task Col",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507695717:510,config,configuration,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507695717,1,['config'],['configuration']
Modifiability,"I don't really like the idea of a `--fix_misencoded_quality_scores` arg at the `GATKTool` level hooked into the data source -- I'd prefer to see your read transformer PR (https://github.com/broadinstitute/gatk/pull/2085) get merged, and then make read transformers a plugin that can be turned on/off on the command line, like we can with read filters. I've asked @cmnbroad to add a comment here outlining what would have to be done to make read transformers a plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245959488:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245959488,2,['plugin'],['plugin']
Modifiability,"I don't think that M2 should be spewing out a concatenation of all alleles,; since that does imply ploidy via the VCF spec. Multiallelics on multiple; lines violates the spec too, right?. On Mon, Sep 18, 2017 at 9:28 PM, David Benjamin <notifications@github.com>; wrote:. > @chandrans <https://github.com/chandrans> @chapmanb; > <https://github.com/chapmanb> @ldgauthier <https://github.com/ldgauthier>; > The -ploidy argument is one of several arguments inherited from; > AssemblyRegionWalker that apply only to HaplotypeCaller and do nothing in; > Mutect2. (We should refactor this once the engine team's workload; > lightens enough to review lower-priority things like this.) The GT field; > emitted by Mutect is just the concatenation of all called alleles -- 0/1,; > 0/1/2, 0/1/2/3 etc -- and doesn't imply anything about the ploidy. Maybe we; > should get rid of it entirely since AF is so much more informative.; >; > I like the idea of splitting multiallelics into multiple lines. It would; > make filtering a lot easier. @LeeTL1220 <https://github.com/leetl1220> do; > you have an opinion?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330401348>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2Bmhl2i8xOPd4zC_WqQ9GtNbRKNks5sjxi1gaJpZM4PTWbd>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330527699:455,inherit,inherited,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330527699,2,"['inherit', 'refactor']","['inherited', 'refactor']"
Modifiability,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:1225,flexible,flexible,1225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,2,"['extend', 'flexible']","['extend', 'flexible']"
Modifiability,"I don't think the `@author` tags will show up in the doc; they generally don't show up in javadoc, and our doc system doesn't include them in the generated doc.`@author` tags are a bit sketchy to begin with, because they're block tags, so the extend all the way to the start of the next tag. But since they're stripped out, putting such a tag at the start of the javadoc can result in everything after it being silently dropped from the output. We found such a case in GATK a while back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349354828:243,extend,extend,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349354828,1,['extend'],['extend']
Modifiability,I fixed these by update the dataproc image from 1.0 -> 1.1. . @davidbernick Is there a way to make that into a variable that's settable in one place and shared between every jenkins spark job?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289#issuecomment-264497363:111,variab,variable,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289#issuecomment-264497363,1,['variab'],['variable']
Modifiability,"I found some edge cases when implemented a new walker based on GATKTool that should be considered in the base class to avoid some mistakes by developers. They should either being correctly handled or documented as not-applicable for some walkers:. - [ ] If a tool/walker overrides getPluginDescriptors to remove the `GATKReadFilterPluginDescriptor`, the method `makeReadFilter` will not return a filter with the `getDefaultReadFilters`. This is important for implementing tools where the user shouldn't be able to override the filters. I suggest to either handle the case where the plugin cannot be detected and return a filter with only defaults, or to specify in the documentation that `makeReadFilter` should be overriden in that case. - [ ] Transformer methods for reads (`makePreReadFilterTransformer`and `makePostReadFilterTransformer`) only have effect in `ReadWalker`(and extensions). I think that the `ReadsContext` should have a method to set pre/post transformers, and call this methods to integrate with every extension of `GATKTool`. Otherwise, it should be documented that it has no effect in most of the cases. @droazen - could you give me some way to proceed here? I think that the best way is to implement the proper behavior, but maybe the engine team has a different opinion...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4651:582,plugin,plugin,582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651,1,['plugin'],['plugin']
Modifiability,"I got it to work by using the runtime switch --disable-sequence-dictionary-validation . . If that is not used it crashes. . . Docker commandline. . /gatk Funcotator --disable-sequence-dictionary-validation \. -R mydata/refs/Homo_sapiens_assembly19.fasta \. -V mydata/P50513_mutect2_filtered.vcf \. -O mydata/P50513_mutect2_funcotator.maf \. --output-file-format MAF \. --data-sources-path mydata/dataSourcesFolder/funcotator_dataSources.v1.6.20190124s/ --ref-version hg19. . . . From: Louis Bergelson <notifications@github.com> ; Sent: Wednesday, October 30, 2019 10:26 AM; To: broadinstitute/gatk <gatk@noreply.github.com>; Cc: rdbremel <rdbremel017@gmail.com>; Mention <mention@noreply.github.com>; Subject: Re: [broadinstitute/gatk] Funcotator shuts down (#6182). . @rdbremel <https://github.com/rdbremel> This got missed in the churn of issues. Does this happen repeatedly or is it a 1 time occurrence? We've seen similar issues in the past and tried to wrap them all in layers of retries, but sometimes things slip through. â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub <https://github.com/broadinstitute/gatk/issues/6182?email_source=notifications&email_token=ANCR2VB4ZCHMAJUHBKE2SP3QRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI#issuecomment-547962085> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/ANCR2VHRV5JESZYAYX55YHTQRGRQFANCNFSM4I2MRFQA> . <https://github.com/notifications/beacon/ANCR2VAS2WE5TDCUC6G5LETQRGRQFA5CNFSM4I2MRFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECUTZZI.gif>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382:975,layers,layers,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182#issuecomment-548102382,1,['layers'],['layers']
Modifiability,"I grok the refactoring changes and those are ðŸ‘, but I think I'm going to need a walkthrough for the core spanning deletion work...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1135250576:11,refactor,refactoring,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857#issuecomment-1135250576,1,['refactor'],['refactoring']
Modifiability,"I guess it depends on how general we want to keep this input path. I think most purely read-depth based callers won't really be able to discover events smaller than 800bp or so (maybe 500bp at the lower limit) with any accuracy. I also don't know of any tools that we're considering that will describe individual breakpoints. . What about this for a rule: create two intervals for the start and end of the CNV interval + or - 151 bases (allowing a read length of slop). If the two intervals overlap, merge them together into a single evidence interval. . We could also make the slop amount parameterizable per input file, since different tools might have different characteristics, although that would be a feature we could just make a ticket for until we need it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3542#issuecomment-327499474:590,parameteriz,parameterizable,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3542#issuecomment-327499474,1,['parameteriz'],['parameterizable']
Modifiability,"I have a PR that fixes this. It's on the branch tws_desparkify_svdiscovery, which is a big refactor on some SV code. Someone could have a look and pull out just that bit of code to make a new, smaller PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662574860:91,refactor,refactor,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662574860,1,['refactor'],['refactor']
Modifiability,"I have been running into an issue with Funcotator where some mutations are causing Funcotator to crash because it attempts to query a segment that extends beyond the boundary of the transcript ( see https://github.com/broadinstitute/gatk/issues/6345 ). This pull request addresses the issue by adding a check for transcript length before executing the query. I looked at the code, and Funcotator currently handles problematic sequence queries in `getFivePrimeUtrSequenceFromTranscriptFasta()` by returning an empty string. I modified `getFivePrimeUtrSequenceFromTranscriptFasta()` to also return an empty string when the segment it is trying to retrieve extends beyond the boundary of the transcript. . I have a small VCF that can be used to reproduce the problem using the current code on `master` and the hg38 data source, and I have verified that this pull request allows Funcotator to process the problematic variant without crashing. I did not add the VCF to the tree, but can provide it if that is preferred. Is there any guidance for how to implement integration tests with funcotator? The Funcotator data source I am using is ~12gb, but I would think the problem could be reproduced with 1 transcript and 1 variant. This is my first pull request to GATK, so please let me know if there is anything you would like me to adjust, I'm happy to address any comments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6546:147,extend,extends,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6546,2,['extend'],['extends']
Modifiability,"I have noticed that when running spark tools (e.g. CountReadsSpark or MarkDuplicatesSpark) that running with an input in the form ""CountReadsSpark -I gs://my-bucket-dir/my-file.bam."" The tool crashes with the following unhelpful stacktraces:. ```; java.io.IOException: Error getting access token from metadata server at: http://metadata/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:208); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1072); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.SparkContext.withScope(SparkContext.scala:679); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:684,config,configure,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['config'],['configure']
Modifiability,"I have pull requests in flight for both (1) and (2). They are 1469; <https://github.com/GoogleCloudPlatform/google-cloud-java/pull/1469> and; 1470 <https://github.com/GoogleCloudPlatform/google-cloud-java/pull/1470>. Cheers,; JP. On Tue, Dec 6, 2016 at 3:54 AM, Tom White <notifications@github.com> wrote:. > Yes, Hadoop-BAM uses the NIO API to do file merging, whereas in GATK we; > were using the Hadoop APIs (and therefore the GCS<->HDFS adapter) to do it.; >; > It looks like there are a couple of things needed in GCS-NIO to use the; > NIO API for this.; >; > 1. GoogleCloudPlatform/google-cloud-java#1450; > <https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1450>; > so that we don't have to special-case gs URIs to remove everything; > except the scheme and host when looking up the filesystem (see; > https://github.com/HadoopGenomics/Hadoop-BAM/; > blob/master/src/main/java/org/seqdoop/hadoop_bam/util/; > NIOFileUtil.java#L40; > <https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L40>; > ); > 2. GoogleCloudPlatform/google-cloud-java#813; > <https://github.com/GoogleCloudPlatform/google-cloud-java/issues/813>; > to support path matching (https://github.com/HadoopGenomics/Hadoop-BAM/; > blob/master/src/main/java/org/seqdoop/hadoop_bam/util/; > NIOFileUtil.java#L90; > <https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L90>; > ); >; > There may be more, as I stopped there. The best way forward is probably to; > go back to the old code in GATK while the deficiencies in GCS-NIO are fixed; > and then released.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-266151447:441,adapt,adapter,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-266151447,1,['adapt'],['adapter']
Modifiability,"I have several use cases for this:; 1. I didn't mention it in the first comment, but if a tool does not want to provide custom `ReadFilter`from the user, but allow to disable the ones applied by the tool, this will keep in sync the parameter name.; 2. I would like to have a `ReadFilter` plugin that is applied ""after analysis"", but the parameters here said that they are applied ""before analysis"". This will be misleading for my users. For this case I also opened #2353, to allow custom parameters based on the same read filter plugin descriptor implemented in GATK.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274753122:288,plugin,plugin,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352#issuecomment-274753122,2,['plugin'],['plugin']
Modifiability,"I have the following instruction in a handson tutorial:. > If you haven't already done so, create a symlink to the gatk-launch script. Navigate back to /gatk and test the symlink by listing the tools available.; ```; cd /usr/local/bin; ln -s /gatk/gatk-launch gatk-launch; cd /gatk; gatk-launch â€“-list; ```. @vdauwera says:; > wouldn't it be simpler to export to path?. My reply:; > Environmental variables persist ephemerally. I haven't tested persistence when containers are stopped and restarted. @vdauwera requests:; > hmm, could also add to path in the bash profile... we should ask the devs if it's possible to set that up in the docker itself, for next time. Could we have both an environmental variable and a symlink that invokes the launch script in the Docker from any location? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3899:397,variab,variables,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3899,2,['variab'],"['variable', 'variables']"
Modifiability,"I have three main reasons to propose to move the arguments in CLP to an argument collection that is configurable by downstream tools/projects:. 1. Support hiding some arguments for downstream projects. For example, I do not want to support a config file by the user, but rather decide the settings for the framework and expose only some configuration.; 1. Set custom defaults for some downstream tools (including GATK). For example, a concrete tool might want to force the temp directory to be specified to avoid failures due to no space (and specify that in the documentation).; 1. Support old-style arguments (not kebab-case) for downstream projects that rely on the current argument definitions. I am specially affected by this one, because updating GATK to the 4.0.0 release of January will be a breaking change that will cause some nightmares for my users - and I don't want to do a major version bump yet (I have to re-work a bit my own framework before it). Thus, the first commit of this PR holds the proposal for the new argument collection. As I know that the team is also trying to normalize arguments and documentation, I included two more commits to help with the task (they can be removed if you think that it is better after the argument collection):; * Use `java.nio.Path` for temp directories (to support temp directories in HDFS, for example); * Change arguments moved to the collection to kebab-case (to help with #3853)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998:100,config,configurable,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998,3,['config'],"['config', 'configurable', 'configuration']"
Modifiability,"I havent published to github yet, pending getting these core changes in; however, the purpose is pretty simple: allow VariantEval to inherit from MultiVariantWalker, but not require it to include the required argument -V. this seemed comparable to VariantWalkerBase (no arguments), and VariantWalker (specifies -V). GATK3's VariantEval uses the --eval argument and I generally tried to keep everything in this port in sync with GATK3, within reason. If there is another way to subclasses to negate some @argument defined by a superclass this would work too. If you want to see more I'll push to github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776:133,inherit,inherit,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-379803776,2,['inherit'],['inherit']
Modifiability,I imagine that @skwalker's scripts could be adapted for the task -- I'll try to set up a meeting with her next week to discuss.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947:44,adapt,adapted,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947,1,['adapt'],['adapted']
Modifiability,I inherited broadinstitute/gatk-protected#795 from @davidbenjamin.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-276161535:2,inherit,inherited,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2267#issuecomment-276161535,1,['inherit'],['inherited']
Modifiability,"I just pulled to the latest version and was surprised to see `gradlew clean` not work!; ```; $ ./gradlew clean; (...); Could not find org.broadinstitute:barclay:1.0.0-24-g87c3fa2-SNAPSHOT; ```. Reverting to 1.0.0-17-g30db73c-SNAPSHOT didn't work (same error).; Reverting to 1.0.0 made it fail somewhere else, with:; Could not resolve org.broadinstitute:gatk-bwamem-jni:1.0.0-rc1-SNAPSHOT. What's going on? Is there something wrong with my configuration?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579:439,config,configuration,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579,1,['config'],['configuration']
Modifiability,"I like option 2. If one of the other options is specified, use the specified value. For example, the following would use 0.1 for interval-psi-scale:. --set-defaults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:482,parameteriz,parameterized,482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['parameteriz'],['parameterized']
Modifiability,"I looked at the first new commit and skimmed the second. I'm assuming; tests still pass. If you want another set of eyes on the test data; refactor it'll have to wait until Monday. -L. On Fri, Oct 21, 2016 at 12:19 PM, David Benjamin notifications@github.com; wrote:. > @ldgauthier https://github.com/ldgauthier I _think_ you approved; > without needing it to send back to you, but I'm paranoid and not fully; > confident with the new github review system. Also, the changes to the test; > code to get rid of the duplicated likelihood-setting were pretty big.; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gatk/pull/2185#issuecomment-255445145,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AGRhdEDyfInQUcZiSZ5qBFhrAOnJpNZiks5q2RBVgaJpZM4KFNEm; > .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-256139184:139,refactor,refactor,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-256139184,1,['refactor'],['refactor']
Modifiability,"I made the change you suggested but just realized I forgot to push before; leaving for my flight. I'll push when I get back. On Tue, Nov 20, 2018, 12:04 PM ldgauthier <notifications@github.com wrote:. > *@ldgauthier* commented on this pull request.; >; > Looks lovely, thanks for the quick fix! I have one refactoring suggestion; > -- take it or leave it.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java; > <https://github.com/broadinstitute/gatk/pull/5435#discussion_r235089237>:; >; > > return Arrays.asList(squareSum,totalDP);; > } catch (final NumberFormatException e) {; > throw new UserException.BadInput(""malformed "" + GATKVCFConstants.RAW_RMS_MAPPING_QUALITY_KEY + "" annotation: "" + rawDataString);; > }; > }; >; > + /**; > + * Private getter function to replace VariantContext::getAttributeAsIntList in instances where there is a chance; > + * that ints will overlow beyond Integer.MAX_VALUE; > + * @return VariantContext attribute indexed by key, as list of long.; > + */; > + static private List<Long> getAttributeAsLongList(final VariantContext vc, final String key, final Long defaultValue) {; >; > I agree with your reticence to put this into htsjdk. The type handling; > there is super awkward, but I don't think it's worth dealing with until it; > gets improved in 3.0. However, I might suggest making this method public; > and moving it to a utility class. GATKProtectedVariantContextUtils has a; > lot of similar methods; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5435#pullrequestreview-176875488>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCBnSuwz30W-52kw9uZTXywWUCua-ks5uxDYbgaJpZM4Yp2Gl>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5435#issuecomment-440649639:306,refactor,refactoring,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5435#issuecomment-440649639,1,['refactor'],['refactoring']
Modifiability,I moved Mark duplicates into it's own package (instead of a single file). There is more refactoring that could be done. I might stop here for now and sync with @jean-philippe-martin on his planned changes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/770:88,refactor,refactoring,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/770,1,['refactor'],['refactoring']
Modifiability,I realized that the `--disable X --disable X` cannot blow up because the argument is a `Set`. Every repeated argument is hidden from the plugin. Either the signature should change to throw the exception or ignore that issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276979349:137,plugin,plugin,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276979349,1,['plugin'],['plugin']
Modifiability,I refactored SplitNCigar reads to use the ReadWalker interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1919:2,refactor,refactored,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1919,1,['refactor'],['refactored']
Modifiability,I removed an unused type parameter and refactored some functionality into a class MultiPloidyGenotyper which handles dealing with multiple ploidies in a somewhat unified way. It could probably be renamed and expanded slightly.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8598:39,refactor,refactored,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8598,1,['refactor'],['refactored']
Modifiability,"I should have been more specific. `--genotype-given-alleles` is HaplotypeCaller functionality, which is very similar if not outputting GVCFs. If providing dbSNP is similar to something you might want to do, then I think this approach will work. My concern was that if you were interested in a particular locus, but had never seen a variant there in any sample (i.e. didn't have an ALT allele to compare to), that implementation could get tricky. We should be able to output 100% reference sites with what I have in mind. @davidbenjamin while you're waiting for the restoration of the TCGA data, would you be interested in extending GGA mode to GenotypeGVCFs? (Obviously without the graph part.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549503239:622,extend,extending,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549503239,1,['extend'],['extending']
Modifiability,I suspect this is related to the jar configuration (3 separate uber- jars) that is unique to the docker CI tests. I'll debug and resolve this after #6351 is merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1251462525:37,config,configuration,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1251462525,1,['config'],['configuration']
Modifiability,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:492,adapt,adapters,492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762,2,"['adapt', 'flexible']","['adapters', 'flexible']"
Modifiability,I think now that we have the NIO plugin working we can probably replace everything that used AuthHolder with NIO.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277828180:33,plugin,plugin,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277828180,1,['plugin'],['plugin']
Modifiability,"I think that it is necessary to have a way for downstream projects to override some of the top-level arguments in the base CLP class. For example, the config file is for documentation purposes, but I don't want to expose users to that argument because I will set the defaults programmatically. Another example is the GCS retries, which might not be useful for a software that is not planning to support GCS even if it is already implemented (or does not want to expose). As a downstream developer, for me it is important to being able to configure arguments and expose/hide them to my final users; with the current implementation, my main issue is to have an argument that are irrelevant for the toolkit user and that I get questions about why and how to use them (the most clear example, the config file). If the main problem is to change an interface, a default value for new methods can be added to keep the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183:151,config,config,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-361876183,6,['config'],"['config', 'configure']"
Modifiability,"I think that no `VariantAnnotation` is documented yet, because there is no even a plugin. It will be nice to have them anyway...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342818332:82,plugin,plugin,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342818332,1,['plugin'],['plugin']
Modifiability,I think that the annotation plugin is assigned to @jamesemery here: https://github.com/broadinstitute/gatk/issues/3287. But of course it can be documented before it is a plugin. I just wanted to point out that all of them should be annotated properly...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823849:28,plugin,plugin,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-342823849,2,['plugin'],['plugin']
Modifiability,"I think that this is a nice feature (at least for me) and not a bug. For example, if in GATK someone runs a tool with `-RF read_filters.args`, then the pipeline cannot be reproduced in a different dataset unless the file is accessible. I can understand that it could be also nice to preserve the `-RF read_filters.args` to be able to modify the file an re-run the tool with different parameters, but for me the purpose of storing the command line in the header or other places is keep track of the exact params that I used: if a file is modified, then it is impossible to trace the params. For input files, this is expected (if the input has changed, it is expected that the result change), but for arguments it shouldn't be the case (independently of the file changing, the tool was running with exactly that parameters). I vote for solve this in Barclay in a configurable way, to allow users to decide which kind of verbosity of the command line they want (I definitely prefer to expand as currently).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092:861,config,configurable,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3797#issuecomment-342798092,1,['config'],['configurable']
Modifiability,"I think that this is prepared for merging, @cmnbroad. As soon as it gets in, I would submit a PR fixing all the problems found in the plugin descriptor for discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-278391496:134,plugin,plugin,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2385#issuecomment-278391496,1,['plugin'],['plugin']
Modifiability,"I think that this is related with https://github.com/broadinstitute/gatk/issues/1880, and I've already develop a rough system in a small project to have a plugin for variant annotation. I would love to contribute to this, and I kind of started with https://github.com/broadinstitute/gatk/pull/2534",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2542#issuecomment-290173938:155,plugin,plugin,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2542#issuecomment-290173938,1,['plugin'],['plugin']
Modifiability,I think the only failure left is a Travis thing. Back to @vruano after heavy refactoring.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216:77,refactor,refactoring,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831#issuecomment-519630216,1,['refactor'],['refactoring']
Modifiability,"I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, https://github.com/dbpedia/distributed-extraction-framework/issues/9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169:466,Config,Configuration,466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169,1,['Config'],['Configuration']
Modifiability,I think we can finally unblock this using PAPIv2's ability to request machine types on Google Cloud. We just have to configure our jenkins server to use PAPIv2.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4356#issuecomment-415862887:117,config,configure,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4356#issuecomment-415862887,1,['config'],['configure']
Modifiability,I think we will add a logger for GenomicsDB with configurable verbosity - but this is low priority for us.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300298389:49,config,configurable,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-300298389,1,['config'],['configurable']
Modifiability,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:58,variab,variable,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215,1,['variab'],['variable']
Modifiability,"I tried setting that environment variable, but it did not resolve the issue. Please note that I have this filesystem mounted as `CIFS` not `NFS`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453657941:33,variab,variable,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453657941,1,['variab'],['variable']
Modifiability,I want to point out a bug regarding the read plugin that I reported in #357 (and fix with in #2359): disable a default filter with arguments blows up as a `CommandLineException` as if the user provided the arguments for a disabled filter. This is quite important in this regard of insane combinations.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276625536:45,plugin,plugin,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276625536,1,['plugin'],['plugin']
Modifiability,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1752:737,extend,extends,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752,1,['extend'],['extends']
Modifiability,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:156,plugin,plugin,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,5,"['Refactor', 'plugin']","['Refactor', 'plugin']"
Modifiability,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:304,variab,variable,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['variab'],['variable']
Modifiability,"I was thinking that if we relied on PyPI for distribution, it would only be for released builds, not a release for every repo merge commit. But, I'm increasingly inclined to think that in the short term we should just include the python archive/zip file right in the gatk distribution zip, and modify the env .yml to install from that. Then every configuration (docker image, git clone user, and end user) could use exactly the same method to establish the environment. That seems like the simplest solution for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343:347,config,configuration,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964#issuecomment-352279343,2,['config'],['configuration']
Modifiability,"I will do a big PR with a commit for each of the issues that we found in the plugin, and including tests. Although the plugin interface is going to change in Barclay, I think that before that the PR should setup all the tests for the issues to ensura that the change does not broke anything. Any opinion on this, @cmnbroad, @droazen, @lbergelson?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278248587:77,plugin,plugin,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-278248587,2,['plugin'],['plugin']
Modifiability,"I would like to have this feature for a new project that will rely on the built-in walkers of GATK. The main problem that I am facing is while grabbing the info from the METAINF file. For fixing this, I have a proposal:. * Create a new interface with the single method to print the startup message; * Create a base-class with the current code in GATK, which can be extended to override some parts of the startup message, but not all.; * Make a non-final static field in `CommandLineProgram` , which is settable. This could be set in the `Main` for toolkits (similarly to how the config file is set). The default will be the GATK implementation. If you agree with the proposal, let me know to implement it and submit a PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-381504458:365,extend,extended,365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-381504458,2,"['config', 'extend']","['config', 'extended']"
Modifiability,"I would like to keep in some of my tools the read group arguments in sync with the `AddOrReplaceReadGroup` in picard, but currently there is no way of access them. This is a very simple and trivial patch to extract the short/long names to a static String variable to be able to use them. In addition, I refactored the variable names to the camel-case java convention.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2260:255,variab,variable,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2260,3,"['refactor', 'variab']","['refactored', 'variable']"
Modifiability,"I would like to specify what passing a `ReadFilter` to some of my tools means, so maybe passing an `ArgumentCollection` will be simpler than this one, I agree. Although #2085 may solve the issue regarding the `ReadTransformer`/`ReadFilter` ordering, I would like to have in the plugin a way to specify different parameters (maybe some of then hidden before expose to users or advanced in the case of disabling). I will open a new PR for that change, but I will really appreciate if I can get something like that in this and other plugins (if implemented).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983:278,plugin,plugin,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-275082983,4,['plugin'],"['plugin', 'plugins']"
Modifiability,"I would like to start a new project to extend the engine of GATK (mostly walker types, e.g. https://github.com/broadinstitute/gatk/issues/1198 and common utilities), and thus I require to have an idea of how the versioning scheme is related to the public API if at all. This will allow me to say that the extensions works with GATK between 4.0.0.0 and 4.1.0.0, for example, and to know when some work is required to move to the next released version. Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4603:39,extend,extend,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603,1,['extend'],['extend']
Modifiability,"I'd be OK with adding some sort of experimental tag, but I'm not sure where to put it. Does that goes in the docs or somewhere in the code to post a warning to the log? I think the new tests should cover that it's working as intended. There is definitely room for future work (e.g. paying closer attention around the boundaries, the refactoring ideas, etc), but for now the docs should describe the expected behavior well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847926227:333,refactor,refactoring,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847926227,1,['refactor'],['refactoring']
Modifiability,"I'd like to get to the point where most/all GATK tools extend `GATKTool` rather than `CommandLineProgram` directly, so I think we have to keep this one open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358025246:55,extend,extend,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358025246,1,['extend'],['extend']
Modifiability,"I'll also mention that as part of https://github.com/broadinstitute/gatk/issues/4341 we plan to give tools more control over the arguments inherited from `GATKTool`, including selectively disabling and redefining engine arguments, so once a mechanism is in place for that `CalculateGenotypePosteriors` could make `--sequence-dictionary` required. Currently this ability only exists for a few `GATKTool` arguments, and the tool has to override methods like `requiresReads()` to make use of it. Until the ability to do this is generalized, recommend the stopgap solution with the check in `onTraversalStart()` + a note in the tool's docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4383#issuecomment-364497042:139,inherit,inherited,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4383#issuecomment-364497042,1,['inherit'],['inherited']
Modifiability,"I'll be adding documented feature tags to the 57 annotation modules. Before I get to these, I need a new ANNOTATORS category to exist in HelpConstants.java. . I've taken the liberty to name the category ANNOTATORS. Here is the relevant info I added to HelpConstants.java:. - group name variable and descriptor: DOC_CAT_ANNOTATORS = ""Annotation Modules""; - group summary variable and descriptor: DOC_CAT_ANNOTATORS_SUMMARY = ""Annotations available to HaplotypeCaller, Mutect2 and VariantAnnotator""; - super category: Utilities (same group as read filters)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3835:286,variab,variable,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3835,2,['variab'],['variable']
Modifiability,I'll take it. The tab completion task is printing out too many warnings for all of the new instances of unresolvable backtrace variables due to Picard. I'll do....something with it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3710#issuecomment-337386911:127,variab,variables,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3710#issuecomment-337386911,1,['variab'],['variables']
Modifiability,"I'm adding some issues and PRs for make the plugin usable in other cases too, @cmnbroad. Maybe you prefer that solution instead of make it extensible. Just let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544:44,plugin,plugin,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275376544,2,['plugin'],['plugin']
Modifiability,"I'm always confused with static-block initializers, and I just wondered in the config PR if the static block in `Main` it is correctly setting everything in sub-classes. I understood that this won't work in Main-derived classes; if that's not the case, feel free to close this PR...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324627658:79,config,config,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324627658,1,['config'],['config']
Modifiability,"I'm assuming you will have the subset of samples before creating a GenomicsDBFeatureReader object (and before creating the corresponding Protobuf export configuration object). More precisely, you are NOT requesting a line by line filter similar to:; At pos 100, compute INFO fields etc including only the samples whose QUAL > 5; At pos 102, compute INFO fields etc including only the samples whose QUAL > 5; ....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322:153,config,configuration,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322,1,['config'],['configuration']
Modifiability,"I'm exploring the usage of the `AssemblyRegionWalker` engine for custom tools and I realized that there is very little support for customization. One of the things that will be nice is to use a custom `ActivityProfileState.Type` for assigning custom reasons to be an active region (or non active at all). My proposal is the following:. * Abstract `ActivityProfileState.Type` with one method to get the value as a `Number`.; * Make the current enum extend this abstract class.; * Change the method to get the value to delegate into the `Number`. This will allow custom types for the activity profile, and value range for the result value (now it should be positive or null).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2539:448,extend,extend,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2539,1,['extend'],['extend']
Modifiability,"I'm glad it's working now, but a PS since you asked about `-independent-mates`: several months ago we made Mutect2 force paired reads to share the latent random variable indicating which haplotype they are derived from in the somatic genotyping model. This is correct because paired reads come from the same molecule of DNA. `-independent-mates` disables this and tells Mutect2 to forget about pairing. We only created the option because some synthetic validation data is generated by spiking in variation without regard to pairing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833:161,variab,variable,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6230#issuecomment-596165833,1,['variab'],['variable']
Modifiability,"I'm going to close this issue because it's not a bug. Several things in the code of Mutect2 and FilterMutectCalls adapt as they traverse the genome and it's possible that some learned parameter shifts minutely. For example, the assembly graph pruning algorithm uses knowledge of previously assembled regions to better distinguish between errors and somatic variation. It's also possible that somewhere we forgot to give something a fixed random seed. In full honesty, I _wish_ that I knew exactly what causes the 3142 to become 3143, and I regret that I don't have time for it. Nonetheless, in principle it is not cause for alarm.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338:114,adapt,adapt,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152#issuecomment-1983783338,2,['adapt'],['adapt']
Modifiability,"I'm looking into migrating custom GATK3 variant Info/GenotypeAnnotations to GATK4. The annotate() method in GATK3 was passed a sizable amount of context. This is greatly reduced in GATK4. I understand a desire to simplify, such as not passing the Walker. FeatureContext in particular would be helpful, is there another way to access that from VariantAnnotations?. Stepping back: the one scenario I want to support is to annotate genotype concordance between the input VCF and a reference VCF. In our GATK3 implementation, the user supplied that VCF on the command line when executing VariantAnnotator. This plugin used GATK3's walker.getResourceRodBindings(), which seems analogous to GATK4 FeatureContext, to find that binding. It then queries that VCF to find any VariantContext from the current site. . I realize this is raising a couple issues: a) access FeatureContext from within annotate(), , b) efficiently query VariantContext from another resource, and c) plugin that would ideally provide its own command-line argument. . Are there any existing GATK annotations or other plugins that deal with these issues?. Thanks in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930:607,plugin,plugin,607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'm more familiar with working with interval data files so I can't speak much to variant call formats except that VCF tends to be a bit wonky with interval data (such as SV calls) because, as @samuelklee mentioned, most of the fields are likely to be irrelevant and thus waste space. I feel strongly that BED-like formats are the way to go for interval data, i.e. #contig start end x1 x2 x3 x4 ...; chr1 2938 3949 3.9 0 + cat ...; ... where x1, x2, x3, x4 ... are columns for variables of different types (which could be specified by additional header lines like in VCF).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457:476,variab,variables,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457,1,['variab'],['variables']
Modifiability,"I'm not sure exactly what's happening but I suspect it has something to do with the way the files are mounted. My guess is that there is some sort of transient interruption happening in the connection between the EC2 instance and the file server, and it's causing an error in gatk. When reading from a local file GATK does not expect any errors since errors in local files are usually fatal problems caused by a broken disk. Its probably some sort of bug in amazon's fuse implementation which isn't properly hiding network problems from the software. . I expect that your output is truncated at the point the error occured, and you probably need to rerun those shards. Instead of mounting them with amazon's fuse, you could try to either copy the files to a local disc, or access them using an NIO filesystem plugins like this plugin https://github.com/awslabs/aws-java-nio-spi-for-s3 or as signed URLs using https://github.com/broadinstitute/http-nio/ (included in gatk 4.6).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942:809,plugin,plugins,809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735#issuecomment-2214915942,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"I'm not sure how this worked before, but adding a variable fixes it. @ahaessly, let me know what you think!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6939:50,variab,variable,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6939,1,['variab'],['variable']
Modifiability,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:1612,extend,extend,1612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,1,['extend'],['extend']
Modifiability,"I'm not sure why the `gs` provider doesn't get installed (I think JP was seeing this before), but in any case there's a workaround in GATK for it: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java#L380-L390. However, since the Hadoop-BAM code is doing the path lookups, it can't call that code directly (the dependency is the wrong way round). It would be best if we could fix the underlying problem of course, so that it gets picked up properly - I wonder if this can be done by fixing the service provider so it survives relocation (see http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer). BTW I'm afraid I'm travelling this week, so I won't have time to look at it until next week either :(",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-263997131:635,plugin,plugins,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-263997131,2,['plugin'],"['plugin', 'plugins']"
Modifiability,I'm not totally clear from your response but I think you've resolved the problem? . If you're encountering a bug merging bai files could you open an issue describing that with your stack trace and any relevant information about the configuration you're running?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623:232,config,configuration,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547956623,2,['config'],['configuration']
Modifiability,"I'm thinking about lists such as filters packages (in the filter plugin): I can imagine a list with long package names separated by commas, which might be complicated to read due to the repetition of the same organization name. I think that it might be also more organize if the configuration file is an YML with sections for the different configurations: this will make the file more readable and easier to modify. Something like the codecov configuration will be interesting, separating configurations for spark, plugins, feature codecs, etc. For example, if I want to use a custom codec for BED files while including the HTSJDK codec packages, I would find a problem. Doing it in a granular level may be interesting for having something like:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - org.magicdgs.htsjdk.codecs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307803675:65,plugin,plugin,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078#issuecomment-307803675,6,"['config', 'plugin']","['configuration', 'configurations', 'plugin', 'plugins']"
Modifiability,"I'm trying to genotype 2388 whole genome samples of a wild species. This species has a large genome and lots of diversity. I've created my genomicsDB for my combined set and genotypeGVCF gets stuck when trying to make the VCF. ; I've been giving it 120G of ram, 32 cores and 30 minutes and it only prints out the first variable 12 sites (corresponding to about 800bp of the genome) to the VCF . I understand that is certainly going to be slow, and I'm prepared to heavily parallelize it, but this is currently unusable to me. Is there any way to speed it up?. Here's my command:. ```; /gatk/gatk-launch --java-options ""-Xmx120G"" GenotypeGVCFs \; -R /home/user/bin/ref/reference.fa \; --intervals $contig \; -V gendb://${chr}_$pos \; -O /scratch/wild_gwas/$genomicsdb/${chr}_$pos.tmp.vcf.gz \; --seconds-between-progress-updates 5 --verbosity DEBUG; ```; Here's standard out:. > 21:13:04.092 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/install/gatk/lib/gkl-0.8.2.jar!/com/intel/gkl/native/libgkl_compression.so; > 21:13:04.108 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/gowens/libgkl_compression3380966567685792416.so; > 21:13:04.218 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.219 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.0.0; > 21:13:04.219 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; > 21:13:04.219 INFO GenotypeGVCFs - Executing as user@cdr806.int.cedar.computecanada.ca on Linux v3.10.0-693.5.2.el7.x86_64 amd64; > 21:13:04.219 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-2ubuntu1.16.04.3-b11; > 21:13:04.219 INFO GenotypeGVCFs - Start Date/Time: January 15, 2018 9:13:04 PM UTC; > 21:13:04.219 INFO GenotypeGVCFs - ------------------------------------------------------------; > 21:13:04.220 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:319,variab,variable,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['variab'],['variable']
Modifiability,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2033:257,extend,extending,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033,3,['extend'],['extending']
Modifiability,"I'm using spark 2.1.0. I can confirm it works with the command ; ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; --sparkMaster yarn; ```; so I guess it's really a minor issue. I can see it confusing other spark users though, who might expect spark configuration arguments to go through `spark-submit` rather than the application args, especially since the --sparkMaster app arg is optional. Just my two cents.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301945697:572,config,configuration,572,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301945697,1,['config'],['configuration']
Modifiability,"I've added a few commits that clean up some of the code inherited from VQSR regarding the use of labeled resources when using allele-specific annotations. This should be ready for review and/or experimentation with importing into the WARP repo, @meganshand. There are a few unrelated failing tests, which I think others are seeing in their branches as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1323953256:56,inherit,inherited,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1323953256,1,['inherit'],['inherited']
Modifiability,"I've added a new end-to-end test for SelectVariants that writes to GCS. Sadly, the IntegrationTestSpec class uses Files throughout, so it wasn't possible to do this simply without first completely refactoring IntegrationTestSpec (which should probably be its own pull request). . Doing this refactoring would have the advantage that changing existing end-to-end tests from local to GCS would be trivial. For now instead I went with an ad-hoc approach. It works, and the test passes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612:197,refactor,refactoring,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455686612,4,['refactor'],['refactoring']
Modifiability,"I've had several Travis test failures (on my picard removal branch) that appear to be failures during kryo serialization of a mocked ReferenceMultiSource object (based on the failing class name, (org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource$$EnhancerByMockitoWithCGLIB$$b0dc631f, which looks like the CGLIB names mentioned [here](https://github.com/mockito/mockito/issues/319)). We're on an ancient version of mockito anyway, and newer versions no longer use cglib, so it seemed like a good time to upgrade. To do so I also had to replace usage of the method getArgumentAt, which has been [deprecated](https://github.com/mockito/mockito/pull/373) in favor of getArgument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581:267,Enhance,EnhancerByMockitoWithCGLIB,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581,1,['Enhance'],['EnhancerByMockitoWithCGLIB']
Modifiability,"I've pulled the problem VCF and a couple of successful ones locally and I can confirm that when running with 4 VCFs:. - VCFs that succeeded through JointGermlineCNVSegmentation in our pipeline succeeded for me locally; - The VCF that was flagged in the error message `Exception thrown at chrX:6383391 [VC SAMPLE_ID.segments.vcf.gz ...` completes just fine with other successful partners; - The VCF that was not identified in the error message, but was inferred to be a sex chromosome aneuploidy causes a failure with any combination of other VCFs; - If there are more than 2 VCFs run together, including the failing VCF/aneuploid sample, the error message indicates the problem originates in a non-aneuploid VCF, which misleading and makes this hard to treat. This behaviour was consistent in `4.5.0.0`. Command used in my toy dataset:. ```; gatk --java-options ""-Xms4000M -Xmx6000M"" JointGermlineCNVSegmentation -R /data/Homo_sapiens_assembly38_masked.fasta -O /data/out.vcf.gz -V /data/SAM1.segments.vcf.gz -V /data/SAM2.segments.vcf.gz -V /data/SAM3.segments.vcf.gz -V /data/SAM4.segments.vcf.gz --model-call-intervals /data/preprocessed.interval_list -ped /data/inferred_sex_pedigree.ped; ```. - In this configuration, `SAM4` is aneuploid, and `SAM1` is always the flagged VCF; - If I remove `SAM1` and re-run with 3 VCFs, `SAM3` is mentioned in the error message. It's not derived from alphabetical order, first argument specified with `-V`, or first in the PED file",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736:1208,config,configuration,1208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834#issuecomment-2123897736,1,['config'],['configuration']
Modifiability,INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2874,Config,ConfigFactory,2874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"ISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4483,Config,ConfigFactory,4483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.L",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4949,variab,variable,4949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,Ideally using a standard mechanism like Apache `Configuration`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368:48,Config,Configuration,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368,1,['Config'],['Configuration']
Modifiability,"If I understand correctly, @bbimber, this sounds a lot like the `--genotype-given-alleles` change David B. made recently -- discover new sites in the provided samples, but _also_ output calls against the variants specified in the resource file. This is a small change from what you propose because the resource file would specify a particular ALT allele, so if you were interested in positions where you haven't seen a variant yet we'd need to extend functionality to call against <NON_REF> and output or you could put some random placeholder allele, although the identity of that allele would affect your likelihoods. On the other hand, having an ALT specified from a previous cohort would improve your likelihoods for all-reference cohorts compared to what you get now for non-variant site output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549456607:444,extend,extend,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239#issuecomment-549456607,1,['extend'],['extend']
Modifiability,"If a tool requires a reference, for example, it should be able to indicate so via an annotation, instead of manually checking the inherited argument value for null. The engine should perform the check on behalf of the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/120:130,inherit,inherited,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/120,1,['inherit'],['inherited']
Modifiability,"If it helps, this is a dummy walker that illustrates the kind of parallelization I'm exploring whether I can implement. The main sticking point I think exists relates to different threads interacting with FeatureManager and the feature reading code:. I would appreciate any thoughts on how to approach this in GATK. ```. /**; * This is a contrived example. It is designed to explore multithreading; */; public class DemoMultiVariantEval extends MultiVariantWalkerGroupedOnStart {; protected List<VariantEvalEngine> engines = new ArrayList<>();. @ArgumentCollection; protected VariantEvalArgumentCollection variantEvalArgs = new VariantEvalArgumentCollection();. @Override; protected void initializeDrivingVariants() {; getDrivingVariantsFeatureInputs().addAll(VariantEvalEngine.getFeatureInputsForDrivingVariants(variantEvalArgs));. super.initializeDrivingVariants();; }. private ExecutorService pool = null;; private final int threads = 2;. @Override; public void onTraversalStart() {; //Again, contrived. The real case would set up multiple engines with different parameters:; engines.add(new VariantEvalEngine(variantEvalArgs, this, getSequenceDictionaryForDrivingVariants(), getSamplesForVariants(), logger));; engines.add(new VariantEvalEngine(variantEvalArgs, this, getSequenceDictionaryForDrivingVariants(), getSamplesForVariants(), logger));. final ThreadFactory threadFactory = new ThreadFactoryBuilder(); .setNameFormat(""dummywalker-thread-%d""); .setDaemon(true); .build();. pool = Executors.newFixedThreadPool(threads, threadFactory);; }. @Override; public void apply(final List<VariantContext> variantContexts, final ReferenceContext referenceContext, final List<ReadsContext> readsContexts) {; // Parallelization should help; however, these steps interact with FeatureManager and the FeatureInputs.; // Is there an appropriate way to approach this?; Utils.runInParallel(threads, () -> {; this.engines.parallelStream().forEach(engine -> {; engine.apply(variantContexts, referenceContext);;",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7013#issuecomment-750442132:437,extend,extends,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7013#issuecomment-750442132,1,['extend'],['extends']
Modifiability,"If the assembler has not discovered an allele it could be simply that the samples does not have that allele but it could also be the case that it failed to assemble due to some edge case. . If the former it makes sense to assign depth 0 but for the latter that would be misleading and a ""I don't know"" output (""."") would be more appropriate. For example, what about those HOM-REF sites that end up with PL=0,0,0 because the reference-confidence-model found reads that don't support the reference sequence yet the assembly did not produce a concrete alternative. Fast forward and the same sample is joint-genotyped with in a cohort with other samples for which HC assembled the alternative haplotype/allele (correctly). Then we will assign AD=0 to those alternative alleles in the original (no-quite)-hom-ref sample. . I think the better answer would be AD=""."" in light of the lack of confidence on the hom-ref call. . Would this even extend to cases where we are confident on hom-ref? Unless any single read is exactly the reference at that site there is a potential for that allele to have gone unnoticed. . Would make sense that if someone wants to know the AD for every alt. allele at a sample where some weren't discovered in, he must re-run HC in GGA mode with the full list of alt alleles?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7290:934,extend,extend,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7290,1,['extend'],['extend']
Modifiability,"If the encapsulation of the datasources is causing issues for tools that extend `GATKTool` directly, we can relax it -- it was intended to prevent walker authors from directly manipulating the datasources used for the traversal, but it may be doing more harm than good at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358333054:73,extend,extend,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358333054,1,['extend'],['extend']
Modifiability,If the requested key is missing in an Avro record:. - Avro 1.11 [throws](https://github.com/apache/avro/blob/release-1.11.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L267-L269); - Avro 1.8 [returns null](https://github.com/apache/avro/blob/release-1.8.0/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L208). Most of the code here was written for Avro 1.8 behavior; these changes adapt for Avro 1.11.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8266:430,adapt,adapt,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266,1,['adapt'],['adapt']
Modifiability,"If this passes, we can probably remove encrypted_29f3b7c4d8c3_key from the repo variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4627:80,variab,variables,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4627,1,['variab'],['variables']
Modifiability,"If you manually specify the fullName attribute for an argument in an Option annotation, it is not respected, and instead the full name of the argument gets set to the name of the annotated member variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/111:196,variab,variable,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/111,1,['variab'],['variable']
Modifiability,Implement 1D and 2D adaptive quadrature,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3318:20,adapt,adaptive,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3318,1,['adapt'],['adaptive']
Modifiability,"Implement ReadsHtsgetData source, refactor HtsgetReader",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6662:34,refactor,refactor,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6662,1,['refactor'],['refactor']
Modifiability,"Implement a `PythonScriptExecutor` that is similar to the existing `RScriptExecutor` (invokes Python with a given set of arguments). Then ask the CNV team to prototype an example tool or two that use `PythonScriptExecutor` to call into a Python machine-learning library, and do an assessment of maintainability, etc. `PythonScriptExecutor` will come with an attached set of conditions for its use, intended to address the most serious issues raised by the engine and support teams with having Python code in the GATK. We should document these conditions in the docs for `PythonScriptExecutor` when it's implemented:. 1. All tools that use `PythonScriptExecutor` must have a Java-based front-end, with standard GATK (barclay-based) arguments. We put a lot of development effort into our arg parser and into striving for user-interface consistency across tools, and cannot afford to duplicate this effort in Python. Geraldine (CC'd) and the rest of the support team can back me up on this one!. 2. An honest effort should be made to minimize the amount of code written in Python -- as much of each tool's work as possible should be done in Java. In particular, reading/writing final inputs and outputs should happen in Java. This is important for a number of reasons, including the engine team's goal of ensuring universal GCS support, consistent Google authentication handling, etc. Again, we really don't want to have to duplicate that work in Python, or for the tools that call into Python to be inconsistent with the rest of the toolkit. 3. All dependencies (Python and native) of Python libraries used will be clearly documented, and included in the default GATK docker image. I don't think I need to explain why this one is important :) . 4. Before we go any further down this path, we prototype one or two tools using `PythonScriptExecutor`, and do a fair assessment of maintainability and other concerns of the engine/support teams, such as whether it will even be possible to package all depend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:295,maintainab,maintainability,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,1,['maintainab'],['maintainability']
Modifiability,"Implements two new tools and updates some methods for a revamp of the `CombineBatches` cross-batch integration module in [gatk-sv](https://github.com/broadinstitute/gatk-sv). - `SVStratify` - tool for splitting out a VCF by variant class. Users pass in a configuration table (see tool documentation for an example) specifying one or more stratification groups classified by SVTYPE, SVLEN range, and reference context(s). The latter are specified as a set of interval lists using `--context-name` and `--context-intervals` arguments. All variants are matched with their respective group which is annotated in the `STRAT` INFO field. Optionally, the output can be split into multiple VCFs by group, which is a very useful functionality that currently can't be done efficiently with common commands/toolkits.; - `GroupedSVCluster` - a hybrid tool combining functionality from `SVStratify` with `SVCluster` to perform intra-stratum clustering. This tool is critical for fine-tuned clustering of specific variants types within certain reference contexts. For example, small variants in simple repeats tend to have lower breakpoint accuracy and are typically ""reclustered"" during call set refinement with looser clustering criteria.; - `SVStratificationEngine` - new class for performing stratification.; - Updates to breakpoint refinement in `CanonicalSVCollapser` that should improve breakpoint accuracy, particularly in larger call sets. Raw evidence support and variant quality are now considered when choosing a representative breakpoint for a group of clustered SVs.; - Added `FlagFieldLogic` type for customizing how `BOTHSIDE_PASS` and `HIGH_SR_BACKGROUND` INFO flags are collapsed during clustering.; - `RD_CN` is now used as a backup if `CN` is not available when determining carrier status for sample overlap.; - Removed no-sort option in favor of spooled sorting.; - Bug fix: support for empty EVIDENCE info fields; - Bug fix: in one of the JointGermlineCnvDefragmenter tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8990:255,config,configuration,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8990,1,['config'],['configuration']
Modifiability,Improvements and refactoring of Nucleotide.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4846:17,refactor,refactoring,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846,1,['refactor'],['refactoring']
Modifiability,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857:284,Adapt,AdaptiveChainPruner,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"In `PicardCommandLineProgram` there is an `instanceMain` that gets called when a subclass program is called. This ""main"" sets a number of static variables to values parsed from the arguments, then calls `super.instanceMain`. The problem is that the values haven't been parsed before they are referenced in this function, so the default values are _always_ used. The ignored flags are:; - `VALIDATION_STRINGENCY`; - `COMPRESSION_LEVEL`; - `MAX_RECORDS_IN_RAM`; - `CREATE_INDEX`; - `CREATE_MD5_FILE`. These flags are ignored for _all_ Picard tools, of which we have dozens. A rough sketch of the solution is here:; https://github.com/broadinstitute/gatk/compare/da_fix_picard; If this seems like a reasonable approach, I'll retake the bug and fix this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1155:145,variab,variables,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1155,1,['variab'],['variables']
Modifiability,"In high-depth calling (eg @meganshand's work with mitochondria) it is necessary to tweak the `min-pruning` argument. If it is too low, base errors render the assembly graph nearly dense, causing a loss of sensitivity when the assembly engine essentially chooses random haplotypes. If it is too high, we also lose sensitivity because true variants are pruned. Setting the command line argument differently for each sample is not only cumbersome. It also doesn't solve the problem because depth varies within the same bam. Thus, pruning must adapt to each assembly region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4867:540,adapt,adapt,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867,1,['adapt'],['adapt']
Modifiability,"In my case, as an API user, my main usage of GATK is for traverse `GATKRead` and `VariantContext`, so I would like to have in `GATKTool` a simpler way of access to the `FeatureInput<VariantContext>` instead of getting them from `FeatureManager features`. It will be useful in the `VariantWalker` as a step to issue #692, to get all the variants provided by the user in the same walker. My idea is modify the `GATKTool` to include:; - A `public abstract boolean requiresVariant()`, which will be used to determine if we should detach or not all the variants inputs from the `FeatureManager features`.; - A `private void initializeVariants()`, which will implement a way to extract the `FeatureInput<VariantContext>` from `features` and initialize a `FeatureManager variants` or a extended class which includes only `VariantContext` inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1710:779,extend,extended,779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1710,1,['extend'],['extended']
Modifiability,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:235,variab,variable,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,6,"['config', 'variab']","['config', 'variable']"
Modifiability,"In particular add output GATKTool.getDefaultToolVCFHeaderLines to the VCF header, and rewrite the integration test for GenerateVCFFromPosteriors so that it validates the equivalence of variant context records, instead of file equivalency",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4267:86,rewrite,rewrite,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4267,1,['rewrite'],['rewrite']
Modifiability,"In terms of the two tools, I don't think it's necessary at this point to make an inheritance structure. `CallVariantsFromAlignedContigsSAMSpark` is more of a one-off for dealing with de novo assembly files and I'm not sure if it will be supported long term. However, I did extract a `callVariantsFromAlignmentRegions` method in `CallVariantsFromAlignedContigsSpark` that `CallVariantsFromAlignedContigsSAMSpark` can use, which reduces code duplication a lot. There's not much left in `CallVariantsFromAlignedContigsSAMSpark` except for the logic to convert GATKReads into AlignmentRegions, which seems appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240514475:81,inherit,inheritance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240514475,1,['inherit'],['inheritance']
Modifiability,"In the `ReadFilter` plugin, the `--disableAllReadFilters` option disable all read filters that are provided by the tool and by the user. From my point of view, it does not make sense to disable all the filters and provide some, but I think that the option may be a shortcut to disable all the provided ones and allow the user to set their own filters (or none, if they prefer it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2361:20,plugin,plugin,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2361,1,['plugin'],['plugin']
Modifiability,"In the latest master, running for example `java -jar build/libs/gatk.jar FixVcfHead` returns:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:366,adapt,adapters,366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['adapt'],['adapters']
Modifiability,"In the news file of a structural variant software I use, I read. >Added FIX_SA and FIX_MISSING_HARD_CLIP; >FIX_SA: rewrites split read SA tags; >corrects GATK indel realignment SA tag data inconsistency; >FIX_MISSING_HARD_CLIP: infers missing hard clipping if split read records have different lengths; >corrects for GATK indel realignment stripping hard clipping when realigning. Could such issues perhaps be resolved in an update to GATK?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6459:115,rewrite,rewrites,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6459,1,['rewrite'],['rewrites']
Modifiability,"In the process of unifying CalculateTargetCoverage / SparkGenomeReadCounts for the rewrite of the CNV pipeline, we decided to experiment with switching over to fragment-based counts due to a request from CGA. For each fragment, CollectFragmentCounts adds a count to *the bin that overlaps with the fragment center*. We filter to properly-paired, first-of-pair reads in order to have well formed fragments and avoid double counting. We also filter out duplicates. In contrast, CalculateTargetCoverage added a count to *all bins that overlapped with a read* and SparkGenomeReadCounts added a count to *the bin that contained the read start*. These tools kept duplicates. However, none of these collection strategies have been rigorously evaluated. Using a small set of WGS SV tandem-duplication calls from @mwalker174 as a truth set, I did some experimenting with changing the count-collection strategy. (We initially thought we were missing some of these simply due to over-denoising/filtering by the PoN, but as we'll see below, the count-collection strategy plays a non-trivial role.). Subsetting to chr3, I built a small PoN of 12 normals (including the case normal) at 100bp and denoised using bin medians only (i.e., `--number-of-eigensamples 0`) to avoid denoising away common events. In chr3, the case sample had three events:. ````; chr3	8559423		8560126; chr3	64547471	64549936; chr3	90414457	90415989; ````. I tried the following, running `ModelSegments` using fairly sensitive parameters (`--number-of-changepoints-penalty-factor 0.1 --maximum-number-of-segments-per-chromosome 10000 --window-size 16 --window-size 32 --maximum-number-of-smoothing-iterations 0` in copy-ratio-only mode:. 1) CollectFragmentCounts. This only recovered event 2.; 2) CollectReadCounts - same as CollectFragmentCounts, but removing the properly-paired and first-of-pair filters and adding a count for each read to the bin containing its start. This recovered all 3 events.; 3) CollectFragmentOverlaps - same filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:83,rewrite,rewrite,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['rewrite'],['rewrite']
Modifiability,"In this case, the FDR threshold is not honored. The explanation of this is complex, but essentially has to do with the Benjamini-Hochberg procedure not playing well with suppression factor when extended to more than one artifact mode. The definition of ``FilterByOrientationBias`` will have to be changes from ""guaranteeing less than 1% FDR over all mutations"" to ""guaranteeing less than 1% FDR in each specified artifact mode"". This could make the filter more aggressive, so we may have to adjust the FDR threshold. - [x] code fix; - [x] doc updates",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3344:194,extend,extended,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3344,1,['extend'],['extended']
Modifiability,"In working on #8296 we have discovered that in the `MafOutputRendererConstants.java` there are myriad constants that hard code aliases with the pattern ""Gencode_34_hugoSymbol"". This can lead to bad behavior if a non-bundled Gencode version is used in Funcotator, specifically it can cause the MAF file to be missing most of its hard-coded fields as they will be mis-identified by the output-renderer resulting in mostly empty outputs. We should rewrite the logic in the MAF code to be completely agnostic to Gencode versions used to generate the Funcotations to drop this hard-coding all-together.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8482:445,rewrite,rewrite,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8482,1,['rewrite'],['rewrite']
Modifiability,"Includes latest Gencode and an implicit fix for #6564. Had to make some code changes for latest liftover Gencode data(v34 -> hg19). . The associated DS test release correctly annotates data on hg19 and hg38. Left to do:. - [x] Update data sources downloader.; - [x] Update data source version validation code. Code updates:; - Now both hg19 and hg38 have the contig names translated to `chr__`; - Added 'lncRNA' to GeneTranscriptType.; - Added ""TAGENE"" gene tag.; - Added the MANE_SELECT tag to FeatureTag.; - Added the STOP_CODON_READTHROUGH tag to FeatureTag.; - Updated the GTF versions that are parseable.; - Fixed a parsing error with new versions of gencode and the remap; positions (for liftover files).; - Added test for indexing new lifted over gencode GTF.; - Added Gencode_34 entries to MAF output map.; - Minor changes to FuncotatorIntegrationTest.java for code syntax.; - Pointed data source downloader at new data sources URL.; - Minor updates to workflows to point at new data sources. Script updates:; - Updated retrieval scripts for dbSNP and Gencode.; - Added required field to gencode config file generation.; - Now gencode retrieval script enforces double hash comments at; top of gencode GTF files. Bug Fixes:; Removing erroneous trailing tab in MAF file output. - Fixes #6693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660:1104,config,config,1104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660,1,['config'],['config']
Modifiability,"Includes:. * Configuration for packages to search read filters; * Configuration for packages to search annotations. In addition, it changes the behavior of `VariantAnnotatorEngine` to use the annotation packages from the configuration, and mimic what the plugin is doing. This closes https://github.com/broadinstitute/gatk/issues/2155",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4611:13,Config,Configuration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4611,4,"['Config', 'config', 'plugin']","['Configuration', 'configuration', 'plugin']"
Modifiability,Initial support for spanning deletions and refactoring for testability,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857:43,refactor,refactoring,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857,1,['refactor'],['refactoring']
Modifiability,"Initial version should consist of:; - A superinterface called `NativeLibrary` that has `getLibraryPath()` and `isSupported()` methods.; - A `PairHmmBinding` interface (name open to negotiation!) that extends `NativeLibrary` and has signatures for `jniComputeLikelihoods()` and other PairHmm JNI methods. Once created, we need to publish a jar on maven for this repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1801:200,extend,extends,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1801,1,['extend'],['extends']
Modifiability,"Instead of calling setHeader() to temporarily give headerless reads; a header and then calling into htsjdk's SAMRecordCoordinateComparator,; adapt the htsjdk code directly to work with headerless reads. This should; be safer (especially in a multithreaded context), as mutating the objects; being compared within a comparator is a violation of contract.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1276:141,adapt,adapt,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1276,1,['adapt'],['adapt']
Modifiability,Instrument FuncotationEngine to provide SEG file configs as a parameter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5962:49,config,configs,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962,1,['config'],['configs']
Modifiability,"Interesting! Thanks for generating these. I am already convinced by #4519 we should at least switch over to a â€˜CollectReadCountsâ€™ strategy for initial evaluations. A few comments:. -Iâ€™m guessing that the equal insert size and uniform sampling is enhancing many of these artifacts to a level that we probably donâ€™t see in the real world. Can we take a look at some real-world examples?. -Same goes for the fact that homs will be unlikely. -Not sure about the dropouts. Might be worth running without SNPs as a confounding factor. -How flexible is SVGen? Might be worth putting together a more realistic simulated data set. Any chance @MartonKN might be able to use it to cook up some realistic tumor data?. -I donâ€™t recall having a `CollectBaseCallCoverage` type tool in betaâ€”which tool are you thinking of? On a related note, it seems there is some demand to port `DepthOfCoverage` from GATK3. However, Iâ€™d prefer that we roll a CNV-specific version of the tool even if it does get ported. In any case, I think along with findings from the other issue, we should issue a quick PR for `CollectReadCounts` and go ahead to change the `CollectCounts` WDL task to call itâ€”itâ€™s for this very reason that the task is named generically! @sooheelee note that we may have to update the tutorials, etc. at some point, but perhaps the right time will be until all evaluations are more complete. Speaking of which, this PR should not delay getting the first round of automated evaluations up and running. Again, the whole point of those is to have a reproducible baseline metric against which we can easily experiment with and adopt these sorts of changes. Although these sorts of theoretical/simulated/thought experiments are clearly useful to us, unfortunately, they may not be as compelling to some of our users as demonstrable improvement seems on real data!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976:534,flexible,flexible,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375122976,2,['flexible'],['flexible']
Modifiability,"Interesting, that's somewhat disturbing news, I wonder if we're paying for ssd's without actually being able to use them... It's also possible there's a different setting that's configuring the ssd's to be used for shuffle output. . We should investigate this further and 1) see if setting spark.local.dir makes a performance difference 2) ask the dataproc team about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564:178,config,configuring,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564,1,['config'],['configuring']
Modifiability,"Interesting, this is the first time that I have seen a CentOS-7 install without zlib and uuid - even the minimal installations include it. Your options are:; - Install zlib and uuid (yum -y install zlib libuuid); - Ask your admins whether these packages are installed in some other location. For example, if the zlib library is at /opt/my_install/lib64/libz.so, then you can set your environment variable LD_LIBRARY_PATH; ```; export LD_LIBRARY_PATH=/opt/my_install/lib64:/opt/my_install/lib:$LD_LIBRARY_PATH; ```; - Wait for the next GenomicsDB binary jar to show up",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357067214:396,variab,variable,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357067214,1,['variab'],['variable']
Modifiability,"Intermittent failure at https://travis-ci.com/github/broadinstitute/gatk/jobs/297047618. ```; [TileDB::FileSystem] Error: hdfs: Cannot list contents of dir gs://hellbender-test-logs/staging/703469fc-52fe-441d-b6e0-8092a114fe2c//chr20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:448,config,configuration,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,4,"['Config', 'config']","['ConfigurationUtil', 'configuration', 'configure']"
Modifiability,"IntervalWalker, VariantWalker enhancements, and GenomeLoc -> SimpleInterval migration in the engine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/297:30,enhance,enhancements,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/297,1,['enhance'],['enhancements']
Modifiability,Is it guaranteed that one of the configurations won't include any of the MQ0 regions? Why is that?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-364663550:33,config,configurations,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-364663550,1,['config'],['configurations']
Modifiability,Is there a way to have java load a config file as system properties on startup?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998:35,config,config,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998,1,['config'],['config']
Modifiability,"Is this the only `CommandLineException` which should be an `UserException`? If not, how is going to work the development of new exceptions in Barclay. For instance, in https://github.com/broadinstitute/barclay/pull/11 there is a new exception that I made for values out of range, which extends `BadArgumentValue`. I agree that this errors should be decoupled from the ones that are not, but in this case I think that this is already implemented:. * `UserException` are handled in the `mainEntry()`, distinguishing errors that comes from the user's side regarding some specifications in the tools/framework.; * `CommandLineException` are handled in `parseArgs()`, distinguishing errors that comes from the command line from the user side while parsing. I expect that any command line error that is not `CommandLineParserInternalException ` or `ShouldNeverReachHereException` comes from the user's side. The contract in Barclay says that are `CommandLineException` are _""Exceptions thrown by CommandLineParser implementations.""_, and I think that if other parts of the code (outside arg parsing) is throwing this exception is a bug that does not come from the user. I guess that this is the problematic part.; * Any other `Exception` is thrown in `Main.handleNonUserException()`, which may be caused by non-user exceptions. I propose that `CommandLineException` is handled as currently to separate ""errors that are the user's fault regarding input and/or assumptions"" (`UserException`), ""errors that are the user's fault while providing parameters to the command line"" (`CommandLineException`) and ""errors that are not the user's fault"" (other `Exception`s). Actually, this is reasonable because the exit status is different for any kind of errors in the current `Main`. The only problem that I see with this approach is the silently failing of a ""bug"" in tools/engine code, which can be rethrow easily in `CommandLineProgram.instanceMain`` as following:. ```java; public Object instanceMain(final Strin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161:286,extend,extends,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2324#issuecomment-268773161,1,['extend'],['extends']
Modifiability,Issue 2968 collect allelic counts extend lw,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3203:34,extend,extend,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203,1,['extend'],['extend']
Modifiability,Issue spotted in GATK3 and fixed by pull-request https://github.com/broadinstitute/gsa-unstable/pull/1377. Original issue: https://github.com/broadinstitute/gsa-unstable/issues/1340. Needs to be ported to GATK4 as part of a larger fix involving the refactoring of AFCalculators (Issue TBA).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858:249,refactor,refactoring,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858,1,['refactor'],['refactoring']
Modifiability,"Issue: Integer overflow error caused Mutect2 v4.1.4.0 to generate a stats file with a negative number. Solution is to change the int data type to long. User report:. Hello, I've just adapted my pipeline to the new filtering strategies, while looking at the files I noticed that for a WGS run I obtained a stats file with a negative number:; [egrassi@occam biodiversa]>cat mutect/CRC1307LMO.vcf.gz.stats; statistic value; callable -1.538687311E9. Looking around about the meaning of the number I found https://gatkforums.broadinstitute.org/gatk/discussion/24496/regenerating-mutect2-stats-file, so I'm wondering if I should be worried by having a negative number of callable sites :/; What's more puzzling is that FilterMutectCalls after ran without any error. Before running mutect I used the usual best practices pipeline, then:; ; gatk Mutect2 -tumor CRC1307LMO -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa -I align/realigned_CRC1307LMO.bam -O mutect/CRC1307LMO.vcf.gz --germline-resource /archive/home/egrassi/bit/task/annotations/dataset/gnomad/af-only-gnomad.hg38.vcf.gz --f1r2-tar-gz mutect/CRC1307LMO_f1r2.tar.gz --independent-mates 2> mutect/CRC1307LMO.vcf.gz.log; ; gatk CalculateContamination -I mutect/CRC1307LMO.pileup.table -O mutect/CRC1307LMO.contamination.table --tumor-segmentation mutect/CRC1307LMO.tum.seg 2> mutect/CRC1307LMO.contamination.table.log; ; gatk LearnReadOrientationModel -I mutect/CRC1307LMO_f1r2.tar.gz -O mutect/CRC1307LMO_read-orientation-model.tar.gz 2> mutect/CRC1307LMO_read-orientation-model.tar.gz.log; ; gatk FilterMutectCalls -V mutect/CRC1307LMO.vcf.gz -O mutect/CRC1307LMO.filtered.vcf.gz -R /archive/home/egrassi/bit/task/annotations/dataset/gnomad/GRCh38.d1.vd1.fa --stats mutect/CRC1307LMO.vcf.gz.stats --contamination-table mutect/CRC1307LMO.contamination.table --tumor-segmentation=mutect/CRC1307LMO.tum.seg --filtering-stats mutect/CRC1307LMO_filtering_stats.tsv --ob-priors mutect/CRC1307LMO_read-orientation-model.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6302:183,adapt,adapted,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6302,1,['adapt'],['adapted']
Modifiability,"It *looks like* it doesn't. I ran a job and looked at the ""environment"" tab in the Spark page for the job and didn't see ""spark.local.dir"" mentioned in the list of properties or the command line. Based on [the documentation](http://spark.apache.org/docs/latest/configuration.html), the setting must thus still be at its default value of ""/tmp"". . /tmp is on the HDD, the SSD one would have to be on /mnt/1/.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283210934:261,config,configuration,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283210934,1,['config'],['configuration']
Modifiability,"It can easely be extended to multiple sequences... it just happened that I didn't need it personally. . I took a quick pick to your branch... do you really care about the contig descriptions?.... I would say that this ""aligner"" class should not be responsible of compose the multi sequence fasta file but rather accept one as a constructor argument and the construction of the fasta is delegated back to the invoker code; in this new more general aligner the current single contig could be implemented as a public static method call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587:17,extend,extended,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587,1,['extend'],['extended']
Modifiability,"It could just be natural variability in the user's runtime environment, but it's worth doing some longer-running tests to be sure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-324453449:25,variab,variability,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-324453449,1,['variab'],['variability']
Modifiability,"It depends why it fails the filter. One reason is consecutive indel elements, but consider for example:. ref: ACGTTTA; read: AC TTTTA; cigar: 2M1D1I4M. Especially in long-read technologies with a lot of indel errors it seems draconian to throw out reads where this happens once. And okay, I understand that an aligner could represent this as a G->T subsitution, but what about the same thing but with 2D followed by 2I? That strikes me as a much better cigar than calling it a DNP. In general, a bad cigar should mean either that the aligner is bad (in which case why are we filtering isolated reads and not just rejecting the entire BAM?) or the read is malformed. But consecutive indels in a technology with many indel errors is neither of these!. Anyway, I don't think there's a problem with allowing these reads in the GATK, and if there is the refactoring in this #6403 should let us fix any problem easily enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079:849,refactor,refactoring,849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6433#issuecomment-583415079,1,['refactor'],['refactoring']
Modifiability,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into â€˜/home/travis/site-libraryâ€™; (as â€˜libâ€™ is unspecified); Error: (converted from warning) dependencies â€˜rlangâ€™, â€˜vctrsâ€™ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6072:182,config,config,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072,7,['config'],['config']
Modifiability,"It looks like all the changes in my original commit with the raw GATK3 code (except for one file) got squashed out somehow, so I can't see just the changes from GATK3 anymore. I'll probably have to go back and re-commit those when you're ready to make this tractable to review. I'll wait to comment on #1 until that happens. As for a default plugin descriptor, I'd prefer not to take one unless its fully implemented, with tests. Plus, although we could develop it here, it should really live in the Barclay repo if its truly generic. More importantly, I'm not sure the plugins in this PR should be plugins at all. Historically, plugins have required a lot of test development and iteration because they have command line arguments (the plugin system is for extending the command line parser with discoverable, re-useable components that are shared across multiple tools, and need shared command line arguments). I haven't looked at the new ones closely, but I'm not sure they're a good fit. As for the files, it look like about 400MB (?) Thats pretty big - you should try to squeeze them down or target some existing files if you can.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008:342,plugin,plugin,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431839008,6,"['extend', 'plugin']","['extending', 'plugin', 'plugins']"
Modifiability,It looks like there's some minor refactoring in your new graph handler. I'm not a real stickler about sneaking that in but just want to check it was intentional.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518:33,refactor,refactoring,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518,1,['refactor'],['refactoring']
Modifiability,"It seems plausible to me, though, that the Google auth library may have been patched to perform checks that it wasn't performing previously. Maybe our project permissions have always been mis-configured :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762:192,config,configured,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762,1,['config'],['configured']
Modifiability,"It seems that there are a lot of soft clips that aren't bacterial reads.; What's your mean insert size? I've seen lots of aberrant soft clips when; the insert size is small and Picard doesn't catch adapter sequences with; multiple mismatches. Does the Picard percent adapter in alignment summary; metrics seem high? I've also seen lots of soft clips when the chimera rate; is high, sometimes because of bad sample extraction. What's the percent; chimeras in your alignment summary metrics? 5% is bad and I've seen up to; 15%, but that was an FFPE tumor sample. On Mon, Mar 25, 2019 at 8:48 PM jjfarrell <notifications@github.com> wrote:. > When the --dontUseSoftCliiped flag is used, the GQ=0 is much lower- N=1355; > for '0/0' calls.; >; > zcat; > A-ADC-AD004288-BL-NCR-15AD82285.hg38.realign.bqsr.dontUseSoftclipped.g.vcf.gz; > |tr '\t' '\n'|grep '0/0'|tr ':' '\t'|cut -f2,3|awk '$2 == ""0"" {print; > $0}'|cut -f1|sort -n|uniq -c; > 1355 0; > 6 0,0,0; > 7 0,0,0,0; > 602 1; > 537 2; > 520 3; > 595 4; > 441 5; > 511 6; > 583 7; > 701 8; > 403 9; > 468 10; >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476431178>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdA_gZKYn3vuqNDvvDadvM9tgzQqGks5vaW5IgaJpZM4YxgEF>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476654990:198,adapt,adapter,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-476654990,2,['adapt'],['adapter']
Modifiability,"It should be fairly easy to create a read transformer plugin; pretty much follow the pattern of read filter plugins: clone and modify GATKReadFilterPluginDescriptor, and add an instance of the new descriptor to the list of plugins passed in to the command line parser in (appropriate) tool base classes. And of course tests!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245968764:54,plugin,plugin,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245968764,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"It turns out I was mistaken that setting the environment variables fixes the problem (stupid error on my part). It's possible the BaseTest message is unrelated. I haven't tested this branch out yet, but building from the commit immediately before the update works. I am going to try the next version to see if it helps. Edit: #3594 does not fix the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419:57,variab,variables,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3592#issuecomment-330896419,1,['variab'],['variables']
Modifiability,It will be useful if this is added to the configuration system (#2368 and #3081).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2155#issuecomment-316078724:42,config,configuration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2155#issuecomment-316078724,1,['config'],['configuration']
Modifiability,It will be very useful to have an abstract class for the plugin arguments (as I did for the read filters plugin in #2355) to be able for downstream projects to change default values or hide arguments to the final user.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921:57,plugin,plugin,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3292#issuecomment-316079921,2,['plugin'],['plugin']
Modifiability,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:313,config,configuration,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,5,['config'],"['configuration', 'configure', 'configured']"
Modifiability,It would be good for progress meter to be more flexible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-575773895:47,flexible,flexible,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-575773895,1,['flexible'],['flexible']
Modifiability,"It's looking like we might have to fix the issues with NIO here after all @tomwhite @jean-philippe-martin, as @lbergelson has been unable to get this working reasonably with the GCS adapter (it runs, but veeeerrryyy slowly).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271691417:182,adapt,adapter,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271691417,1,['adapt'],['adapter']
Modifiability,"It's not clear to me that we want these tools in Gatk4. We deliberately didn't port them because we felt they were unnecessary going forward. . I understand that there are some legitimate use cases that require them: ex low coverage naive variant calling from high ploidy pools which haplotype caller would do poorly on. (Also, do we know that haplotype caller doesn't do well on those sorts of things? Maybe we should consider modifications there if it doesn't?) I'm not sure that supporting that use case is worth the added complexity of maintaining and supporting these tools. Especially since we don't provide a pileup based variant caller as part of gatk4... . @vdauwera Can you comment? . @sooheelee I'm not sure I agree with you that supporting this for mutect 1 is useful. ; A) We don't want to support the use of mutect 1 anymore and would like to encourage people to switch to mutect 2 which I think we now believe is a better variant caller for both snps and indels. ; B) Mutect 1 users are already using gatk3, so they have access to these tools already. Mutect 1 also requires co-cleaning which I believe is a different but related tool to indel realignment. . For the variant review issue, we have thoughts on implementing a much better solution for variant review by creating an assembly plugin for igv.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988:1303,plugin,plugin,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-308451988,2,['plugin'],['plugin']
Modifiability,"It's weird, usually java should output an error message if it runs out of memory. The exception would be when java is assigned so much memory that the SYSTEM kills it instead of java killing itself. ; You could try adding `dmesg | tail -100` to your wdl after m2 runs to see if there are any messages from the OOMkiller. . What's your total available memory on the machine and your -Xmx setting? You typically need to leave some memory room for the OS and other native sofware. (although by default our pipelines SHOULD have that configured correctly.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414:530,config,configured,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-939070414,1,['config'],['configured']
Modifiability,Its use in `ValidateBasicSomaticShortMutations` seems limited to the integration test. Can I rewrite the test to do without `AnnotatedInterval` and call it a day?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913:93,rewrite,rewrite,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526876913,1,['rewrite'],['rewrite']
Modifiability,"Iâ€™ve also revisited this work for MalariaGEN, additionally including further cleanup of the canonical part of the WDLs (mostly low hanging fruit like adding structs, which help a lot for cutting down parameter cruft on Terra). For ease of iteration, this work broke things up into 3 pushes of a button: 1) data collection, 2) preclustering (done in a relatively modular way, so you can swap in whatever clustering script you like, as long as it outputs hard/soft responsibilities) +random selection of training cohorts, and 3) cohort mode + scattered case mode on all clusters. But no reason we couldnâ€™t link some of those up. No problem running 16k samples, with 6 clusters and 300 training samples per, but also note I was only running a single genomic shard containing CNVs of interest for this use case. (I did manage to break Terra for a few days when I tried to attach collected counts to the data model in what I wouldâ€™ve thought would be a relatively trivial way, but thatâ€™s another matter.). Iâ€™ve shared some version of these WDLs over Slack previously, but happy to also open up a branch here. I think some of this work may be replicated in GATK-SV and Iâ€™m also not sure what we want to make canonical. Surely most users will run only a single cluster. But from the perspective of our MalariaGEN collaborators, the more of what I put together for them being made canonical, the better, as this will ease future maintainability. But will leave it up to other current stakeholders.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360:1421,maintainab,maintainability,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5633#issuecomment-894527360,1,['maintainab'],['maintainability']
Modifiability,"JDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4310,Config,ConfigFactory,4310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,Jexl Support for Extended Attributes Doesn't Work with Lists,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4631:17,Extend,Extended,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4631,1,['Extend'],['Extended']
Modifiability,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:65,refactor,refactored,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['refactor'],['refactored']
Modifiability,"Just an idea: it will be nice to propagate the configuration from `Main` to the tools, and obtain from it the packages/classes to include in the command line tools (this is one of the things that I implemented in #2322).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309680944:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309680944,1,['config'],['configuration']
Modifiability,"Just checked and this is not new behavior. I was afraid this would be an unintended consequence of the DRAGEN branch but that doesn't seem to be the case. Looking a little closer into the code I actually think the only difference this makes is explicitly for the contamination and nothing else. There are a few layers where we filter reads before the annotations are called (filtered by QC before active region determination, MQ/etc-filtered/un-softclipped/overlap-score-adjusted before assembly, filtered based on poor concordance with existing haplotypes, reads are realigned and re-filtered by position, and again filtered due to contamination downsampling). It seems to me that the two likelihoods arrays fed to the `prepareReadAlleleLikelihoodsForAnnotation()` have had all of the above steps applied to them EXCEPT for the contamination downsampling step applied to them looking thorough the code in the HaplotypeCaller. I guess the question is whether there is a strong reason to make the annotations with/without the contamination adjustment... I think the argument `--use-filtered-reads-for-annotations` is misleadingly labeled though the description looks correct to me since it really does only seem to make a difference for the contamination step. . There is another wrinkle to all of this. For DRAGEN-GATK we evaluated calculating the overlaps/annotations exactly how they do it in DRAGEN and decided against it. In DRAGEN they retain the original reads from the bam (i.e. no realignment/no score adjustments/etc...) and use THOSE for annotation and for genotyping. I would have to pick through their debug output to tell just which subsets get used for genotyping (for example they still use non-haplotype-matching reads for FRD and BQD but I don't remember if those are also used for calculating annotations).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324:311,layers,layers,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7144#issuecomment-800380324,1,['layers'],['layers']
Modifiability,"Just for posterity:. jhess 1:55 PM; just to clarify: what is the logic behind approximating Ïƒ_(Ï„/min/maj) â‰ˆ (post90 - post10)/2? (edited) ; 1:55; what is the scale factor of 1/2 for?; 1:58; one other thing â€” how come Ïƒ_(min/maj) is the sum of the total CR segmentâ€™s variance (i.e. Ïƒ_Ï„) and the allelic segmentâ€™s variance?; 2:00; this would imply that the allelic segments are actually a sum of the random variables corresponding to the allelic and total segmentation, which Iâ€™m not sure is the case?. slee 5:32 PM; Sorry, just now seeing your questions!; 5:33; The scale factor of 1/2 is pretty arbitrary. Just trying to give an estimate of posterior width when the credible interval might be skewed. A better approach would be to refit posteriors with Gaussians/Betas as mentioned previously.; 5:35; However, I'm not actually convinced that these credible intervals are what we want to pass to the sigmas. As I also mentioned above, if sigma.tau is supposed to be a global quantity, probably the posterior median of the parameter that controls the global variance (given in the .cr.params file) might be a better thing to use. However, I never got a straight answer from anybody about whether this was a segment-level or global quantity---any idea?. slee 5:41 PM; As for using the sum of the CR variance and the MAF variance, you're right---we should be propagating error for the product of the two random variables. Not sure what I was thinking...probably just a brain fart. Not sure if it will make a difference for ABSOLUTE, but thanks for catching that!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494:405,variab,variables,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5804#issuecomment-652411494,2,['variab'],['variables']
Modifiability,Just noting for posterity that removing build strings in the conda YML seems to have improved portability to different OS environments: https://gatk.broadinstitute.org/hc/en-us/community/posts/360061666671-Broken-conda-env-create-n-gatk-f-gatkcondaenv-yml,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774:94,portab,portability,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-628860774,1,['portab'],['portability']
Modifiability,"Just occured to me--Are we saying the application of a _germline workflow_ extends to the creation of a PoN consisting of germline normals for the _somatic workflow_?. If so, we should reorganize the directory structure of the CNV scripts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3163#issuecomment-310873860:75,extend,extends,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163#issuecomment-310873860,1,['extend'],['extends']
Modifiability,Just some minor instances that slipped through during the rewrite.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4119:58,rewrite,rewrite,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4119,1,['rewrite'],['rewrite']
Modifiability,"K Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/goo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3990,Config,ConfigFactory,3990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,Karthik had suggested setting the environment variable TILEDB_DISABLE_FILE_LOCKING=1 for NFS (see https://github.com/broadinstitute/gatk/issues/4753) -- maybe give that a shot?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453600476:46,variab,variable,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453600476,1,['variab'],['variable']
Modifiability,Keep variable names/strings consistent.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/989:5,variab,variable,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/989,1,['variab'],['variable']
Modifiability,"L; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3931,Config,ConfigFactory,3931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"LAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3921,Config,ConfigFactory,3921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,LIBS/LocusWalker refactoring and overlapping read-pairs handling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041:17,refactor,refactoring,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041,1,['refactor'],['refactoring']
Modifiability,"Largely taken from Lee's sample code, see JIRA ticket for details. Spins up a Hail cluster and runs a script to extract from a VDS to VCF files on a per-chromosome basis. Includes some refactoring to move some of the workspace-sniffing that was part of bulk ingest into more generic utility code. In terms of cluster tracking:. - Cluster name is calculated in shell script and visible in the logs; - Cluster name is written to a file which is delocalized even if the workload script fails. . Unintended but useful example [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/a96667a7-e08c-43f4-abad-b55fbe7f0c06) where not only is the cluster name logged and written to an output file which is delocalized, but the cluster gets shut down anyway by cleanup code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8525:185,refactor,refactoring,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8525,1,['refactor'],['refactoring']
Modifiability,"Lee, just letting you know I've tagged you in a forum question. ---; The oncotated maf output includes many rejected mutations (using the configuration from the public spaces). This is bad practice. The unfiltered VCF (or preferably a tsv) should have these sites but we should not be annotating them or putting them in final outputs. . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11440/m2-gatk4-oncotated-maf-output-includes-rejected-mutations/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4421:138,config,configuration,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4421,1,['config'],['configuration']
Modifiability,"Let us take an example. Suppose, we configure GenomicsDB with 3 column partitions - 0-10, 10-100, 100-300 and want to run GenomicsDBImport tool with an interval [0,100]. In this case, the import tool will only write contigs between 0 and 100 into first two partitions (according to the loader JSON file). Is this what you had in mind? The command line will look like:; $ gatk-launch GenomicsDBImport -L 0-100 --loaderJSONfile loader.json --streamIdJSONFile stream.json. This can definitely be done. However, the client still needs to know the column partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931:36,config,configure,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931,1,['config'],['configure']
Modifiability,"Let's create a mock up of a possible future configuration setup using the Owner library (https://github.com/lviggiano/owner). For the mock up, I recommend we have two configuration files, one containing system properties and the other containing a few general engine settings. . We can select a few system properties from `gatk-launch` for inclusion in the system properties config file (eg., `samjdk.compression_level`, `samjdk.use_async_io_read_samtools`, etc.). . For the engine settings file, I recommend including `codecPackages` (a `List<String>` currently hardcoded in `FeatureManager.CODEC_PACKAGES`), `cloudPrefetchBuffer`/`cloudIndexPrefetchBuffer` (int values) from `GATKTool`, and `createOutputBamIndex` (boolean), also from `GATKTool`. As part of this, we'll have to prove that we can inject the system properties sufficiently early on that libraries such as htsjdk will pick them up.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:44,config,configuration,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,3,['config'],"['config', 'configuration']"
Modifiability,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:195,flexible,flexible,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['flexible'],['flexible']
Modifiability,"Let's hear what others say, but I think I would strongly prefer to simply take over VariantEval in another repo if this was something you'd consider. I'd likely do much of what you propose anyway (certainly WRT testing); however, perhaps not the microscope we went through with the core GATK changes earlier. On plugins: I like what seems to be shaping up w/ Barclay. I carried over the Stratifier and Evaluator as plugins because it seems like it would make sense to allow tools to provide extensions (VariantEval, our tool, does). If I took this PR a step further, I would have migrated many arguments currently top-level on VariantEval into the plugins themselves (a good feature in Barclay). As an aside: I dont think VariantAnnotator is migrated yet, but we have many GATK3 plugins related to annotation, and hope that tool retains Annotator plugins when it get migrated. My impressions of barclay are probably a little out of date. I agree the main argument parsing framework is pretty robust. Specifically on plugins, it seems a little less so, or at least there are not many tools I visibly see exercising that part of the code. For example, there really should be a default implmentation or base class between Barclay's plugins and ReadFilter plugins. I'm guessing if more tools in GATK4 were using plugins this would have happened. I created something like this for VariantEval, and without a ton of work that could probably get generalized; however, doing so would throw a lot higher bar on me and as noted above I'm trying to take on less, not more at the moment. If we do take over VariantEval, I'm certainly happy to try to contribute code and experiences to improve the core, through more targeted PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501:312,plugin,plugins,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407202501,18,['plugin'],['plugins']
Modifiability,"Let's see if we can get Azure Blob support working in GATK using the existing azure-storage-blob-nio NIO plugin:. https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/storage/azure-storage-blob-nio. https://search.maven.org/artifact/com.azure/azure-storage-blob-nio/12.0.0-beta.8/jar. Currently auth info needs to be manually passed in, unlike with the Google NIO plugin, but I've opened a feature request to get the auth info automatically from the environment:. https://github.com/Azure/azure-sdk-for-java/issues/23653",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7425:105,plugin,plugin,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7425,2,['plugin'],['plugin']
Modifiability,Limiting scattering size in ingest to keep beta customers under quota. Successful run on NHGRI AnVIL dataset: https://app.terra.bio/#workspaces/gvs-dev/NHGRI_AnVIL_3K%20hatcher/job_history/a9c2a81b-d81c-4f7a-a433-4096ccc7b579. Quota behavior during successful run:; ![Screenshot 2023-02-08 at 3 37 36 PM](https://user-images.githubusercontent.com/110987709/217656881-793e8a87-3e8c-40fc-90a6-6f640ff1c976.png). Next successful run after minor refactoring: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/8a34477f-af3b-4e19-8184-862ed1c2cba3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8193:442,refactor,refactoring,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8193,1,['refactor'],['refactoring']
Modifiability,"LoadData `maxRetries` parameterized, default increased [VS-383]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7791:22,parameteriz,parameterized,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7791,1,['parameteriz'],['parameterized']
Modifiability,Look into adaptive pruning in GATK 4.2.0.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:10,adapt,adaptive,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['adapt'],['adaptive']
Modifiability,"Looking at MetricFile and with it heavy use of Reflexion looks a bit nasty, if there is a better alternative the better. I guess a refactoring of MetricFile would use annotations to allow one to customize output variable name... force one to have those not-so-good looking CAPITAL_FIELD_NAMES for the sake of it is harsh. Don't understand why One has to commit to ; particular type for all histograms either. . Anyway, only if the use of MetricFile is an overkill I would ask you to do your custom one (i.e if it can be done in a few lines of code).... probably not the case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-336521031:131,refactor,refactoring,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-336521031,2,"['refactor', 'variab']","['refactoring', 'variable']"
Modifiability,"Looking back into this PR... at some point you are using 'N' to pad what seem to be gaps on the read sequence. Although the end result would be the same perhaps is better to be more explicit and just use '-' instead. In that case my suggestion of using `Nucleotide.intersect` wouldn't cover for the '-' character so you need a explicity ""&&"" or ""II"". When you compare the cost of each different alignment the gap-open and gap-ext are ignored (you only look at base call mismatches). I wonder whether it would be more correct to actually take them in consideration... so imagine that there is no gaps in the original alignment what-soever and that adding a 1bp gap decreases the number of mis-matches by just 1 which is typically Q30 increase in the Lk but the gap itself default penalty is Q45 so can one say that that read wouldn't still support the reference over a 1-bp gap alternative? . Example with a 2-bp gap making the trick:; ```; Ref: ....GCATGTGATATATATATATATATATATATACACACAC....; Read: ....GCATGTG--ATATATATATATATATATATAC <end-of-the-read>; ```. That could happen in STRs with impurities... but if the original alignment did not added itself the gap to reduce the number of mismatches is because precisely due to the added cost of the gap-open and necessary extends that we would be ignoring here. This is all hypothetical until some one quantifies how often this might occur ... so I'm happy to keep ignoring the gaps for now until we get a report on a real-live dataset that would benefit of such a change or some enthusiastic dsde-methods member investigates this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5172#issuecomment-420743269:1270,extend,extends,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5172#issuecomment-420743269,1,['extend'],['extends']
Modifiability,"Looks great!. One quick note: I don't get the idea behind `Poisson` -- shouldn't we simply use negative binomials w/ modeled `mu_sj` and `alpha_sj`, evaluated at observed counts (`tt.arange(min_count, max_count + 1)`), and weighted with the number bins for each count (`_hist_sjm`)? i.e. if one observes an empirical distribution `P_obs(x)` rather than `x` draws, then the appropriate max likelihood objective function is `\sum_x P_obs(x) log P_model(x | \theta)`. Perhaps this is exactly what you've done and I don't get it. Another quick note: what I had in mind was _either_ modeling `mu_sj` at quantized ploidy states, _or_ let the ploidy state be unrestricted w/ a penalty via. a Bernoulli process (possibly w/ different per-contig penalties to account for e.g. higher rate of X/Y loss). We have enough samples in the cohort to select the quantized model (and those samples pin down the per-contig biases `b_j`). The samples that do not conform to quantized ploidy states can then choose whatever (variable) ploidy state they wish by paying a (hefty) price. We would also need to mask contigs that have variable ploidy calls from gCNV.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536:1003,variab,variable,1003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376286536,4,['variab'],['variable']
Modifiability,"Lots of refactoring was done for the Segmenter classes in #6499. At least for segmentation, all use cases (CR-only, AF-only, CR+AF, single-sample, multi-sample) now go through `MultisampleMultidimensionalKernelSegmenter`. `AlleleFractionKernelSegmenter` and `CopyRatioKernelSegmenter` classes still exist, but both simply call the `MultisampleMultidimensionalKernelSegmenter` class; this was done so preexisting tests for those two classes could be reused. I'm fine with calling this done. We can always open a new issue in the unlikely event we refactor the modelling code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908:8,refactor,refactoring,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625#issuecomment-900609908,4,['refactor'],"['refactor', 'refactoring']"
Modifiability,Low-hanging gcnvkernel refactoring and code improvement,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4058:23,refactor,refactoring,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4058,1,['refactor'],['refactoring']
Modifiability,"MTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 14:39:24.083 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 14:39:24.083 INFO DetermineGermlineContigPloidy - Initializing engine; 14:39:26.111 INFO DetermineGermlineContigPloidy - Shutting down engine; [May 26, 2019 2:39:26 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1511522304; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python -c import gcnvkernel. Stdout:; Stderr: Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:2849,config,configdefaults,2849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configdefaults']
Modifiability,"M_READER_FACTORY :; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4014,Config,ConfigFactory,4014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,Major changes:. - remove workspace datamodel updating from GvsAssignIds; - refactored GvsImportGenomes to remove all bq load code; - added new load status table append the load status of a sample; - changed SetIsLoaded and CheckForDuplicateData to read from the partitions AND the load status table,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573:75,refactor,refactored,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573,1,['refactor'],['refactored']
Modifiability,Make HaplotypeCallerSpark extend AssemblyRegionWalkerSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5386:26,extend,extend,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5386,1,['extend'],['extend']
Modifiability,Make ReadsSparkSource Sorting configurable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4859:30,config,configurable,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4859,1,['config'],['configurable']
Modifiability,Make RobustBrentSolver more flexible,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2971:28,flexible,flexible,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2971,1,['flexible'],['flexible']
Modifiability,Make adaptive pruner smarter in complex graphs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520:5,adapt,adaptive,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520,1,['adapt'],['adaptive']
Modifiability,Make annotations a barclay plugin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287:27,plugin,plugin,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287,1,['plugin'],['plugin']
Modifiability,Make several Funcotator methods and fields protected so it is easiest to extend the tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8166:73,extend,extend,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8166,1,['extend'],['extend']
Modifiability,Make sure to run the following to be able to push to GAR from your machine:; ```; gcloud auth configure-docker us-central1-docker.pkg.dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8783:94,config,configure-docker,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8783,1,['config'],['configure-docker']
Modifiability,"Mappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may be experimented with, but it seems unlikely in the near future (their own quirkiness, lack of API for JNI, etc); it seems more and more likely to me that eventually it's inevitable to have a custom alignment m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1740,config,configuration,1740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,4,['config'],['configuration']
Modifiability,MarkDuplicates performance optimizations. This includes:; - a small refactor of the original MarkDuplicatesDataflow so that most of the core code can be reused directly in the optimized version; - helper classes to keep the optimized code organized and dataflow-like; - reworked input for performance; - the optimized code spends a lot less time moving data across machines; - performance bugfixes:; - UUID generation; - format conversion,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/863:68,refactor,refactor,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/863,1,['refactor'],['refactor']
Modifiability,"Maybe I misunderstand the underlying model, but if some Pedigree annotations only need to know which samples are founders (ExcessHet ?) , and some need to know the full relationships (PossibleDeNovo), then I'm suggesting we change the class hierarchy to reflect that:. PedigreeAnnotation; |--TrioAnnotation; |----PossibleDeNovo; |--ExcessHet (assuming ExcessHet only needs founders...); ... Then the plugin could deterministically validate whether the user has provided sufficient args for the set of requested annotations; and if so, propagate them accordingly. A TrioAnnotation could only be populated (from the command line at least) from a file, whereas the others could be populated from either a file or just a set of IDs. I think it would simplify the annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550:400,plugin,plugin,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-463372550,2,['plugin'],['plugin']
Modifiability,MendelianViolation class needs to be refactored,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5034:37,refactor,refactored,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5034,1,['refactor'],['refactored']
Modifiability,Migrate FuncotateSegments to use Owner for its configuration files,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5963:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963,1,['config'],['configuration']
Modifiability,Migrate GATK engine to new configuration mechanism,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3081:27,config,configuration,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3081,1,['config'],['configuration']
Modifiability,Minor enhancements to match VariantRecalibrator tweaks,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2780#issuecomment-309635824:6,enhance,enhancements,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2780#issuecomment-309635824,1,['enhance'],['enhancements']
Modifiability,Mock up an example configuration setup using Owner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126:19,config,configuration,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126,1,['config'],['configuration']
Modifiability,Modify the PythonScriptExecutor to allow environment variables.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6256:53,variab,variables,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6256,1,['variab'],['variables']
Modifiability,"Modifying what I wrote earlier, got confused with another issue. I am not familiar with Lustre and Lustre configuration. Did the excessive file locking from Lustre(FUTEX_WAIT_PRIVATE?) go away with `--genomicsdb-shared-posixfs-optimizations`? . Is there anyway to configure Lustre buffer sizes for writing? If not, can you try setting environment variable TILEDB_UPLOAD_BUFFER_SIZE to something like 5242880(5M) and try `GenomicsDBImport`? Does it help with performance? Is the amount of file locking lower than before?. If the performance is still not acceptable... What version of gatk are you using? Can you use the latest gatk and try using the `--bypass-feature-reader` option with `GenomicsDBImport`? Does this help with performance?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554:106,config,configuration,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554,3,"['config', 'variab']","['configuration', 'configure', 'variable']"
Modifiability,"More concrete runtime numbers are forthcoming but the profiler shows the following. Note that these numbers are generated on this branch hanging off of master ca. November and does not include the other optimizations to this part of the code that have been made recently.; Before:; <img width=""1068"" alt=""screen shot 2019-01-25 at 2 59 53 pm"" src=""https://user-images.githubusercontent.com/16102845/51772199-6c541880-20b9-11e9-8823-7249e7f4d874.png"">; ; After: ; <img width=""1092"" alt=""screen shot 2019-01-25 at 3 00 05 pm"" src=""https://user-images.githubusercontent.com/16102845/51772174-57778500-20b9-11e9-9d74-9f93d76358a0.png"">. Beyond the tests that I have written explicitly to illuminate discrepancies, I have run HaplotypeCaller in GVCF mode on the input bams in large and explicitly checked for places where the refactored method mismatches from the previous code and it appears to be matching over a wide range of cases, there probably could be more. . Resolves #5488",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5607:821,refactor,refactored,821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607,1,['refactor'],['refactored']
Modifiability,More flexible integer copy number transition priors in gCNV,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2998:5,flexible,flexible,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2998,1,['flexible'],['flexible']
Modifiability,More flexible matching of dbSNP variants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6626:5,flexible,flexible,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6626,1,['flexible'],['flexible']
Modifiability,More refactoring PDHCE and preparing for joint detection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8467:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8467,2,['refactor'],['refactoring']
Modifiability,More refactoring of Mark duplicates and pipeline hookup,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/770:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/770,1,['refactor'],['refactoring']
Modifiability,"Most ReadWalkers apply the WellformedReadFilter, but their spark equivalents; were not doing so. This creates an inherited method GATKSparkTool.makeReadFilter(); that defaults to the WellformedReadFilter and gets automatically applied to the; RDD of reads returned from GATKSparkTool.getReads(). Only BaseRecalibratorSpark needed to override this method to apply a custom; read filter in order to match the walker filtering settings. Resolves #1158",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159:113,inherit,inherited,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159,1,['inherit'],['inherited']
Modifiability,"Most of these changes are to support automated evaluation of GATK CNV. - Updates `AnnotatedIntervals` (formerly `SimpleAnnotatedGenomicRegion`) to use the tribble framework for reading. Writing is done in a way that should be concordant with a future tribble writing framework, as per discussion with @droazen.; - Changes to `XsvLocatableTableCodec` to support usage of arbitrary config files. This cannot be done when using tribble features in the CLI. Already reviewed with @jonn-smith . Support for SAM File headers and comments is included.; - *Note:* The reading of `AnnotatedIntervals` cannot be done automatically on the command line, unless the config file is a sibling. The tools below do not even attempt this, since the use cases involved will never have a sibling config file.; - Created a default config file in the jar file resources to read tsvs with locatable fields from the CNV collection files. This is much less strict than the framework used by the CNV tools. The reader will accept any columns (or subset of the columns). CLIs (both experimental quality): ; - `TagGermlineEvents` is a simple tool that attempts to identify events in a tumor seg file that correspond to a germline events. ; - This is done purely with concordance on the breakpoints of the events (within some padding). ; - Input germline segments must have calls. ; - If a germline call is broken into multiple segments, this tool will handle that appropriately (ditto if there are multiple tumor segments overlapping the germline call). - `MergeAnnotatedRegions` will merge all overlapping regions and resolve annotation value conflicts. Closes #3995",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276:380,config,config,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276,4,['config'],['config']
Modifiability,"Most spark tools use the one in GATKSparkTool, but some have some special requirements that make it not work for them. They have to specify different sequence dictionaries or something like that in a way that isn't exposed. Maybe something could be refactored there, but they needed manually adjusting to match the new behavior because of their special handling of the writing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389:249,refactor,refactored,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6458#issuecomment-594167389,1,['refactor'],['refactored']
Modifiability,Most things were addressed here. A bunch of follow on tickets created to address more complicated refactorings that we don't have time to hit now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-368648535:98,refactor,refactorings,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3803#issuecomment-368648535,1,['refactor'],['refactorings']
Modifiability,Move ReadFilter plugin integration up to GATKTool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2218:16,plugin,plugin,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2218,1,['plugin'],['plugin']
Modifiability,Move read filter plugin initialization to GATKTool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2175:17,plugin,plugin,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2175,1,['plugin'],['plugin']
Modifiability,"Moved R dependencies to conda environment, cleaned up R/python dependencies, and updated base Docker/Travis configuration.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026:108,config,configuration,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026,1,['config'],['configuration']
Modifiability,"Moving to [GenomicsDB 1.4.1 ](https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.1)release will allow for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the `--genomicsdb-use-gcs-hdfs-connector` option. Using the native client with gcs allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc. The defaults are all hardcoded to match what is in gatk at present. It also helps with performance issues with gcs, see #7070. This version also contains fixes for #7089, although it will require additional support from gatk(will be part of a separate PR).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7224:463,config,configuring,463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7224,1,['config'],['configuring']
Modifiability,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:1118,config,configuration,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,1,['config'],['configuration']
Modifiability,"Mutect2 Adaptive Pruning issue as discussed in GATK OH meeting. ; Here is the original post:. This request was created from a contribution made by Nabeel Ahmed on April 07, 2021 09:13 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077647812-Why-do-a-clear-expected-variant-not-show-up-in-the-Mutect2-vcf-file). \--. I am running Mutect2 on a sample in tumor-only mode. This sample has mutations introduced and known to be true positive calls. However, I am unable to detect some of these calls in the vcf file after Mutect2 is run that have very clear read support as seen in IGV. I have used the â€“bam-output option to show the output bam and in IGV, it shows that there is no assembly in this region and no mutation event was detected. I am showing the IGV screenshot for one of such calls (chr12:25398285). ![](https://gatk.broadinstitute.org/hc/user_images/46GjRo3tH-Y456j6ApIsqw.png). I am using the latest version GATK 4.2.0.0 and the following is the full Mutect2 command from the log file. java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Mutect2 -R ../resources/hg19.fa -L ../resources/coding\_regions.bed -I bam\_files/sample1.bam --pon ../resources/pon.vcf.gz --germline-resource ../resources/af-only-gnomad.raw.sites.hg19.vcf.gz --bam-output sample1.mutect2\_out.bam --recover-all-dangling-branches true -min-pruning 1 --min-dangling-branch-length 2 --debug --max-reads-per-alignment-start 0 --genotype-pon-sites True --f1r2-tar-gz vcf\_files/f1r2.sample1.tar.gz -O vcf\_files/unfiltered.sample1.vcf Â . In the debug mode, the following log messages are generated for this region. 08:01:26.086 INFOÂ  Mutect2Engine - Assembling chr12:**2539**8242-**2539**8320 wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:8,Adapt,Adaptive,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['Adapt'],['Adaptive']
Modifiability,Mutect2 WDL: Add Funcotator Adjustable diskspace and Memory variables,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680:60,variab,variables,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680,1,['variab'],['variables']
Modifiability,Mutect2 WDL: Funcotate task has useless variables - no way to increase memory for Funcotate task only,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:40,variab,variables,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,1,['variab'],['variables']
Modifiability,"Mutect3 dataset enhancements: optional truth VCF for labels, seq error likelihood annotation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7975:16,enhance,enhancements,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7975,1,['enhance'],['enhancements']
Modifiability,"My $0.02:. 1. In general it's ok with me to not provide a template for WDLs in the GATK repo as long as you guys help us (ie @bshifaw) produce appropriate templates to include in the gatk-workflows repo and in FireCloud. . 2. Re: Picard tools, going forward they should be invoked from the GATK jar by default. Among other benefits, that will reduce support entropy wrt possible combination of versions of tools people might be using. 3. I like the idea of focusing on the auto-generated wrappers for improvements like the string variable for adding arbitrary extra args.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358488159:530,variab,variable,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188#issuecomment-358488159,1,['variab'],['variable']
Modifiability,"My 2 cents: actually the argset strategy would be nice also for plugins. For example, in ReadFilters it might allow to specify ""recommended"" filters but not necessary; and in the annotations to convert the groups to a argument set. +1 to the argset for many use-cases and not only this one!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758:64,plugin,plugins,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758,1,['plugin'],['plugins']
Modifiability,"My lab has a variety of custom walkers. Many subclass MultiVariantWalkerGroupedOnStart, which is a useful iteration pattern. We tend to scatter/gather on our cluster, where each job is given an interval set. When doing this, handling variant spanning those borders is critical. We just had an issue around this, which stems from MultiVariantWalkerGroupedOnStart and the fact that ignoreIntervalsOutsideStart defaults to false. For our usage, we almost never want this to be true, and it's a really subtle problem if the user doesnt remember to set this. So my question is: is there a best-practice way for subclasses to override / remove or set default on inherited arguments? Granted, individual walkers could simply change the value of ignoreIntervalsOutsideStart during the init phase, but I dont like that solution since it basically leaves an useless/ignored argument. . thanks in advance for any ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7287:656,inherit,inherited,656,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7287,1,['inherit'],['inherited']
Modifiability,"My recollection is that this is a use case we never put any priority on so there's no test in the GATK suite for access to private files. There should be, of course. The feature is there and (at least locally) it worked when I tried it. NIO does not use the API_KEY, it uses default credentials. Those are environment variables that are set by the `gcloud` command or pre-set for you in the case of virtual machines on Google. There are two cases: local execution and Spark. . I just tested local execution and it worked fine for me:. ```; $ ./gatk-launch PrintReads -L Broad.human.exome.b37.small.interval_list -I gs://jpmartin-private/bench/WGS-G94982-NA12878.bam -O t_gcs.bam; ```. this command worked even though (unless I'm mistaken) neither the bucket nor the file are public. One challenge however is that the way to set default credentials has changed recently. Calling `gcloud auth login` used to be enough but now we have to call (IIRC) `gcloud auth application-default login`. For Spark, the default credentials are set as whoever owns the dataproc environment that's used to run the show. So it should be set so it has access to the buckets necessary. NIO has mechanisms for accessing buckets that belong to someone other than who is running the Spark job, but they are not hooked into GATK yet.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658:318,variab,variables,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277832658,1,['variab'],['variables']
Modifiability,"My worry about camel case is that it trips up people a lot, especially those whose native language doesn't have a concept of case (like Chinese). . Maybe long arguments with lots of dashes need to be refactored to have fewer... can you give some examples?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323747625:200,refactor,refactored,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2596#issuecomment-323747625,1,['refactor'],['refactored']
Modifiability,"NAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3380,Config,ConfigFactory,3380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,NFO GenomicsDBImport - HTSJDK Defaults.CREATE_MD5 : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4300,Config,ConfigFactory,4300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3748,Config,ConfigFactory,3748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2810,Config,ConfigFactory,2810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"NVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3832,Config,ConfigFactory,3832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,Namespaced arguments in barclay are something we've talked about before that could help with this. So multiple argument collections / plugins objects could declare the same argument and it would be de-ambiguated by the full name of the plugin/collection. Something like `--ReadNameFilter.invert --MappingQualityFilter.invert`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502173995:134,plugin,plugins,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502173995,2,['plugin'],"['plugin', 'plugins']"
Modifiability,Need to create a tool that allows a user to import / create a simple delimited data source (i.e. from a given CSV / TSV file). See how Oncotator structured its config files for insights on how to do this. It may be possible to simply reuse that config file format.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3785:160,config,config,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3785,2,['config'],['config']
Modifiability,"Needs to:. -support overriding config settings via a simple mechanism (like providing an override config file); -use a simple, easy-to-edit file format like Java Properties (name = value); -be widely used in the Java community & well-maintained.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3078:31,config,config,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3078,2,['config'],['config']
Modifiability,"New implementation of `SlidingWindowWalker` with some ideas from the discussion in #1528. The thinks that are requested in #1198 still holds, but now it is more general: padding option is added and construction of windows are done by interval. The code contain a lot of TODO because it relies on changes implemented in #1567, and because it is suppose to be a walker over `ReadWindow` instead of `SimpleInterval`+`ReadsContext` if reads are available. I think that with these changes it could be general to be extended by `ReadWindowWalker` and by users that needs a different way of ""slide"" over intervals.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708:510,extend,extended,510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708,1,['extend'],['extended']
Modifiability,"New tool aiming to call all types of precise variants detectable by long read alignments (not fully functioning yet in the sense that not all types of variants are detected yet&mdash;to be handled by later PRs in this series).; This new tool splits the input long reads by scanning their alignment characteristics (number of alignments, if strand switch is involved, if mapped to the same chromosome, if have equally good alignment configurations based on the scoring tool, etc), and send them down different code path/logic units for variant type inference and VCF output.; This PR would only deal with simple INSDEL, for long reads having exactly 2 alignments (no other equally good alignment configuration) mapped to the same chromosome without strand switch or order switch (translocation or large tandem duplications), because we already have this type of variant covered in master. __UPDATE__; See updated roadmap in #2703. NEEDS TO WAIT UNTIL PART 1 IS IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3456:432,config,configurations,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3456,2,['config'],"['configuration', 'configurations']"
Modifiability,"No problems. The walkers have no built in parallelism so there's no problem with using state. It makes it harder to adapt to spark, but that's probably not a big deal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4447#issuecomment-368091726:116,adapt,adapt,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4447#issuecomment-368091726,1,['adapt'],['adapt']
Modifiability,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:287,adapt,adapt,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981,1,['adapt'],['adapt']
Modifiability,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:881,variab,variable,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,1,['variab'],['variable']
Modifiability,"Not really an issue, just wanted to document some surprising behavior. @tmelman has been reviving/reimplementing some ancient CNV/ModelSegments evaluations (dating as far back as 4.0.2.1!) and trying to understand whether observed differences---intentional or otherwise---are due to method changes I might have made, or if she might've introduced changes in her reimplementation of the evaluation code. I ran some checks on the stability of ModelSegments using an old set of inputs (normal/tumor allelic counts and denoised copy ratios for SM-74P4M WES). Behavior has remained largely stable since at least 4.1.0.0. Namely:. 1) We evaluated and signed off on a change that went into 4.1.0.0. See comments in https://github.com/broadinstitute/gatk/pull/5575.; 2) A slight numerical difference in the MCMC-sampled allele fractions was introduced by changes made to some MathUtils code for calculating logs/factorials/etc. between 4.1.0.0 and 4.1.1.0 in https://github.com/broadinstitute/gatk/pull/5814. Note that no CNV code was directly changed, it's just that we call out to that changed MathUtils code---namely, to calculate log10factorial. The overall result in my test was a very slight change to the number of segments found, from 516 to 522.; 3) No further numerical changes have been introduced through the current 4.2.4.1, so any additional code changes I made were indeed true refactors, at least from the perspective of this simple test. Phew!. I was indeed surprised to find that very slight differences in the log10factorial behavior (which result from changing the recursive calculation of cached values to a direct one, and appear in something like the 13th decimal place) led to non-negligible changes in the MCMC estimates of the allele fractions---and thus, changes in the number of segments. Although these are also relatively slight differences in terms of practical impact, they are perhaps much larger than one might guess, given their humble origins.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7649:1385,refactor,refactors,1385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7649,1,['refactor'],['refactors']
Modifiability,"Not sure if this is outside the scope of a simple port, but I think it would be great if the fitting of a `GaussianMixtureModel` was made a little bit more generic and extracted. Right now the method `maximizeGaussian` takes in `List<VariantDatum>`, but it should be trivial to refactor it to take in a `double[]` or `List<Double>`. Fitting a GMM could be more generally useful for other methods, after all. It might even be useful to extract the k-means clustering code used to initialize the model, if this is retained in the port. Perhaps also outside the scope, but it'd also be nice if variable names were changed to match the notation in Bishop Ch. 10 (on which the variational-Bayes algorithm is based). I think this would make the code much easier to parse from a mathematical standpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146:278,refactor,refactor,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236003146,4,"['refactor', 'variab']","['refactor', 'variable']"
Modifiability,"Not sure if this is related, but @chandrans and I had some trouble with the dataproc launcher yesterday (didn't recognize some yarn argument). Changing the ""image"" setting in the cluster config solved it, afaiu.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2230#issuecomment-278729124:187,config,config,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2230#issuecomment-278729124,1,['config'],['config']
Modifiability,"Note separate method configuration, but uses the same WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811803:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811803,1,['config'],['configuration']
Modifiability,"Note that before this is merged, we'll need to do a datasource release in which the following property is added to the gencode config files:. ```; # Required field for GENCODE files.; # NCBI build version:; ncbi_build_version = X; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522#issuecomment-447141409:127,config,config,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522#issuecomment-447141409,1,['config'],['config']
Modifiability,"Note that there is an AnnotateIntervals tool in the CNV pipeline (awaiting review in sl_denoising) that will output a TSV with column headers CONTIG, START, END, and GC_CONTENT. It takes -L, which can do the padding for you. If this doesn't exactly fit the bill for you, then it's probably best if you roll your own implementation rather than modify or refactor that code---should be easy enough.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3859#issuecomment-345807469:353,refactor,refactor,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3859#issuecomment-345807469,1,['refactor'],['refactor']
Modifiability,"Note to self: the gcloud API changes a bit with the new release, apply the changes in [jp_gcloud_17_snapshot](https://github.com/broadinstitute/gatk/tree/jp_gcloud_17_snapshot) to adapt.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2822#issuecomment-306241927:180,adapt,adapt,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822#issuecomment-306241927,1,['adapt'],['adapt']
Modifiability,"Note: these are not hooked up to the code anywhere, I have another branch where I am doing the work to plug these in. This also does not currently resolve the equivalent issue to https://github.com/broadinstitute/gatk/issues/3848 but it does add tests to both plugins enforcing what the correct behavior should be. . fixes #3624",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3851:260,plugin,plugins,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3851,1,['plugin'],['plugins']
Modifiability,"Noticed this minor typo while doing some refactoring for a new feature branch. The issue is that the MultidimensionalKernelSegmenter incorrectly uses `maxNumChangepointsPerChromosome = maxNumSegmentsPerChromosome`, when it should be using `maxNumChangepointsPerChromosome = maxNumSegmentsPerChromosome - 1` like the CopyRatioKernelSegmenter and AlleleFractionKernelSegmenter do. @fleharty mind reviewing?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6497:41,refactor,refactoring,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6497,1,['refactor'],['refactoring']
Modifiability,"Now that there's been some refactoring of this code, it might be relatively straightforward to rewire the GVCFBlockCombiner to take in likelihood data from the pileup without creating a VariantContext, then pass the combined data to a VC and then to the writer.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5618#issuecomment-590953474:27,refactor,refactoring,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618#issuecomment-590953474,1,['refactor'],['refactoring']
Modifiability,"Now that we have important `VariantWalker` tools that use reads as a side input (such as @lucidtronix 's `CNNScoreVariants`), we need to add caching to `ReadsContext` for good performance on nearby reads queries during a traversal. The caching should be modeled after the existing caching in `FeatureContext` as implemented in the `FeatureCache` class, but it should include the ability to cache ""around"" the current locus, rather than just ahead of it as in `FeatureCache`. Ideally, the tool itself should be able to configure the default caching behavior via arguments to control bases to cache before and after current locus.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4781:518,config,configure,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4781,1,['config'],['configure']
Modifiability,"Now that we're using git lfs to manage our large test resources, we need to configure travis to install/init git lfs before running the test suite.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/840:76,config,configure,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/840,1,['config'],['configure']
Modifiability,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:560,variab,variable,560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,2,['variab'],['variable']
Modifiability,"OK so just following along; the problem appears related to the Google Cloud Storage Connector and its configuration. When running on Cloud we need to ask for the `https://www.googleapis.com/auth/devstorage.read_write` scope, as described in [the install docs](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md). But you're right that `https://www.googleapis.com/auth/cloud-platform` should imply that so it should work... The command line argument is `--scopes` (plural) and not `--scope` but that's probably not the issue, the tool would have complained if you actually typed `scope` in there. . Perhaps the code is trying to do the non-cloud setup and that's what's making it not work on cloud?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331047616:102,config,configuration,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331047616,1,['config'],['configuration']
Modifiability,"OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by. `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`. This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:. ````; [global]; base_compiledir = PATH/TO/COMPILEDIR_#; ````. Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?. This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort. However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?. @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809:272,variab,variable,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"OK, thanks. I tried to keep edits here limited and protected. I'm happy to describe more about what I'm trying to do in VariantQC if that's helpful. Also - i have not forgotten about trying to refactor VariantQC to better handle arguments (i.e. dont pass the walker to the VariantEvaluator, and to separate a VariantEvalEngine class, somewhat like VariantAnnotationEngine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502259266:193,refactor,refactor,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998#issuecomment-502259266,1,['refactor'],['refactor']
Modifiability,"OK. Another thing: since in GATK4 it inherits from LocusWalkerByInterval, -L is now required. the usage examples still say -L is optional. . Tangentially is there a shortcut way to pass ""all intervals in the genome"" to GATK in the -L argument? Is there some trick using the DICT file or something like this? Certainly it's not that hard to convert a .dict file to an interval list, but not automatic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634784068:37,inherit,inherits,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634784068,1,['inherit'],['inherits']
Modifiability,"OK. As a reference, how does GATK deal with max-alternate-alleles for normal human variant calling? Presumably really high alternate alleles would primarily happen in repetitive/index prone-regions? FWIW, When we execute GenotypeGVCFs, we run as ~1000 jobs where each takes an even chunk of the genome, by base pairs. . Yes, I did see the bypass-feature-reader option, but we have jobs in-flight and I'm reluctant to change too many things as once. We will try this when possible though. As far as number of batches imported: I would need to check, but I believe it's only ~5 batches with perhaps 50-100 samples/ea. So I guess it's not that many new batches in the scheme of things, but anecdotally we have noticed that with the last couple rounds of import we needed to reduce batch size to make it work (i.e. not get hung). It is conceivable there is some other factor that is causing that variable performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581:892,variab,variable,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581,1,['variab'],['variable']
Modifiability,"OK. I found a potential solution. For this solution, we do not need to add or remove any dependencies. The only change is to the `log4j.properties` file (which configures log4j 1.x) to match the config specified in `log4j2.xml` (which configures log4j2). Now, GKL will use log4j 1.x to log, but the format will match the rest of GATK, which uses log4j2. This means that we have only one GKL for both GATK 3 and 4, at the expense of having to keep to config files, `log4j.properties` and `log4j2.xml`, in sync (which they probably should have been anyway, thought they weren't).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320779413:160,config,configures,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320779413,4,['config'],"['config', 'configures']"
Modifiability,"OR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4981,Config,ConfigFactory,4981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"O_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5466,Config,ConfigFactory,5466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"O_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6105,Config,ConfigFactory,6105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"O_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFacto",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4167,Config,ConfigFactory,4167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"O_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5643,Config,ConfigFactory,5643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,Obviated by ModelSegments rewrite.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3181#issuecomment-356697320:26,rewrite,rewrite,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3181#issuecomment-356697320,1,['rewrite'],['rewrite']
Modifiability,"OfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; > + }; > +; > + // get the sequence dictionary; > + final SAMSequenceDictionary sequenceDictionary = getBestAvailableSequenceDictionary();; > + final List<SimpleInterval> intervals = hasIntervals() ? intervalArgumentCollection.getIntervals(sequenceDictionary); > + : IntervalUtils.getAllIntervalsForReference(sequenceDictionary);; > +; > + // create an IntervalList by copying all elements of 'intervals' into it; > + IntervalList intervalList = new IntervalList(sequenceDictionary);; > + intervals.stream().map(si -> new Inte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:4993,inherit,inherited,4993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['inherit'],['inherited']
Modifiability,"Oh, I hadn't noticed that there was a compilation warning causing the test to fail. ```; /gatk/src/test/java/org/broadinstitute/hellbender/MainTest.java:55: warning: [serial] serializable class ExitNotAllowedExcepion has no definition of serialVersionUID; private static final class ExitNotAllowedExcepion extends SecurityException {; ^; error: warnings found and -Werror specified; ```. Please fix that also :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772:306,extend,extends,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4283#issuecomment-361661772,1,['extend'],['extends']
Modifiability,"Oh, also note that there might be some variable-name references in the Javadocs for the VQSR-lite tools that are not rendered properly in online docs; see https://github.com/broadinstitute/gatk/issues/8146 for more context. However, if you're just looking at the Javadocs via IntelliJ, everything should look fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414063049:39,variab,variable-name,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1414063049,1,['variab'],['variable-name']
Modifiability,"Oh, interesting. That's a real problem. We inherited that code from picard and I don't think anyone ever paid attention to it. I'm assuming it's in there because someone encountered a tmp dir they couldn't read/write to but could set permissions on at some point, which seems weird. . At most we should be setting it for owner only I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4513#issuecomment-371635375:43,inherit,inherited,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4513#issuecomment-371635375,1,['inherit'],['inherited']
Modifiability,Ohh. This looks like what we have really wanted when we refactor the test suite to test spark and other tools using the same methods. This should bring restful nights to us all. Unfortunately it looks like most of the docker tests have failed with errors along the lines of this: ; ```; org.gradle.api.internal.tasks.testing.TestSuiteExecutionException: Could not complete execution for Gradle Test Executor 1.; 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(Exec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:56,refactor,refactor,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['refactor'],['refactor']
Modifiability,"On the first question, we definitely appreciate how much work this will take. Often, porting the code is the easy part; developing new tests and test data can be a huge effort. I can try to find out if it would be possible for you to take the tool over - I know this kind of thing has come up before for other tools, but I'd have to ask around to find that out. @vdauwera do you have input on this ?. As for the plugins, currently in your branch `VariantStratification` and `VariantEvaluator` are modeled as Barclay command line plugin descriptors, and I was questioning whether thats necessary. Being a plugin is not necessarily required - `ReadFilter` and `Annotation` are both plugins, but they didn't have to be, and it takes quite a bit of work (again, mostly test development) to get a plugin right. Also, I'd consider the Barclay plugin framework to be pretty developed at this point, so I'd be curious to learn more about what issues you see. And yes, definitely don't check any of the large GATK3 test files into the repo, even temporarily. Take a look at [General guidelines for GATK4 developers](https://github.com/broadinstitute/gatk#dev_guidelines) if you haven't already. As you pointed out, new GATK4 tests that use smaller files would have to be developed. We'd want those to be included, and passing tests on the CI server, before we started reviewing the branch, so we know we're reviewing code that works and is covered by tests as much as possible. The second commit in my list above would have only your GATK3 java test files, etc (but not the big files, which you appear to have locally). The third commit would have your ported tool code, as well as the new test code, with the new tests enabled, as well as the smaller input files and expected results files. At the end we'd remove commit #2.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633:412,plugin,plugins,412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407185633,12,['plugin'],"['plugin', 'plugins']"
Modifiability,"Once we choose the library to use for GATK configuration, let's have a design meeting to make sure we come up with something that works for Spark, downstream projects, our users, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079:43,config,configuration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079,1,['config'],['configuration']
Modifiability,"One concern I have is the maintainability of the test (having been burned by this in other places myself). When we add a new output field, etc we need a very easy way to update/generate these results. At the very least some instructions would be helpful (and imagine someone to follow those as part of a PR)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7192#issuecomment-821234533:26,maintainab,maintainability,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7192#issuecomment-821234533,1,['maintainab'],['maintainability']
Modifiability,"One major goal is to replace all the system properties in `gatk-launch`. Config options would be a combination of:; -Java system properties (like in gatk-launch); -Other engine-wide settings (like codec package names in `FeatureManager`, or NIO retries)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467920:73,Config,Config,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467920,1,['Config'],['Config']
Modifiability,"One more thing: I'm also wondering if it would be possible to get a quick, preliminary evaluation of such a process without actually doing the work of adding it into the training tool. It's probably possible to do a slightly more ""manual"" validation split (say, using one or a few chromosomes), run the score tool on that validation set, use some external code to calculate the desired threshold from the resulting scores, and then use that threshold going forward. Actually, now that I've written it out, that sounds a lot cleaner and more flexible! Let me try to hack together the corresponding workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611:541,flexible,flexible,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1065543611,1,['flexible'],['flexible']
Modifiability,"One observation that illustrates the need for care when optimizing metrics: for a few of the F1 optimizations, the haplotype-to-reference match-value parameter gets driven to its minimal value (1). Not 100% sure, but I'm guessing this might effectively boost precision by somehow cutting down on the complexity of proposed haplotypes---it depends on what the exact behavior of our SW algorithm is for negative scores. @davidbenjamin any thoughts on this behavior?. Something I don't quite understand yet is if we can impose some effective constraints on the parameters or otherwise reduce the number of independent dimensions. For example, it seems reasonable to me to fix the gap-extend penalties to -1 and let all other parameters be defined w.r.t. them. But perhaps we can also fix the match values similarly?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193:681,extend,extend,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193,1,['extend'],['extend']
Modifiability,One of the non-cloud tests in BQSRSparkIntegrationTest tries to use the API key when its not required; as a result the test fails when the key isn't set even though it should pass. Introduced in the test refactoring that was part of https://github.com/broadinstitute/gatk/pull/1533.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1590:204,refactor,refactoring,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1590,1,['refactor'],['refactoring']
Modifiability,"One part of this ticket is done: https://github.com/broadinstitute/gatk/pull/4964 added accessors that allow direct descendants of `GATKTool` to directly access engine datasources, while still forbidding direct access for tools that extend a Walker base class (except for Walker types living in the engine package, which still have access).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878:233,extend,extend,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-483829878,1,['extend'],['extend']
Modifiability,"One proposal for moving forward would be to have a default properties file with a known name/location that is included in the gatk jar (say, ""gatk.default.properties""), which is always loaded and populates the initial configuration, and then use the classloader getResources method to also load all resources with some other known name (say, ""gatk.properties""). That way any properties files on the classpath with the known name would be automatically discovered and loaded. The apache commons API allows looks like it has good support for handling this using a [composite](http://commons.apache.org/proper/commons-configuration/userguide/howto_compositeconfiguration.html#Composite_Configuration_Details) configuration. We would have to define some rules around override semantics, but it looks like the api provides a lot of control over that as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954:218,config,configuration,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954,3,['config'],['configuration']
Modifiability,"One thing to try is to configure cromwell to retain the log directory via a workflow option when we run the tests. Then at the end of the build we can copy them somewhere, either always, or via the travis after_failure entry in the build matrix. Then we'd be able to see exactly what failed in the travis environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4130#issuecomment-357059677:23,config,configure,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4130#issuecomment-357059677,1,['config'],['configure']
Modifiability,"One variable that we need to control for is OpenJDK vs. Oracle JDK. Apparently these errors happened with OpenJDK, which is known to be flakier in the networking department than Oracle JDK. We should test with Oracle's JDK and see if the errors persist.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874:4,variab,variable,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300313874,1,['variab'],['variable']
Modifiability,"Oof, that's a nasty problem. We can definitely do something about it. It feels more like a Microsoft bug than a GATK one though. It seems crazy that each layer pull has to be a separate web request and there's no batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:400,layers,layers,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,2,['layers'],['layers']
Modifiability,"Ops reported several instances in which the allele-specific filtering failed. In the case I examined, the MQ distribution is much tighter around the mode at 60, which causes lin alg failures because that variable is effectively constant. Added more jitter, which has served well in the past.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6262:204,variab,variable,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6262,1,['variab'],['variable']
Modifiability,"Options include:. -Do nothing, and enforce via coding conventions (and hope the dataflow team comes to its senses). -Making a copy of the input before every `apply()` / `processElement()` / etc. (only affects dataflow code, but is inefficient (since it copies even when there's no mutation) and brittle since it only affects tools that go through our interface). -Make our types immutable, and add builders for mutation that perform copies (affects/penalizes non-dataflow code as well, since it will no longer be possible to modify in place). -Have both mutable and immutable views of each type, plus a builder (hard to manage given dataflow's poor support for polymorphism)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702:661,polymorphi,polymorphism,661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702,1,['polymorphi'],['polymorphism']
Modifiability,Options only used when running locally. Internally options are passed either by appending to java invocation directly with; command-line options or using the JAVA_OPTS environment variable. Also added environment variable printout with --dryRun; Other minor formatting / whitespace changes. resolves #2694,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2783:180,variab,variable,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2783,2,['variab'],['variable']
Modifiability,"Original report by @samuelklee (see https://github.com/broadinstitute/barclay/issues/189):; > I noticed that Javadoc @value tags are not being rendered correctly in e.g. https://gatk.broadinstitute.org/hc/en-us/articles/9570326304155-ScoreVariantAnnotations-BETA-. I used these tags to specify the variables corresponding to argument names (e.g., StandardArgumentDefinitions#INTERVALS_LONG_NAME instead of intervals , USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME instead of use-allele-specific-annotations, etc.), and while they show up correctly when rendering the Javadoc within IntelliJ, it seems the same is not true on the GATK website. Is there an easy fix in the code for generating these docs, or should I just avoid using this tag?. My original response:. > I tested this using the new Java 17 doclets in the hope that it would just work, but the result is the same. However, the new Java language model classes make it easy to interpolate these, so Iâ€™ll fix this in the barclay Java 17 branch. However, in looking more closely, it's not as easy to fix as I first thought, and the problem is a little deeper than I first realized. Although it's easy to detect these using the new Java 17 apis, it's more difficult to retrieve the actual values. And even then, because the gatkdoc process only consumes a subset of the classes consumed by the javadoc process (it only sees `@DocumentedFeature`s), it's quite easy to reference something in the javadoc comment that can be resolved by javdoc, but not by gatkdoc. But it appears that even the javadoc process isn't rendering these tags correctly. Here is the raw javadoc comment:; ```; * Input VCF file. Site-level annotations will be extracted from the contained variants (or alleles,; * if the {@value USE_ALLELE_SPECIFIC_ANNOTATIONS_LONG_NAME} argument is specified).; ```; The rendering in javadoc (the argument name is missing entirely, but it should be interpolated):; <img width=""780"" alt=""Screen Shot 2023-01-05 at 12 17 43 PM"" src=""https://",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8146:298,variab,variables,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8146,1,['variab'],['variables']
Modifiability,"Originally from @vruano . Depending of what ploidy we use AR may return different active region boundaries. This differences cause the haploid assembly to fail with the larger region hightlight the lack of robustness of the current approach. More concretely the problem seem to be the presence of cycle in the larger region. Files are located in . ```; /humgen/gsa-hpprojects/dev/valentin/bug-reports/non-rubsassembly-with-ploidy4. cd $THAT_DIR; sh ./run.sh; ```. in CEUTrio*ploidy4.vcf the variant 20:22064431 is missing (as some other in the same region) which is a TP in knowledge base. . If you look into the debug output ploidy2.err and ploidy4.err, the latter attempts to assemble a larger region failing due to a cycle. . AR traversal comes out with different active region boundaries because the engine used takes as a parameter the ploidy. That is not by itself a bug and a bad think is just that the assembly fails for the extended region. . The task here is to improve the assembly algorithm to cope with this situations better (perhaps handle cycles appropriately).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/267:933,extend,extended,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/267,1,['extend'],['extended']
Modifiability,"Originally, we just had the normal be optional. You also had automated tests in the WDL Travis. . In FC, for tumor only, you would probably want a separate method configuration that ran on sample entity type. I'm open to other suggestions, but I can't think of another way. This could in theory be used for germline calling, too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811723:163,config,configuration,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362811723,1,['config'],['configuration']
Modifiability,"Our data source classes are an inconsistent mess -- let's refactor so that we have ONE centralized reads source used by all tools (walkers and spark), one reference source, etc. This will have the side benefit of making it easier for new features like CRAM support to propagate transparently to all tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/959:58,refactor,refactor,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/959,1,['refactor'],['refactor']
Modifiability,OutputStream.java:1529) ~[?:?]; at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438) ~[?:?]; at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181) ~[?:?]; at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350) ~[?:?]; at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; 11:00:54.078 INFO AbstractConnector - Stopped,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:22987,adapt,adapted,22987,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"Overall the refactoring looks good and makes senseâ€¦ but I'm not seeing how this fixes the problem of eating exceptions we saw during a recent run. Can you explain what was happening before, and how the new code addresses it?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-927995357:12,refactor,refactoring,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-927995357,1,['refactor'],['refactoring']
Modifiability,Override mechanisms (in order of priority). -Individual config options specified on the command line manually / Or explicit config file ; -Override config file packaged into a downstream project; -Default GATK config file,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520:56,config,config,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2368#issuecomment-307467520,4,['config'],['config']
Modifiability,Overriding inherited options,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/149:11,inherit,inherited,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/149,1,['inherit'],['inherited']
Modifiability,"Overview: see [this presentation](https://docs.google.com/presentation/d/1jPKYcaMcpT_e1l8L6D3wn7wBvC-yKt4GVrgeeTRBrss/edit#slide=id.g7f3200a976_0_97). ![image](https://user-images.githubusercontent.com/1423491/136983924-338faca1-30f0-4f1e-92c7-b34f091050ca.png). WDL; * updated WDLs to support parameterized loading of PET and/or RANGES; * enhanced inline schemas in WDL to JSON to allow for declaring required fields. Common; * updated AvroFileReader to use GATKPath instead of String for file, allows us to read from gs:// directly; * changed ""mode"" from EXOMES/GENOMES/ARRAYS (unused) to PET/RANGES; * promoted GQStateEnum to top-level class (it was inside PetTsvCreator but used across the codebase); * added numerical GQ value to GQStateEnum; * max deletion size is 1000bp . Import; * added flags to enable writing of PET and/or VET; * code to create RefRanges with pluggable writer and TSV/Avro implementations; ; Extract; * add parameter to parameterize inferred GQ value; * support to read VET/Ranges data from Avro files (to support testing); * Entire implementation of ranges support; * Note there is a maximum supported DELETION size. Upstream deletions larger than this will not generate downstream spanning indels. Testing; * added new integration test for ranges extract; * added various unit tests; * (IN PROCESS) scientific tieout against 1k; * scale testing up to 90k once we've move to v2 reblocking. How to perform scientific tieout; 1. Run the ""GvsIngest"" pipeline with load_ref_ranges = true, this will load both the PET and REF_RANGES tables; 2. Run Create Alt Allele, Training, etc as normal; 3. Extract a callset twice -- once with mode = 'PET' (the default) and once with mode = 'RANGES'; 4. Compare the resulting VCFs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7498:294,parameteriz,parameterized,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7498,3,"['enhance', 'parameteriz']","['enhanced', 'parameterize', 'parameterized']"
Modifiability,"PPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5249,Config,ConfigFactory,5249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"PPY_COMPRESSOR : false; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5888,Config,ConfigFactory,5888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"PPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3933,Config,ConfigFactory,3933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,PRs like https://github.com/broadinstitute/gatk/pull/2156 make it clear that we need some master configuration mechanism in the GATK that can be overridden by clients/downstream projects. . One promising option is `commons-configuration` (https://commons.apache.org/proper/commons-configuration/userguide/user_guide.html) using properties files -- we should look into this to see whether it does what we want.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2297:97,config,configuration,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2297,3,['config'],['configuration']
Modifiability,Package example GATK config file in the startup dir of the GATK docker image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4485:21,config,config,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4485,1,['config'],['config']
Modifiability,Parameterize the logging frequency for ProgressLogger.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8662:0,Parameteriz,Parameterize,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8662,1,['Parameteriz'],['Parameterize']
Modifiability,Parsing the GATK config file currently overrides any command-line specified config options for system-level parameters. Options explicitly specified on the command-line should override what is in the config file. The `GATKConfig.properties` file is missing from the packaged binary release (as created by`gradle bundle`). Gradle must be updated to include it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4436:17,config,config,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4436,3,['config'],['config']
Modifiability,"Passing in the properties file did work (with the --gatk-config-file option). However, of the four tools I tested (MarkDuplicates, BaseRecalibrator, ApplyBQSR, and HaplotypeCaller) all of the tools accepted the --gatk-config-file option except for MarkDuplicates, which complains that it is not a recognized option. Perhaps this should be turned into a separate issue?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-368036324:57,config,config-file,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435#issuecomment-368036324,2,['config'],['config-file']
Modifiability,PathSeq Illumina adapter trimming and simple repeat masking,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354:17,adapt,adapter,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354,2,['adapt'],['adapter']
Modifiability,"PathSeqFilterSpark and PathSeqPipelineSpark clear all the sequences from the input header file, as the Bwa step only accepts unaligned reads. However, the header sequences were being cleared before the reads were loaded, causing WellformedReadFilter to remove any mapped reads (by failing to find the corresponding sequence name in the header). This PR fixes this bug by creating a deep copy of the header. It also refactors this code, which is used in both the Filter and Pipeline tools, into a utility function `checkAndClearHeaderSequences()` in PSUtils. Tests have also been added/updated accordingly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3453:415,refactor,refactors,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3453,1,['refactor'],['refactors']
Modifiability,"Per discussion with @droazen, we'll do the Spark tool equivalent of https://github.com/broadinstitute/gatk/issues/365 (once the various branches with ReadsSparkSink refactorings are merged in).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1480:165,refactor,refactorings,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1480,1,['refactor'],['refactorings']
Modifiability,"Per discussion with @kgururaj, this will probably take the form of a flag that suppresses materializing the genotype data. Also see the protobuf-based enhancements described [here](https://github.com/broadinstitute/gatk/issues/3689).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-336965003:151,enhance,enhancements,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-336965003,1,['enhance'],['enhancements']
Modifiability,"Picard had its Optical duplicate finding code refactored recently, additionally it has been noticed as part of #4656 that we are currently not properly accounting for the read groups when we stratify reads in MarkDuplicatesSpark which will likely cause problems for bams with more than one read group. Additionally better test coverage for multiple read groups should be added to ensure we are handling them sanely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4700:46,refactor,refactored,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4700,1,['refactor'],['refactored']
Modifiability,"PipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloud",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4903,Config,ConfigFactory,4903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"Please feel free to close this out. Just noticing small typos as I read through the docs for #1027; Not sure whether y'all prefer to avoid making trivial changes until a larger refactoring occurs, or would want them fixed on their own when they are noticed..",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1048:177,refactor,refactoring,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1048,1,['refactor'],['refactoring']
Modifiability,"Please need help : . I ran the script for VQSR . gatk-4.2.0.0/gatk VariantRecalibrator\; -V variants_sitesonly.vcf.gz\; 	-trust-all-polymorphic\; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0\; -an FS -an ReadPosRankSum -an MQRankSum -an QD -an SOR -an DP\ ; -mode INDEL\; -max-gaussians 4\; -resource:mills,known=false,training=true,truth=true,prior=12:Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\; -resource:axiomPoly,known=false,training=true,truth=false,prior=Axiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz\; -resource:dbsnp,known=true,training=false,truth=false,prior=2:Homo_sapiens_assembly38.dbsnp138.vcf\; -O cohort_indels.recal\; --tranches-file cohort_indels.tranches. ERROR - >. A USER ERROR has occurred: Argument resource was missing: Argument 'resource' is required. Any help would be really great !. thank you; Smeeta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593:132,polymorphi,polymorphic,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-885465593,1,['polymorphi'],['polymorphic']
Modifiability,Please use the template in the WDL GATK repo doc that was shared. Or we can modify that template. I'd like the document to match what is generated automatically. The template in that document includes optimizations and is quite portable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295:228,portab,portable,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295,1,['portab'],['portable']
Modifiability,Plugin descriptors (filters and annotations) should use configurable package lists,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4036:0,Plugin,Plugin,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4036,2,"['Plugin', 'config']","['Plugin', 'configurable']"
Modifiability,Plugin for read transformers,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2160:0,Plugin,Plugin,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2160,1,['Plugin'],['Plugin']
Modifiability,"Plugins can define their own arguments, such as VariantAnnotation classes. We have a number of cases where multiple plugins share arguments. In other words, plugins A and B both require argument X. If either A or B is used, this argument is required. They cannot have independent values for argument X. Is there any way to accommodate this?. I created an ArgumentCollection class to define that argument, and then added this @ArgumentCollection to each plugin. Something like:. public class GenotypeConcordanceBySite extends PedigreeAnnotation implements InfoFieldAnnotation { â€‹; â€‹@ArgumentCollection; â€‹public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. â€‹. .etc......; }. public class GenotypeConcordance extends PedigreeAnnotation implements InfoFieldAnnotation {; â€‹@ArgumentCollection; â€‹public GenotypeConcordanceArgumentCollection args = new GenotypeConcordanceArgumentCollection();. . etc......; }. public class GenotypeConcordanceArgumentCollection {; â€‹@Argument(doc=""Reference genotypes VCF"", fullName = ""reference-genotypes-vcf"", shortName = ""rg"", optional = true); â€‹public FeatureInput<VariantContext> referenceVcf = null;; }. When I run VariantAnnotator with both plugins, I get an error from within Barclay about arguments with duplicate names. Ideally these plugins would not be aware of each other (since they can be used independently). Is there a way to define arguments that might be declared in different plugins, but are somehow resolved as identical and therefore allowed?. Thanks for any help or ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213:0,Plugin,Plugins,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213,9,"['Plugin', 'extend', 'plugin']","['Plugins', 'extends', 'plugin', 'plugins']"
Modifiability,Possible enhancements to MCMC.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2824:9,enhance,enhancements,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2824,1,['enhance'],['enhancements']
Modifiability,Possibly inaccurate warning about mismatching parameter configs in PostprocessGermlineCNVCalls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6994:56,config,configs,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6994,1,['config'],['configs']
Modifiability,"PostProcessGermlineCNVCalls is currently single-sample, using input calls and model for the whole cohort. Specifying a sample index is not particularly user friendly. Given that we already output calls as a directory of files, including a sample map could enable the user to specify a sample name rather than an index. This would involve changes to GermlineCNVCaller as well. Alternatively, extending PostProcessGermlineCNVCalls to process all the samples at the same time would eliminate this problem and allow us to avoid some irritating transposes by parallelizing by shard instead of by sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659:391,extend,extending,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659,1,['extend'],['extending']
Modifiability,"PostprocessGermlineCNVCalls performs a check of the denoising/calling hyperparameter configs used to generate the model in GermlineCNVCaller cohort mode against those used to generate the case-mode result passed to PostprocessGermlineCNVCalls. However, although some of these hyperparameters are not exposed in case mode (since they have no effect on the sample-level parameters inferred in case mode, e.g., `psi_t_scale`), their python default values are nevertheless written to the case-mode config. I think that this results in a spurious mismatch between the cohort/case mode configs, which causes PostprocessGermlineCNVCalls to emit the following warnings in case mode when non-default values are used:. ````; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different denoising configuration between model and calls -- proceeding at your own risk!; WARNING gcnvkernel.postprocess.viterbi_segmentation - Different calling configuration between model and calls -- proceeding at your own risk!; ````. I'm pretty sure that inference is actually performed correctly, but we may want to double check and clean up these warnings. We should probably just copy the non-exposed values from the model config on the python side when running GermlineCNVCaller in case mode. Not sure if there's any way to emit sensible warnings on the Java side. These hyperparameters are still exposed to the Java command line in case mode, they just aren't passed on to the python command line. So the user can change their values from their engine defaults without having any effect at all, but this is probably what we want. Perhaps we can document, though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6994:85,config,configs,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6994,6,['config'],"['config', 'configs', 'configuration']"
Modifiability,"Preparation for some refactoring related to TileDB integration. Extracted classes are package-protected for now, since they are not intended for direct use outside of the engine package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1929:21,refactor,refactoring,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1929,1,['refactor'],['refactoring']
Modifiability,"Preparation for upcoming SAMRecord -> Read refactoring. Wanted to; separate out this trivial package rename, as it was cluttering; the diff on my main branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/352:43,refactor,refactoring,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/352,1,['refactor'],['refactoring']
Modifiability,Prevents a bug that occurs when a file path contains characters that are illegal in URIs. Specific example was when using `--tmp-dir file:///tmp/workflow#main` GATK would initially correctly interpret this as `/tmp/workflow#main` but then when setting the Java temp directory in `CommandLineProgram.java` line 164 it would send `/tmp/workflow#main` to `IOUtils.getAbsolutePathWithoutFileProtocol` which would then mangle it by turning it into a URI and then removing `file://` resulting in `/tmp/workflow%23main` which later causes issues when things like the Codecs attempt to write configuration files to the temp directory that doesn't exist. CWLTool often creates path names that contain `#` so workflows made by CWLTool and containing GATK can fail because of this bug.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6769:584,config,configuration,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6769,1,['config'],['configuration']
Modifiability,"Previously we had an issue where our travis builds would use the wrong; commit for the docker builds in the travis pull-request builds but not for the push; builds. This caused the tests from master to run and usually pass. However,; since we are mounting the test data from the correct commit into the; docker, this would result in confusing mismatches where old tests would; try to run on new test data. Fixing the problem by using the $TRAVIS_COMMIT environment variable; instead of the TRAVIS_BRANCH. fixes #3216",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3217:465,variab,variable,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3217,1,['variab'],['variable']
Modifiability,"Previously, a temporary table is created as part of extract of the VQSR features, and it goes into a separate `temp_tables` dataset in the current project -- that is no longer true, and it now goes into the default dataset as a short living temp table with the task name and a hash. This pr should:. - default to the current dataset (with the VET etc tables) rather than a different dataset. - give a prefix to the temp tables so we know which one came from which step. - temp table TTL---not a changeable option, but default to 24 hours across the board. Still to discuss:; Parameterization of the location (dataset) to create the temp table in (default to the default dataset); manual clean up/TTL is a changeable option and TTL is parametrizable (currently the TTL is a parameter for the prepare step -- but then we set a default as 24 hrs in the WDL) . ![Screen Shot 2022-06-10 at 1 27 58 PM](https://user-images.githubusercontent.com/6863459/173121781-4486c1d1-ef7a-4ab8-aa62-fdc5018fd3b9.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7742:575,Parameteriz,Parameterization,575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7742,1,['Parameteriz'],['Parameterization']
Modifiability,"Prior to assembly (in `AssemblyBasedCallerUtils.assembleReads`, we transform reads in several ways that are meant to be permanent (that is, we want to use them in both assembly and genotyping) within `finalizeRegion`. (Additionally, we error reads within `ReadThreadingAssembler.runLocalAssembly`, but this is done on temporary copies of reads that are used for kmers and discarded). These transformations include hard clipping low-quality ends, adaptor sequences, and, optionally, soft-clipped bases, as well as correcting the base qualities of overlapping mates. According to the git history, these transformations have been accidentally temporary for quite a while. Let's look at the relevant code. First, in `Mutect2Engine.callRegion` we have (comments added and code simplified for clarity). ```; final AssemblyRegion assemblyActiveRegion = AssemblyBasedCallerUtils.assemblyRegionWithWellMappedReads(originalAssemblyRegion . . .);. // assembleReads finalizes region, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:446,adapt,adaptor,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['adapt'],['adaptor']
Modifiability,Probably git. See `core.autocrlf` at https://git-scm.com/book/id/v2/Customizing-Git-Git-Configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431474812:88,Config,Configuration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-431474812,1,['Config'],['Configuration']
Modifiability,"Problem was ocurring in the presence of insertions and deletions. fixes #6139. 1. Changed ReadClipper unit tests:; - The tests in many cases assumed that the unclipped alignment locations do not change when the read is clipped. This is not true: for example if start for cigar; 1M1I3M is 100, the unclipped start for 2H3M is 99. All assertUnclipped calls were; removed; - Alignment now check that the read length remains to be consistent with the CIGAR, that the aligned bases span are consistent with the CIGAR and that the number of clipped bases from the read is consistent with the requested clipping; 2. Hard clipping in ClippingOps was buggy, thus we introduced new tests for it.; 3. Text was refactored for readability; 4. Clipping in ClippingOps did not treat insertions and deletions in the clipped parts of the CIGAR correctly. This was fixed; 5. Alignment re-calculation after clipping did not work correctly if the initial CIGAR contained insertions and deletions; 6. Hard clipping applied to the hard clipped read did not behave correctly",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6280:699,refactor,refactored,699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6280,1,['refactor'],['refactored']
Modifiability,"ProgressMeter: allow labels to be configurable per-traversal/tool, and hook up to GenomicsDBImport",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2690:34,config,configurable,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690,1,['config'],['configurable']
Modifiability,Promote gradle build change that helps with certain spark config errors during build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:58,config,config,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,1,['config'],['config']
Modifiability,"Propose to reduce redundantly cracking open a path/stream to discover the correct feature codec. We do this twice for each feature input, which for multi-variant walkers with large # of inputs can be a lot. This caches the codec class in a FeatureInout the first time we find it. Ideally FeatureManager would remember it, but not all of the FeatureDataSources are created by Feature Manager (and fixing that is a bigger refactoring).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2740:420,refactor,refactoring,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2740,1,['refactor'],['refactoring']
Modifiability,"Prototype a PythonScriptExecutor, and assess maintainability of an example tool that calls into a Python machine-learning library",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:45,maintainab,maintainability,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,2,['maintainab'],['maintainability']
Modifiability,Qual = 0 sites don't count as polymorphic for GVCF mode,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4967:30,polymorphi,polymorphic,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4967,1,['polymorphi'],['polymorphic']
Modifiability,"READER_FACTORY :; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4418,Config,ConfigFactory,4418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,REATE_MD5 : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:4785,Config,ConfigFactory,4785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4245,Config,ConfigFactory,4245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"RENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3844,Config,ConfigFactory,3844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"RROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.mis",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5850,variab,variable,5850,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,"RY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:3867,Config,ConfigFactory,3867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"R_FACTORY :; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3457,Config,ConfigFactory,3457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"R_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3974,Config,ConfigFactory,3974,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"RankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz --use-allele-specific-annotations`. #### Error Message; ```; Using GATK jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms24g -jar ~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCred",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:2185,polymorphi,polymorphic,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['polymorphi'],['polymorphic']
Modifiability,"Read counts at different stages of the PathSeq pipeline are now logged using `MetricsFile`. The filter metrics contains the number of reads remaining and number of reads filtered at each step (after filtering pre-aligned reads, low quality/complexity reads, host reads, and duplicates). The score metrics give number of pathogen-mapped and unmapped reads. These metrics are now validated in the PathSeq integration tests, which have also been refactored to use DataProviders instead of separate functions.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611:443,refactor,refactored,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611,1,['refactor'],['refactored']
Modifiability,ReadFilter plugin tests does not live in the same package than the plugin itself,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2532:11,plugin,plugin,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532,2,['plugin'],['plugin']
Modifiability,"Really we need some tests for gs:// files in ReadsSparkSinkUnitTest - e.g. a GCS version of testWritingToFileURL. This needs knowledge of how to configure the Hadoop GCS connector (outside dataproc), which I lack. Perhaps someone else knows how to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-270615942:145,config,configure,145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-270615942,1,['config'],['configure']
Modifiability,RecalibrationArgumentColleciton arguments need to be refactored to new standard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3974:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3974,1,['refactor'],['refactored']
Modifiability,"RecalibratorEngine.java:43); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibrator.onTraversalSuccess(VariantRecalibrator.java:625); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:895); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. I believe this is derived from an error earlier in the log, since the `stderr` gives the same Java heap space error: ; ```; [2019-09-16 19:05:59,50] [error] WorkflowManagerActor Workflow 9f7a01a4-0632-4817-8622-aa51e520abf1 failed (during ExecutingWorkflowState): Job JointGenotyping.SNPsVariantRecalibratorClassic:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /path/to/stderr.; ```. I have read past issues (https://gatkforums.broadinstitute.org/gatk/discussion/23880/java-heap-space) regarding this that may suggest it is a bug. It has pointed me to increasing the available heap memory through the primary command of -Xmx. Is this the way to do it? ; ```; java -Xmx600G -Dconfig.file=' + re.sub('input.json', 'overrides.conf', input_json) + ' -jar ' + args.cromwell_path + ' run ' + re.sub('input.json', 'joint-discovery-gatk4.wdl', input_json) + ' -i ' + input_json; ```; where I substitute in the corresponding config, json, and wdl files. . Is 600G enough? Each vcf is around 6G large and since I have 150, does that mean I should be allocating more than 900G (6G x 150)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6165:2809,config,config,2809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6165,1,['config'],['config']
Modifiability,Recent refactoring seems to have introduced a bug in pileup mode that failed to enforce the limit on the number of haplotypes to be considered. With this patch:. - HaplotypeCaller once again respects the limit on haplotypes before genotyping.; - Changed some `HashSet`s to `LinkedHashSets` to preserve determinism.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8489:7,refactor,refactoring,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8489,1,['refactor'],['refactoring']
Modifiability,Refactor *Context classes to return empty Collections/iterators when there is no backing data source,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/249:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/249,1,['Refactor'],['Refactor']
Modifiability,Refactor AlleleListUtilsUnitTest to have no skips and be robustly deterministic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,1,['Refactor'],['Refactor']
Modifiability,Refactor ArtificialReadUtils to make it easier to select backing implementation (SAMRecord vs. Google Read),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/641:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/641,1,['Refactor'],['Refactor']
Modifiability,Refactor CNV collection classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3976:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3976,1,['Refactor'],['Refactor']
Modifiability,"Refactor Dataflow transforms by ""top level"" transform",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/651:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/651,1,['Refactor'],['Refactor']
Modifiability,"Refactor Dataflow transforms by ""top leveltransform",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/649:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/649,1,['Refactor'],['Refactor']
Modifiability,"Refactor Funcotator scripts to take arguments, not internal configurations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5346:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5346,2,"['Refactor', 'config']","['Refactor', 'configurations']"
Modifiability,Refactor GATKTool so that more tools can comfortably extend it directly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341,2,"['Refactor', 'extend']","['Refactor', 'extend']"
Modifiability,Refactor GATKVariantContextUtils' variant context merging methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/132:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/132,1,['Refactor'],['Refactor']
Modifiability,Refactor GVCFWriter to allow push/pull iteration.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5311:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5311,1,['Refactor'],['Refactor']
Modifiability,Refactor GenomeLocParser and GenomeLoc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100,1,['Refactor'],['Refactor']
Modifiability,Refactor MT wdl to make validations easier,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5708:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708,1,['Refactor'],['Refactor']
Modifiability,Refactor ReadsSparkSource to accommodate non-sorting bam output,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4818:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4818,1,['Refactor'],['Refactor']
Modifiability,Refactor VariantEvalUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5441:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5441,1,['Refactor'],['Refactor']
Modifiability,Refactor VcfFuncotationFactoryCache to go into any funcotation factory that does not need Gencode/Transcript Funcotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4974:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974,1,['Refactor'],['Refactor']
Modifiability,Refactor WGS and WES coverage collection to be more analogous.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2964:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2964,1,['Refactor'],['Refactor']
Modifiability,Refactor WGS coverage collection and GC annotation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3153:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3153,1,['Refactor'],['Refactor']
Modifiability,Refactor `CallVariantsFromAlignedContigsSpark` to prep for calling SV.INS & SV.DEL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2258:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258,1,['Refactor'],['Refactor']
Modifiability,"Refactor backend LocusWalker iterator (currently LocusIteratorByState) to support emitting uncovered/""empty"" loci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2678:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2678,1,['Refactor'],['Refactor']
Modifiability,Refactor com.intel.genomicsdb package references to org.genomicsdb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,1,['Refactor'],['Refactor']
Modifiability,Refactor control flow in ModelSegments and gCNV CLIs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3951:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3951,1,['Refactor'],['Refactor']
Modifiability,Refactor dockerfiles to reduce docker layer count,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8686:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8686,1,['Refactor'],['Refactor']
Modifiability,"Refactor exceptions: add UserException, remove ReviewedHellbenderException, rename HellbenderException to GATKException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/85:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/85,1,['Refactor'],['Refactor']
Modifiability,Refactor gatk-launch to use argparse,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1330:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1330,1,['Refactor'],['Refactor']
Modifiability,Refactor het genotyping code in ModelSegments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915,1,['Refactor'],['Refactor']
Modifiability,"Refactor python code from extract dir into a scripts directory. Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f85602d0-6dc5-49d6-82d1-eb58e9966021); Passing VAT Creation work [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/ddc7fcf9-5fb7-44e2-8117-721389d4f858), [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/0d705f21-3362-4890-b925-5bed2646fe4d), and [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/15cfe125-e700-44c8-b9d0-c3e98d7db4c0)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9017:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9017,1,['Refactor'],['Refactor']
Modifiability,Refactor segment classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2836:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2836,1,['Refactor'],['Refactor']
Modifiability,Refactor the docker image to correspond to the minimum necessary for user execution,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3930:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3930,1,['Refactor'],['Refactor']
Modifiability,Refactor the test suite on Github Actions to run faster,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7798:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7798,1,['Refactor'],['Refactor']
Modifiability,Refactor to put 'Lite' functionality into ExtractCohort.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8295:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8295,1,['Refactor'],['Refactor']
Modifiability,Refactor two VariantEval methods to allow subclasses to override,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998,1,['Refactor'],['Refactor']
Modifiability,Refactor/improve allele-specific annotation reduce interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3293:0,Refactor,Refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3293,1,['Refactor'],['Refactor']
Modifiability,Refactored CalcNIndelInformativeReads() use dynamic programming and cached results,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5607:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607,1,['Refactor'],['Refactored']
Modifiability,Refactored JointVcfFiltering WDL and expanded tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074,1,['Refactor'],['Refactored']
Modifiability,Refactored and enhanced ArgumentsBuilder,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6474:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6474,2,"['Refactor', 'enhance']","['Refactored', 'enhanced']"
Modifiability,Refactored the docker build script to only only include the gatk bundle in order to shrink the docker image size,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955:0,Refactor,Refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955,1,['Refactor'],['Refactored']
Modifiability,Refactoring / housekeeping Mutect2IntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6184:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6184,1,['Refactor'],['Refactoring']
Modifiability,Refactoring a confusing method in GATKVariantContextUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8690:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8690,1,['Refactor'],['Refactoring']
Modifiability,Refactoring gCNV WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5176:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5176,1,['Refactor'],['Refactoring']
Modifiability,Refactoring of ModelSegments Segmenter and Modeller backend classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625,1,['Refactor'],['Refactoring']
Modifiability,"Refactoring of the structs and utilities involved calling simple inversions.; This helps preparing for calling simple insertions and deletions in SV.; Most changes are simple changes, no changes are made to the algorithm itself.; A simple fix of orginal code in `AlignmentRegion` was put in.; Major re-engineering of `getVariantContextForBreakpointAlleleAlignmentList()` in caller was done and explained in the temporary comment that will be removed after review is done. @cwhelan would you please review? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2258:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2258,1,['Refactor'],['Refactoring']
Modifiability,Refactoring of variant context and genotype comparison code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6417:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6417,1,['Refactor'],['Refactoring']
Modifiability,Refactoring read orientation model to run within Mutect2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5840:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840,1,['Refactor'],['Refactoring']
Modifiability,Refactoring windows and padding for assembly and genotyping,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6358:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6358,1,['Refactor'],['Refactoring']
Modifiability,Refactoring won't happen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6877#issuecomment-1377803387:0,Refactor,Refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6877#issuecomment-1377803387,1,['Refactor'],['Refactoring']
Modifiability,ReferenceDependentFeatureCodecs are broken and need to be refactored,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/234:58,refactor,refactored,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/234,1,['refactor'],['refactored']
Modifiability,Remove hardcoded system properties from gatk frontend script that collide with those in the GATK config file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4484:97,config,config,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4484,1,['config'],['config']
Modifiability,Repackage ReadFilter plugin tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3525:21,plugin,plugin,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3525,1,['plugin'],['plugin']
Modifiability,Replace literal arguments with variables in several integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4416:31,variab,variables,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4416,1,['variab'],['variables']
Modifiability,Request: fine-grained configuration for codec packages for downstream projects,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:22,config,configuration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,1,['config'],['configuration']
Modifiability,Request: read filter plugin method for get a list with default filters and CMD instances,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362:21,plugin,plugin,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362,1,['plugin'],['plugin']
Modifiability,Resolved the issue of adding gs:// to the beginning and / to the end of the environment variable GATK_GCS_STAGING. Tested it locally.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5452:88,variab,variable,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5452,1,['variab'],['variable']
Modifiability,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:257,config,configuration,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890,2,['config'],['configuration']
Modifiability,"Reviving this. This will essentially be a major refactor/rewrite of CreatePanelOfNormals to make it scalable enough to handle WGS. - [x] CombineReadCounts is too cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:48,refactor,refactor,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,2,"['refactor', 'rewrite']","['refactor', 'rewrite']"
Modifiability,"Reworks classes used by `JointGermlineCNVSegmentationIntegration` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally. A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentationIntegration`. This is the first of a series of PRs for an experimental Java-based implementation of some modules in `gatk-sv` pipeline, including SV vcf merging, clustering, evidence aggregation, and genotyping.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243:388,enhance,enhanced,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243,1,['enhance'],['enhanced']
Modifiability,Rewrite complex SV functional annotation in SVAnnotate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8516:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite detection of interval file types,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/167:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/167,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite haplotype construction methods in PDHapComputationEngine,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8367:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8367,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite of the FilterVariantTranches tool without python dependencies. Uses @takutosato's shiny new TwoPassVariantWalker. @takutosato or @cmnbroad care to review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4800:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800,1,['Rewrite'],['Rewrite']
Modifiability,Rewrite strand artifact docs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4477:0,Rewrite,Rewrite,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4477,1,['Rewrite'],['Rewrite']
Modifiability,"Right now our docker image is much larger than it needs to be. This is at least in part because it contains our entire git clone as well as the packaged jars. This is not necessary and could potentially come with it shrinking our docker image substantially. . This change would involve a major refactoring of how we execute our tests through gralde inside the docker image, as removing the test dependencies will mean we probably have to externally mount the git clone from the docker image in order to pull in the proper dependencies.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3930:294,refactor,refactoring,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3930,1,['refactor'],['refactoring']
Modifiability,"Right now the docker image is too large. It appears that the if the build_docker.sh script its set to run the tests, then it is uploading the test resources into the docker and removing them, resulting in multiple layers which uses unneeded space. We need to remove the resources files from the docker.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3414:214,layers,layers,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3414,1,['layers'],['layers']
Modifiability,"Right now, we pair reads with overlapping variants. We can generalize this for any two PCollections of classes that extend Locatable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/663:116,extend,extend,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/663,1,['extend'],['extend']
Modifiability,"Right now, when we create a bucket-based test, we upload files into `gs://hellbender/test/resources/`, which Travis uses for `HELLBENDER_TEST_INPUTS` (the environment variable used by dataflow tests). These files are currently unversioned, which is bad -- we need to come up with a better way of managing our dataflow test inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/739:167,variab,variable,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/739,1,['variab'],['variable']
Modifiability,"Right, the `try` block needs to catch the `java.lang.UnsatisfiedLinkError` exception. We'll fix that in the next GKL release. As a workaround, you can try defining this environment variable: `export GKL_USE_LIB_PATH=1`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265859639:181,variab,variable,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265859639,1,['variab'],['variable']
Modifiability,"Right. I want to subset by sample name, effectively taking a slice of the; position by sample genotype matrix and computing Info annotations based; only in the kept samples. On Mon, Mar 4, 2019, 8:52 PM Karthik Gururaj <notifications@github.com>; wrote:. > I'm assuming you will have the subset of samples before creating a; > GenomicsDBFeatureReader object (and before creating the corresponding; > Protobuf export configuration object).; >; > More precisely, you are NOT requesting a line by line filter similar to:; > At pos 100, compute INFO fields etc including only the samples whose QUAL; > > 5; > At pos 102, compute INFO fields etc including only the samples whose QUAL; > > 5; > ....; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469502322>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMZjIbDJ2eDZcB69XHiUycnumzHrks5vTc3PgaJpZM4Z7pF2>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469680991:416,config,configuration,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5570#issuecomment-469680991,1,['config'],['configuration']
Modifiability,Run the GATK tests using the refactored rans code in htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8730:29,refactor,refactored,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8730,1,['refactor'],['refactored']
Modifiability,"Running a particular bam sort takes ~20minutes with hdd and 16 minutes with ssd. So it's definitely being used somehow. It looks like spark.local.dir is over ridden by the environment variable LOCAL_DIRS, and I don't see that set, but it's possible it's being set but not recorded correctly in the UI or something like that. Someone will need to poke at a bit more to be more clear about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370:184,variab,variable,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283481370,2,['variab'],['variable']
Modifiability,"Running:; spark-submit --master yarn-client --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar PrintReadsSpark -I /gatk4/output.bam -O /gatk4/output_2.bam --sparkMaster yarn-client; 14:19:09.870 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:19:10.155 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 11, 2017 2:19:10 PM CST] PrintReadsSpark --output /gatk4/output_2.bam --input /gatk4/output.bam --sparkMaster yarn-client --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 11, 2017 2:19:10 PM CST] Executing as hdfs@mg on Linux 3.10.0-514.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:1261,variab,variables,1261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4527,Config,ConfigFactory,4527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,SSION_LEVEL : 2; 11:35:40.188 INFO Mutect2 - HTSJDK Defaults.CREATE_INDEX : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3286,Config,ConfigFactory,3286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,Saw it again [here](https://travis-ci.com/broadinstitute/gatk/jobs/180435353) (now restarted). If I'm reading the serialization stack in the right order:. ```; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); ```. it looks like we're trying to serialize a ClassLoader. The FieldSerializer does appear to use a ClassLoader to load classes during serialization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740:262,Config,Configuration,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740,1,['Config'],['Configuration']
Modifiability,"ScoreVariantAnnotations:. Scores variant calls in a VCF file based on site-level annotations using a previously trained model. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use trained models that contain both SNP and INDEL scorers as input) ; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Double check or add behavior for handling previously filtered input, clearing present filters, etc. Future work:. - [ ] The `score_samples` method of the sklearn IsolationForest is single-threaded. See (possibly stalled) PR at https://github.com/scikit-learn/scikit-learn/pull/14001 and some workarounds using e.g. `multiprocessing` ibid.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563:199,config,configurations,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948563,2,['config'],['configurations']
Modifiability,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5587:160,refactor,refactoring,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587,2,['refactor'],['refactoring']
Modifiability,"See https://github.com/broadinstitute/gatk/issues/4888, which is an older report for the same issue. As mentioned there, I think we should patch our fork of `google-cloud-java` to do a channel reopen on `UnknownHostException` for now as a quick fix. @jean-philippe-martin is eventually going to add an official configuration mechanism for clients of `google-cloud-java` to customize which errors should trigger a retry/reopen, which should provide a better way to deal with these errors as they crop up without having to modify the NIO library itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420:311,config,configuration,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412134420,1,['config'],['configuration']
Modifiability,See individual commit messages; - fix median bug; - parameterized sample_list and enable subsetting; - support pre-query bytes processed estimate,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6374:52,parameteriz,parameterized,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6374,1,['parameteriz'],['parameterized']
Modifiability,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:144,rewrite,rewrite,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,2,['rewrite'],['rewrite']
Modifiability,SelectVariants JEXL filter fixes and refactor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092:37,refactor,refactor,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092,1,['refactor'],['refactor']
Modifiability,Set gcloud config directory in travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7525:11,config,config,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7525,1,['config'],['config']
Modifiability,"Several backwards-incompatible changes in VCF 4.3 (eg., escape sequences) have made it difficult to update without first doing a major refactoring in HTSJDK to better version/isolate our parsers. @cmnbroad can provide further details.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-471719969:135,refactor,refactoring,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-471719969,1,['refactor'],['refactoring']
Modifiability,Several improvements to SV contig alignment configuration picker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326:44,config,configuration,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326,1,['config'],['configuration']
Modifiability,Should we refactor datasources to have two separate class hierarchies for segments vs. small mutations?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5961:10,refactor,refactor,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5961,1,['refactor'],['refactor']
Modifiability,SimpleAnnotatedGenomicRegion refactoring to use Tribble,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3738:29,refactor,refactoring,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3738,1,['refactor'],['refactoring']
Modifiability,"Since Geraldine is away till the end of the week, and we are under the Nov 23 deadline for review, I will proceed with changes. I think it useful for me to go through the motions and see what other discussion items turn up. Notes on factors I think are of interest to users re annotators:; - cohort vs sample level annotation; - InfoFieldAnnotation; - GenotypeAnnotation; - minimum number of samples, e.g. 10 for inbreedingcoefficient; - standard annotations for each tool (HC, M2 and VariantAnnotator), standard allele-specific annotations.; - StandardMutectAnnotation; - PerAlleleAnnotation; - StandardAnnotation (extends Annotation); - StandardHCAnnotation; - VariantAnnotation; - noticing `public class` vs `public final class`. Not annotating `abstract` class nor `public interface`.; - What is a reducible annotation?; - I would really find helpful the acronym for the annotation, e.g. MBQ, be listed with the annotation summary, e.g. Median base quality of bases supporting each allele.; - Annotations that are specific to a tool. E.g. DepthPerSampleHC can only be used by HaplotypeCaller and not VariantAnnotator. Doc doesn't say anything about Mutect2. ; - Not sure what VariantOverlapAnnotator does ~~but went ahead and summarized as ""Annotate ID field and attribute overlap FLAG"".~~ `did not tag`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344427246:616,extend,extends,616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344427246,1,['extend'],['extends']
Modifiability,"Since we are going to change many of those argument names (camel-back to kebab-case) I think we should take this opportunity to use constants to specify argument names in the code and use them in our test code so further changes in argument names don't break tests. . Take as an example [CombineReadCounts](https://github.com/broadinstitute/gatk/blob/3ec7399a54ccf89d2b323b2be71b8b7e4931174c/src/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java). Extract enclosed below. It might be also beneficial to add public constant for the default values. ```java; public final class CombineReadCounts extends CommandLineProgram {. public static final String READ_COUNT_FILES_SHORT_NAME = StandardArgumentDefinitions.INPUT_SHORT_NAME;; public static final String READ_COUNT_FILES_FULL_NAME = StandardArgumentDefinitions.INPUT_LONG_NAME;; public static final String READ_COUNT_FILE_LIST_SHORT_NAME = ""inputList"";; public static final String READ_COUNT_FILE_LIST_FULL_NAME = READ_COUNT_FILE_LIST_SHORT_NAME;; public static final String MAX_GROUP_SIZE_SHORT_NAME = ""MOF"";; public static final String MAX_GROUP_SIZE_FULL_NAME = ""maxOpenFiles"";; public static final int DEFAULT_MAX_GROUP_SIZE = 100;. @Argument(; doc = ""Coverage files to combine, they must contain all the targets in the input file ("" +; TargetArgumentCollection.TARGET_FILE_LONG_NAME + "") and in the same order"",; shortName = READ_COUNT_FILE_LIST_SHORT_NAME,; fullName = READ_COUNT_FILE_LIST_FULL_NAME,; optional = true; ); protected File coverageFileList;. @Argument(; doc = READ_COUNT_FILES_DOCUMENTATION,; shortName = READ_COUNT_FILES_SHORT_NAME,; fullName = READ_COUNT_FILES_FULL_NAME,; optional = true; ); protected List<File> coverageFiles = new ArrayList<>();. @Argument(; doc = ""Maximum number of files to combine simultaneously."",; shortName = MAX_GROUP_SIZE_SHORT_NAME,; fullName = MAX_GROUP_SIZE_FULL_NAME,; optional = false; ); protected int maxMergeSize = DEFAULT_MAX_GROUP_SIZE;. @ArgumentCollection; protect",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346175904:618,extend,extends,618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346175904,1,['extend'],['extends']
Modifiability,"Since we're using Mutect2 for mitochondrial calling, we want some reference confidence representation for joint calling. I did my best, but further refactoring suggestions appreciated. Tests to follow. @davidbenjamin can you take a look at the LODs in the integration test results? I'm not entirely surprised that at the same depth, the variant LOD is higher than the reference LOD. I'm not sure that the NON_REF LOD at variant sites is coming out right though. Is there an effective negative LOD asymptote?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312:148,refactor,refactoring,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312,1,['refactor'],['refactoring']
Modifiability,"Size=0,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2 --jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar -- CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn; Job [acdae2af-e0ce-4822-87f5-dcd165d85cf4] submitted.; Waiting for job output...; 20:39:42.869 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:39:43.053 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/acdae2af-e0ce-4822-87f5-dcd165d85cf4/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [November 27, 2017 8:39:43 PM UTC] CountReadsSpark --input gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [November 27, 2017 8:39:43 PM UTC] Executing as root@droazen-test-cluster-m on Linux 3.16.0-4-amd64",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:2637,variab,variables,2637,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"So I just updated one of the newer tests, and now all of the tests for HaplotypeCaller seem to be passing locally. The previous commits updating the copy code were preserved when Louis reverted, so there were basically no changes I had to make to get this ""working."" That does leave us with one question now:. When looking into this a little with James and Louis earlier, we realized that the code for setting up the ActiveRegionGenotyper uses a weird partial copy of the standard CLI args method that has existed in the code for whoever knows how long. Conceptually this seems like a bad idea, but changing it now would possibly cause some older tests to fail, if they were based on this faulty method reasoning. Should we try to merge the PR as it is now, with all tests passing, and hopefully consistency with previous behavior, or try to update the logic around this genotyper as well at the same time? It's possible we can try to address the latter point as well at some point in the future when we try to get Louis's refactor code actually working. Maybe there could be some quarter goal around a HaplotypeCaller code revamp sometime inspired by some of these ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216:1023,refactor,refactor,1023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8609#issuecomment-1847916216,1,['refactor'],['refactor']
Modifiability,So we typically override the config files on the command line. We'll have to make sure we wire the log4j 1.x logger to respect our command line overrides if it doesn't already. You can check that by testing if you can control the log output with the --verbosity command. If not we'll have to update `LoggingUtils.setLoggingLevel()`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320787794:29,config,config,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416#issuecomment-320787794,1,['config'],['config']
Modifiability,"Solves to some extend #2362, by adding a new getter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2364:15,extend,extend,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2364,1,['extend'],['extend']
Modifiability,"Some changes after the comments from @droazen (in commit #2360):; * Change the argument name from `--disableAllReadFilters` to `--disableToolDefaultReadFilters`; * Make `--disableToolDefaultReadFilters` mutex w.r.t. `--disableReadFilter`; * As pointed out in a different PR (#2355), make all the plugin arguments common.; * Make `isDisabled()` and `getAllInstances()` honor the `--disableToolDefaultReadFilters` argument (solves #2363). I think that this makes more sense than disable absolutely all the read filters, including the ones provided by the user. The cases where this is more useful are:; - Process the data without filters: provide just `--disableToolDefaultReadFilters`; - Process the data without default filters and add any other: provide `--disableToolDefaultReadFilters` and `--readFilter` with the rest of filters; - Process the data with the default filters and include more: as in the previous behaviour, provide `--readFilter` with the rest of filters; - Process the data with some default filters but change the order: provide `--disableToolDefaultReadFilters` and the list of filters in the new order. . Can you have a look to this one, @cmnbroad and/or @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2365#issuecomment-275633852:296,plugin,plugin,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2365#issuecomment-275633852,1,['plugin'],['plugin']
Modifiability,"Some comments/questions for the review:; - I'll add a separate ticket to rewrite the integration tests, all of which pass and most of which are disabled since they require access to large files on the broad file system. In the meantime I need to add a couple of small tests to get the coverage back up, and would like to get the CR process started.; - I ported a bunch of support files but need feedback on whether they're in the right location.; - Somewhere I saw something that said GATK no longer supports .ped files ? If not, what should the replacement be in the tests require pedigree input?; - Is it a requirement to support Ploidy > 2 ? The current GATK tool, and thus the HB tool, do not; - I did not port the WalkerTestSpec.disableShadowVCF? Is that needed in Hellbender ?; - Are there other headers I should be applying to the output variant file ?. Command Line Arguments:; - I didn't port the GATK command line argument ""-no_cmd_line_in_header"". Should I ? And if not, should the command line args automatically be propagated to the output vcf file ? I didn't see GATK do this anywhere.; - There was one test that used --variant:dbsnp on the command line but I couldn't find the code that processed that in GATK, not sure what the means on the command line.; - I replaced ""-U LENIENT_VCF_PROCESSING"" with ""--lenient"" (testFileWithoutInfoLineInHeaderWithOverride needs this to pass).; - I replaced ""-L"" with --interval since HB seems to use -L for ""lane"" ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027:73,rewrite,rewrite,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/792#issuecomment-128798027,1,['rewrite'],['rewrite']
Modifiability,Some info on Spark configurations:. https://stackoverflow.com/questions/29441316/specifying-an-external-configuration-file-for-apache-spark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322552565:19,config,configurations,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3079#issuecomment-322552565,2,['config'],"['configuration-file-for-apache-spark', 'configurations']"
Modifiability,"Some offline discussions have led us to the conclusion that this is best handled by tools upstream. Adapters should not be simply soft-clipped, so it shouldn't be the responsibility of M2 or HC to include logic to remove adapters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816:100,Adapt,Adapters,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346#issuecomment-575334816,3,"['Adapt', 'adapt']","['Adapters', 'adapters']"
Modifiability,"Some questions before this is code reviewed in detail:. 1) A number of query methods in GoogleGenomicsReadAdapter adapter; throw if the corresponding field is not present in the underlying read. For some; of these there are guard methods you can call to avoid this (see for example; the changes in ReadUtils.java), but for some of the others I'm not sure how to; usefully query the state without already knowing the answer, ie.:. -isSupplementaryAlignment; -isSecondaryAlignment,; -failsVendorQualityCheck; -isDuplicate; -mateIsReverseStrand. To have fidelity with SAMRecord.getSAMString , we need to be able to query these; (as does ReadUtils.getFlags, which has a similar problem, but I changed that to; use guard methods to prevent throwing). In a couple of cases I had to change; the Read adapter to not throw. We need to figure out if this kind; of change is ok. or what the alternative is. 2) This is incidental to this PR, but there are a few inconsistencies between how; GenomicsConverter.makeSAMRecord and ReadUtils compute derived state values, ie. flags.; I can work around these in the getSAMString tests (I'm using Read->SAMRecord; conversions to validate the tests), but the underlying format conversions; are inconsistent. Should we align them ?. For example, GenomicsConverter sets the firstInPair flag on the SAMRecord if readNumber==0,; even if numberOfReads==1, whereas the ReadUtils/GoogleReadAdapter requires readNumber==0; and numberOfReads==2. Likewise the unmapped flag is determined differently: Genomics converter: (http://google-genomics.readthedocs.org/en/latest/migrating_tips.html):; final boolean unmapped = (read.getAlignment() == null || ; read.getAlignment().getPosition() == null || ; read.getAlignment().getPosition().getPosition() == null);; ReadUtils:; private boolean positionIsUnmapped( final Position position ) {; return position == null ||; position.getReferenceName() == null || position.getReferenceName().equals(SAMRecord.NO_ALIGNMENT_REFERENCE_NAME) ||; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/871:114,adapt,adapter,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/871,2,['adapt'],['adapter']
Modifiability,"Some read tags get lost when we convert SAM to fastq. This tool allows us to get those tags back once we are done processing the fastqs (some tools e.g. adapter clippers cannot take SAMs as input so the conversion is unavoidable.) So this tool works like Picard MergeBamAlignment, except that we are putting the tags from the unaligned bam to the aligned bam, rather than adding alignment info to the unaligned bam. We will use this in our new TCap RNA pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7739:153,adapt,adapter,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739,1,['adapt'],['adapter']
Modifiability,Some refactoring of where the main WDLs live. Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/009b92ea-9b51-4ebe-8ddd-924c53f28a55).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8970:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8970,1,['refactor'],['refactoring']
Modifiability,"Some travis jobs are still failing even with the reduced set of CRAN mirrors. This is a possible alternative solution that restores the previous set of fallback repos, but relaxes the treatment of remote warnings. Might be overkill but it seems to work. See `R_REMOTES_NO_ERRORS_FROM_WARNINGS ` under https://github.com/r-lib/remotes#environment-variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5602:346,variab,variables,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602,1,['variab'],['variables']
Modifiability,"Sorry @droazen @LeeTL1220, can you give me a bit more context? @LeeTL1220 is no longer using any of the CNV-specific collections classes that I had hoped might be Tribble-ized in the future, so I'm OK with any decisions you guys make that are specific to his classes (does @jonn-smith have an opinion?) I think that moving towards storing the config in the header is a good thing, in general. If we need to make corresponding changes to the CNV-specific collections classes, then we should talk more. Not all of those collections describe locatables, so I'm not sure how we could fit them in the Tribble framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330:343,config,config,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4276#issuecomment-369734330,1,['config'],['config']
Modifiability,"Sorry for generating a big one:. I've tried to put the relevant commits together. . 10cdeba is the biggest, but mostly refactoring the breakpoint and complication logic into 2 classes: `BreakpointsInference` and `BreakpointComplications`, which now both have class hierarchies. After that cleanup, in the following commits I've put all the efforts for inferring the alt haplotype sequence into the class `BreakpointsInference`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4305:119,refactor,refactoring,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4305,1,['refactor'],['refactoring']
Modifiability,"Sorry, again confounded by static-blocks and inheritance (dup of my own issue https://github.com/broadinstitute/gatk/issues/3483)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946:45,inherit,inheritance,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946,1,['inherit'],['inheritance']
Modifiability,"Sorry, but this bug still isn't fixed as of v4.2.6.1. Reproduce as follows:. ```; --read-filter MateDistantReadFilter; --mate-too-distant-length 1500; ```. Instead of a run-time exception (as in v4.2.5.0), HaplotypeCaller simply produces no variant calls at all. Expected behavior would be to exclude paired-end mappings whose TLEN exceeds the parameterized value. Perhaps there is an implementation bug, unrelated to the original problem, that contains faulty logic for doing this. Thanks...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692:344,parameteriz,parameterized,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1102943692,2,['parameteriz'],['parameterized']
Modifiability,"Sorry, it's difficult for me to spot git notifications in my email. . > Maybe @bshifaw can chime in? Are the featured workspaces covered by tests elsewhere? What is the current SOP for taking workflows from this repo, turning them into featured workspaces, and populating their configurations?. Example JSONs with input test data are usually introduced in the gatk-workflows git repos and carried over to the featured workspaces. That isn't to say they are not welcomed from the gatk repo. > @bshifaw related to what Sam was saying - we also have a few standard resources needed to run the workflows that we would like to share with users. What is the standard procedure for doing so? Ideally they would be bundled with featured workspaces, but also accessible from outside of Terra. Workflow resources files that are not already in [broad-references](https://console.cloud.google.com/storage/browser/broad-references) would be saved in the [gatk-best-practices](https://console.cloud.google.com/storage/browser/gatk-best-practices) bucket. In the past i've separated the resources files per workflow directory (e.g. pathseq, cnn-hg38) but you can organize them a different way if the resources files would be shared by other workflows (e.g. somatic-hg38, somatic-b37).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507703719:278,config,configurations,278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017#issuecomment-507703719,1,['config'],['configurations']
Modifiability,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:548,Config,Configured,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193,1,['Config'],['Configured']
Modifiability,"Sounds like a good idea, thanks! I will use a copy of the current plugin in my project til all these changes of the plugin descriptor are in (both Barclay and my proposals here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-275707975:66,plugin,plugin,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2362#issuecomment-275707975,2,['plugin'],['plugin']
Modifiability,Spark metrics collector refactoring checkpoint.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:24,refactor,refactoring,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,1,['refactor'],['refactoring']
Modifiability,Spark multilevel metrics collection refactoring.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1959:36,refactor,refactoring,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959,1,['refactor'],['refactoring']
Modifiability,"SparkCommandLineArgumentCollection does not support ""="" in the values of spark configuration variables",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3687:79,config,configuration,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3687,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,"Spin up a public jenkins server for long-running validation tests (and other tests that can't or shouldn't run in travis), or switch from travis to a more flexible CI provider",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1400:155,flexible,flexible,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400,1,['flexible'],['flexible']
Modifiability,SplitNCigar reads walker refactor 1864,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1919:25,refactor,refactor,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1919,1,['refactor'],['refactor']
Modifiability,Stack trace from failed job:. ```; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Null value when trying to read system resource. Cannot find: org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval/annotated_region_default.config; 	at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsFile(Resource.java:90); 	at org.broadinstitute.hellbender.utils.codecs.AnnotatedIntervalCodec.<init>(AnnotatedIntervalCodec.java:55); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.lang.Class.newInstance(Class.java:442); 	at org.broadinstitute.hellbender.engine.FeatureManager.getCandidateCodecsForFile(FeatureManager.java:511); 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:464); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:324); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:304); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.Abstra,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:259,config,config,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,1,['config'],['config']
Modifiability,Standardize on system properties vs. environment variables,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1666:49,variab,variables,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1666,1,['variab'],['variables']
Modifiability,"Stems from https://github.com/broadinstitute/gsa-unstable/issues/1406. Unless something changed in the port from GATK3 to GATK4, this is how pairs of overlapping mates are handled by ReadUtils when a tool seeks to determine adaptor boundaries:. <img src=""http://cd8ba0b44a15c10065fd-24461f391e20b7336331d5789078af53.r23.cf1.rackcdn.com/gatk.vanillaforums.com/FileUpload/41/48ae8ddb4ba74d5a02310b75135347.png"" align=""right"" height=""45""/> When inserts are small such that mapped mates overlap, we clip off the non-overlapping regions based on the assumption that they are adapter sequence. . @ldgauthier suggests that this is a dumb way to handle them because ""there will be cases where the reads overlap, but don't yet read into the adapter and we're throwing away data"". The task here is to propose and implement a better way to do this. If this code is no longer used in GATK4, please point out by what it has been replaced.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2238:224,adapt,adaptor,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2238,3,['adapt'],"['adapter', 'adaptor']"
Modifiability,Store the NCBI build version in the Gencode datasource config files; in order to resolve an issue where this value was not always available; when annotating IGR variants. Resolves #4404,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5522:55,config,config,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5522,1,['config'],['config']
Modifiability,"Subsampling seems to be the way to go, see #2858. For the record, I did try to implement caching, but this results in excessive cache checking. In general, I think a better solution is to structure code so that expensive global quantities are not unnecessarily recomputed locally. At some point, this sort of undesirable recomputation snuck in during a refactoring of the allele-fraction likelihood code, probably when we tried to make the method for computing site likelihoods pull double duty based on the presence or absence of an allelic PoN. With an allelic PoN, we need to compute a log gamma at each site based on the site-specific bias hyperparameters; without a PoN, we only need to do this once for all sites, since the bias hyperparameters are now global, but the code naively recomputes it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709:353,refactor,refactoring,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709,1,['refactor'],['refactoring']
Modifiability,"Substituting `STANDARD_CONFIDENCE_FOR_CALLING/3` for `STANDARD_CONFIDENCE_FOR_EMITTING` seems wrong, given that `STANDARD_CONFIDENCE_FOR_CALLING` is a user-configurable value. We talked to @vdauwera just now and she agrees -- we're going to close this PR here and open a ticket against GATK3 to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2194#issuecomment-259794842:156,config,configurable,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2194#issuecomment-259794842,1,['config'],['configurable']
Modifiability,"Sure thing. I can find out which changes I needed to make in gatk to get certain tools to work, like `PrintReads` and `MarkDuplicates`, though they were certainly not exhaustive. We will also see about open sourcing our s3 nio library which is basically a rewrite of https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 with changes for handling s3 endpoints.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319206378:256,rewrite,rewrite,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-319206378,1,['rewrite'],['rewrite']
Modifiability,"T : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - creat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4561,Config,ConfigFactory,4561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"TA : null; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5280,Config,ConfigFactory,5280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,TODO: refactor duplicated VC generation code from PosteriorProbabilitiesUtilsUnitTest in ReblockGVCFUnitTest by extracting to VariantContextTestUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932:6,refactor,refactor,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932,1,['refactor'],['refactor']
Modifiability,"TOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3473,Config,ConfigFactory,3473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"TSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3757,Config,ConfigFactory,3757,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,Test GATK4 with the existing S3 NIO plugin and get basic S3 read support working,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708:36,plugin,plugin,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708,1,['plugin'],['plugin']
Modifiability,"Testing the tool behavior when given an incomplete PED file. PED; <img width=""847"" alt=""screenshot 2019-01-22 16 42 07"" src=""https://user-images.githubusercontent.com/11543866/51567234-b42e3200-1e64-11e9-942c-2934980dc04a.png"">. Command; ```; gatk CalculateGenotypePosteriors \; -V precomputed/trioGGVCF.vcf.gz \; -ped duo.ped \; --skip-population-priors \; -O sandbox/duoCGP.vcf.gz; ```. Results; <img width=""842"" alt=""screenshot 2019-01-22 16 44 01"" src=""https://user-images.githubusercontent.com/11543866/51567337-ef306580-1e64-11e9-8ca6-051ccb3fa18d.png"">. The line of interest reads:; ```; 16:27:00.401 INFO CalculateGenotypePosteriors - No PED file passed or no *non-skipped* trios found in PED file. Skipping family priors.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-456575220:361,sandbox,sandbox,361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5409#issuecomment-456575220,1,['sandbox'],['sandbox']
Modifiability,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5847:414,polymorphi,polymorphic,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847,1,['polymorphi'],['polymorphic']
Modifiability,"Thank you @kshakir. What I see there is that the code sets the default NIO option, and as part of this is creates a google cloud `StorageOptions` object. Sadly for us, when this object is created it determines which Google credentials to use, and if nothing was specified by the user it will send some network messages to try to figure out whether it's running on a Google Compute Engine machine. When we wrote the default-setting code we didn't realize that setting the number of retries was going to cause a network message to be sent, with the associated potential retries and delays. We can't change the way Google Compute Engine works, or how the Google authentication works either. Ideally we'd want some way to only search for credentials when we know NIO is going to be used. The point of these defaults is that they're used for anything that uses NIO, including third-party library code. We can't fully replicate this behavior in a different way from the outside. So I think the ""correct"" fix would be to go deep inside the Google NIO library and change it so that instead of providing a default configuration (that the user would have to put together, causing the problem you've seen), we can provide a *callback* that sets the configuration when the Google Cloud NIO provider is loaded. This is harder for future developers to wrap their heads around, but at least it would prevent this delay if NIO is not used. I'd like to think about this some more before doing something quite this drastic, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504:1105,config,configuration,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504,2,['config'],['configuration']
Modifiability,"Thank you @mwalker174 for the suggestions. I ended up writing for loops to test which configurations work. Driver memory: 2-50g; executor memory: 2-50g; executor cores: 1-20; bamPartitionSize: 1-64m. Some combinations failed in minutes, some failed in hours, and some finished without errors. Bellow are three of which work for a ~33X WGS data:; ```; ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 20 ; --executor-memory 10g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 4000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.cores.max=600 ; --executor-cores 5 ; --executor-memory 50g ; --conf spark.driver.memory=50g. ../gatk-4.beta.1/gatk-launch BwaAndMarkDuplicatesPipelineSpark ; --bamPartitionSize 64000000 ; -I hdfs://bigdata/user/myname/gatk4test/wgs.sub4.unaligned.bam ; -O hdfs://bigdata/user/myname/gatk4test/wgs.sub4.BwaAndMarkDuplicatesPipelineSpark_out.bam ; -R hdfs://bigdata/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit ; --bwamemIndexImage /hadoop/myname/GRCh37.fa.img ; --disableSequenceDictionaryValidation ; -- --sparkRunner SPARK ; --sparkMaster spark://ln16:7077 ; --conf spark.core",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314:86,config,configurations,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313981314,1,['config'],['configurations']
Modifiability,"Thank you @vruano for your diligent review. I've implemented logger classes to encapsulate the metrics classes. Unfortunately the metrics classes must remain public in order to write output using `MetricsUtils.saveMetrics()`, but at least the tools aren't using them directly. There are two logging class groups - one for Filter and one Score. For Filter, there is an interface `PSFilterLogger` that is implemented by a file-logging class `PSFilterFileLogger` and a dummy class `PSFilterEmptyLogger` that does nothing. There are analogous classes for Score, but there is no Empty logger because it's not actually necessary. This adds a lot of new classes (maybe you can think of a better way) but usage has been greatly simplified. As we discussed in person, I don't think there is a faster way to count the reads in Spark. If you wanted to count the reads as they pass through, you would have to use some kind of atomic type that would be slow. Also it may be impossible to account for cases when tasks fail and restart. @lbergelson @droazen In this PR, I wanted to use htsjdk's MetricsFile and MetricBase classes for writing metrics to a file. I notice that these classes are mostly used for picard-related things. Is this the preferred way to do things? They do force you to expose public variables and also use an upper-case naming convention. On the other hand, they are somewhat convenient.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160:1292,variab,variables,1292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3611#issuecomment-334308160,2,['variab'],['variables']
Modifiability,Thank you for all the work on GATK4 and for including a wrapper script to help in setting up Java options. I've included GATK4 in bioconda (https://anaconda.org/bioconda/gatk4) with the `gatk-launch` wrapper and wanted a way to be able to pass java options to the local run. This PR uses the `GATK_JVM_OPTS` environmental variable to pass Java options like memory specification to the gatk-launch script. Thanks for considering this change,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2778:322,variab,variable,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2778,1,['variab'],['variable']
Modifiability,"Thanks @davidbenjamin for essentially refactoring this code three times now!. Looking forward to reviewing your latest changes, but it may have to wait until early next week. Apologies for the delay. In the meantime, I see that there is a minor rebase conflictâ€”up to you if you want to address it now, no biggie if you want to wait until after review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1070960072:38,refactor,refactoring,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1070960072,1,['refactor'],['refactoring']
Modifiability,"Thanks @droazen. Because you assigned it to me, I would like to know a couple of details on how this should be implemented in GATK4:. * GATK3 use to have a the `MisencodedBaseQualityReadTransformer` always on, with a switch for checking/fixing the qualities. If we follow this approach in GATK, the only change for this is to include the checking step every n reads and then #2160 will do the rest. Nevertheles, I think that it's quite dangerous to allow an user to disable it with the plugin (because the name suggest that it is only fixing the qualities), so I suggest to integrate in the read data source an iterator for checking every x reads if the qualities are misencoded, independently on the transformer.; * GATK3 throws an UserException for ""putatively misencoded"" qualities, using 60 as maximum base quality for throwing. I think that in the case of GATK4 could be more useful to use a warning if it is over 60 (I do not know what is the reasoning behind this value), and use `SAMUtils.MAX_PHRED_SCORE` for throwing. I'd be happy to implement this if there is a consensus about what to do here, so I'll wait for your ideas...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2082#issuecomment-288760814:486,plugin,plugin,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2082#issuecomment-288760814,1,['plugin'],['plugin']
Modifiability,"Thanks @lbergelson for looking into this. Users can definitely squash the image after pulling, and then push it to their private registries - that's the best workaround here, so this is likely a low-priority issue. Docker images can only be pulled by layers currently; there's no way to pull an image that has multiple layers with one HTTP request. In the [TES runner](https://github.com/microsoft/ga4gh-tes), we are also increasing the docker pull retry count to help. I'll try to update the `dockerfile` and send a PR, thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1935007776:251,layers,layers,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1935007776,2,['layers'],['layers']
Modifiability,"Thanks @lbergelson! I agree that it might be good to break into more layersâ€”could be worth talking to SV team and seeing what lessons they learned in putting together their hierarchy of images. Also, note that I pushed the install of miniconda into the base, but I did not push down the setup of the GATK conda environment itself (which takes the bulk of the time during the main-image build, as it requires lots of downloading). I think I commented elsewhere that a good strategy might be to set up the conda environment with the non-GATK python dependencies in the base, and then update the environment via a pip install of the GATK python packages in the main image. This would let us make python code changes without having to rebuild the base, but might require a bit of scripting to create a final yml for non-Docker users. I also agree that it would be nice to cut down the Travis time, might be worth taking a look at other strategies to do thatâ€”could save everyone a lot of time!. Will try to add the test you suggested sometime tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662:69,layers,layers,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-621487662,2,['layers'],['layers']
Modifiability,"Thanks @mwalker174! I think I responded to or addressed everything. The code paths for reading TSVs all go through the abstract CNV collection classes. Those require a bit of boilerplate, but were IMO a huge improvement over the horrowshow of utility methods from the old code... Happy to discuss possible further refactoring and improvement (and there are already catch-all issues open), if needed. If we decide to stream other locatable collections, we can start to extract more of these streaming/subsetting methods to `AbstractLocatableCollection`, which would give us something like the `LocatableTableReader` you're envisioning in your edit. We've discussed using @jonn-smith's `XSVLocatableTable` machinery as well. I think the only downsides are the conventional reliance on extensions/config files for decoding, as well as the need to accommodate CNV headers. Encoding is also not handled. We also still need to represent non-Locatable TSVs, ideally with a minimal number of code paths, although that probably won't present any major refactoring issues. Also recall that we discussed moving from Files -> Paths in previous PRs, so we should instead go from Files -> FeatureDataSources where it makes sense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770:314,refactor,refactoring,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6266#issuecomment-558720770,3,"['config', 'refactor']","['config', 'refactoring']"
Modifiability,"Thanks @ruqianl, you may want to read through the comments at https://github.com/broadinstitute/gatk/issues/6235 and the corresponding PR https://github.com/broadinstitute/gatk/pull/6244, which both address this issue. See also the following bit of documentation added in that PR:. > Advanced users may wish to set the THEANO_FLAGS environment variable to override the GATK theano configuration. For example, by running THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR"" gatk GermlineCNVCaller ..., users can specify the theano compilation directory (which is set to $HOME/.theano by default). See theano documentation at https://theano-pymc.readthedocs.io/en/latest/library/config.html. So you can specify a unique compilation directory for each of your jobs to avoid the compilelock, e.g., `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/0"" gatk GermlineCNVCaller ...`, `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/1"" gatk GermlineCNVCaller ...`, etc. Alternatively, you can increase `config.compile.timeout` as discussed in those comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899:344,variab,variable,344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899,4,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:226,config,config,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432,1,['config'],['config']
Modifiability,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:183,parameteriz,parameterized,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['parameteriz'],['parameterized']
Modifiability,"Thanks for indulging me on this. To me it seems like `UnfilledReadsLikelihoods` diverges too much from `ReadsLikelihoods` to extend it. In effect it's letting `ReadsLikelihoods` sometimes be a wrapper for something that is not a `ReadsLikelihoods`. I haven't worked this out but I would hope that it's possible to construct a `ReadsLikelihoods` from a pileup. I mean, the idea of pileup calling is that you use just a single base for the likelihoods and not the whole read (via Pair-HMM), so we should be able to fill the likelihoods from the base qualities.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856:125,extend,extend,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856,1,['extend'],['extend']
Modifiability,"Thanks for looking into this @davidbenjamin. I followed the best practices using bwa mem, mark duplicates etc., to create these input bams for HaplotypeCaller. This is Novaseq 2 x 150 data, I ran Fastqc on the reads and everything looks really good, the only thing I can find that might explain the soft-clipping is that there's some Nextera adapter read through on a small percentage of the reads. I haven't been using -Y with bwa (I see it's used in GATK 4 wdls), so it seems like there should be less soft-clipping than normal. I'll admit these are definitely messy regions we're dealing with, but we really need to make the F5 calls for our clinical pipeline. I just tried --dont-use-soft-clipped-bases and I wasn't able to pick the SNP up in the 55-55003_F5_region.bam, but using forceActive/dontTrimActiveRegions does work on this call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402690747:342,adapt,adapter,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-402690747,1,['adapt'],['adapter']
Modifiability,"Thanks for reporting this, @Stikus! That change you highlighted indeed fixes the issue. There was an oblique mention of issues with the previously specified version of pip in the comments of that PR. Note that we now use conda 23.10.0 with the libmamba solver in the GATK Docker image. Please feel free to reopen if you have issues with that specific configuration!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618#issuecomment-1851818671:351,config,configuration,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618#issuecomment-1851818671,1,['config'],['configuration']
Modifiability,"Thanks for that info and for sharing the files, @asmirnov239. I suspect that there are essentially two types of bins: ""nice"" and ""not so nice"". The sampling noise in the former is determined by Poisson observation noise, whereas that in the latter is determined by uncertainty in the bias posteriors. This is a bit hard to see in the plots above, and even in this version where I tried to adjust the point size and alpha:. ![image](https://user-images.githubusercontent.com/11076296/137733810-16a79ea9-ea7b-47cc-a42f-40130a949015.png). However, plotting a measure of the difference in the dCRs (from 20 and 200 posterior samples) vs. the dCR is more suggestive:. ![image](https://user-images.githubusercontent.com/11076296/137734587-1b9f6551-74b2-4097-a02c-f51d7341251c.png). As are the dCR histograms:. ![image](https://user-images.githubusercontent.com/11076296/137733867-ce0f5573-a5cc-412c-9060-56fbb09d1ef0.png). I would guess that the nice spike around CR ~ 2 and the fatter base extending up to dCR ~ 100 are distinct populations of bins. So the punchline would be that differences at high dCR are probably just noise within the noise. For ""nice"" bins at dCR ~ few, the sampling noise looks to be <1%. Not really sure what's going on at very high dCR, but I think it's safe to say that these are ""not so nice"" bins!. I've seen this pattern in other WES cohorts when plotting the posterior means vs. std devs for the biases; tried to dig up the plots on Slack, but I can't find them at the moment. Perhaps something along those lines might be worth visualizing in your model-criticism notebooks, if you don't already?. Again, hard to say this is indeed the case from the dCRs alone, but if so, it might be worth baking this sort of mixture into future versions of the model or coming up with other strategies to deal with such bins.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946:985,extend,extending,985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946,1,['extend'],['extending']
Modifiability,"Thanks for the feedback, @cmnbroad. Â @droazen, should I open a ticket for implement the plugin and close this issue? What's about the checking of the quals?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652:88,plugin,plugin,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245969652,2,['plugin'],['plugin']
Modifiability,"Thanks for the suggestion, @droazen! I did this PR before the read filter plugin was included, and actually I was thinking about remove this PR because it is very clunky. Should I close this and open a discussion about the plugin?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245960477:74,plugin,plugin,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2084#issuecomment-245960477,2,['plugin'],['plugin']
Modifiability,"Thanks guys!. On Sat, Sep 23, 2017 at 11:38 PM, David Benjamin <notifications@github.com>; wrote:. > *@davidbenjamin* requested changes on this pull request.; >; > Done with my review. Mainly the usual stuff about writing more idiomatic; > Java that all C++ coders go through!; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646010>:; >; > > +import org.broadinstitute.hellbender.utils.IntervalUtils;; > +import org.broadinstitute.hellbender.utils.SimpleInterval;; > +; > +import java.io.File;; > +import java.util.List;; > +; > +; > +; > +@CommandLineProgramProperties(; > + summary = ""Split intervals into sub-interval files."",; > + oneLineSummary = ""Split intervals into sub-interval files."",; > + programGroup = VariantProgramGroup.class; > +); > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; >; > @samuelklee <https://github.com/samuelklee> is the boss of the copy; > number code, but personally I don't see the need to be extremely concise; > with short names and would prefer width.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646054>:; >; > > +@DocumentedFeature; > +public class CreateBinningIntervals extends GATKTool {; > + public static final String WIDTH_OF_BINS_SHORT_NAME = ""bw"";; > + public static final String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + pri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:967,extend,extends,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['extend'],['extends']
Modifiability,That code checks whether DP is 0 or the maximum PL value is 0. If any of the conditions is satisfied then plugin will assign nocall ./. to that site.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-2117184307:106,plugin,plugin,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-2117184307,1,['plugin'],['plugin']
Modifiability,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:975,config,config,975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816,2,['config'],['config']
Modifiability,"That sounds like a good thing to look at. If someone has already written it, it would be great to not have to rewrite it..",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4086#issuecomment-356366388:110,rewrite,rewrite,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4086#issuecomment-356366388,1,['rewrite'],['rewrite']
Modifiability,"That's a good sign. However the code above only checks the providers at the client. It'd be interesting to also check what happens at the workers. I wrote ExampleNioCheckFS for this purpose earlier, you can use it. It's only in a test branch of mine (since it's just test code) but it's pretty short. Looks like this:. ````java; /**; * Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector.; */; @CommandLineProgramProperties(; summary = ""Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector"",; oneLineSummary = ""Example of how to use Spark on Google Cloud Storage directly, without using the GCS Hadoop Connector"",; programGroup = ReadProgramGroup.class; ); public class ExampleNioCheckFS extends SparkCommandLineProgram {; private static final long serialVersionUID = 1L;. @Argument(fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME, shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, doc = ""Output file (if not provided, defaults to STDOUT)"", common = false, optional = true); private File OUTPUT_FILE = null;. @Argument(fullName = ""inputPath"", shortName = ""P"", doc = ""Input path (eg. gs://foo/bar.bam)"", optional = false); private String path = null;. // Typically set to number of executors times number of cores per executor.; @Argument(fullName = ""parts"", doc = ""number of partitions"", optional = false); private int parts = 3;. private void countReads(JavaSparkContext ctx) {; PrintStream outputStream;. try {; outputStream = OUTPUT_FILE != null ? new PrintStream(OUTPUT_FILE) : System.out;; }; catch ( FileNotFoundException e ) {; throw new UserException.CouldNotReadInputFile(OUTPUT_FILE, e);; }. NioBam input = new NioBam(path, path + "".bai"");; List<String> ret = input.getReads(ctx, parts).mapPartitions(ExampleNioCheckFS::getFS).collect();; outputStream.println(""**** Results **** : "" + String.join("", "", ret));; }. private static Iterator<String> getFS(Iterator<SAMRecord> rs) {",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267424466:785,extend,extends,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267424466,1,['extend'],['extends']
Modifiability,"That's why I am not using in ReadTools and other developmental toolkit the base class from GATK, due to the polluted command line with unused arguments. I think that for give flexibility, some of that arguments should be configurable by extending classes. For example, some tools that does not require reads at all should be able to turn off the read arguments. That will be very useful, although I am not sure how to do it in a proper way without adding more and more interfaces for argument collections. In context case of this PR, I think that adding it does not have any real effect on the GATK codebase, and a lot is gained by downstream projects. For example, if the wrapper script adds another argument that should be parsed in `Main` and documented, the GATK team just add it to its class. If a toolkit has a similar wrapper script, it can also add its own only-doc argument by simply overriding the method...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090:221,config,configurable,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371822090,4,"['config', 'extend']","['configurable', 'extending']"
Modifiability,"The AlleleFrequencyQC tool subclasses VariantEval, but doesn't provide it's own tool annotations, so it inherits VariantEval's `@BetaFeature` status and command line description. It also appears to directly clobber several of the command line argument values provided by the user, including the name of the output file. It should have its own `@CommandLineProgramProperties` and `@BetaFeature/@Experimental` annotations, and preferably better argument handling. Longer term, when https://github.com/broadinstitute/gatk/issues/5439 is done, it should be refactored so it uses the `VariantEval` engine class that will be part of that work, instead of subclassing the VariantEval tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6997:104,inherit,inherits,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6997,2,"['inherit', 'refactor']","['inherits', 'refactored']"
Modifiability,The Engine Team discussed this internally and we're going to pull out a subset of all the configuration options into the config file. These options should be those that will change only infrequently (like the data sources directory).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685:90,config,configuration,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685,2,['config'],"['config', 'configuration']"
Modifiability,"The HaplotypeCaller has slightly different behavior between VCF and GVCF output in some cases, which means that the variants at the edges of the active region may not be called in both. This is due to the following line:; https://github.com/broadinstitute/gatk/blob/89ea9e01225db5c9bbe262c888a0abb74509f94c/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyRegionTrimmer.java#L318. The behavior for VCF mode should be made to conform to GVCF mode by defining `callableRegion = originalRegion.trim(callableSpan, extendedSpan);`. Super easy fix, but will break a bunch of tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5676:543,extend,extendedSpan,543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5676,1,['extend'],['extendedSpan']
Modifiability,"The MendelianViolation class, as ported from GATK3, is used both to get mendelian violation state for a single variant/set of samples, as well as an accumulator for counting violations for multiple variants. It contains two isViolation methods, one of which clobbers the cumulative state without warning. The class should be refactored to make the two usage patterns distinct and less prone to accidental misuse.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5034:325,refactor,refactored,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5034,1,['refactor'],['refactored']
Modifiability,"The VCFHeader and header line hierarchy classes in htsjdk need refactoring to fix a number of bugs, and to support clean handling of new versions of VCF and BCF. This work is mostly done, PR is [here](https://github.com/samtools/htsjdk/pull/835).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2601:63,refactor,refactoring,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2601,1,['refactor'],['refactoring']
Modifiability,The actual code for the funcotation factories is all set up for this. The required update is that `GencodeFuncotationFactory` needs to be refactored to take in the name of the data source. Right now it's assumed that it can only be `Gencode`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3956#issuecomment-378314286:138,refactor,refactored,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3956#issuecomment-378314286,1,['refactor'],['refactored']
Modifiability,"The behavior of the GATK3 CombineVariants was very inconsistent and the arguments weren't entirely clear. I also suspect that some operations weren't possible with the arguments given. Rather than port that old broken version, I would advocate for an overhaul or rewrite. @bhanugandham it's going to be a big project to collect requirements and expected behavior for this tool. For example, what should the MQ be for the combined VCF for two different input VCFs with different MQ values? Much of the confusion stemmed from the old ability to merge VCFs containing the same sample. In the case where we take one genotype for each sample name (e.g. the old ` -genotypeMergeOptions PRIORITIZE`) then I believe the old behavior was wrong in some cases, taking the filter status from an input VCF at random. We also need to clarify `FilteredRecordMergeType` options, e.g. https://github.com/broadinstitute/gsa-unstable/issues/935",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167:263,rewrite,rewrite,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/17#issuecomment-430229167,1,['rewrite'],['rewrite']
Modifiability,"The broad artifactory moved to https://broadinstitute.jfrog.io/broadinstitute/. There is a redirect in place which as been working for downloads, but uploads are failing with `401 Unauthorized`. It seems like updating the url fixes the problem. As a second issue, our builds try to upload archives for every integration test build, which worked when we only had 1 integration test build, but now that we have multiples we are uploading duplicates which isn't good. We should fix that, probably by adding either a new environment variable to the travis build, or a final build stage to perform the upload.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3068:529,variab,variable,529,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3068,1,['variab'],['variable']
Modifiability,"The build.gradle code below builds the native shared library for AVX PairHMM using gcc and copies the .so file to the desired location. The jar task will archive the .so file in the GATK jar file. ``` gradle; apply plugin: 'cpp'; model {; components {; VectorLoglessPairHMM(NativeLibrarySpec) {; binaries.withType(SharedLibraryBinarySpec) { binary ->; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include""; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include/linux""; cppCompiler.args ""-mavx""; linker.args ""-static-libgcc"". task copySharedLib(type: Copy) {; from binary.tasks; into ""build/classes/main/org/broadinstitute/hellbender/utils/pairhmm""; }; jar.dependsOn copySharedLib; }; // skip static library build; binaries.withType(StaticLibraryBinarySpec) { binary ->; buildable = false; }; }; }; }; ```. The gradle gcc plugin expects to find the C++ source code in the default location shown below. We can use a different directory structure, if desired. ```; src/; |-- main; |-- test; `-- VectorLoglessPairHMM; |-- cpp; `-- headers; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1492:215,plugin,plugin,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1492,2,['plugin'],['plugin']
Modifiability,The client secret file or the api should be able to be set as an environment variable or in a properties file somewhere so they don't have to be entered every single time someone runs a program.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/701:77,variab,variable,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/701,1,['variab'],['variable']
Modifiability,The concise message is:. ```; cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle; > Configure project :; Executing: git lfs pull --include src/main/resources/large. > Task :condaStandardEnvironmentDefinition; Created standard Conda environment yml file: gatkcondaenv.yml. > Task :pythonPackageArchive; Created GATK Python package archive in /home/cb2/gatk/build/gatkPythonPackageArchive.zip. > Task :gatkDoc FAILED; Unable to find the 'javadoc' executable. Tried the java home: /usr/lib/jvm/java-11-openjdk-amd64 and the PATH. We will assume the executable can be ran in the current working folder. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Get more help at https://help.gradle.org; ```. And stacktrace flag output looks like:. ```; `cb2@cb2-VirtualBox:~/gatk$ ./gradlew bundle --stacktrace; > Task :gatkDoc FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'. * Try:; Run with --info or --debug option to get more log output. Run with --scan to get full insights. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':gatkDoc'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:166); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$3.accept(ExecuteActionsTaskExecuter.java:163); at org.gradle.internal.Try$Failure.ifSuccessfulOrElse(Try.java:191); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(Execu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:77,Config,Configure,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['Config'],['Configure']
Modifiability,"The current FindBreakpointEvidence code is classifying reads pairs that overlap such that the start position of the reverse read is before the start position of the forward read as ""OutiesPair"" discordant read pair evidence. However, these are likely due to sequencing of very short inserts that causes some of the adapter to be sequenced and potentially aligned. This change requires a read pair to not be overlapping to be counted as an 'OutiesPair'. On the CHM dataset this causes the number of intervals discovered to drop from 23152 to 21633, and the number of called variants to drop from 3467 to 3366. . @tedsharpe could you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2515:315,adapt,adapter,315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2515,1,['adapt'],['adapter']
Modifiability,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1213:181,config,configuring,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213,2,['config'],"['configuration', 'configuring']"
Modifiability,"The current initialization action for dataproc workers puts the reference image in different places depending on whether or not an SSD is mounted. Preemptible dataproc workers don't have SSDs, so a mixed cluster will have references mounted on different paths depending on the worker. This change symlinks the SSD mount point onto the HDD so that paths can be consistent. . Also increases several cluster configuration parameters relating to retries, which I saw recommended if using preemptible workers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4493:405,config,configuration,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4493,1,['config'],['configuration']
Modifiability,"The debug variable of AsseemblyResultSet wasn't set anywhere, and therefore the command line argument debug didn't propagate to the function buildEventMapsForHaplotypes in EventMap.java. I added the function setDebug to AssemblyResultSet, and set its value in HaplotypeCallerEngine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5455:10,variab,variable,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5455,1,['variab'],['variable']
Modifiability,"The docker image only uses OpenJDK. However, the way travis is configured, the docker tests will be run once for OpenJDK and once for OracleJDK, but that distinction has no meaning, since the native JVM on the travis VM is irrelevant to what is happening in the docker image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2748:63,config,configured,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2748,1,['config'],['configured']
Modifiability,"The entire test suite aborts if HELLBENDER_TEST_INPUTS isn't set because an exception is thrown when loading the VariantWalkerGCSSupportIntegrationTest class. With this change, the tests will still fail, but the rest of the test suite will run. Depending on what the intent for these tests is, another possibility would be to add a dependsOn method with a hard dependency so the tests would be skipped in the case of no env variable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2404:424,variab,variable,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2404,1,['variab'],['variable']
Modifiability,"The fact that it's circular means that there are reads and bases right up against the edge of the contig and often have soft clips that extend beyond the contig. So it looks a bit like this:. <img width=""945"" alt=""screen shot 2018-07-19 at 11 37 45 am"" src=""https://user-images.githubusercontent.com/13020550/42953344-3ba4f544-8b48-11e8-9250-db75e56c2069.png"">. So yes, circular means it's probably throwing in a weird edge case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5036#issuecomment-406322137:136,extend,extend,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5036#issuecomment-406322137,1,['extend'],['extend']
Modifiability,The fact that you can print stack traces on UserException is not very discoverable. We should probably include instructions to do so in the UserException message itself. We might want to write the stack trace to a file so that people don't have to rerun the program to get it as well. . It's also weird that it's set through an environment variable instead of as an argument. (Although it may be difficult to implement as an argument since it has to be set correctly even if argument parsing fails. @cmnbroad Any thoughts on that? ),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2443:340,variab,variable,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2443,1,['variab'],['variable']
Modifiability,"The file size only went from 1.9MB to 3MB -- there's no perceptible; difference in test runtime that I can see. On Wed, Apr 5, 2017 at 2:04 PM, droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In src/test/java/org/broadinstitute/hellbender/tools/spark/; > ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest.java; > <https://github.com/broadinstitute/gatk/pull/2540#discussion_r109987028>:; >; > > +; > +; > +public class ParallelCopyGCSDirectoryIntoHDFSSparkIntegrationTest extends CommandLineProgramTest {; > +; > + @Override; > + public String getTestedToolName() {; > + return ParallelCopyGCSDirectoryIntoHDFSSpark.class.getSimpleName();; > + }; > +; > + @Test(groups = {""spark"", ""bucket""}); > + public void testCopyFile() throws Exception {; > + MiniDFSCluster cluster = null;; > + try {; > + final Configuration conf = new Configuration();; > + // set the minicluster to have a very low block size so that we can test transfering a file in chunks without actually needing to move a big file; > + conf.set(""dfs.blocksize"", ""1048576"");; >; > Instead of switching to a larger file, is it possible to just decrease the; > block size further? (thinking about test runtimes here); >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2540#discussion_r109987028>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZYV_vD1XwS5IPvZiNZKOe6QzDJDVks5rs9e2gaJpZM4MtGXX>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-291948506:555,extend,extends,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2540#issuecomment-291948506,3,"['Config', 'extend']","['Configuration', 'extends']"
Modifiability,"The follow error messages popped up after d25894b3bc80e450210cf8a9124c4171e65f3717. The program seems to function properly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:236,variab,variable,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,2,['variab'],['variable']
Modifiability,"The following test class may fail with the error: ""Values were supplied for (ReadLengthReadFilter) that is also disabled"":. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; public class GATKReadFilterPluginDescriptorUnitTest extends BaseTest {. @CommandLineProgramProperties(summary = ""Test read filter plugin with default arguments"",; oneLineSummary = ""Test read filter plugin with default arguments"",; programGroup = TestProgramGroup.class); private static class TestWithDefaultReadFilters extends CommandLineProgram {. private final List<ReadFilter> defaultFilters;. public TestWithDefaultReadFilters(final List<ReadFilter> defaultFilters) {; this.defaultFilters = defaultFilters;; }. protected List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(defaultFilters));; }. @Override; protected Object doWork() {; return null;; }; }. @Test; public void testWithDefaultReadFiltersWithParams() throws Exception {; // this ReadFilter have parameters --maxReadLength/--minReadLength, that are set because of the default filter; final CommandLineProgram clp = new TestWithDefaultReadFilters(Collections.singletonList(new ReadLengthReadFilter(10, 50)));; // disable the read filter should not blow up because of that parameters, because they are not provided by the user; clp.instanceMain(new String[]{""--"" + StandardArgumentDefinitions.DISABLE_READ_FILTER_LONG_NAME, ""ReadLengthReadFilter""});. }; }; ```. I don't know if this may be solved in GATK or in Barclay, but at least a workaround for this logging a warning instead of blowing up will be better than throwing, because that means that default filters with parameters cannot be disabled.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2357:237,extend,extends,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2357,5,"['extend', 'plugin']","['extends', 'plugin']"
Modifiability,The generated online doc currently doesn't include the names of the default annotations or annotation groups used by various tools. The values are already propagated from the annotation plugin to the freemarker map by Barclay; it should be easy to update the freemarker template to display these similar to the way we display default read filters.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5577:186,plugin,plugin,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5577,1,['plugin'],['plugin']
Modifiability,"The help message was wrong when an environment variable was missing. I've changed it so the same string is used to lookup the variable and report it missing so that can't ever be broken again. This does change it from loading the variables once at startup to loading them every time they are queried. I assumed that isn't an issue, but I can change it to cache them if someone can see a problem with that.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/572:47,variab,variable,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/572,3,['variab'],"['variable', 'variables']"
Modifiability,The htsjdk downstream tests were put together before gradle had composite builds and are very hacky. They should be refactored to use composite builds instead of installing a strangely named maven artifact. . We should also split them into unit/ integration tests to reduce wallclock time. This should be easy since we already to it in travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:116,refactor,refactored,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,1,['refactor'],['refactored']
Modifiability,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:78,config,configurable,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183,1,['config'],['configurable']
Modifiability,"The latest Picard release introduces a dependency on the Google Cloud NIO library that conflicts with GATK's dependency. We are going to have to blacklist the Picard NIO dependency for now. . Longer term, we might want to consider having both projects depend upon a build of htsjdk that comes with the NIO plugin.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4556:306,plugin,plugin,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556,1,['plugin'],['plugin']
Modifiability,The list of packages used by GATKReadFilterPluginDescriptor and GATKAnnotationPluginDescriptor to find plugin instances should be a configurable setting.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4036:103,plugin,plugin,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4036,2,"['config', 'plugin']","['configurable', 'plugin']"
Modifiability,The main issue with this task was that the query results were being limited to 100 by default. So we use the -n param now in the query. Another issue was that we were running bq show on a table variable $TABLE which is never defined.; I also changed this because the approach (returning all the samples names of the samples that have been loaded) didn't seem scalable. I wanted to only return at most the number of samples we are trying to ingest.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230:194,variab,variable,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230,1,['variab'],['variable']
Modifiability,"The master branch failed on BaseRealibratorSpark when running WGS. Try to test this branch, but got hit by a strange error message. The jar file looks right to me. @tomwhite did you have some environment variables? . ````Using GATK jar /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar; Running:; /home/genomics/Projects/spark/bin/spark-submit --master spark://n001:7077 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --executor-memory 25G --driver-memory 5G /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar BaseRecalibratorSpark -I hdfs://n001:54310/GATK4TEST/LargeBroadData/WGS-G94982-NA12878.bam -knownSites hdfs://n001:54310/GATK4TEST/DBSNP/dbsnp_138.hg19.vcf.gz -R hdfs://n001:54310/GATK4TEST/OldData/human_g1k_v37.2bit -O hdfs://n001:54310/GATK4TEST/LargeOutput/WGS_BQSR --sparkMaster spark://n001:7077; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; java.lang.ClassNotFoundException: org.broadinstitute.hellbender.Main; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.ja",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877:204,variab,variables,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877,1,['variab'],['variables']
Modifiability,"The model currently gives each read an independent latent variable indicating which haplotype it was derived from. This latent variable should be a property of fragments, not reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5085:58,variab,variable,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5085,2,['variab'],['variable']
Modifiability,"The naming of the different classes (`AlignmentRegion`, `AssembledBreakpoint`, `BreakpointAllele`) was very confusing especially with the id variable `breakpointId` in several classes. I've renamed `AssembledBreakpoint` to `BreakpointAlignment`, since I think that the main thing it's trying to capture is a split alignment that's indicating there might be a breakpoint at a given location. Then, the actual breakpoint is represented by `BreakpointAllele` which the information about the breakpoint junction, but not the alignment-related fields. Does that make more sense?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240475247:141,variab,variable,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2079#issuecomment-240475247,1,['variab'],['variable']
Modifiability,"The output printed at startup (`CommandLineProgram.printStartupMessage()`) should be easy for toolkits that build on top of GATK to customize. Currently it can be customized via overriding methods, but this is awkward if you want to extend built-in walker classes rather than your own subclass of `CommandLineProgram`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101:233,extend,extend,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101,1,['extend'],['extend']
Modifiability,"The packages for codecs is a key feature for downstream tools implementing new codecs for other formats or to include overrides of codecs already included. Nevertheless, the current implementation (at version 4.0.0.0) the only way of configuring this is at the package level using the `codec_packages` configuration. I request support for the following fine-grained configuration:. * Add/Remove concrete codec classes; * Exclude single classes from a concrete `codec_package` specified (this can be done by the previous requirement if it uses fully qualified codec names); * Exclude sub-packages from a concrete `codec_package` specified. Representing this in an YML format, I would like to have the ability to configure the codecs as following:. ```yml; - codecs:; - packages:; - htsjdk.variant; - htsjdk.tribble; - exclude_class: bed.BEDCodec; - org.broadinstitute.hellbender.utils.codecs; - exclude_package: gencode; - org.magicdgs.htsjdk.codecs; - classes:; - org.external.htsjdk_codecs.CustomBedCodec; ```. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4180:234,config,configuring,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4180,4,['config'],"['configuration', 'configure', 'configuring']"
Modifiability,"The pedigree-checking warning for the PossibleDeNovo annotation is always output because it happens in the constructor, and in GATK4 all the InfoFieldAnnotations get instantiated. It's a little weird to have this warning even when the annotation is not requested. But if this is the cost we pay for getting rid of the PluginManager I will gladly deal with it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3714:318,Plugin,PluginManager,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3714,1,['Plugin'],['PluginManager']
Modifiability,"The problem is that htsjdk supports reading multiple versions of vcf, but only knows how to write the current version. Traditionally this has worked because older vcf versions could be trivially written out as a newer version. But v4.3 restricts some values to a narrower range than previous versions, so its not always possible to write out a pre-v4.3 version as a v4.3 compliant file in a non-destructive way. Hence the need to refactor to better support full versioning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-472037659:430,refactor,refactor,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2602#issuecomment-472037659,1,['refactor'],['refactor']
Modifiability,The problem looks like it's due to the Hadoop version. Hadoop 2.7.0 or later is required for https://issues.apache.org/jira/browse/HDFS-3689. We rely on this change to concatenate the VCF parts together (which are variable lengths).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6218#issuecomment-546884621:214,variab,variable,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6218#issuecomment-546884621,1,['variab'],['variable']
Modifiability,The problem seems to be fixed in picard with @cmnbroad's change to the cloud configuration. Thew pom for 2.18.1+ looks like it won't include nio.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4556#issuecomment-375432298:77,config,configuration,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4556#issuecomment-375432298,1,['config'],['configuration']
Modifiability,"The problem seems to be that some of the ""deep"" filters inherited from ReadWalker require to look into the read-base/qualities (just to compare their lenghts) which seems to be quite costly to decode and some of the read-attributes when this particular tool does not need to look into those fields. . It seems to me that we could override these check with a more lightweight alternative. . <img width=""1009"" alt=""screen shot 2018-09-27 at 3 03 36 pm"" src=""https://user-images.githubusercontent.com/791104/46168553-d32a0800-c266-11e8-96fd-d60d7c0380d5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425207954:56,inherit,inherited,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425207954,1,['inherit'],['inherited']
Modifiability,"The properties needed are listed here (for testing in this case): https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java#L83-L86. They are; * `fs.gs.impl`; * `fs.AbstractFileSystem.gs.impl`; * `fs.gs.project.id`; * `google.cloud.auth.service.account.json.keyfile`. Note that to set them as Spark configuration values, they need to be prefixed with `spark.hadoop`. So from the `spark-submit` command line you would write. ```; --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500745498:374,config,configuration,374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996#issuecomment-500745498,1,['config'],['configuration']
Modifiability,"The regression tests added as part of #4344 fulfill this requirement. However, they need to be refactored to take advantage of the newly-included full `hg19` and `hg38` reference sequences.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5295#issuecomment-430312265:95,refactor,refactored,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5295#issuecomment-430312265,1,['refactor'],['refactored']
Modifiability,"The script is dangerous in its current state because it uses rm -Rf ${dir} arguments which can result in unwanted deletion if something goes wrong with the input arguments. We should either fix those arguments or remove the necessity for the script to be run with root permissions altogether to avoid any future problems that might arise and make the script safer for users. Additionally, the whole script could be refactored to be cleaner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3798:415,refactor,refactored,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3798,1,['refactor'],['refactored']
Modifiability,"The snapshot builds get published to an artifact repository, but I don't think those are accessible from outside of Broad. The build from this morning with your branch is [here](https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot-local/org/broadinstitute/gatk/4.0.11.0-30-g9c4a27b-SNAPSHOT/) if you can access it. Otherwise, for local development, you can do the following:. - pull gatk master from today so it includes your commit; - run `git fetch --tags` (this is optional but it will give your local build a more reasonable version tag); - run `./gradlew install printVersion` to install the locally built gatk into your local machine's maven repository; - change your VariantQC gradle project to include the `maven` gradle plugin if its not already there; - add `mavenLocal()` to your projects' `repositories `closure; - change your gatk dependency to the version number printed out by 'printVersion'; - rebuild VariantQC. Having said all that, what code are you dependent on ? I expect the command line interface to VariantEval, and the VariantUtils and StratificationManager and friends classes all to undergo some refactoring and evolve a bit before the tool has the beta tag removed and the interfaces are stabilized. See https://github.com/broadinstitute/gatk/issues/5439 and https://github.com/broadinstitute/gatk/issues/5440.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440782148:737,plugin,plugin,737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440782148,3,"['evolve', 'plugin', 'refactor']","['evolve', 'plugin', 'refactoring']"
Modifiability,"The task here is to simply move the code while changing as little as possible, and then validate that. Once that's done, we can do whatever refactoring/changes we want to VQSR, or replace it completely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525:140,refactor,refactoring,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062#issuecomment-236014525,2,['refactor'],['refactoring']
Modifiability,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1465:255,inherit,inherited,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465,1,['inherit'],['inherited']
Modifiability,"The underlying issue here is is that the GATK conda env environment isn't established since bioconda doesn't appear to configure it. The NPE needs is fixed by #7816. In this particular case it appears that some of the requirements are satisfied, since the code gets past the initial check to see if the GATK python code is available. But then the actual CNN code can't be loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269:119,config,configure,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269,1,['config'],['configure']
Modifiability,"The use of Targets to refer to genomic intervals is unnecessary and confusing. It obfuscates the fact that most of the tools and code can be applied to not only counts from WES targets, but also counts from WES baits, WGS bins, etc. Requiring that Targets be named also adds unnecessary storage and memory burden. We should just use SimpleIntervals everywhere. We should also get rid of the target file format. In terms of external visibility, we can just rename tools and edit javadoc. Internally, there will be many classes that need to be both renamed and refactored. I instead suggest that we rebuild new versions of the classes and tools as necessary in the tools/copynumber package. - [ ] Rename tools: AnnotateTargets -> AnnotateIntervals, TargetCoverageSexGenotyper -> ReadCountSexGenotyper. ; - [ ] Deprecate tools: CalculateTargetCoverage, ConvertBedToTargetFile, and PadTargets will be replaced by @asmirnov239's new CollectReadCounts tool and on-the-fly padding specified by --interval_padding parameters.; - [ ] Deprecate target file format and change all other affected file formats.; - [ ] Refactor/rename/rebuild classes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3246:559,refactor,refactored,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3246,2,"['Refactor', 'refactor']","['Refactor', 'refactored']"
Modifiability,"The varianteval package (VariantEval, evaluators/stratifiers and stratification manager) was ported directly from GATK3 to minimize diffs for review, and needs a code-style cleanup pass:. - rename variables with names in ALL_CAPS; - remove redundant type instantiation params in favor of <> operator; - add finals; - revisit the use of generic type params and required casts, etc in StratificationManager and stratifier classes",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5440:197,variab,variables,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5440,1,['variab'],['variables']
Modifiability,The various *Context objects should be refactored to return empty lists upon lack of input instead of being Optional,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/244:39,refactor,refactored,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/244,1,['refactor'],['refactored']
Modifiability,"The workflow in cnv_somatic_panel_workflow.wdl defines:; "" Int? mem_gb_for_create_read_count_pon"". But when it's used in the call to CreateReadCountPanelOfNormals, the variable is referred to as ""mem_for_create_read_count_pon"" (i.e. no ""_gb""). This causes the workflow to fail...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4281:168,variab,variable,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4281,1,['variab'],['variable']
Modifiability,"Theory from @cmnbroad is below:. ```; I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, dbpedia/distributed-extraction-framework#9.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579:504,Config,Configuration,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579,1,['Config'],['Configuration']
Modifiability,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5764:334,Config,Configure,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764,1,['Config'],['Configure']
Modifiability,"There are a number of Python-based unit tests for `NVScoreVariants` in `src/main/python/org/broadinstitute/hellbender/scorevariants/tests`, which use the standard Python `unittest` framework (https://docs.python.org/3/library/unittest.html). We should ideally hook these up to the GATK test suite, and run them either via a gradle plugin for Python tests, or via a `PythonScriptExecutor` from a Java-based TestNG test. We'll need to figure out how to parse the test report for these tests and get the results to display nicely in github alongside the Java-based test results. We'll also need to look into whether the tests are cleaning up temp files properly, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9011:331,plugin,plugin,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9011,1,['plugin'],['plugin']
Modifiability,"There are a number of skipped tests on a successful run of `AlleleListUtilsUnitTest` These are deliberately skipped because the tests share a single data provider, but each test can only use a subset of the data. These should be refactored to avoid skipping tests. These tests also make use of random number generators. It looks like these may not be properly isolated and may introduce coupling between what should be independent tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/607:229,refactor,refactored,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/607,2,"['coupling', 'refactor']","['coupling', 'refactored']"
Modifiability,"There are no Java code changes in this PR. Tests were done manually. As a reminder, the modified files are still considered experimental. Changes:; - combine_tracks.wdl: Fixes bug where string was compared to a float. Closes #5284 ; - combine_tracks.wdl: Converts the processed seg file into a format for GISTIC2. This is a trivial conversion. Closes #5283 ; - Other changes in `aggregate_combine_tracks.wdl` to support the above, including aggregation of individual GISTIC2 seg files into a single GISTIC2 seg file.; - Added gs urls for necessary auxiliary files in the documentation.; - Added multiple output types for the ABSOLUTE skew parameter to support heterogeneous execution configurations. File, Float, and String. All are the same value.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5287:684,config,configurations,684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5287,1,['config'],['configurations']
Modifiability,"There are still too many variables here. Do you know that the input bams are the same? Are you using BWA-MEM? My theory is that it's choosing different secondary alignments in MQ0 cases, preferring bases that are capital. If you can show that the same reads going in produce different variants with your two different references then this will be a lot easier to debug.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6825#issuecomment-707755358:25,variab,variables,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6825#issuecomment-707755358,1,['variab'],['variables']
Modifiability,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:78,inherit,inherit,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189,1,['inherit'],['inherit']
Modifiability,"There have been requests for some additional clarity on ""how; much test coverage is enough"" for hellbender tools. Rather than; mandate a particular coverage target, I proposed a more flexible; set of guidelines which I've added to the README in this commit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/382:183,flexible,flexible,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/382,1,['flexible'],['flexible']
Modifiability,"There is a first attempt to have some configurable settings in #2322. If in that PR this could be included, feel free to let me know how do you want to do this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272485359:38,config,configurable,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272485359,1,['config'],['configurable']
Modifiability,"There is an existing NIO filesystem provider for Amazon S3 that has been used successfully with GATK4 by at least one user (with some minor tweaks to the engine). We should add the S3 plugin as a dependency, add basic tests for read support, and make whatever changes are needed to get it working.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3708:184,plugin,plugin,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3708,1,['plugin'],['plugin']
Modifiability,"There is no conceivable worst case for this PR -- the only reason for having a `CountSet` was to be be able to have quick `min` and `max` operations (in log(n) time), *but*. * these operations are not used anywhere outside of unit tests, so to make an illuminating worst case you would have to rewrite the assembly engine.; * Even if we did use these operations they would be done once per assembly region, and therefore we could make this class 1000x slower and we would add about a second to the run time of a WGS bam.; * a plain old `TreeSet`, which is what this PR replaces the `CountSet` with, also has these operations in log time.; * the number of kmer sizes used is usually 2, and will be up to 6 in very rare cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5467#issuecomment-443463884:294,rewrite,rewrite,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5467#issuecomment-443463884,1,['rewrite'],['rewrite']
Modifiability,"There is some duplication of code to handle CR-only, AF-only, and CR+AF that could be eliminated with some refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5625:107,refactor,refactoring,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625,1,['refactor'],['refactoring']
Modifiability,"There seems to be no obvious way to read thru the unmapped read pairs in a bam file in Spark. Looking at the code in ```ReadSparkSource#getParallelReads(String, String, List, long)``` it seems that ; perhaps it is possible by setting the appropriate property in Configuration returned by ```ctx.hadoopConfiguration()``` however there is no documentation as to what property that could be. . @droazen I assign it to you initially so that you route it to whoever might be most suited to address this issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2572:262,Config,Configuration,262,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2572,1,['Config'],['Configuration']
Modifiability,"There were a few issues with this case. First, the data source was not constructed 100% correctly. The config file is correct. . The index file is for the tar.gz version of the source data and not for the uncompressed version that they're using. The index should correspond to the source data in the file referenced by the config file itself (not a zipped or otherwise transformed version). Secondly, the source `tsv` data file has the header line for the table commented out. The Xsv codec is aware of leading hash marks as comments and will ignore any such lines. Because of this, the leading hash in the table header is ignored and the file cannot be properly parsed. The fix is simple - just remove the leading hash from the table header (the preceding line with the two hash marks is correctly interpreted as a file header because of the leading hashes acting as comments). Lastly, even if the user fixed the file they would still need to index it with`IndexFeatureFile`. At some point the code underlying this in `HTSJDK` was broken such that no Xsv files can currently be indexed. I have submitted a pull request in `HTSJDK` (https://github.com/samtools/htsjdk/pull/1429) for this and have another ready to go in GATK (#6224) that includes a test for this case so this reversion cannot happen again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183:103,config,config,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223#issuecomment-545186183,4,['config'],['config']
Modifiability,"There's such feature for GATKTool, but not yet for GATKSparkTool. Much of the code is copied from the GATKTool version; Engine team, please comment if a refactor is needed and how. Thanks!; (Tagging @droazen @lbergelson and @cmnbroad )",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4981:153,refactor,refactor,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4981,1,['refactor'],['refactor']
Modifiability,"These codecs require a `GenomeLocParser` (and therefore a sequence dictionary), and so are currently broken in hellbender, which does not assume the presence of a sequence dictionary for Feature-containing files. We need to either refactor these codecs to not require a `GenomeLocParser` (and remove the `ReferenceDependentFeatureCodec` interface), or delete them if they are no longer needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/234:231,refactor,refactor,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/234,1,['refactor'],['refactor']
Modifiability,"These kind of errors are typically seen when the field description in the VCF header is incorrect. For example, describing the field length to be a fixed integer when the field is really variable length. I would recommend closely scanning the VCF header for inconsistencies first.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035:187,variab,variable,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407450035,1,['variab'],['variable']
Modifiability,"These measurement are useful when tuning performance (or hunting down performance anomalies), but they have a measurable overhead (10% difference on a test with 1000 intervals, 5x the standard deviation on 10 runs). So turn them off by default. Also refactor a few of those into a try-finally to avoid repetition and its associated risks on correctness.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391:250,refactor,refactor,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391,1,['refactor'],['refactor']
Modifiability,"Things left for later:; * `GenotypeIndexCalculator` sometimes interacts with primitive arrays, sometimes with `GenotypeAlleleCounts`; * `GenotypeLikelihoodCalculator` has extraneous responsibilities and doesn't interact with `GenotypeAlleleCounts` as well as it should.; * `alleleCountsToIndex(final GenotypeAlleleCounts newGAC, final int[] newToOldAlleleMap)` in `GenotypeIndexCalculator` needs refactoring.; * `GenotypeLikelihoodCalculators` is really just a cache of `GenotypeAlleleCounts`.; * `GenotypeAlleleCounts` has some unused and barely-used methods, and it precomputes a lot of quantities that are not often needed and could be computed on-the-fly without difficulty or expense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217:396,refactor,refactoring,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217,1,['refactor'],['refactoring']
Modifiability,This PR adds sputnik CI https://sputnik.ci as a code reviewer on pull Reqs. I have configured it to use only FindBugs to limit messages to potentially useful ones. @droazen @lbergelson wdyt? we could give it a try and see if it helps us or annoys us.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1747:83,config,configured,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1747,1,['config'],['configured']
Modifiability,"This PR attempts to eliminate long-running, useless assemblies that significantly extend runtime on some samples:. - Conducts a scan over the genome to find intervals of excessive depth, defined as an interval where coverage is greater than a lower factor times the average coverage of the sample and containing a coverage peak greater than an upper factor times the average coverage.; - Nearby high-coverage regions within one read-length of each other are merged together.; - Excludes reads that map exclusively inside high coverage regions from evidence gathering.; - Excludes reads that map exclusively inside high coverage regions from QName finding for seeding assemblies. In addition, after observing that many long-running assemblies occur on non-primary reference contigs, we also exclude reads that map to non-primary contigs (as defined by the ""cross-contig to ignore set"") from evidence gathering. Runtime on the CHM mix sample with this change is approximately 38 minutes, and our NA19238 snapshot now takes only 22 minutes, a significant drop in runtime. There are a few changes in the resulting call set but they appear to be minimal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4438:82,extend,extend,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4438,1,['extend'],['extend']
Modifiability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to ; * the same chromosome with reference order switch but without strand switch, or; * different chromosomes. This brings us (unfiltered) ~6000 mated BND records, half of which are on canonical chromosomes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3571:91,config,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3571,1,['config'],['configuration']
Modifiability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, but NOT significantly overlapping each other. We used to call inversions from such alignments, but it is more appropriate to emit BND records because a lot of times such signal is actually generated from inverted segmental duplications, or simply inverted mobile element insertions. To confidently interpret and distinguish between such events, we need other types of evidence, and is better to be dealt with downstream logic units. Inverted duplications are NOT dealt with in this PR and is going to be in the next. NEEDS TO WAIT UNTIL PART 1 & 2 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3457:91,config,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3457,1,['config'],['configuration']
Modifiability,"This PR deals with long reads with exactly two alignments (no other equally good alignment configuration), mapped to the same chromosome with strand switch, significantly overlapping each other on their reference spans. We used to call inversions from such alignments when feasible, but it is more appropriate to emit inverted duplication records. NEEDS TO WAIT UNTIL PARTS 1, 2 AND 3 ARE IN.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3464:91,config,configuration,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3464,1,['config'],['configuration']
Modifiability,"This PR dynamically sets the logging level for command line tools at runtime using the current version of log4j (we were headed down a path of downgrading to a previous version of log4j in order to implement this). However, it uses an API that is normally used in code for extending log4j rather than acting as a client to it, and requires an explicit cast of the value returned from LogManager.getContext. The Apache project site illustrates the use of this api in the first line of code in an example here: https://logging.apache.org/log4j/2.x/manual/customconfig.html#AddingToCurrent. We need to decide if we want to take this and stay on the current version or continue with the downgradeâ€¦",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/603:273,extend,extending,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/603,1,['extend'],['extending']
Modifiability,"This PR fixes two bugs. First, the SplitIntervals task would enter WeightedSplitIntervals and hang. I added an extra boolean argument to extract so you can specify that no, you really don't want to use a weighted bed. Relatedly, the code branch for running the original GATK SplitIntervals code wasn't correct, as passing weight-bed-file to it as an argument caused a failure. It uses a slightly hacky method of defining a string in WDL to be empty or not depending on if we use weighted beds, interpolating that string into the bash, then checking to see if it's empty there to transmit that state. There is likely a cleaner way to do this, and in the next revision I will likely rewrite this part cleaner. Second, after SplitIntervals passed we hit an error during ExtractTask. The way it expanded intervals to handle large deletions could sometimes subtract past the start of a chromosome, so that logic needed to be patched in a few separate places to handle the interval for the mitochondrial dna that started much closer to the beginning (instead of having a 10k base pair buffer). This PR has those changes too. Successful run here: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/a006a959-9300-42cf-84a7-38c70a35ee21. Successful run after incorporating PR changes: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/e2ee3abd-288e-4f1d-b5be-f78cf5400ce9. Successful run after last PR refactoring that allowed me to revert almost all changes to GvsUtils.SplitIntervals: https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Test/job_history/94fed63a-98ca-466e-8d4c-ac97f24adf37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8113:681,rewrite,rewrite,681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8113,2,"['refactor', 'rewrite']","['refactoring', 'rewrite']"
Modifiability,"This PR fixes two problems I noticed relative to how we are treating cross-contig evidence that comes from the alt contigs:. - The cross-contig exclusion rule in `ReadClassifier` was incorrect. Due to the structure of the `if`-`else if` logic in `checkDiscordantPair()`, `SameStrandPair` or `OutiesPair` evidence was still being created for cross-contig pairs based on the strand configuration of the two reads (even though they were aligned to different contigs).; - The `runWholePipeline` script we've been using was not actually setting the cross-contig kill list parameter, meaning no cross-contig evidence was being filtered out. The change makes the number of intervals for CMHMIX WGS1 drop from 31962 to 27645.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3262:380,config,configuration,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3262,1,['config'],['configuration']
Modifiability,"This PR is a finalized version of https://github.com/broadinstitute/gatk/pull/5017. I've copied the branch into our repo so that travis will run cloud tests on it. Currently, only Posix filesystem paths can be passed as workspaces and arrays to GenomicsDB via GenomicsDBImport and SelectVariants. This PR will allow for hdfs and gcs (and emrfs/s3) URIs to be supported as well. ; Examples; ```; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path hdfs://master:9000/gdb_ws -L 1:500-10000; ./gatk GenomicsDBImport -V /vcfs/sample.vcf.gz --genomicsdb-workspace-path gs://my_bucket/gdb_ws -L 1:500-10000; ```; ```; ./gatk SelectVariants -V gendb.hdfs://master:9000/gdb_ws -R hs37d5.fa -O out.vcf; ./gatk SelectVariants -V gendb.gs://my_bucket/gdb_ws -R hs37d5.fa -O out.vcf; ```; GenomicsDB supports GCS via the [Cloud Storage Connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage). Set environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the GCS Service Account json file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197:949,variab,variable,949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197,1,['variab'],['variable']
Modifiability,"This PR is intended to introduce several new tools related to the CleanVcf workflow in GATK-SV, which the use of these tools being documented in https://github.com/broadinstitute/gatk-sv/pull/733. These tools are intended to introduce several enhancements over the existing implementation, including but not limited to:; - Introduce various unit and integration tests into the workflow.; - Create more robust and generalizable tools that can be used independent of _CleanVcf_.; - Improve runtime and execution speed by leveraging Java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8996:243,enhance,enhancements,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8996,1,['enhance'],['enhancements']
Modifiability,This PR is the culmination of work from myself and @lbergelson to improve the runtime for MarkDuplicatesSpark on a single machine. This involved a rewrite of the tool as well as a number of improvements which should bring it into closer agreement with MarkDuplicates from picard. . Note: this is merely a checkpoint and there is still work that must be done to bring the work into agreement with recent MarkDuplicates development in picard. . Resolves #3706,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4656:147,rewrite,rewrite,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656,1,['rewrite'],['rewrite']
Modifiability,"This PR is the initial stage of implementing the calling of IMPRECISE variants in the SV pipeline. It introduces the concept of an evidence-target link, which joins an evidence interval to its distal target. This is an extension of the 'coherent' evidence concept previously used in determining evidence thresholds for assembly. The code in this PR contains the following changes:. - Evidence intervals and distal targets now are treated as stranded, and evidence-target link clustering depends on overlaps between both intervals and strands.; - Evidence target interval and distal target interval calculations have been modified to make sure that evidence supporting the same event clusters together (has overlapping intervals). This includes several changes such as extending the 'rest-of-fragment-size' calculation to try to capture almost all non-outlier fragment sizes in the library; increasing the split read location uncertainty a little; and being more precise about the boundaries of distal target intervals by taking advantage of information in the MD and MC tags if available.; - Evidence target links are gathered for every piece of evidence supporting a high-quality distal target. ; - Evidence target links are clustered together and store the amount of split-read and read-pair evidence that went into each cluster.; - All evidence target link clusters that are composed of at least 1 split read or at least 2 read pairs are collected in the driver and emitted in a BEDPE formatted file specified in the command line parameters.; - A `PairedStrandedIntervalTree` data structure is introduced to allow `SVIntervalTree`-style lookups for paired intervals. To finish this work, future PRs will 1) use the collected evidence target links to annotate our assembly called-variants with the number of split reads and read pairs observed in the original mappings and 2) create IMPRECISE VCF records for events that have enough evidence-target-link support, first for deletions and then possibl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469:768,extend,extending,768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469,1,['extend'],['extending']
Modifiability,This PR merges the BQ Write API with the ref ranges table and refactors the Import wdl to be single sample.; I pulled out the create tables into it's own wdl. It doesn't make sense to try this for each individual sample. For now it need to be run separately. We are also not setting the is_loaded field in the sample_info table - this needs to be resolved when we decide how we want to track that info. See https://docs.google.com/document/d/1_ox38x7YjSeQx1I-6K_6kB4TTlonaEah2LRGevN9GmM/edit# for details,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7530:62,refactor,refactors,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7530,1,['refactor'],['refactors']
Modifiability,"This PR only has a subset of the tools, but I wanted put something out there quickly to get comments and make sure I'm on the right track.; - Created tools.picard subpackage.; - Extend CommandLineProgram with PicardCommandLineProgram.; - Ported the following CLPs, with tests and small test files from Picard:; - AddCommentsToBam; - CleanSam; - CreateSequenceDictionary; - FastqToSam; - MergeBamAlignment; - RevertSam; - SamFormatConverter; - SamToFastq; - ValidateSamFile. Some notes:; - doWork() returns null for most CLPs. The exception is ValidateSam; in Picard, it returns a meaningful exit code (0 if input SAM is valid, 1 if not). Various unit tests were relying on this behavior. For now, I preserved it by returning a boolean.; - MergeBamAlignment actually involves a fair amount of logic, a la MarkDuplicates. It combines an aligned BAM with an unmapped BAM. Its helper classes have been placed in utils.sam.mergealignment. More information can be found there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/124:178,Extend,Extend,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/124,1,['Extend'],['Extend']
Modifiability,This PR tries to solve several issues:; - Refactoring constructors for LIBS (#1879); - Adding maximum depth per sample argument to `LocusWalker` to avoid memory overload; - Fixing `Pileup`/`CheckPileup` tools for command line read filters; - Method for fix overlaps in `ReadPileup` in the same way as samtools (#2034),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154:42,Refactor,Refactoring,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154,1,['Refactor'],['Refactoring']
Modifiability,"This addresses https://github.com/broadinstitute/gatk/issues/1015 and https://github.com/broadinstitute/gatk/issues/1094. The idea is to remove the single reducer sort (which doesn't scale), by performing a totally ordered parallel sort on the reads, then writing each partition as a (headerless) BAM file. Finally, the BAM files are concatenated together after writing an initial header. This is very similar to the approach that Hadoop-BAM takes, but adapted to work on Spark. I haven't done extensive benchmarking, but when I ran MD on a ~75MB BAM the runtime dropped from >30 mins to around 8 mins. This is still worse than the walker equivalent for small files, but it's an improvement that means many jobs that didn't finish before now do. Note that this includes the changes from https://github.com/broadinstitute/gatk/pull/1127. I'll rebase once that is committed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1174:453,adapt,adapted,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1174,1,['adapt'],['adapted']
Modifiability,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5608:260,variab,variable,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608,1,['variab'],['variable']
Modifiability,"This adds a hard filter for low variant allele fraction calls. We will not turn this on by default in any of our pipelines, but it will give users an easy option to filter everything below a certain VAF that they don't care about. It also adds a hard filter for low alt depth calls based on a threshold from the median autosomal coverage (that must be supplied as an argument). It takes the cutoff from a Poisson with a mean of 1.5 * median coverage (to account for NuMTs with 3 copies in the autosome) and is tuned to catch 99% of the false positives (which we know will also catch lots of true positives). . It also removes the Polymorphic NUMT annotation (since that's basically what's going into the filter at this point).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842:630,Polymorphi,Polymorphic,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842,1,['Polymorphi'],['Polymorphic']
Modifiability,This allows `PrintReads` and other classes that inherit from `ReadWalker` to split files over an arbitrary number of intervals and not get any repeated reads across all of the split data files. - Added reads-must-start-within-intervals flag to ReadWalker to allow for; splitting of files over an interval set without seeing repeats. - Added a new iterator type to filter reads by this interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6054:48,inherit,inherit,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6054,1,['inherit'],['inherit']
Modifiability,This also means the `GeneListOutputRenderer` will need to accept a config file as a parameter. `SimpleTsvOutputRenderer` already does this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5962#issuecomment-494907060:67,config,config,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962#issuecomment-494907060,1,['config'],['config']
Modifiability,This branch does 2 things. ; 1. It makes ProgressMeter async #; 2. It makes ProgressMeter and interface so it could be made more flexible for non-locatables. This could be a first step to making it more flexible for https://github.com/broadinstitute/gatk/issues/6390 and https://github.com/broadinstitute/gatk/issues/5178,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6484:129,flexible,flexible,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6484,2,['flexible'],['flexible']
Modifiability,"This bug has become part of a bigger effort to address how configure the gatk. We're working on a general solution to avoid this sort of issue in the future. We haven't addressed this specific subcase yet though. For now the workaround I described above should work for you. If it doesn't, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565:59,config,configure,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565,1,['config'],['configure']
Modifiability,"This can improve some build configurations for GATK (ony noted the ones in 4.6, but not previos ones):. * `failFast` property for test tasks. This would be useful for PRs; * Declare reasons for dependency resolution rules and constraint dependencies. This could be useful for explaining why some dependencies are not the latest (e.g., protobuf).; * Allow options in the command line. This could be nice for the doc generation.; * Default jacoco is 0.8.0, which improves the coverage report by filtering out some empty constructors",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4659:28,config,configurations,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659,1,['config'],['configurations']
Modifiability,"This code (building off of Louis' fixes) adds the following:; - AuthHolder, a replacement for the PipelineOptions. It stores the authentication info we need for GCS and supports both API_KEY and client-secrets.json. I adapted a few classes to accept an AuthHolder.; - BaseRecalibratorOptimizedSpark, a port of the ""shard"" approach I first did on the Dataflow side. Note that currently this code only performs reasonably for small inputs if you specify -L on the command line (for large inputs it doesn't matter).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987:218,adapt,adapted,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987,1,['adapt'],['adapted']
Modifiability,"This codec should check for the existence of a config file next to the specified file which will inform the parser of the columns from which to parse the `contig`, `start`, and `end` locations from columns.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3898:47,config,config,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3898,1,['config'],['config']
Modifiability,"This comes from pre-Barclay code, but it is an issue when looking for tests for the plugin:. * `ReadFilterPluginUnitTest` lives in org.broadinstitute.hellbender.engine.filters while the plugin lives in `org.broadinstitute.hellbender.cmdline.GATKPlugin`; * In addition, the test is called in a different way that the plugin, which is `GATKReadFilterPluginDescriptor`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2532:84,plugin,plugin,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2532,3,['plugin'],['plugin']
Modifiability,"This contains the streaming Python executor, implemented by a (Python-independent) StreamingProcessController. The StreamingProcessController, and the existing ProcessController are refactored to use a shared ProcessControllerBase, and changed from using raw Threads and Runnables to using an ExecutorService with Futures and Callables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3872:182,refactor,refactored,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3872,1,['refactor'],['refactored']
Modifiability,"This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM). As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via `gcloud auth`, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on. The message is useful, for if we *were* running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095:419,variab,variable,419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095,1,['variab'],['variable']
Modifiability,"This extends Variant Eval to compare AFs between variants in binned AF buckets based on Thousand Genomes VCF, between the expected AF from Thousand Genomes and the seen one in the actual VCF, to be used as a QC metric for our arrays pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6039:5,extend,extends,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6039,1,['extend'],['extends']
Modifiability,"This feature was initially opened in this PR: https://github.com/broadinstitute/gatk/pull/8750, after which @lbergelson and @droazen made comments here; https://github.com/broadinstitute/gatk/pull/8752. . The driving use-case is that we took over the GATK3 MergeVariantsAndGenotypes tool at DISCVRseq and users have been requesting the older behavior on VCF merges, such as: https://github.com/BimberLab/DISCVRSeq/issues/313. . The original PR has been languishing since March and I'm hoping to finalize this feature. Because I cant write to the GATK repo and b/c @lbergelson made some suggestions on a GATK-based branch I am going to put every together into one clean PR, which responds to the code review from the thread above. . To recap background: . - In GATK3, when merging variants, the IDs of all the source VCFs were retained. The GATK4 code path seems like it intended to do this, since the variantSources set is generated, but that variant isnt used for anything (I assume GATK3 code was partially carried forward to incompletely refactored?). . - This PR is designed to allow code to opt-in to the old GATK behavior of retaining the IDs of source VCFs in the ID field. It will not change the default behavior for existing code. - I dont think I can kick off the test suite, but these tests did pass here: https://github.com/broadinstitute/gatk/pull/8752. Again, @lbergelson and @droazen both reviewed the original PR and seemed fine with it in principle. The primary concern raised by @droazen was to avoid changing the current defaults and to not create additional burden (such as adding sorts). I believe this addresses both of those concerns. @jamesemery commented on the thread at one point as well. . Is there anything I can do to help move this forward? Thanks for your time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9032:1041,refactor,refactored,1041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9032,1,['refactor'],['refactored']
Modifiability,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787:12,Inherit,Inherited,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787,4,"['Inherit', 'extend', 'inherit']","['Inherited', 'extends', 'inherit', 'inherited']"
Modifiability,"This fixes several bugs in CompareSAMs, and adds test coverage:; - Bug 1: Program couldn't handle multiple reads with the same read name + start coordinate. To get around this, we add a UniquePrimaryAlignmentKey class and use that. ; - As a consequence of this, some unit tests were incorrect (they relied on the existence of the aforementioned bug). A test file was modified to fix this.; - Bug 2: When comparing unsorted files, if the read names stop being equal at any point, the program would throw an exception. Instead, it should finish but return ""false"".; - Bug 3: The case of queryname-sorted or unsorted inputs were not tested. At all. That counts as a bug, IMO.; - Added unit tests for all these cases. ; - Did some refactoring (I had initially hoped to abstract out the traversal method, but that will have to wait).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/183:727,refactor,refactoring,727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/183,1,['refactor'],['refactoring']
Modifiability,"This includes wrappers to present `SAMRecords` to the tools; Also adding 4 simple tools as examples; `FlagStatsDataflow`; It makes use of dataflow's built in hierarchical aggregation; `CountBasesDataflow`; Simple walker that makes use of the SAMRecord conversion; `CountReadsDataflow`; Does what it says; `PrintReadsDataflow`; This is a very limited version of our print reads walker; It prints `SAMRecords` as strings to an unordered text file; It could potentially be useful as method for examining bam output before we have a proper bam writer. These tools exist in two parts:; A transform extending from `PTransformSAM` (A subclass of `PTransform<Read,O>` which facilitates conversion to `SAMRecord`; A command line tool implementing a complete pipeline; These pipelines can apply arbitrary `ReadFilter`s/ `ReadTransformer`s which are applied before the main transform; (a list of transforms and a list of filters can be applied, it's currently not handled very efficiently though, better to pre-comine them into a single meta transform). Currently, only tests which use local files are running on travis.; There is code included to run on files in buckets, but the tests for it are currently disabled due to travis configuration issues (will be resolved in a seperate ticket). Some changes were made to existing classes to make them Serialize properly; Some test files were moved to help normalize test data locations (although not all tests are normalized, should be done in separate ticket); the new storage locations are based on the complete package name rather than just the tool name",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/443:593,extend,extending,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/443,2,"['config', 'extend']","['configuration', 'extending']"
Modifiability,This is a bit of a pain because the sam files produce by htsjdk now all say they're version 1.5. We need to rewrite our sam/bam files to be compliant with the new version (or at the very least update the version strings. ) Alternatively we could change the comparator to ignore versions when comparing.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/758:108,rewrite,rewrite,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/758,1,['rewrite'],['rewrite']
Modifiability,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesnâ€™t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1827:161,refactor,refactoring,161,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827,4,"['evolve', 'extend', 'refactor']","['evolve', 'extend', 'refactoring']"
Modifiability,"This is a patch to fix the integration test that is broken in the EchoCallset.; There was refactoring done on GvsExtractAvroFilesForHail (in the EchoCallset branch) that has broken the inputs to the integration test on that branch. ; I'm not sure this is the perfect solution, but I'd like to get it merged into EchoCallset so we can unify EchoCallset and ah_var_store",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8737:90,refactor,refactoring,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8737,1,['refactor'],['refactoring']
Modifiability,This is a strict copy-paste job. I'll do further refactoring after this. I'm doing this in two steps so it's easier to read the diffs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/759:49,refactor,refactoring,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/759,1,['refactor'],['refactoring']
Modifiability,This is a suggested refactor of PrintSVEvidence to use a single output stream and parameterize the type of evidence.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7045:20,refactor,refactor,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7045,2,"['parameteriz', 'refactor']","['parameterize', 'refactor']"
Modifiability,"This is a useful class when strictly dealing with pileups. However, it only supports point mutations. Extend to indels....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3641:102,Extend,Extend,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3641,1,['Extend'],['Extend']
Modifiability,"This is a very minimal change of the testing framework to allow users of the framework to use `IntegrationTestSpec` with their own classes. It solves the problem of a custom `Main` class to run the command line test in programs using the framework (through overriding default behavior), and the loading of `GenomeLocParser` by the `BaseTest` if the test is simply extending `CommandLineProgramTest`. More details for this issue in #2033. Now API users could implements and modify default behavior of `CommandLineProgramTestInterface` and use this test classes in `IntegrationTestSpec`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122:364,extend,extending,364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122,1,['extend'],['extending']
Modifiability,"This is because the GencodeFuncotationFactory will force the datasource name (`getName()`) to return ""Gencode"". However, some areas of the code (e.g. `Funcotator.java`) will query the name from the config file. If these do not match, confusion ensues. You will get IGRs for everything, since all queries into the datasource will yield no found features/transcripts.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4791:198,config,config,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4791,1,['config'],['config']
Modifiability,This is fixed for now with a travis environment variable. I'm testing lb_add_region_to_dataproc to make sure that the fix works before removing that variable and merging a change to the dataproc code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6129#issuecomment-525941840:48,variab,variable,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6129#issuecomment-525941840,2,['variab'],['variable']
Modifiability,"This is meant as a group discussion that could happen several places, and here is as good as any. We know that shipping around the header is has a _huge_ cost. So, we need to find a way to effectively strip it from the `SAMRecord` without breaking it. I propose the following.; - Modify `SAMRecord` to use getter methods for the header; - Create a `HeaderSAMRecord` that extends `SAMRecord` and that has a static field for the header. This class would override `getHeader` to return the static; - Use `Broadcast` with `mapPartitions` to set the static on each worker. An alternative would be audit the field usage and do a combination of performing all necessary calls that require the header to when we load the reads and, if possible, making the still offending methods inaccessible. So, @tomwhite , @akiezun , @droazen , @lbergelson , @jean-philippe-martin , what do you all think?. I know @lbergelson previous expressed he didn't like the usage of statics for this purpose.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900:371,extend,extends,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900,1,['extend'],['extends']
Modifiability,"This is not ready for merge -- I just want to see if tests pass with this configuration. There are still some unresolved vulnerabilities:. ```; [1/7] - pkg:maven/com.google.protobuf/protobuf-java@4.0.0-rc-2 - 3 vulnerabilities found!; [2/7] - pkg:maven/log4j/log4j@1.2.17 - 6 vulnerabilities found!; [3/7] - pkg:maven/org.codehaus.janino/janino@3.1.9 - 1 vulnerability found!; [4/7] - pkg:maven/net.minidev/json-smart@2.4.7 - 1 vulnerability found!; [5/7] - pkg:maven/org.codehaus.jettison/jettison@1.1 - 3 vulnerabilities found!; [6/7] - pkg:maven/org.eclipse.jetty/jetty-util@9.4.48.v20220622 - 1 vulnerability found!; [7/7] - pkg:maven/org.eclipse.jetty/jetty-http@9.4.48.v20220622 - 1 vulnerability found!; ```. Some of these we may be unable to resolve. Eg., the `protobuf-java` version in this branch appears to be the most recent one, but still has open vulnerabilities filed against it. The ancient log4j 1.x version is used by two of our dependencies (`hdf5-java-bindings` and `spark-mllib_2.12`), and is the most recent version. Note that this is completely unrelated to the infamous log4j 2.x vulnerability, which was patched in GATK a long time ago.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581408853:74,config,configuration,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8352#issuecomment-1581408853,1,['config'],['configuration']
Modifiability,This is now configurable via Owner -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-358072659:12,config,configurable,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-358072659,1,['config'],['configurable']
Modifiability,"This is only changing codepaths related with the help. So this change will change the usage to say that all filters are valid for disabeFilter to say that only the available ones are valid. The only problem could be in the docgen code, but not in the behavior of the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845:267,plugin,plugin,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2360#issuecomment-275496845,1,['plugin'],['plugin']
Modifiability,"This is rebased off of https://github.com/broadinstitute/gatk/pull/3716, since it depends on code there. Hence, only the second commit needs to be reviewed in this PR. The code and tests are quite similar to that for PlotSegmentedCopyRatio/PlotACNVResults. However, I've changed the R scripts to be more efficient (WGS plots no longer take several hours). Furthermore, PlotModeledSegments is more flexible than PlotACNVResults in that it plots CR, AF, or both on the fly depending on the available inputs. I've also added some more input validation, changed some terminology, and moved over to data.table for reading TSVs in R.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3729:397,flexible,flexible,397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3729,1,['flexible'],['flexible']
Modifiability,"This is related to #7287. . By default MultiVariantWalkerGroupedOnStart will iterate over any variant that spans the user-provided intervals. This is not what one would typically want when running scatter/gather jobs, since variants spanning interval borders would be included in both jobs. There is a variable/argument for IGNORE_VARIANTS_THAT_START_OUTSIDE_INTERVAL, but it's private and therefore subclasses cant read it. I would like our MultiVariantWalkerGroupedOnStart to view this value and at least log a warning if the current job hasUserSuppliedIntervals(), and ignoreIntervalsOutsideStart=false. In this PR I just make that variable protected, but I could also add formal getter/setters if you prefer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7301:302,variab,variable,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7301,2,['variab'],['variable']
Modifiability,"This is the set of fixes for the filter plugin, @cmnbroad. The first commit is the change introduced in #2385 for less verbose test output, so it should be drop once it is accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-278899912:40,plugin,plugin,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-278899912,1,['plugin'],['plugin']
Modifiability,"This is to overhaul tests on SV assembly-based non-complex breakpoint and type inference code. ------. What is done:. * new AssemblyBasedSVDiscoveryTestDataProvider classes which hold manually computed expected values. Unit tests simply load the expected values and compare with actual values calculated on the fly; * fix bugs in BND formatted variants and tested (using the structure above). Classes affected in `main` ; * `AnnotatedVariantProducer`: methods are grouped together and renamed to reflect that the annotations added are assembly-specific or using short reads; * `BreakEndVariantType`: more detailed types (mostly about how alt allele, with the ref bases and square brackets); * `BreakpointComplications` and `BreakpointsInference`: mostly to add trivial methods used only in tests; `DiscoverVariantsFromContigAlignmentsSAMSpark`: now a thin CLI, where functionalities are refactored into new class `ContigChimericAlignmentIterativeInterpreter`; * `SimpleChimera`: new documented and tested method `firstContigRegionRefSpanAfterSecond ` and trivial test-related code",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4835:887,refactor,refactored,887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4835,1,['refactor'],['refactored']
Modifiability,"This likely has to do with your spark configuration. Check on the Spark job's progress through the web interface, which should be something like http://<driver_address>:4040 (see https://spark.apache.org/docs/latest/monitoring.html). . If your BAM is very small, you can also try increasing the number of partitions by reducing --bamPartitionSize.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312316932:38,config,configuration,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312316932,1,['config'],['configuration']
Modifiability,This list was generated using a not-yet merged version of my CRAM metadata tool that uses my not-yet-merged refactored CRAM code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6018#issuecomment-505925995:108,refactor,refactored,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018#issuecomment-505925995,1,['refactor'],['refactored']
Modifiability,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:132,extend,extended,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['extend'],['extended']
Modifiability,"This moves all logging to log4j (previously we had a mix of log4j and SAMTools logging). . Questions:There are about 35 places where we were using the SAMTools ProgressLogger, which assumes SAMTools logging. I reproduced that class in hellbender, but implemented the SAMTools ProgressLoggerInterface in order to retain compatibility with SAMWriters, since we use those in a couple of places. The hellbender ProgressLogger class is in utils.runtime, not sure if there is a better place for it. Also I'm not sure how to handle the source code attribution of it since it was lifted from SAMTools but slightly modified for hellbender ? Can I do that or do I need to rewrite it from scratch ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/612:662,rewrite,rewrite,662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/612,1,['rewrite'],['rewrite']
Modifiability,This must be the most discussed tool in the GATK. I am relieved to close the book on this PR. Very nice work @mwalker174 -- thanks for all the refactoring!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243#issuecomment-936636793:143,refactor,refactoring,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243#issuecomment-936636793,1,['refactor'],['refactoring']
Modifiability,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:254,refactor,refactoring,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501,1,['refactor'],['refactoring']
Modifiability,"This new PathSeq WDL redesigns the workflow for improved performance in the cloud. Downsampling can be applied to BAMs with high microbial content (ie >10M reads) that normally cause performance issues. . Other improvements include:. * Removed microbial fasta input, as only the sequence dictionary is needed.; * Broke pipeline down to into smaller tasks. This helps reduce costs by a) provisioning fewer resources at the filter and score phases of the pipeline and b) reducing job wall time to minimize the likelihood of VM preemption.; * Filter-only option, which can be used to cheaply estimate the number of microbial reads in the sample.; * Metrics are now parsed so they can be fed as output to the Terra data model.; * CRAM-to-BAM capability; * Updated WDL readme; * Deleted unneeded WDL json configuration, as the configuration can be provided in Terra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6536:800,config,configuration,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6536,2,['config'],['configuration']
Modifiability,This optimizes the defaults in mitochondria-mode for WGS mitochondria calling. It changes the `pruning-lod-threshold` in adaptive pruning and the `lod-divided-by-depth` threshold in `FilterMutectCalls`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544:121,adapt,adaptive,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544,1,['adapt'],['adaptive']
Modifiability,"This pull request is focused on resolving occurrences of Sonar rule squid:S1197 - Array designators ""[]"" should be on the type, not the variable. You can find more information about the issue here: https://dev.eclipse.org/sonar/coding_rules#q=squid:S1197. Please let me know if you have any questions. M-Ezzat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1915:136,variab,variable,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1915,1,['variab'],['variable']
Modifiability,"This replaces a secret that requires a pr to fix, and updates the name of one of the others.; Requires 1 more step after this.; * Switch travis variable name from DOCKER_SERVICE_PASS -> DOCKER_SERVICE_TOKEN for clarity; * Replace gcloud encrypted key",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7521:144,variab,variable,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7521,1,['variab'],['variable']
Modifiability,"This request was created from a contribution made by Yanis Chrys on August 19, 2021 11:35 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-](https://gatk.broadinstitute.org/hc/en-us/community/posts/4405429551515-JEXL-expression-for-filtering-on-AD-SelectVariants-FastaAlternateReferenceMaker-). \--. Hi, ; ; I am working on haploid bacterial data and I ran into a limitation of the program that I either can't solve or it would be nice to add a funtion for it in the future. I'll explain the issue:. Let's say I have (low coverage) data that I want to turn into an alternate fasta reference where: ; ; REF: A. ALT: AAGT,T,CA. If I want to keep variants where the AD > \[threshold\] I can't do. \-select 'vc.getGenotype(""sample"").getAD.1'. because for my sample it could be that the called ALT is getAD.2 and so far I haven't been able to use anything other than a number as an index to getAD. This would be solved if we could do:. getAD.getGT OR getAD.IndexOfAlleleWithHighestCount. but to my knowledge none of these will work because JEXL will give an error. Maybe extending JEXL java operation to the AD array could fix it? Because even getAD\[0\] gives an error. Do you have a solution to this?. PS. I am sorry if this should have been under General Questions<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/177956'>Zendesk ticket #177956</a>)<br>gz#177956</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7448:1180,extend,extending,1180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7448,1,['extend'],['extending']
Modifiability,"This requires also a finer control for the codecs, once the configuration-code is implemented, to ignore default packages, and include/exclude single classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-324272895:60,config,configuration-code,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-324272895,1,['config'],['configuration-code']
Modifiability,"This script will look for a small input that trips BaseRecalibrator. However, it can be adapted for debugging pretty much anything else, so long as you have two versions of the code: a ""known good"" one to compare against, and a ""under test"" one that has the bug you're trying to generate a minimal input for.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/913:88,adapt,adapted,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/913,1,['adapt'],['adapted']
Modifiability,"This seems like a consequence of the fact that we use `java.nio.file.Path`for a lot of things in gatk. This requires a custom `java.nio.file.spi.FileSystemProvider` to be available for each type of path you want to be able to resolve. Spark native uses `org.apache.hadoop.fs.Path` for a lot of things. It's seems likely that that maprfs provides a hadoop file system plugin, which many spark applications can consume, but it's unlikely that it also provides a java.nio.file.Path implementation. ; ; I don't think we'd be able to implement a provider for maprfs ourselves. We don't have any systems with maprfs and don't have the bandwidth to take it on right now. Implementing a file system provider isn't a terribly complicated project, but it's not a trivial one either. However, there's an implementation for hadoop here https://github.com/damiencarol/jsr203-hadoop which is sufficient for what gatk does. If maprfs provides a hadoop file system, it would probably not be too difficult to take that project as a template and modify it to use the maprfs implementation. . I think the only things you'd have to implement for the spark tools to work are the basic Path operations that support the simple operations like `Paths.get()`,`Files.exists()`, and `Path.resolve()`. (although that's not a complete list. . If you are interested in writing a plugin like that, you can add it to the gatk class path at runtime. We might also be open to packaging such a plugin with the gatk if there was wide demand for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555:367,plugin,plugin,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936#issuecomment-350070555,6,['plugin'],['plugin']
Modifiability,"This seems like a lot of machinery (introducing two new types and a new method) just to hide the config file argument. What if we just mark it `@Hidden` (I know thats prohibited, but this is kind of a special case). The only reason it even exists is because we wanted it to appear in the command lines we display on output and embed in output files. If its `@Hidden` it will still be reflected there when it's used, but it wouldn't be displayed in tool help/usage. Its already always displayed in help as an arg for the gatk wrapper.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371570897:97,config,config,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4474#issuecomment-371570897,1,['config'],['config']
Modifiability,"This seems to be a regression with GATK 4.1.0.0. The code does check for compatible versions before beginning traversal. However, the following log was reported using the M2 WDL:; ```; Runtime.totalMemory()=58851328; ***********************************************************************. A USER ERROR has occurred: Bad input: Config file for datasource (file:///cromwell_root/funcotator_dataSources.v1.4.20180615/gencode/hg19/gencode.config) does not contain required key: ""ncbi_build_version"". ***********************************************************************; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005:328,Config,Config,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660#issuecomment-463020005,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"This seems to happen in the cloud auth layers, which I don't control. . One potential workaround would be to add a command-line option to disable GCS support. This would only help the original reporter if they don't use GCS paths, of course. Is this something we think may be worth doing at all?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427413074:39,layers,layers,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427413074,1,['layers'],['layers']
Modifiability,This seems to have already been refactored at some point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2102#issuecomment-590488457:32,refactor,refactored,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2102#issuecomment-590488457,1,['refactor'],['refactored']
Modifiability,"This set of optimizations brings the GATK4 HaplotypeCaller performance into line; with GATK3.x performance. Note that HaplotypeCallerSpark is not touched by this PR (that is for a future PR). Summary of changes:. * AssemblyRegionWalker: query all intervals on each contig simultaneously, rather than individually; * GATKRead: Cache adaptor boundary, soft start/end, and cigar length; * GATKRead: add getBasesNoCopy() / getBaseQualitiesNoCopy(); * ReadPileup: speed up stratified constructor; * LIBS.lazyLoadNextAlignmentContext(): don't keep pileup elements unnecessarily separated by sample during pileup creation; * Restore faster GATK3 version of ReferenceConfidenceModel.sumMismatchingQualities(); * RefVsAnyResult: nest within ReferenceConfidenceModel, and allow direct field access; * Remove redundant getBases() call in ReadThreadingGraph; * Fix BaseGraph Utils.validateArg() call; * ReadPileup: replace Collections.unmodifiableList(pileupElements).iterator() with direct return of an iterator that forbids removal; * Kill expensive bounds checking in GATKRead getBase()/getBaseQuality()/getCigarElement(); * Kill nonNull checks in PileupElement; * Kill expensive PileupElement and ReadPileup arg validation; * GATKRead adapter: clear cached values upon mutation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031:332,adapt,adaptor,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031,2,['adapt'],"['adapter', 'adaptor']"
Modifiability,This ticket aims to centralize small documentation errors such as typos and syntax errors that can be addressed in bulk. - [x] HaplotypeCaller doc has some syntax errors in links causing entire paragraphs to be included in the link. ; - [x] CollectAllelicCounts has a syntax error that causes a code format block to extend to most of the page (probably a missing closing tag). ; - [x] CalculateContamination has a missing `</pre>` tag that also causes a code format block to be extended to the rest of the page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3173:316,extend,extend,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3173,2,['extend'],"['extend', 'extended']"
Modifiability,"This ticket is just to make the codec packages configurable, which would be resolved by https://github.com/broadinstitute/gatk/pull/3447. If you need more fine-grained control than this, we could discuss as part of a separate ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337651849:47,config,configurable,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337651849,1,['config'],['configurable']
Modifiability,This ties into the URI class design meeting we're having next week -- I'd say wait until then before starting any refactor of this part of the code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4480#issuecomment-369943177:114,refactor,refactor,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4480#issuecomment-369943177,1,['refactor'],['refactor']
Modifiability,"This tool should be a ReadWalker, but because of the way it uses the reference it may require a bit of refactoring to port it to the ReadWalker interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/123:103,refactor,refactoring,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/123,1,['refactor'],['refactoring']
Modifiability,This uses the new defaults with adaptive pruning in version 4.1.0.0 in Mutect and removes the old ad hoc pruning argument. @ldgauthier can you please take a look when you get a chance?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5669:32,adapt,adaptive,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669,1,['adapt'],['adaptive']
Modifiability,"This version introduces a change that (at least on my machine) fixes the mysterious ""happens only on the command line"" test failure. Also uses a newer version of genomics-dataflow because I had to fix a bug there for API_KEY to work in our setting. Finally, this version also moves the files around so they match the local tree, and changes the environment variables naming scheme to be a little more consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535:357,variab,variables,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535,1,['variab'],['variables']
Modifiability,This was an issue with propagating polymorphic std::exception code from the native library's logger utility and has been fixed in the [1.3.2 release ](https://mvnrepository.com/artifact/org.genomicsdb/genomicsdb/1.3.2) of the GenomicsDB library. Also note that using java option `GATK_STACKTRACE_ON_USER_EXCEPTION` with gatk will also output a C/C++ limited stacktrace as requested by @lbergelson.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6852:35,polymorphi,polymorphic,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6852,1,['polymorphi'],['polymorphic']
Modifiability,"This worked until we kebabified. The Freemarker template looks for the hardcoded string ""readFilter"", but the doc system populates the Freemarker map using the display name self-reported by the (ReadFilter) plugin, which is in turn derived from the standard argument name for read filters. These matched when the arg was ""readFilter"". But after kebabification, its now ""read-filter"". We should probably change the plugins to use a fixed display.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4387:207,plugin,plugin,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4387,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"Those sound like issues with the IndelRealigner tool from GATK3, which is; not part of our pipeline anymore. Is this still a problem with 4.1.4.1?. On Wed, Feb 19, 2020, 1:00 AM Dario Strbenac <notifications@github.com>; wrote:. > In the news file of a structural variant software I use, I read; >; > Added FIX_SA and FIX_MISSING_HARD_CLIP; > FIX_SA: rewrites split read SA tags; > corrects GATK indel realignment SA tag data inconsistency; > FIX_MISSING_HARD_CLIP: infers missing hard clipping if split read records; > have different lengths; > corrects for GATK indel realignment stripping hard clipping when realigning; >; > Could such issues perhaps be resolved in an update to GATK?; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6459?email_source=notifications&email_token=ABSGC5E7CIUF53HYCPS76FDRDTDHBA5CNFSM4KXSMK22YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4IOQ3U6A>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABSGC5DYKL5KH6EZS5ZU66DRDTDHBANCNFSM4KXSMK2Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6459#issuecomment-588488049:351,rewrite,rewrites,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6459#issuecomment-588488049,1,['rewrite'],['rewrites']
Modifiability,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8049:310,inherit,inherited,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049,1,['inherit'],['inherited']
Modifiability,To add to this ticket. In #7876 we have had to expand the JumboAnnotations to work in the HaplotypeCaller as well. Unfortunately this has created problems since there aren't evidences objects in the HC so we have had to change the erasure of the annotate() methods somewhat and some hacky code is now part of the `VariantAnnotatorEngine` which currently has some code in the `addInfoAnnotations()` method that has to resolve the complicated spiderwebs of which likelihoods objects do or don't exist at any given time and then cast them to what they likely are. This really needs to be revisited and refactored to handle the extra annotation inputs more gracefully.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7543#issuecomment-1191802150:599,refactor,refactored,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7543#issuecomment-1191802150,1,['refactor'],['refactored']
Modifiability,To allow implementation of other `ReadFilter` plugins with the same parameter names.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2352:46,plugin,plugins,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2352,1,['plugin'],['plugins']
Modifiability,"To clarify what needs to be done here:. -Add a new `--javaOptions` argument to `gatk-launch`. -When running with a packaged local jar, the value of `--javaOptions` should be injected into the command line built by `formatLocalJarCommand()`. -When running with the ""wrapper script"" (as a result of building with `./gradlew installDist` instead of `./gradlew localJar`), propagate the value of `--javaOptions` to the `JAVA_OPTS` environment variable the wrapper script expects. You can inspect the wrapper script itself by running `./gradlew installDist` and then examining `build/install/gatk/bin/gatk`. -When running on Spark, you'll need to add the `--javaOptions` to `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2694#issuecomment-305007868:439,variab,variable,439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2694#issuecomment-305007868,1,['variab'],['variable']
Modifiability,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:75,config,configured,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551,1,['config'],['configured']
Modifiability,"To provide some more background, the idea is to generate output as generated by [CollectAllelicCounts](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_copynumber_CollectAllelicCounts.php) for a pool of normals so that we can correct allelic biases in tumor-only. Would it be possible that CreateSomaticPanelofNormals is extended to cover the CollectAllelicCounts ""special case""? . @samuelklee @davidbenjamin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5649#issuecomment-462058940:380,extend,extended,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5649#issuecomment-462058940,1,['extend'],['extended']
Modifiability,Tools be able to override the behavior of options they have inherited?. How should this work? Should you be able to change a required field to an optional one? Or only the other way around? . If you override the a field that has an `@Option` annotation does it effectively replace it?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/149:60,inherit,inherited,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/149,1,['inherit'],['inherited']
Modifiability,"TrainVariantAnnotationsModel:. Trains a model for scoring variant calls based on site-level annotations. TODOs:. - [x] Integration tests. Exact-match tests for (non-exhaustive) configurations given by the Cartesian product of the following options:; * non-allele-specific vs. allele-specific; * SNP-only vs. SNP+INDEL (for both of these options, we use extracted annotations that contain both SNP and INDEL variants as input); * positive (training with *.annot.hdf5) vs. positive-unlabeled (training with *.annot.hdf5 and *.unlabeled.annot.hdf5); * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:177,config,configurations,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,2,"['Refactor', 'config']","['Refactor', 'configurations']"
Modifiability,"Turns out a typo prevents running the ""manage_sv_pipeline"" script, saying GATK_DIR is an unbound variable. Please fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318746083:97,variab,variable,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318746083,1,['variab'],['variable']
Modifiability,Unbound variable bug fixed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318751586:8,variab,variable,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3370#issuecomment-318751586,1,['variab'],['variable']
Modifiability,"Unit tests for tool X should not rely on the behavior of instanceMain or doWork in tool Y. . In particular, unit tests that involve comparing/validating outputs should not reference CLPs like CompareSAMs or ValidateSamFile directly. Instead, these CLPs should just be thin wrappers around other classes that have the actual logic. This is already the case for ValidateSamFile, which is just a wrapper for SamFileValidator in HTSJDK. CompareSAMs should be refactored to match this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/145:455,refactor,refactored,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/145,1,['refactor'],['refactored']
Modifiability,Update to latest version of VQSR Lite; Refactor GvsCreateFilterSet.wdl to move VQSR Classic code to its own WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8269:39,Refactor,Refactor,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8269,1,['Refactor'],['Refactor']
Modifiability,Update variable names in mutect2_pon.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259:7,variab,variable,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259,1,['variab'],['variable']
Modifiability,Update variable names in mutect2_pon.wdl to reflect changes in mutect2.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259:7,variab,variable,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259,1,['variab'],['variable']
Modifiability,Updates to `gatk` launch script to fix properties and config file definitions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653:54,config,config,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653,1,['config'],['config']
Modifiability,"Upgrade to Barclay 3.0.0, with changes for FeatureInput discovery based on Barclay refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4523:83,refactor,refactoring,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4523,1,['refactor'],['refactoring']
Modifiability,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7609:90,plugin,plugins,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"Use SampleLocatableMetadata if you want CombineSegmentBreakpoints to only operate on segment files from a single sample. It's conceivable that you want it to be more flexible, in which case I would use LocatableMetadata. Also, go ahead and move the collection class into the collection package, rather than expose the abstract classes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352762883:166,flexible,flexible,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352762883,1,['flexible'],['flexible']
Modifiability,"Use argument for execution project id for queries, specifically in the sample query. This was a gap since the project id was already used for Storage API usage. I think the SampleList class could also use some refactoring, but wanted to keep this PR small",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7136:210,refactor,refactoring,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7136,1,['refactor'],['refactoring']
Modifiability,Use default plugin instances (i.e. readFilters) as the default value for args in gatkDoc.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6646:12,plugin,plugin,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6646,1,['plugin'],['plugin']
Modifiability,Use portable Python arg parsing from Docker build script [VS-995],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8380:4,portab,portable,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8380,1,['portab'],['portable']
Modifiability,Use the correct variable when freeing github runner space: AGENT_TOOLSDIRECTORY.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8373:16,variab,variable,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8373,1,['variab'],['variable']
Modifiability,"User Reported this issue as shown below: . I used GenomicsDBImport and GenotypeGVCFs to call SNPs in GATK4. Due to large reference genome (1G) and sample size (100). I want to separate the work for each chromosome. It seems that GenomicsDBImport works for all chromosomes, but GenotypeGVCFs only works for chromosome1. Could you please give me some suggestions. Below are commands and log information for chromosome 2. Look forward to hearing from you soon.; Best regards,; Baosheng. ### command, all variables are defined before command lines.; $GATK --java-options ""-Xmx24g"" \; GenomicsDBImport \; ${InputVCF} \; --genomicsdb-workspace-path ${OUTDIR}/chr02 \; -L Qrob_Chr02. $GATK --java-options ""-Xmx48g"" \; GenotypeGVCFs \; -R ${REF} \; -V gendb://${OUTDIR}/chr02 \; -all-sites \; -O ${OUTDIR}/chr02.vcf. ## log file; 23:41:40.274 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:42:41.990 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.990 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 23:42:41.990 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:42:41.991 INFO GenomicsDBImport - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 23:42:41.991 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:42:41.991 INFO GenomicsDBImport - Start Date/Time: January 26, 2019 11:41:40 PM CST; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865:501,variab,variables,501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865,1,['variab'],['variables']
Modifiability,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3301:731,sandbox,sandbox,731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301,1,['sandbox'],['sandbox']
Modifiability,"User writes: . > It's great that the GATK walkers can follow symlinks to bam files. But I've never understood, why GATK UG and HC can't look for the index file in the same location as the bam. Instead one has to also create symlinks to the index files in the same location as the symlinks to the bam files. This prevents one from using as input a file with bam symlinks. There are several ways to work around this, but it's just such an annoying obstacle, which I run into every 3 or 6 months, because I forget about this behaviour. Please please fix and you can come and raid my mini fridge at work full of chocolate (currently also holds LEGO). If you choose to ignore this enhancement request, then GATK is still a pretty good tool :). Sounds legit to me. Not something I think is worth spending time on in GATK3, but sounds like a reasonable feature request for 4. This Issue was generated from your [forums](http://gatkforums.broadinstitute.org/discussion/5944/symlinks-to-bam-files/p1)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/781:676,enhance,enhancement,676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/781,1,['enhance'],['enhancement']
Modifiability,"Users are supposed to enable/disable async I/O options by passing java option flags to the gatk launch shell script:. `; ./gatk-4.0.1.2/gatk --java-options ""-Dsamjdk.use_async_io_read_samtools=true -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=true"" <Tool and its flags>; `. It appears that no matter how these flags are passed, and even if one hard codes the variables in the actual shell script (change the defaults within the script), they are ignored. **Three examples follow:**. (Irrelevant sections are omitted with "". . ."" in their place for readability) ; (These examples use BaseRecalibrator as an example tool). **Example 1: No Flags Passed (but -Xmx12G)** ; `; ./gatk-4.0.1.2/gatk --java-options ""-Xmx12G"" <Tool specific commands>; `. ```; Using GATK jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar ; Running: ; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx12G -jar /projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar BaseRecalibrator -I /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/hg38_aligned.sorted.dedupped.bam --known-sites /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/dbsnp_hg38/dbsnp_146.hg38.vcf -O /projects/bioinformatics/TestingGATK4/hg38_GATK4/async_options_run/newOutputs/recal.table -R /projects/bioinformatics/DataPacks/human/NA12878_fullsize_bundle_GATK4_copy_2/reference_hg38/Homo_sapiens_assembly38.fasta; 10:45:49.790 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/projects/bioinformatics/TestingGATK4/gatk-4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:45:49.887 INFO BaseRecalibrator - ------------------------------------------------------------ ; 10:45:49.888 INFO BaseRecalibrator - The Genome Analysis Toolkit (GAT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4435:392,variab,variables,392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4435,1,['variab'],['variables']
Modifiability,Using adaptive pruning in mitochondria pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5669:6,adapt,adaptive,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669,1,['adapt'],['adaptive']
Modifiability,Utils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.021 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.092 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 15:41:50.093 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.093 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.config; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; WARNING 2020-08-19 15:41:50 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:50.159 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hg19_All_20180423.vcf.gz -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.159 INFO Data,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:14753,config,config,14753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['config'],['config']
Modifiability,"VCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4115,Config,ConfigFactory,4115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,VS-1159 - Enhance GVSWithdrawSamples,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8599:10,Enhance,Enhance,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8599,1,['Enhance'],['Enhance']
Modifiability,VS-1490 - Refactor python code from extract dir into a scripts directory.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9017:10,Refactor,Refactor,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9017,1,['Refactor'],['Refactor']
Modifiability,Variable depth bins in Mutect2 autoval,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4339:0,Variab,Variable,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4339,1,['Variab'],['Variable']
Modifiability,Variant context and genotype comparison have been heavily refactored for readability and to provide more easily customized comparisons,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6417:58,refactor,refactored,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6417,1,['refactor'],['refactored']
Modifiability,VariantFiltration - port and a ~half-rewrite for GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/614:37,rewrite,rewrite,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/614,1,['rewrite'],['rewrite']
Modifiability,"VariantQC is a tool we made that is somewhat analogous to FastQC. Given an input VCF, it runs VariantEval to generate various summary tables of data, and then makes an HTML report (borrowing a lot from the tool MultiQC) summarizing that VCF. . I wrote this originally by forking GATK3 and wrote a new walker that internally called and run VariantEval. That was never the final plan. I dont know what this will need to look like in GATK4 yet. I'm fine with the expectation that GATK4 VariantEval will evolve and we'd need to update our code wrapping it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440806347:500,evolve,evolve,500,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440806347,1,['evolve'],['evolve']
Modifiability,"VariantWalker/MultiVariantWalker create two FeatureDataSources for each input, which results in redundant dynamic discovery of which codec to use. For a MultiVariantWalker with a lot of inputs, like VariantRecalibrator, this can be a lot of path/stream opening/closing. Propose this small change to cache the codec in the FeatureInput. Ideally FeatureManager would keep track of this, but thats bigger refactor as not all of the FeatureDataSources are created by FeatureManager.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2737:402,refactor,refactor,402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2737,1,['refactor'],['refactor']
Modifiability,"VariantsSpark - Done initializing engine; 19/02/18 16:58:10 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/02/18 16:58:10 INFO org.spark_project.jetty.util.log: Logging initialized @8431ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.Server: Started @8536ms; 19/02/18 16:58:11 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 19/02/18 16:58:11 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 19/02/18 16:58:12 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m/10.240.0.11:8032; 19/02/18 16:58:13 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m/10.240.0.11:10200; 19/02/18 16:58:15 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1550508751046_0004; WARNING	2019-02-18 16:58:23	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; WARNING	2019-02-18 16:58:23	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 19/02/18 16:58:25 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 19/02/18 16:58",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:4890,config,configuration,4890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,Very simple implementation of #2297 using a custom `GATKConf` class to allow both promatically (`GATKConfBuilder`) and by commons-configuration API (constructor). Only includes:. * Packages/Classes to include in the CLP on startup.; * Packages to look for codecs on startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322:130,config,configuration,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322,1,['config'],['configuration']
Modifiability,"WIth the 4.2.2.0 ReblockGVCF it is running fine. This was without rerunning the HaplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:256,variab,variable,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['variab'],['variable']
Modifiability,"WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Request",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4075,Config,ConfigFactory,4075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"We build GATK-SV docker images on GitHub runners. We use the following to set up the environment to use `--squash` flag. https://github.com/broadinstitute/gatk-sv/blob/52813222b64bf2d15fb9a1aae068590bee184511/.github/workflows/sv_pipeline_docker.yml#L199-L204. I am unsure if you can configure the runtime env on Google Cloud build, but if you can, hopefully, the above can hint some directions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1979099671:284,config,configure,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1979099671,1,['config'],['configure']
Modifiability,We can either add `to*LegacySegmentCollection` methods to ModeledSegmentCollection or add static utility methods to ModelSegments. No real preference here as the coupling is minimal.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407170543:162,coupling,coupling,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5037#issuecomment-407170543,1,['coupling'],['coupling']
Modifiability,We could combine the filters and transformers into 1 plugin that can intersperse them. It would be more complicated but be maximally expressive.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246002976:53,plugin,plugin,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246002976,1,['plugin'],['plugin']
Modifiability,We currently use a confusing mix of system properties and environment variables for settings not controlled by command-line arguments. We should choose one or the other.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1666:70,variab,variables,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1666,1,['variab'],['variables']
Modifiability,"We generate indices on output files, but we decided not to auto-generate indices on input files in GATK4. We included tools `IndexFeatureFile` and `BuildBamIndex` that can generate these indices on-demand. Indexing inputs automatically is inherently racy/dangerous in the face of multiple processes sharing inputs unless you do things like file locking, which comes with all sorts of portability issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2647#issuecomment-298768350:384,portab,portability,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2647#issuecomment-298768350,1,['portab'],['portability']
Modifiability,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5998:32,extend,extends,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998,3,"['adapt', 'extend', 'refactor']","['adapt', 'extends', 'refactor']"
Modifiability,"We haven't resolved the issue with our project configuration yet, so we need to publish this ourselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4008:47,config,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4008,1,['config'],['configuration']
Modifiability,"We need a framework for easily comparing annotation values in test suite outputs (actual vs. expected), with a per-annotation configurable tolerance before failure. Exact-match comparisons are too brittle for our needs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3275:126,config,configurable,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3275,1,['config'],['configurable']
Modifiability,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5396:348,config,configurations,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396,1,['config'],['configurations']
Modifiability,We ported a bunch of code that is marked as deprecated. We should delete it and refactor anything that relies on it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/162:80,refactor,refactor,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/162,1,['refactor'],['refactor']
Modifiability,"We probably can't remove `ReferenceWindowFunction`, as it's being used by tools in gatk-protected. How can we guarantee that `JoinStrategy.OVERLAPS_PARTITIONER` will provide at least as much reference context as requested by the configured `ReferenceWindowFunction`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2254:229,config,configured,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2254,1,['config'],['configured']
Modifiability,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4990:130,refactor,refactored,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990,1,['refactor'],['refactored']
Modifiability,"We recently discovered that some of the tests we didn't think required google cloud authentication require that gcloud be initialized. Travis didn't catch this because we always initialize gcloud in order to do log uploading. We should change this so it's only initialized during the tests for the cloud tests. . The actual error we discovered didn't require that credentials be correct, only that a default project had been configured so simply logging out isn't enough to trigger it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2706:425,config,configured,425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2706,1,['config'],['configured']
Modifiability,"We recently introduced some new log4j error messages on spark. ```; og4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; ```. These are likely the result of an additional transitive log4j dependency from GenomicsDB introduced in #2389. . We should can probably stop them with additional exclusions in our spark build.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2622:174,variab,variable,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2622,2,['variab'],['variable']
Modifiability,"We recently updated (PR #4858) the default Smith-Waterman parameters for realigning reads to their best haplotype. Although there is no reason for the alignment of haplotypes to the reference to use the same parameters, it seems like we are similarly favoring indels too much. There is a forum discussion to this effect:. https://gatkforums.broadinstitute.org/gatk/discussion/23230/gatk-haplotypecaller-mnp-output-problem. Here are the parameters we use:. * match: 200; * substitution: -150; * indel start: -260; * indel extend: -11. These parameters, which are essentially a prior on biological variation, prefer an indel, with a cost of 260, to a SNP, with a cost of 350. This does not seem correct. It almost never comes up because the correct alignment is usually unambiguous, but when it does, shouldn't we break the tie in favor of the SNP?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564:521,extend,extend,521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564,1,['extend'],['extend']
Modifiability,"We recommend backing up data just because it is the ""cleanest"" way to roll back. If backing up data is really such a pain point, you could skip doing that. Just back up the callset.json file, and don't turn on `--consolidate` when you're doing incremental import. If a failure happens, just roll back the callset.json and re-do the import. The downside is that the failed import will hang around and take up disk space, but hopefully it is a rare enough occurrence that it doesn't matter - and you will have saved yourself backing up the data. In response to 2) - I guess you're implying that the overhead of cluster/job scheduling won't amortize any benefits from parallelism there? I suppose that could be true, but doesn't seem to be worth optimizing towards that. What I'm asking is whether split and merge are purely an instrument to allow you to choose the granularity of parallelism you want to use? Or is there something else? As I said before, we are considering enabling other ways to do distributed import which would work for the former. It might go something like:; - Create a workspace/initialize configuration+intervals to be imported; - Actually do the import by kicking off (multiple) import(s). User can pick the number of intervals each import is responsible for. User must ensure that no interval gets specified in multiple import processes. P.S: regarding 1000s of small contigs - the current GenomicsDBImport doesn't so so well with large number of contigs (unless you do concatenate the contigs into fewer groups). We hope to have some changes coming soon that will help with that by adding an option for the tool to merge multiple contigs into a single folder in the workspace.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548:1111,config,configuration,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548,1,['config'],['configuration']
Modifiability,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377:20,plugin,plugin,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377,1,['plugin'],['plugin']
Modifiability,We should definitely try to centralize setting of the system properties if possible (perhaps using a master config file) -- though I vote that we put in a quick fix for this first.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267123755:108,config,config,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267123755,1,['config'],['config']
Modifiability,"We should probably adapt the IntervalTree from htsjdk to work for us. We've run into a number of cases where this is needed, in Valentine's exome code and in Tom's hadoop reader. We could use both. `boolean overlaps(Locatable locatable, IntervalTree<Locatable> locatables)`. and . `Set<Locatable> getOverlapping(Locatable locatable, IntervalTree<Locatable> locatables)`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/559:19,adapt,adapt,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/559,1,['adapt'],['adapt']
Modifiability,"We should rely on the GATK config file for the default values for these system properties, rather than hardcoding them into the `gatk` frontend script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4484:27,config,config,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4484,1,['config'],['config']
Modifiability,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3317:372,adapt,adaptive,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317,1,['adapt'],['adaptive']
Modifiability,"We use the GATKGCSOptions class to hold GCP authentication. It inherits from the Dataflow hierarchy and works well there, but since it doesn't implement Serializable it's cumbersome to work with in Spark. We've created AuthHolder as a replacement. It can do all the things GATKGCSOptions can do, and more (well, except for holding Dataflow debug options but we don't need that anymore). Once #978 is merged in, we need to migrate the code from GATKGCSOptions to AuthHolder. One benefit is that this will allow the Spark code to support client-secrets.json (for access to private GCS files, unlike the API key which only grants access to world-readable GCS files).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1002:63,inherit,inherits,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1002,1,['inherit'],['inherits']
Modifiability,"We're seeing messages like the following when running `GenomicsDBImport`:. ```; Column 948660 has too many alleles in the combined VCF record : 61 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; Column 948710 has too many alleles in the combined VCF record : 83 : current limit : 50. Fields, such as PL, with length equal to the number of genotypes will NOT be added for this location.; ```. Is this limit of 50 configurable, if we wanted to raise it, and if not, could it be made configurable?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687:489,config,configurable,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687,2,['config'],['configurable']
Modifiability,"We've filed a ticket with github support -- however, the branch has been cleared to merge in its current state, as it's had more than enough reviews. We can file tickets to improve/refactor once it's in master.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625:181,refactor,refactor,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3945#issuecomment-351092625,2,['refactor'],['refactor']
Modifiability,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:827,Config,Configuration,827,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['Config'],['Configuration']
Modifiability,"Weird, I would have that that forcing the htsjdk version like we already do would have done it... I don't see anything wrong with adding that exclusion, but I'm confused why we need it. ` force 'com.github.samtools:htsjdk:' + htsjdkVersion`. It sounds like a gradle bug in building the final pom file. I wonder if switching to the javaLibrary plugin would fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292579154:343,plugin,plugin,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2578#issuecomment-292579154,1,['plugin'],['plugin']
Modifiability,"Well, I guess that part of the contract for GATKTool is that GATK tools should report progress as they go. I agree that the ProgressMeter should be made more flexible and allow reporting of progress in terms of things other than genomic location.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577262636:158,flexible,flexible,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577262636,1,['flexible'],['flexible']
Modifiability,"Well, that explains that, sort of. The code snippet you're providing looks like it ought to do what you say it does (i.e., the mates have to be paired, not unmapped, mapped to the same contig, and have a difference in their start positions that is at least `mateTooDistantLength`). . But there are two problems with this:. 1) This filter's behavior is unexpected wrt HaplotypeCaller. It seems to me that an inclusive filter (i.e., process only paired-end mappings whose TLEN falls within a specified range) would be more usable. That would imply a filter implementation that accepts a pair of integers, but the expected behavior would be more obvious and in line with GATK's other range-limited parameterizations (e.g., `MappingQualityReadFilter` comes immediately to mind). 2) I can't tell from where I sit, but the code snippet looks correct only if `getStart()` and `getMateStart()` return a zero-based start position of each mate relative to the start of the strand to which the mate is mapped. If the code is just computing the difference between POS for the mates, the computation is incorrect for forward + reverse-complement (Illumina-style) pairs. In addition, computing TLEN requires not only that you consider the orientation of the individual mate mappings, but also that you make an arbitrary decision about how to handle soft-clipped reads. I hate to say this, but I think this parameter needs some attention. Its potential utility with HaplotypeCaller seems evident to me (i.e., it would be good to be able to exclude outliers with unreasonable TLENs) but its implementation and frugal documentation make it unusable in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220:695,parameteriz,parameterizations,695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7701#issuecomment-1103199220,2,['parameteriz'],['parameterizations']
Modifiability,What is the progress on this @cmnbroad? Is this waiting for the new Barclay plugin interface?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-282995363:76,plugin,plugin,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-282995363,1,['plugin'],['plugin']
Modifiability,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:25,portab,portable,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363,1,['portab'],['portable']
Modifiability,"What is your cluster configuration? . That's a lot of memory for one executor, it may be having trouble allocating workers with that much memory, or using all the memory on 1 very large executor.; Have you tried setting executor cores as well? I would usually set it to something like `--executor-cores 4 --executor-memory 16G` . You want to design your executors so they fit evenly into the worker nodes on your cluster but don't have too many cores per executor. . An aside, you *should* be able to create a bam index as part of SortSamSpark now, we have support for generating it in parallel and merging the indexes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547077382:21,config,configuration,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6233#issuecomment-547077382,1,['config'],['configuration']
Modifiability,"What the rules for when a tool is allowed to be a `CommandLineProgram`? Most of the CNV tools extend `CommandLineProgram` rather than `GATKTool` for various reasons, including: 1) they use sequence-dictionary input in a way that requires custom argument documentation, 2) they use `-I` to specify non-BAM/SAM/CRAM input, and 3) they don't really make use of the argument collections available in `GATKTool` or otherwise fall under the walker paradigm. These reasons are admittedly minor, but they do make the tools a bit nicer to use in the end. Otherwise, whenever it makes sense for a tool to extend `GATKTool`, it does (4 out of 12 of the CNV tools). (A bit of a tangent: in all the cases where we do extend `GATKTool` to e.g. make use of the `-L` functionality, we still have to jump through some extra hoops to make sure we don't get tripped up. For example, the default `--interval-merging-rule` behavior is incorrect for most CNV analyses, so the user has to set this to `OVERLAPPING_ONLY` manually, otherwise we throw an exception---which is quite awkward. Ideally, we'd have some option to not modify the incoming intervals at all, as well.). So I'm comfortable with closing this issue, but we can discuss the pros and cons of moving more of the tools over if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358038497:94,extend,extend,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2471#issuecomment-358038497,3,['extend'],['extend']
Modifiability,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2320:182,refactor,refactoring,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320,1,['refactor'],['refactoring']
Modifiability,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2282:538,extend,extending,538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282,1,['extend'],['extending']
Modifiability,"When running Spark tools with GCS files on Dataproc the [GCS connector](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) is set up and configured for you, but this isn't the case when running with local Spark, even on a GCP VM. We should make the experience easier through documentation and/or configuration improvements.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996:158,config,configured,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996,2,['config'],"['configuration', 'configured']"
Modifiability,"When using the command `./gradlew bundle` to build the GATK project downloaded from GitHub I receive this error:. ```; Starting a Gradle Daemon, 1 incompatible Daemon could not be reused, use --status for details; :createPythonPackageArchive; Creating GATK Python package archive...; Created GATK Python package archive in /datadrive/NGS-SparkGATK/gatk/build/gatkPythonPackageArchive.zip; :compileJava UP-TO-DATE; :processResources; :classes; :gatkTabComplete; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; /datadrive/NGS-SparkGATK/gatk/build/tmp/expandedArchives/picard-2.17.2-sources.jar_2eu0sptfiz8othzntm7sqhvx4/picard/util/LiftoverUtils.java:332: error: unmappable character for encoding ASCII; * Based on Adrian Tan, Gon??alo R. Abecasis and Hyun Min Kang. (2015); ^; 2 errors; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/datadrive/NGS-SparkGATK/gatk/build/tmp/gatkTabComplete/javadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 9.873 secs; ```; Can you please ""rewrite"" the name _GonÃ§alo_? It is always cause of exceptions when building the GATK project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4434:1481,rewrite,rewrite,1481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4434,1,['rewrite'],['rewrite']
Modifiability,"When we make enhancements to the walker engine (eg., modify the `GATKTool` base class to support CRAM, or to validate the sequence dictionaries of the inputs), it would be good if Spark tools could also reap the benefits of these changes automatically. We may need to unify (or better integrate) the `GATKTool` and `SparkCommandLineProgram` base classes somehow to make this possible, as well as classes like `ReadsDataSource` (for walkers) and `ReadsSparkSource` (for Spark tools).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/680:13,enhance,enhancements,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/680,1,['enhance'],['enhancements']
Modifiability,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4989:8,refactor,refactored,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989,2,"['config', 'refactor']","['configuration', 'refactored']"
Modifiability,"While @ldgauthier was presenting LobSTR, an STR indel caller, @fleharty and I were discussing how their probabilistic model was in some sense trying to convey a gap opening penalty and a gap continuation penalty for PCR slippage and related errors. This sort of suggests that one could go whole hog and build this into an enhanced pair-HMM that is aware of multiple error modes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1869:322,enhance,enhanced,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869,1,['enhance'],['enhanced']
Modifiability,"While we normally don't recommend ignoring that wrapper, this seems like a good reason to do so. . The wrapper is pretty simple, most of what it's doing is some munging of the input to allow it to be more standardized in several different gatk use cases. The only thing I can think of that you would want to be sure to copy is that it sets a number of properties. . We set these spark `--conf` properties with the wrapper. I don't actually know how important some of them are anymore. If it works without them then you're probably good.; ```; ""spark.kryoserializer.buffer.max"" : ""512m"",; ""spark.driver.maxResultSize"" : ""0"",; ""spark.driver.userClassPathFirst"" : ""false"",; ""spark.io.compression.codec"" : ""lzf"",; ""spark.executor.memoryOverhead"" : ""600"",; ""spark.driver.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK,; ""spark.executor.extraJavaOptions"" : EXTRA_JAVA_OPTIONS_SPARK; ```. These are htsjdk properties we want to set for spark. ; ```; EXTRA_JAVA_OPTIONS_SPARK= ""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "" \; ""-Dsamjdk.use_async_io_read_samtools=false "" \; ""-Dsamjdk.use_async_io_write_samtools=false "" \; ""-Dsamjdk.use_async_io_write_tribble=false "" \; ""-Dsamjdk.compression_level=2 ""; ```. If you can get this value into your spark environment variables it prevents and anying warning output. `SUPPRESS_GCLOUD_CREDS_WARNING=true`. Let us know how it works for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054:1251,variab,variables,1251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-539073054,2,['variab'],['variables']
Modifiability,"While working on validating #5607 I noticed that at the top of the method `isReadInformativeOfIndelOfSize()` that there is the following breakout condition:; ```; if( read.getLength() - readStart < maxIndelSize || refBases.length - refStart < maxIndelSize ) {; return false;; }; ```; This says that if the readStart is too close to the read.getLenght() then it will break out and not calculate the informativeness of a read. Unfortunately readStart isn't the readbase indexed readStart, its actually the ""IGV view"" offset for the read generated by the pileup for a particular reference position. The actual length that matters to us is: `AlignmentUtils.getBasesAlignedOneToOne(read).length` which is computed later when we realign the read bases to the reference. What this means is that if a read happens to have a long deletion in it then we will end up prematurely marking the read bases as being non-informative despite there being more than enough bases to work with when doing computations. Furthermore, since we realign the read bases later in the codepath, these bases in the gap between the realigned length and `read.getLength()` are still used to compute mismatch likelihood for bases before that point in the read. An example of this issue: I have a read with the cigar ""77M10D24M"", at position 92 of the read (the igv offset so in reality the 5th base into the last element of the cigar) the code returns false due to this condition. In reality `AlignmentUtils.getBasesAlignedOneToOne(read).length - readStart` value is 19, and thus comparable since there are >10 bases left in the read to test. . I have duplicated this behavior in #5607, perhaps it would be easiest to get that branch in first before tackling this issue just so validation for that refactor is easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5646:1764,refactor,refactor,1764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5646,1,['refactor'],['refactor']
Modifiability,Will add a variable to our Protobuf configuration object - the JSON already an option to set this.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300298863:11,variab,variable,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2687#issuecomment-300298863,2,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"Will proposed a novel approach to filtering that could prove useful: instead of running a sequence of filters and discarding items as soon as they fail a filter, use `com.google.cloud.dataflow.sdk.transforms.Partition<T>` to group items into `PCollections` according to which filters they fail, and then allow a configurable transform to be run on each partition. By default we might just count the number of items failing each filter, but we could have the option of doing things like saving the failing items somewhere for debugging purposes, or outputting for each item the list of ALL filters that it fails.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/303:312,config,configurable,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/303,1,['config'],['configurable']
Modifiability,Will refactor and re-open as a different PR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8642#issuecomment-1937109021:5,refactor,refactor,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8642#issuecomment-1937109021,1,['refactor'],['refactor']
Modifiability,"With a service account key set, it worked like a charm:. ```; $ ./gatk-launch PrintReadsSpark -I gs://jpmartin-testing-project/hellbender-test-inputs/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O gs://jpmartin-testing-project/test-output/readcount --shardedOutput true -- --sparkRunner GCS --cluster jps-test-cluster; (...); [November 20, 2017 6:17:08 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=670040064; Job [13c93a62-96d0-456e-91d1-ef7b20f1236b] finished successfully.; ```. Though I understand that [this is expected](https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330650894). So next I tried it without any `HELLBEND*` environment variable and it worked as well!. ```; Job [6e2f2c6b-921a-4fdf-a42e-0706216b2098] finished successfully.; (...); $ gsutil ls -lh gs://jpmartin-testing-project/test-output/readcount/; 0 B 2017-11-20T18:28:27Z gs://jpmartin-testing-project/test-output/readcount/; 0 B 2017-11-20T18:28:52Z gs://jpmartin-testing-project/test-output/readcount/_SUCCESS; 120.25 MiB 2017-11-20T18:28:51Z gs://jpmartin-testing-project/test-output/readcount/part-r-00000.bam; ```. This is with `GOOGLE_APPLICATION_CREDENTIALS` set, as I believe is part of the GATK README instructions. Next I went to my repro code and tried it again with v30. It failed (`StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account.`) I'm not sure why but the new version is certainly an improvement over the previous one since it fixes `PrintReadsSpark`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-345788205:745,variab,variable,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-345788205,1,['variab'],['variable']
Modifiability,"With gnomAD support, we will need to be able to match an HG19 interval against the B37 data source. This can be done by adding a field to the data source config files `dataAreB37` (or similar). Then in the code, `DataSourceFuncotationFactory` sets that to an internal variable based on the config file and uses that setting to call either `getValues(mainSourceFileAsFeatureInput)` or `getValues(mainSourceFileAsFeatureInput, equivalentB37SimpleInterval)` where `equivalentHg19SimpleInterval` is the Hg19 equivalent interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5456:154,config,config,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5456,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"With the addition of the Owner configuration mechanism to GATK, we're going to need to fold (most of) `gatk-launch` into Java, so that the Spark settings can be loaded from Owner before `spark-submit` or `gcloud` are invoked. Note that we'll still need to have a thin GATK launcher script for the sake of the tab completion, which requires that we invoke GATK using a unique command name.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3503:31,config,configuration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3503,1,['config'],['configuration']
Modifiability,"With the exception of the HMM package, all of our R dependencies are available through the conda R or bioconda channels; the HMM package is available only through a user's custom channel. However, the HMM package is only used to generate truth for testing the Java HMM code by @vruano (which is currently unused, but we thought was worth keeping around). I'm sure we could easily rewrite the tests to load the truth from a file. I think we should get rid of the install_R_packages.R script altogether and just roll all of these dependencies into the conda environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920:380,rewrite,rewrite,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920,1,['rewrite'],['rewrite']
Modifiability,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7830:176,config,configuration,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830,1,['config'],['configuration']
Modifiability,"Would be nice if common, rarely-changing dataflow options like the project and client secret could be specified in a config file rather than as part of every hellbender command.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/508:117,config,config,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/508,1,['config'],['config']
Modifiability,"Wow, thanks for the detailed comments so far, @davidbenjamin! But perhaps let's quickly chat before you go any further?. There are a lot of things you commented on---temporary integration tests using local files, lots of code/arguments/etc. intentionally copied verbatim over from VQSR/tranches, and entire tools (the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain)---that are rather in flux or will be scrapped/cleaned up shortly. That said, the comments on the code inherited from VQSR will certainly be useful in this process!. But it might save you some time if we could chat so I can give you a rough orientation and perhaps point out where the vestigial VQSR code remains. I think focusing discussion on the high level design of the tools that are likely to stay would also be most useful at this stage. Feel free to throw something on my calendar!. In the end, I think we will probably just retain the BGMM backend + the versions of the tools in the ""scalable"" package. I left the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain tools in this branch so I could do one round of tieout. That tieout came out OK, so I think we'll abandon the monolithic tools, along with all the associated code outside of the scalable package. If it helps, I can go ahead and remove that stuff from this draft PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942:478,inherit,inherited,478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942,1,['inherit'],['inherited']
Modifiability,XsvLocatableTableCodec and AnnotatedIntervalCodec should inherit from the same abstract class,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580:57,inherit,inherit,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580,1,['inherit'],['inherit']
Modifiability,"Y : ; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2930,Config,ConfigFactory,2930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,YXZh) | `0% <0%> (-100%)` | `0% <0%> (-8%)` | |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `3.448% <0%> (-94.253%)` | `2% <0%> (-8%)` | |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `0% <0%> (-91.429%)` | `0% <0%> (-23%)` | |; | [.../org/broadinstitute/hellbender/utils/IGVUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9JR1ZVdGlscy5qYXZh) | `0% <0%> (-88.889%)` | `0% <0%> (-3%)` | |; | [...alkers/mutect/filtering/PolymorphicNuMTFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvUG9seW1vcnBoaWNOdU1URmlsdGVyLmphdmE=) | `0% <0%> (-88.235%)` | `0% <0%> (-9%)` | |; | [...aplotypecaller/HaplotypeCallerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `0.431% <0%> (-87.5%)` | `2% <0%> (-85%)` | |; | [...alkers/mutect/SomaticReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/5803/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9Tb21hdGljUmVmZXJlbmNlQ29uZmlkZW5jZU1vZGVsLmphdmE=) | `12.5% <0%> (-84.375%)` | `1% <0%> (-7%)` | |; | [...tils/variant/writers/SomaticGVCFBlockCombiner.ja,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5803#issuecomment-473417970:2816,Polymorphi,PolymorphicNuMTFilter,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5803#issuecomment-473417970,1,['Polymorphi'],['PolymorphicNuMTFilter']
Modifiability,"Y_COMPRESSOR : false; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:2997,Config,ConfigFactory,2997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:134,adapt,adapter,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,1,['adapt'],['adapter']
Modifiability,"Yeah, it would be useful (see https://github.com/broadinstitute/gatk/issues/2582). Not sure if/when we'll ever get around to the Barclay changes though. Another simple option that wouldn't require Barclay changes would be to implement it as just another (plugin descriptor) command line argument that could be sued alongside `--read-filter`'. So if you wanted a `ReadNameFilter` and an inverted `ReadLengthFilter`, the syntax would be:. `--read-filter ReadNameFilter --invert-read-filter ReadLengthReadFilter`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306:255,plugin,plugin,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6005#issuecomment-502231306,2,['plugin'],['plugin']
Modifiability,"Yeah, the workaround was simply to add the library jar to the classpath and not try to compile them together. I created the issue to soon, Sorry. . As for the NIO library, it is for AWS S3. We are adapting this one https://github.com/Upplication/Amazon-S3-FileSystem-NIO2 to meet our needs. We didn't like the way it handles s3 endpoints because AWS EMR Spark clusters don't support s3 uri's with that particular syntax. Our version modifies it to support normal s3 uri's without endpoints, instead setting the endpoint with a configuration parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431:197,adapt,adapting,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102#issuecomment-308161431,4,"['adapt', 'config']","['adapting', 'configuration']"
Modifiability,"Yep, that will definitely cause a crash - variable length fields need a length value stored while fixed length fields don't. I can add checks for this scenario in GenomicsDB. Please let me know if the following make sense:; Header says that the field F is a fixed length field with length = N. In the data section, if; * length(F) < N - pad with missing values, no error message, continue; * length(F) > N - error, throw exception and print descriptive error message",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684:42,variab,variable,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684,1,['variab'],['variable']
Modifiability,"Yes it is. Honestly not sure on the u/g config, as an end user I'd really rather not have to care about that ðŸ˜… ; This is the only tool causing this kind of issue so it's got to be the tool itself, no?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078145966:40,config,config,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078145966,1,['config'],['config']
Modifiability,"Yes, Hadoop-BAM uses the NIO API to do file merging, whereas in GATK we were using the Hadoop APIs (and therefore the GCS<->HDFS adapter) to do it. It looks like there are a couple of things needed in GCS-NIO to use the NIO API for this.; 1. https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1450 so that we don't have to special-case `gs` URIs to remove everything except the scheme and host when looking up the filesystem (see https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L40); 2. https://github.com/GoogleCloudPlatform/google-cloud-java/issues/813 to support path matching (https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/util/NIOFileUtil.java#L90). There may be more, as I stopped there. The best way forward is probably to go back to the old code in GATK while the deficiencies in GCS-NIO are fixed and then released. The stacktrace I got for 1 was:. ```; java.lang.IllegalArgumentException: GCS FileSystem URIs mustn't have: port, userinfo, path, query, or fragment: gs://gatk-demo-tom/TEST/markdups.parts/_SUCCESS; 	at shaded.cloud-nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:54); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); ```. And for 2:. ```; java.lang.UnsupportedOperationException; 	at com.google.cloud.s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265132050:129,adapt,adapter,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-265132050,1,['adapt'],['adapter']
Modifiability,"Yes, thank you for jogging my memory @bbimber. @davidbenjamin adding something to that extend in the BadArgumentException message would be helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6263#issuecomment-558740872:87,extend,extend,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6263#issuecomment-558740872,1,['extend'],['extend']
Modifiability,YnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zdi9jbHVzdGVyL1NWQ2x1c3RlckVuZ2luZS5qYXZh) | `93.269% <0.000%> (-1.002%)` | :arrow_down: |; | [...stitute/hellbender/tools/walkers/sv/SVCluster.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3Rlci5qYXZh) | `89.773% <0.000%> (-0.881%)` | :arrow_down: |; | [...tools/walkers/sv/JointGermlineCNVSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L0pvaW50R2VybWxpbmVDTlZTZWdtZW50YXRpb24uamF2YQ==) | `86.047% <0.000%> (-0.752%)` | :arrow_down: |; | [...der/tools/walkers/sv/SVClusterIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L1NWQ2x1c3RlckludGVncmF0aW9uVGVzdC5qYXZh) | `99.496% <0.000%> (-0.004%)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <0.000%> (+0.035%)` | :arrow_up: |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/7858/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7858#issuecomment-1130438520:4985,Adapt,AdaptiveChainPruner,4985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7858#issuecomment-1130438520,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"You can currently specify additional spark configuration with the --conf argument, so you could override the registrator that way. I think you'd have to update gatk-launch as well as build.gradle to get it to work, and currently gatk-launch is shared with gatk-protected. Probably the best solution is going to be to extract all the hardcoded configurations into a configuration file that can be changed on a per project basis. There's some movement to this in gatk public at the moment, but we haven't settled on a solution yet I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272484533:43,config,configuration,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2337#issuecomment-272484533,3,['config'],"['configuration', 'configurations']"
Modifiability,"You can detect whether a tool has failed in bash by checking whether the exit status code is non-zero. In bash, the exit status code of the last command run is stored in the variable `$?`; ; In general, you should ask questions like these on the GATK forum (https://gatkforums.broadinstitute.org/gatk) instead of here, however -- this is for bug reports rather than support requests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349:174,variab,variable,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4242#issuecomment-359955349,1,['variab'],['variable']
Modifiability,You would not have access to docker container options when using the Google backend because the running of your image is all controlled by Pipelines API. You would be able to set that value when running on a local backend but thats probably not portable enough for your workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357313468:245,portab,portable,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4140#issuecomment-357313468,1,['portab'],['portable']
Modifiability,"You'd better verify the consistency of the results step by step, then you can find which step's result become diffierent, finally fix it.; For example, you want to see if the `JavaRDD<Shard<GATKRead>> readShards`results are always same. You can add debug info in `callVariantsWithHaplotypeCaller` just like this:. ```; ...... final JavaRDD<Shard<GATKRead>> readShards = SparkSharder.shard ....; //Debug; String path = ""./dubug/readShardsTest1"";; java.io.File debug = new File(path);; if (!debug.getParentFile().exists()); debug.getParentFile().mkdir();; try (PrintStream printStream = new PrintStream(debug)) {; printStream.println(""List<ShardBoundary> shardBoundaries size : "" + shardBoundaries.size());; printStream.printf(""NumPartitions : %d\n"", readShards.getNumPartitions());. List<ShardDebug> shardDebugs = readShards.mapToPair(shard -> new Tuple2<>(new ShardDebug(shard), null)); .sortByKey((Comparator<ShardDebug> & Serializable) (o1, o2) ->; IntervalUtils.compareLocatables(o1, o2, header.getSequenceDictionary()); ).keys().collect();; printStream.printf(""NumShard : %d\n"", shardDebugs.size());; for (ShardDebug shardDebug : shardDebugs) {; printStream.println(shardDebug.toString());; }; }catch (Exception e){; e.printStackTrace();; }; ```. ```; static class ShardDebug extends ShardBoundary{; int size;. public ShardDebug(Shard<GATKRead> shard) {; super(shard.getInterval(),shard.getPaddedInterval());; size = Iterators.size(shard.iterator());; }. @Override; public String toString() {; return this.getInterval().toString() + ""\t"" + size;; }; }; ```; ; Run twice and compare the differences, do this step by step, you will find the bug. Gook Luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371415511:1280,extend,extends,1280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4231#issuecomment-371415511,1,['extend'],['extends']
Modifiability,"Your solution doesn't address your third listed drawback to the current; approach, though I'm not sure there's any way to do that that wouldn't; require a pretty dramatic change. It's not obvious to me why we wanted the given alleles in the graph; originally. Maybe the use case was variants from UG that we didn't; necessarily believe were aligned properly?. I don't have any objections, but I'd feel better if we had a better guess; at what the original method was trying to do. On Wed, Apr 3, 2019 at 9:56 PM David Benjamin <notifications@github.com>; wrote:. > In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them; > into the ref haplotype, then threading these constructed haplotypes into; > the assembly graph with a large edge weight. There are several drawbacks to; > this approach:; >; > - The strange edge weights interfere with the AdaptiveChainPruner.; > - The large edge weights may not be large enough to avoid pruning when; > depth is extremely high.; > - The alleles may be lost if assembly fails.; > - If the alleles actually exist but are in phase with another variant; > we end up putting an enormous amount of weight on a false haplotype.; >; > We can get around these issue with the following method:; >; > - assemble haplotypes without regard to the force-called alleles.; > - if an allele is present in these haplotypes, do nothing further.; > - otherwise, add a haplotype in which the allele is injected into the; > reference haplotype.; >; > @LeeTL1220 <https://github.com/LeeTL1220> I prototyped this and it seems; > to resolve the missed forced alleles that Ziao found.; >; > @ldgauthier <https://github.com/ldgauthier> Can you think of any; > objections to making this change in HaplotypeCaller?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5857>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdMcaTJg47gn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767:862,Adapt,AdaptiveChainPruner,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857#issuecomment-479916767,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"[Compiler daemon](https://docs.gradle.org/current/userguide/performance.html#compiler_daemon). The Gradle Java plugin allows you to run the compiler as a separate process by setting `options.fork = true`. This feature can lead to much less garbage collection and make Gradleâ€™s infrastructure faster. This project has more than 1000 source files. We can consider enabling this feature. =====================; If there are any inappropriate modifications in this PR, please give me a reply and I will change them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7561:111,plugin,plugin,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561,1,['plugin'],['plugin']
Modifiability,"[bug report] GermlineCNVCaller (gatk 4.2.0.0) ""H5DreadVL_str: failed to read variable length strings 	at ncsa.hdf.hdf5lib.H5.H5DreadVL""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7202:77,variab,variable,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202,1,['variab'],['variable']
Modifiability,"] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] WorkflowManagerActor Starting workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] WorkflowManagerActor Successfully started WorkflowActor-c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2019-10-01 02:53:02,55] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2019-10-01 02:53:02,64] [info] MaterializeWorkflowDescriptorActor [c55a06f3]: Parsing workflow as WDL draft-2; [2019-10-01 02:53:03,80] [info] WorkflowManagerActor Workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to evaluate input 'padding' (reason 1 of 1): For input string: ""Int? (optional)""; Failed to evaluate input 'minimum_interval_median_percentile' (reaso",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:2709,config,configured,2709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configured']
Modifiability,] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3387,config,configuration,3387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:2279,variab,variable,2279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['variab'],['variable']
Modifiability,"_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:3392,variab,variable,3392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['variab'],['variable']
Modifiability,"_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3669,Config,ConfigFactory,3669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3999,Config,ConfigFactory,3999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.50: 0%| | 2/5000 [00:03<2:40:21, 1.93s/it]; 14:14:16.609 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3698,variab,variables,3698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['variab'],['variables']
Modifiability,"_expectation_mode=hybrid --num_samples_copy_ratio_approx=200 --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC cu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:5829,variab,variable,5829,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['variab'],['variable']
Modifiability,_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5420,Config,ConfigFactory,5420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5486,Config,ConfigFactory,5486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.82",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4383,Config,ConfigFactory,4383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4491,Config,ConfigFactory,4491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4015,Config,ConfigFactory,4015,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"`AFCalculator` has a couple of methods that don't belong: `reduceScope()`, which is called _before_ the AF calculation to reduce its computational burden, and `subsetAlleles()`, which is called _after_ the calculation to eliminate alleles that don't exist in called genotypes (in conformance with the dubious VCF spec). These methods are chronologically distinct from the rest of `AFCalculator` and do not appear to use any private variables. Thus they could easily be turned into static methods and removed from `AFCalculator`. Furthermore, `reduceScope()` and `subsetAlleles()` each have two implementations, in `DiploidExactAFCalculator` and `GeneralPloidyExactAFCalculator`. Since these two cases are complementary, they could easily be merged in a single method with an `if (ploidy == 2) . . .`. Finally, the general ploidy code is more complicated than it needs to be and needs editing. Beyond general housekeeping, the main motivation here is to untangle the AF/qual code as much as possible _without_ changing behavior before introducing the new model into the mix (issue #1697).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1891:432,variab,variables,432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1891,1,['variab'],['variables']
Modifiability,`DataSourceUtils` contains string constants for fields in the config file (e.g. `CONFIG_FILE_FIELD_NAME_NAME`). These should be rolled into an enum together. This will facilitate file validation by enabling them to be iterated over automatically using the enum's built in methods.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5465:62,config,config,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5465,1,['config'],['config']
Modifiability,"`FilterMutectCalls` has always and continues to filter multiallelics by default, except in mitochondria mode. You may adjust this with the `max-alt-allele-count` argument. `-max-alternate-alleles` is a `HaplotypeCaller` argument that appeared in `Mutect2` due to excess inheritance in the class hierarchy. I don't believe it ever had any effect in `Mutect2`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6603#issuecomment-629897864:270,inherit,inheritance,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6603#issuecomment-629897864,1,['inherit'],['inheritance']
Modifiability,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5963:63,config,configuration,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963,2,['config'],['configuration']
Modifiability,"`HaplotypeCaller` and `Mutect2` share an argument `min-base-quality-score` that restricts low-quality bases from assembly. This argument gets passed to `ReadThreadingAssembler`, but take a look at the following code in that class:. ```java; private static final byte MIN_BASE_QUALITY_TO_USE_IN_ASSEMBLY = DEFAULT_MIN_BASE_QUALITY_TO_USE;; protected byte minBaseQualityToUseInAssembly = DEFAULT_MIN_BASE_QUALITY_TO_USE;; ```; The latter variable is set from the command line argument, but only the `static` constant is ever used, in particular in line 447:. ```java; final ReadThreadingGraph rtgraph = new ReadThreadingGraph(kmerSize, debugGraphTransformations, MIN_BASE_QUALITY_TO_USE_IN_ASSEMBLY, numPruningSamples);; ```. The fix is extremely simple: just replace `MIN_BASE_QUALITY_TO_USE_IN_ASSEMBLY` with `minBaseQualityToUseInAssembly`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4126:436,variab,variable,436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4126,1,['variab'],['variable']
Modifiability,"`Tribble` codecs can only read data from `Locatable` data sources (those with contig + start + end position). Recently we have found a need for reading in files that do not have locatable data (e.g. tabular data that has a `Gene Name` and a set of attributes, but no start/stop location). Tribble should be updated to have a baseline `Interface` that is generic (and not necessarily `Locatable`). Our current interface / infrastructure can inherit from that for data sources that are `Locatable`. Then a new `Codec` can be created for data sources that are not Locatable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3760:440,inherit,inherit,440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3760,1,['inherit'],['inherit']
Modifiability,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:225,config,config,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183,1,['config'],['config']
Modifiability,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:77,config,configuration,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,3,"['config', 'plugin']","['configuration', 'plugins']"
Modifiability,```; > ls -1 /tmp | grep config$; ...; tmp_read_resource_9070459585787683374.config; tmp_read_resource_9128464625731709220.config; tmp_read_resource_9145440961585524679.config; tmp_read_resource_9164235676580024644.config; tmp_read_resource_959850395283914212.config; tmp_read_resource_979816073947827397.config; tmp_read_resource_983369287551636047.config; tmp_read_resource_993654349410744404.config; ...; ```. Anybody else seeing these files being created but not deleted. I'm running thousands of samples via HaplotypeCaller/GenotypeGVCFs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6771:25,config,config,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6771,9,['config'],['config']
Modifiability,"```; ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; ```. shows up at the top of every run, we should fix this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/216:34,config,configuration,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/216,2,['config'],['configuration']
Modifiability,a:208); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.getPath(CloudStorageFileSystemProvider.java:85); 	at java.nio.file.Paths.get(Paths.java:143); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtilsTest.testNoIllegalArgumentException(BucketUtilsTest.java:38); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); 	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:127); ```. This should be a safe method to call. We should either refactor this method or we change NIO to not throw in this case.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2707:2952,refactor,refactor,2952,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2707,1,['refactor'],['refactor']
Modifiability,"aJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1272,config,configuration,1272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['config'],['configuration']
Modifiability,"able.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:341); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:261); ... 18 more; ```. java version:; ```; java -version; openjdk version ""1.8.0_222""; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1~deb9u1-b10); OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode); ```; I added the cadd folder into data source folder like the structure mentioned in document:; ```; cadd; |- hg19; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; |; |- hg38; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; ```; The config file (cadd.config); ```; name = CADD; version = v1.4; src_file = InDels_inclAnno.tsv; origin_location =; preprocessing_script = UNKNOWN. Whether this data source is for the b37 reference.; Required and defaults to false.; isB37DataSource = false. Supported types:; simpleXSV -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript IDlocatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome locationgencode -- Custom datasource class for GENCODEcosmic -- Custom datasource class for COSMIC vcf -- Custom datasource class for Variant Call Format (VCF) files; type = locatableXSV; Required field for GENCODE files.Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path =. Required field for GENCODE files.; NCBI build version (either hg19 or hg38):; ncbi_build_version =. Required field for simpleXSV files.; Valid values:; GENE_NAME; TRANSCRIPT_ID; xsv_key = GENE_NAME. Required field for simpleXSV files.; The 0-based index of the column c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:4551,config,config,4551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"ace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5911,Config,ConfigFactory,5911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,ache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3405,config,config,3405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,added log4j2.xml to configure logging and remove warning at runtime,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/295:20,config,configure,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/295,1,['config'],['configure']
Modifiability,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7184:283,refactor,refactored,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184,1,['refactor'],['refactored']
Modifiability,"adds ""extends GATKBaseTest"" to all SV unit tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3789:6,extend,extends,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3789,1,['extend'],['extends']
Modifiability,"age](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1405,variab,variables,1405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,2,"['config', 'variab']","['configuration', 'variables']"
Modifiability,ah_var_store - ngs_cohort_extract - add backticks to table name variable,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7050:64,variab,variable,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7050,1,['variab'],['variable']
Modifiability,"al String WIDTH_OF_BINS_LONG_NAME = ""binwidths"";; > +; > + public static final String PADDING_SHORT_NAME = ""pad"";; > + public static final String PADDING_LONG_NAME = ""padding"";; > +; > + @Argument(; > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; >; > binWidth would be a more readable variable name. There's nothing wrong; > with the command line argument and the variable being identical.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646097>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > This tool extends GATKTool, which means that it inherits an; > IntervalArgumentCollection that already includes a padding argument. A; > new one is not needed. BTW @samuelklee <https://github.com/samuelklee>; > does this come up elsewhere in the CNV code? It could be a holdover from; > the days of porting ReCapSeg when I feel we used to write more; > CommandLinePrograms.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646119>:; >; > > + createBins();; > + }; > +; > + /**; > + * Generates binning coverage in the intervals given by the user.; > + * The width of bins, the intervals and the output file's path are given by the user.; > + */; > + public void createBin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:2816,extend,extends,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,2,"['extend', 'inherit']","['extends', 'inherits']"
Modifiability,al.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.Default,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3936,config,configureHierarchy,3936,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configureHierarchy']
Modifiability,allerSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:01:43.730 INFO HaplotypeCallerSpark - Deflater: IntelDeflater; 19:01:43.730 INFO HaplotypeCallerSpark - Inflater: IntelInflater; 19:01:43.730 INFO HaplotypeCallerSpark - GCS max retries/reopens: 20; 19:01:43.730 INFO HaplotypeCallerSpark - Requester pays: disabled; 19:01:43.730 WARN HaplotypeCallerSpark - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:4062,config,configuration,4062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['config'],['configuration']
Modifiability,allowing variant walkers to configure their caching behavior,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:28,config,configure,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,1,['config'],['configure']
Modifiability,"am/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipeline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5391,Config,ConfigFactory,5391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"am/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6030,Config,ConfigFactory,6030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,anding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:16628,Extend,Extended,16628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,angling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:10278,Extend,Extended,10278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3425,Plugin,PluginRegistry,3425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginRegistry']
Modifiability,apted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:14055,adapt,adapted,14055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"ark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:3591,config,configuration,3591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"ark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --num-executors 20 --executor-cores 6 --executor-memory 6g /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 23:10:10.737 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 23:10:10.965 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 23:10:12.679 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.680 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:10:12.680 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:10:12.680 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 23:10:12.681 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 23:10:12.681 INFO CountReadsSpark - Start Date/Time: February 5, 2019 11:10:10 PM EST; 23:10:12.681 INFO CountReadsSpark - -------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:3169,variab,variables,3169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3464,Plugin,PluginRegistry,3464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginRegistry']
Modifiability,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857:931,Refactor,Refactoring,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857,1,['Refactor'],['Refactoring']
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5680,Config,ConfigFactory,5680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6319,Config,ConfigFactory,6319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 20:41:37.620 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 20:41:37.620 DEBUG ConfigFactory - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5497,Config,ConfigFactory,5497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"ation more efficient. . ---. @vruano commented on [Thu May 05 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217267297). Those haplotype scores have not been throughly analyzed but we are already using them to discard haplotypes beyond the maximum allowed per graph kmer size so I don't see the harm in using the for further reduction. . Certainly is a step forward from just throwing an exception back to the user. However, we should output a Warning every time we need to do such a reduction just to keep track. ---. @sooheelee commented on [Fri May 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-217443170). Is it possible for the user to mask this 45SrDNA locus for separate analysis? Assuming of course that this locus is of further interest to their aims. For example, either for more exact mapping then variant calling or separate variant calling. I say this because a quick glance at the literature suggests this is potentially a highly variable region that may be present in multiple copies depending on species. It's a ribosomal DNA locus, that is, a site from which rRNA is transcribed. In mammals (humans & mice) it looks like it is a tandemly repeated locus residing on several chromosomes:. <img width=""424"" alt=""screenshot 2016-05-06 09 37 12"" src=""https://cloud.githubusercontent.com/assets/11543866/15074654/264e199a-136e-11e6-852e-431d8690f2aa.png"">. Some random references:; - [Concerted copy number variation balances ribosomal; DNA dosage in human and mouse genomes](http://www.pnas.org/content/112/8/2485.full.pdf); - [Haplotype Detection from Next-Generation Sequencing in High-Ploidy-Level Species: 45S rDNA Gene Copies in the Hexaploid Spartina maritima.](http://www.ncbi.nlm.nih.gov/pubmed/26530424); - [Non-Random Distribution of 5S rDNA Sites and Its Association with 45S rDNA in Plant Chromosomes.](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0035139). ---. @vruano commented on [Fri May ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:7293,variab,variable,7293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['variab'],['variable']
Modifiability,"aults.CREATE_MD5 : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3406,Config,ConfigFactory,3406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5324,Config,ConfigFactory,5324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:54:55.301 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:54:55.302 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:54:55.302 DEBUG ConfigFactory - Configuration file values:; 17:54:55.320 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:5963,Config,ConfigFactory,5963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"aults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3379,Config,ConfigFactory,3379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"aults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGerm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4846,Config,ConfigFactory,4846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,ava:648); 	at org.gradle.api.internal.file.copy.DefaultCopySpec$DefaultCopySpecResolver.walk(DefaultCopySpec.java:650); 	at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:458); 	at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:38); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$Static,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:4713,plugin,plugins,4713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"b.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30,55] [info] Unspecified type (Unspecified version) workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 submitted; [2020-07-14 05:09:30,66] [info] SingleWorkflowRunnerActor: Workflow submitted 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,68] [info] WorkflowManagerActor Starting workflow 968be82c-eef3-4bdb-a1ab",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:2371,config,configured,2371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configured']
Modifiability,b3e464b642?src=pr&el=desc) will **increase** coverage by `0.036%`.; > The diff coverage is `84.211%`. ```diff; @@ Coverage Diff @@; ## master #4960 +/- ##; ==============================================; + Coverage 80.784% 80.82% +0.036% ; - Complexity 17957 17967 +10 ; ==============================================; Files 1095 1095 ; Lines 64587 64600 +13 ; Branches 10392 10394 +2 ; ==============================================; + Hits 52176 52210 +34 ; + Misses 8388 8372 -16 ; + Partials 4023 4018 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4960?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `86.364% <Ã¸> (Ã¸)` | `1 <0> (Ã¸)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `77.64% <100%> (+1.242%)` | `45 <0> (Ã¸)` | :arrow_down: |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `90.556% <81.25%> (+4.927%)` | `53 <7> (+6)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.491% <0%> (+0.546%)` | `170% <0%> (+2%)` | :arrow_up: |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242:1267,config,config,1267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"bble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-6-amd64 amd64; 21:02:08.890 INFO PrintReadsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-8u171-b11-1~bpo8+1-b11; 21:02:08.890 INFO PrintReadsSpark - Start Date/Time: July 24, 2018 9:02:08 PM UTC; 21:02:08.890 INFO PrintReadsSpark - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:3833,variab,variables,3833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"bi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4639,Config,ConfigFactory,4639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:2576,refactor,refactoring,2576,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['refactor'],['refactoring']
Modifiability,broadinstitute) (9aa31e4) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/72684d0fae3326398c80e2f47d78eeff1fcc14fe?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (72684d0) will **decrease** coverage by `0.001%`.; > The diff coverage is `100.000%`. ```diff; @@ Coverage Diff @@; ## master #7851 +/- ##; ===============================================; - Coverage 86.948% 86.947% -0.001% ; Complexity 36927 36927 ; ===============================================; Files 2219 2219 ; Lines 173673 173674 +1 ; Branches 18755 18755 ; ===============================================; - Hits 151006 151005 -1 ; + Misses 16055 16054 -1 ; - Partials 6612 6615 +3 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/7851?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `97.368% <100.000%> (+0.035%)` | :arrow_up: |; | [.../hellbender/utils/python/PythonUnitTestRunner.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9weXRob24vUHl0aG9uVW5pdFRlc3RSdW5uZXIuamF2YQ==) | `75.410% <0.000%> (-3.279%)` | :arrow_down: |; | [...itute/hellbender/tools/LocalAssemblerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7851/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7851#issuecomment-1126424538:1373,Adapt,AdaptiveChainPruner,1373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7851#issuecomment-1126424538,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/392945202/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/-356078842/Ameloblastoma_FFPE_P5-unfiltered.vcf.gz -O Ameloblastoma_FFPE_P5-filtered.vcf.gz --contamination-table /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/contamination.table --tumor-segmentation /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/0c2281d2-9d90-4ee3-88bb-f0bb015cdb7c/call-Filter/attempt-4/inputs/51739658/segments.table --max-events-in-region 6. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553:3481,variab,variable,3481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553,1,['variab'],['variable']
Modifiability,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2634:324,config,configuration,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634,1,['config'],['configuration']
Modifiability,buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.Factories$1.create(Factories.java:22); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:148); 22:05:55.972 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:4120,config,configure,4120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configure']
Modifiability,builds are now failing due to the changes in the artifical reads generator...I'll make them more flexible so that I can still get more random reads without breaking the other tests....,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6280#issuecomment-559244663:97,flexible,flexible,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6280#issuecomment-559244663,1,['flexible'],['flexible']
Modifiability,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595:180,refactor,refactoring,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595,1,['refactor'],['refactoring']
Modifiability,"c 5 12:51:17 2017 -0500. Updates to handle SAM header changes from sl_wgs_acnv_headers and updates to mb_gcnv_python_kernel. commit d02d04df684a2820308a1d1c2bfda4b7d1c5f05e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Nov 13 12:52:33 2017 -0500. Added CLIs and WDL for python gCNV pipeline. commit 66ed74b68375d43514ef84658e7a6c771ed9053c; Author: Mehrtash Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11083,refactor,refactored,11083,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,2,['refactor'],['refactored']
Modifiability,c=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `86.42%`. ```diff; @@ Coverage Diff @@; ## master #5462 +/- ##; ===============================================; + Coverage 87.075% 87.087% +0.012% ; + Complexity 31334 31225 -109 ; ===============================================; Files 1921 1915 -6 ; Lines 144602 144079 -523 ; Branches 15951 15891 -60 ; ===============================================; - Hits 125912 125474 -438 ; + Misses 12896 12834 -62 ; + Partials 5794 5771 -23; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5462?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...s/walkers/haplotypecaller/graphs/PathUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvUGF0aFVuaXRUZXN0LmphdmE=) | `93.258% <Ã¸> (-0.22%)` | `7 <0> (Ã¸)` | |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `95.349% <100%> (Ã¸)` | `16 <0> (Ã¸)` | :arrow_down: |; | [...ller/readthreading/ReadThreadingGraphUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9yZWFkdGhyZWFkaW5nL1JlYWRUaHJlYWRpbmdHcmFwaFVuaXRUZXN0LmphdmE=) | `95.238% <100%> (+0.018%)` | `55 <0> (Ã¸)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/ChainPrunerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5462/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQ2hhaW5QcnVuZXJVbml0VGVzdC5qYXZh) | `99.194% <100%> (-0.006%)` | `40 <0> (Ã¸)` | |; | [...der/t,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5462#issuecomment-450062027:1281,Adapt,AdaptiveChainPruner,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5462#issuecomment-450062027,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,"cWorkUnitHandler.java:202); at org.broadinstitute.barclay.help.DocWorkUnit.processDoc(DocWorkUnit.java:144); at org.broadinstitute.barclay.help.HelpDoclet.lambda$processDocs$1(HelpDoclet.java:169); at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2742); at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); at org.broadinstitute.barclay.help.HelpDoclet.processDocs(HelpDoclet.java:169); at org.broadinstitute.barclay.help.HelpDoclet.startProcessDocs(HelpDoclet.java:113); at org.broadinstitute.hellbender.utils.help.GATKHelpDoclet.start(GATKHelpDoclet.java:34); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:310); at com.sun.tools.javadoc.DocletInvoker.start(DocletInvoker.java:189); at com.sun.tools.javadoc.Start.parseAndExecute(Start.java:366); at com.sun.tools.javadoc.Start.begin(Start.java:219); at com.sun.tools.javadoc.Start.begin(Start.java:205); at com.sun.tools.javadoc.Main.execute(Main.java:64); at com.sun.tools.javadoc.Main.main(Main.java:54); 1 error; :gatkDoc FAILED; ```. I can reproduce this error with a custom project with the gradle.build from gatk-protected and only the following class:. ```java; /**; * @author Daniel Gomez-Sanchez (magicDGS); */; @CommandLineProgramProperties(oneLineSummary = ""Test plugin doc"", summary = ""Test plugin doc"", programGroup = QCProgramGroup.class); @DocumentedFeature; public class ExampleToolWithPluginDescriptor extends CommandLineProgram {. @Override; public List<? extends CommandLinePluginDescriptor<?>> getPluginDescriptors() {; return Collections.singletonList(new GATKReadFilterPluginDescriptor(new ArrayList<>()));; }. @Override; protected Object doWork() {; return null;; }; }; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2739:3419,plugin,plugin,3419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2739,4,"['extend', 'plugin']","['extends', 'plugin']"
Modifiability,"c_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implemen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5200,Config,ConfigFactory,5200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,ce for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases(CRAMReferenceRegion.java:74); 	at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:450); 	at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); 	at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); 	at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); 	at htsjdk.samtools.CRAMFi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:3142,Adapt,AdaptedCallable,3142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['Adapt'],['AdaptedCallable']
Modifiability,ch 52/65; 19:36:40.808 INFO Â GenomicsDBImport - Done importing batch 53/65; 20:18:42.274 INFO Â GenomicsDBImport - Done importing batch 54/65; 21:01:51.304 INFO Â GenomicsDBImport - Done importing batch 55/65; 21:36:00.458 INFO Â GenomicsDBImport - Done importing batch 56/65; 22:08:38.587 INFO Â GenomicsDBImport - Done importing batch 57/65; 22:40:44.082 INFO Â GenomicsDBImport - Done importing batch 58/65; 23:14:11.202 INFO Â GenomicsDBImport - Done importing batch 59/65; 23:48:23.805 INFO Â GenomicsDBImport - Done importing batch 60/65; 00:20:35.869 INFO Â GenomicsDBImport - Done importing batch 61/65; 00:51:47.408 INFO Â GenomicsDBImport - Done importing batch 62/65; 01:25:23.587 INFO Â GenomicsDBImport - Done importing batch 63/65; 01:59:03.103 INFO Â GenomicsDBImport - Done importing batch 64/65; Using GATK jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) defined in environment variable GATK_LOCAL_JAR; Running:; Â  Â  java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx150g -Xms16g -jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) GenomicsDBImport --sample-name-map sample_map.chr3 --genomicsdb-workspace-path genomicsDB.rb.chr3 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --L chr3 --batch-size 50 --bypass-feature-reader --reader-threads 5 --merge-input-intervals --overwrite-existing-genomicsdb-workspace --consolidate; [farrell@scc-hadoop genomicsdb]$ ls genomicsDB.rb.chr3; __tiledb_workspace.tdb Â chr3$1$198295559 Â vcfheader.vcf Â vidmap.json. ```; It never indicates that it imported batch 65/65.Â No error and theÂ Â callset.json is missing which we found in chr4 to chr22. ; Â Â ; ls genomicsDB.rb.chr4. __tiledb_workspace.tdb Â callset.json Â chr4$1$1902145,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:3638,variab,variable,3638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['variab'],['variable']
Modifiability,"ck broadcast_0 stored as values in memory (estimated size 285.6 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.131.101.159:34044 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.131.101.159:34044 (size: 2.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 1 from broadcast at ReadsSparkSink.java:195; 17/10/11 14:19:18 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir; 17/10/11 14:19:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1; 17/10/11 14:19:18 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 17/10/11 14:19:18 INFO spark.SparkContext: Starting job: saveAsNewAPIHadoopFile at ReadsSparkSink.java:203; 17/10/11 14:19:18 INFO input.FileInputFormat: Total input paths to process : 1; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Registering RDD 5 (mapToPair at SparkUtils.java:157); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Got job 0 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) with 1 output partitions; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Parents of fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:10468,Config,Configuration,10468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['Config'],['Configuration']
Modifiability,clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:1361,variab,variables,1361,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,closing - refactoring to one WDL,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7112#issuecomment-786760463:10,refactor,refactoring,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112#issuecomment-786760463,1,['refactor'],['refactoring']
Modifiability,code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2468,config,configuration,2468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar PrintReadsSpark -I /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam -O /home/centos/storage/output.bam --sparkMaster spark://192.168.1.110:7077; 05:27:50.924 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 05:27:51.034 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/centos/gatk-4.beta.5/gatk-package-4.beta.5-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 3, 2017 5:27:51 AM UTC] PrintReadsSpark --output /home/centos/storage/output.bam --input /home/centos/storage/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam --sparkMaster spark://192.168.1.110:7077 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 3, 2017 5:27:51 AM UTC] Executing as centos@master.novalocal on Lin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:1429,variab,variables,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:11354,Extend,Extended,11354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,"cotationFactory.createDefaultFuncotationsOnVariant(GencodeFuncotationFactory.java:499); 22 Jun 2023 14:54:27,163 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:217); 22 Jun 2023 14:54:27,164 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182); 22 Jun 2023 14:54:27,166 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:152); 22 Jun 2023 14:54:27,167 DEBUG: 		at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); 22 Jun 2023 14:54:27,168 DEBUG: 		at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179); 22 Jun 2023 14:54:27,170 DEBUG: 		at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625); 22 Jun 2023 14:54:27,171 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); 22 Jun 2023 14:54:27,172 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499); 22 Jun 2023 14:54:27,174 DEBUG: 		at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921); 22 Jun 2023 14:54:27,175 DEBUG: 		at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 22 Jun 2023 14:54:27,177 DEBUG: 		at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682); 22 Jun 2023 14:54:27,178 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:162); 22 Jun 2023 14:54:27,180 DEBUG: 		at com.github.discvrseq.walkers.ExtendedFuncotator.enqueueAndHandleVariant(ExtendedFuncotator.java:209); 22 Jun 2023 14:54:27,181 DEBUG: 		at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:878); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226:3423,Extend,ExtendedFuncotator,3423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8363#issuecomment-1603412226,2,['Extend'],['ExtendedFuncotator']
Modifiability,create gradle plugin that automatically configures sparkJar correctly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1453:14,plugin,plugin,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453,2,"['config', 'plugin']","['configures', 'plugin']"
Modifiability,create variable for java xmx size in picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8395:7,variab,variable,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8395,1,['variab'],['variable']
Modifiability,created variable for user to specify java heap size in picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8406:8,variab,variable,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8406,1,['variab'],['variable']
Modifiability,"cription; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:404); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5855:1152,variab,variable,1152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855,1,['variab'],['variable']
Modifiability,"ctory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run.; CollectIlluminaLaneMetrics (Picard) Collects Illumina lane metrics for the given BaseCalling analysis directory.; ExtractIlluminaBarcodes (Picard) Tool determines the barcode for each read in an Illumina lane.; IlluminaBasecallsToFastq (Picard) Generate FASTQ file(s) from Illumina basecall read data. ...; ```. With this change it instead prints the gatk launcher help, which is not the intended result. ; ```; Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after --; GCS: run using Google cloud dataproc; commands after the -- will be passed to dataproc; --cluster <your-cluster> must be specified after the --; spark properties and some common spark-submit parameters will be translated; to dataproc equivalents. --dry-run may be specified to output the generated command line without running it; --java-options 'OPTION1[ OPTION2=Y ... ]' optional - pass the given string of options to the; java JVM at runtime.; Java options MUST be passed inside a single string with space-separated values.; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030:1429,Config,Configuration,1429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030,2,"['Config', 'config']","['Configuration', 'config-file']"
Modifiability,"ctory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4288,Config,ConfigFactory,4288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"ctory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3812,Config,ConfigFactory,3812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"ctory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5356,Config,ConfigFactory,5356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ctory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4253,Config,ConfigFactory,4253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"ctory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_convert",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5547,Config,ConfigFactory,5547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"cuting as myname@ln14 on Linux 3.10.0-514.16.1.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b15; Version: 4.alpha.2-1125-g27b5190-SNAPSHOT; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4504,variab,variable,4504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['variab'],['variable']
Modifiability,cy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL0FzeW5jaHJvbm91c1N0cmVhbVdyaXRlci5qYXZh) | `81.633% <0.000%> (-2.041%)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `96.396% <0.000%> (-1.305%)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `73.750% <0.000%> (-1.250%)` | :arrow_down: |; | [...ools/walkers/annotator/VariantAnnotatorEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9WYXJpYW50QW5ub3RhdG9yRW5naW5lLmphdmE=) | `86.260% <0.000%> (-0.999%)` | :arrow_down: |; | [...org/broadinstitute/hellbender/utils/MathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:4557,config,config,4557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"d I think things look good from this perspective, at least. This tieout uses a subset of the Pf7 samples containing 300 cohort and 1683 case samples (which were indeed treated as a cohort-case cluster in the original Pf7 CNV genotyping analysis). ~4k genomic bins are covered. We compare this branch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1085,variab,variables,1085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['variab'],['variables']
Modifiability,"d be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all of your ""minimal"" changes for the port, including the real, new tests. This should compile, and tests should",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1888,plugin,plugins,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,2,['plugin'],['plugins']
Modifiability,"d evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvalua",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2273,config,configuration,2273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,d on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using these work arounds (change the max number of haplotypes or kmersize) but those are not good solutions in general as he would pay a CPU and sensitivity penalty in other places. . ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265551340). I can see how the current best-path selection algorithm may fail to produce a good coverage of events across the active region depending on the weights on edges .... for some configurations the algorithm may dedicate too much time in exploring alternatives in one section of the graph because these are nearly equaly likely disregarding other possibilities other section just because they can only result in a relatively larger drop in the likelihood of the path. . I quick but elegant solution would be to simulate passes across the graph... first iterations would produce a quickly growing set of haplotypes but eventually repeated sampling would not produce new haplotypes. if after 100 subsequent simulations there is no new discovery or we have reached a limit (128?) we would stop there. This simulation approach could be implemented only in situations the graphs are too complex for an analytical solution. We can determine the maximum number of paths in a graph with a quick deep first traversal to decide whether to use the analytical-exact or the simulation-proximate approach. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-287813366). To be done in GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2954:4673,config,configurations,4673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954,1,['config'],['configurations']
Modifiability,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1644,flexible,flexible,1644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,2,['flexible'],['flexible']
Modifiability,default configuration rather than the path. Fixes #1324.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1433:8,config,configuration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1433,1,['config'],['configuration']
Modifiability,delete PluginManager,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/158:7,Plugin,PluginManager,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/158,1,['Plugin'],['PluginManager']
Modifiability,der.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3530,plugin,plugins,3530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['plugin'],['plugins']
Modifiability,"e --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 17:31:06.046 INFO cohort_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 17:31:16.537 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 17:31:23.180 INFO cohort_denoising_calling - Loading 44 read counts file(s)...; 17:34:27.362 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 17:40:48.713 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)... Stderr:; at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:340); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:13931,variab,variable,13931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['variab'],['variable']
Modifiability,"e GATK/Picard (`IndexFeatureFile `), but they would print inconsistent logs with the rest of my toolkits and they aren't overridable because the classes are final; thus, I would use a decorator over this tools to print the proper startup messages. After a while, I might implement a `VariantWalker`, which will require that I implement another layer (`MyVariantWalker`). Thus, I end up with a lot of naive classes implemented on top of the base walkers and wrappers around bundled GATK/Picard tools. This is very difficult to maintain, because if a change is done at the `CommandLineProgram` abstract class for the logging output (a new method, for example), I will need to update every naive class and wrapper if I bump the GATK version. In addition, extensions of my own toolkit (if any) would need to do the same, making the class-dependency tree so deep that it is difficult to follow (with GATK3, this problem was really driving me crazy when I tried to implement custom tools). On the other hand, there is another use case for the GATK itself: once barclay has a common class for CLP, GATK would be able to run directly Picard tools without the decorator; nevertheless, they will still need it for the log output. This also gives me the impression that the configuration for the CLP output should be at the barclay level, to be shared between Picard/GATK/downstream toolkits to be able to combine them. I think that a way of managing that woul be a new field in the CLP consisting on an interface/abstract class, `CommandLineStartupFormatter`, with the same CLP methods for this kind of operations, that will be passed to the CLP on construction (in `Main`) and defaults to whatever base class is chosen. This will allow custom toolkits to override in their `Main` the formatter and thus make consistent the output of every tool. Another option is to use directly something like the Spring framework, but I think that it is quite complicated for API users without knowledge of Spring (like me).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646:2071,config,configuration,2071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4101#issuecomment-382994646,1,['config'],['configuration']
Modifiability,"e VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1066,Config,ConfigurationContainerInternal,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Config'],['ConfigurationContainerInternal']
Modifiability,"e canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently well; - Sequence duplicates are removed. Other:; -Fixed bugginess in very large LongBloomFilters by changing a size variable from int to long. ; - Also realized we can't get away with using just 1 hash function in the Bloom filter. Before, I was using a single 64-bit hash and splitting it into 2 32-bit hashes, then using the hash1 + i*hash2 trick to generate each hash value. I don't think we can do this now because we allow for tables of size >2 billion bits in a single filter, so we need 2 64-bit hashes to use the trick.; -A couple of utility functions have been moved around",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:2175,variab,variable,2175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['variab'],['variable']
Modifiability,"e interpreted and location inferred by studying local assembly contig alignment signatures, it is time to clean up the corresponding package in the pipeline and make the switch to the updated implementation, which now outputs not only insertion, deletion, small tandem duplication, and inversions, but also novel adjacencies (BND records whose meanings cannot be fully resolved solely from assembly alignment signatures) as well as complex variants that theoretically could be arbitrarily complex (`<CPX>`, as long as we have assembled across the full event). . ## Planed organization. the `discovery` package could be divided roughly now into. ### interface. `SvDiscoveryDataBundle`, `SvDiscoverFromLocalAssemblyContigAlignmentsSpark`, `SvType`, `AnnotatedVariantProducer`. ### alignment prep (sub package). `AlignmentInterval`, `AlignedContig` (refactor `AssemblyContigWithFineTunedAlignments` into `AlignedContig`), `AlignedContigGenerator`, `AlignedAssembly`, `ContigAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:917,refactor,refactor,917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,2,['refactor'],['refactor']
Modifiability,"e typically used to indicate adapter sequence. See reply to jhess in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/35120#Comment_35120>:. > That's correct, Q2 bases are considered to be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic workflows that call down to the lowest allele fraction variants. It seems this ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:1145,adapt,adapter,1145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,1,['adapt'],['adapter']
Modifiability,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:3065,config,configuration,3065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,1,['config'],['configuration']
Modifiability,"e; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5002,Config,ConfigFactory,5002,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,eCNVCaller - HTSJDK Defaults.CREATE_MD5 : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3197,Config,ConfigFactory,3197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,2,['Config'],"['ConfigFactory', 'Configuration']"
Modifiability,"eCaller.java b/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; index cf34b1fb4e..2ee5752f04 100644; --- a/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; +++ b/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/haplotypecaller/HaplotypeCaller.java; @@ -497,10 +497,11 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; protected boolean mergeVariantsViaLD = false;; ; @Advanced; - @Argument(fullName=""tryPhysicalPhasing"", shortName=""tryPhysicalPhasing"", doc=""If specified, we will add physical (read-based) phasing information"", required = false); - protected boolean tryPhysicalPhasing = false;; + @Argument(fullName=""doNotRunPhysicalPhasing"", shortName=""doNotRunPhysicalPhasing"", doc=""If specified, we will not try to add physical (read-based) phasing information"", required = false); + protected boolean doNotRunPhysicalPhasing = false;; ; - public static final String HAPLOTYPE_CALLER_PHASING_KEY = ""HCP"";; + public static final String HAPLOTYPE_CALLER_PHASING_ID_KEY = ""PID"";; + public static final String HAPLOTYPE_CALLER_PHASING_GT_KEY = ""PGT"";; ; // -----------------------------------------------------------------------------------------------; // arguments for debugging / developing the haplotype caller; @@ -634,12 +635,11 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if ( emitReferenceConfidence() ) {; ; if (SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES); - throw new UserException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and Genotyping Giving Alleles at the same time"");; + throw new UserException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and GENOTYPE_GIVEN_ALLELES at the same time"");; ; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:1232,extend,extends,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,2,['extend'],['extends']
Modifiability,"eProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz; ```. #### Steps to reproduce; gatk --java-options -Xms100g VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/ch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:10517,polymorphi,polymorphic,10517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['polymorphi'],['polymorphic']
Modifiability,"e_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - Executing as root@gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m on Linux v4.9.0-8-amd64 amd64; 16:58:10.114 INFO PrintVariantsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-2~deb9u1-b13; 16:58:10.114 INFO PrintVariantsSpark - Start Date/Time: February 18, 2019 4:58:09 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1514,variab,variables,1514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"eano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:269); at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5142,config,configdefaults,5142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configdefaults']
Modifiability,ed[m[50D[1B[1m> :testOnPackagedReleaseJar > Executing test org...help.DocumentationGeneration[m[79D[1B[3A src/main/java/org/broadinstitute/hellbender/utils/variant/writers/SomaticGVCFWriter.java:4: error: package com.google.common.collect does not exist[0K; 2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; 2022-08-16T00:09:07.4436380Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4436641Z location: class CommandLineProgram; 2022-08-16T00:09:07.4436930Z src/main/java/org/broadinstitute/hellbender/engine/FeatureInput.java:120: error: cannot find symbol; 2022-08-16T00:09:07.4437094Z @VisibleForTesting; 2022-08-16T00:09:07.4437369Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4437519Z location: class FeatureInput<T>; 2022-08-16T00:09:07.4437725Z where T is a type-variable:; 2022-08-16T00:09:07.4437925Z T extends Feature declared in class FeatureInput; 2022-08-16T00:09:07.4438276Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:251: error: cannot find symbol; 2022-08-16T00:09:07.4438417Z @VisibleForTesting; 2022-08-16T00:09:07.4438677Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4438873Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4439223Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:271: error: cannot find symbol; 2022-08-16T00:09:07.4439362Z @VisibleForTesting; 2022-08-16T00:09:07.4439618Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4439806Z location: class PosteriorProbabilitiesUtils; 2022-08-16T00:09:07.4465668Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/DefaultGATKVariantAnnotationArgumentCollection.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4466113Z [done in 2417 ms]; 2022-08-16T00:09:07.4466222Z ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:20885,extend,extends,20885,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:1384,config,config,1384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,12,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"ed for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1380,config,configuration,1380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['config'],['configuration']
Modifiability,"ed(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for pon; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 151, column 69:. gatk GenomicsDBImport --genomicsdb-workspace-path pon_db -R ~{ref_fasta} -V ~{sep=' -V ' input_vcfs} -L ~{intervals}; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. Am I using womtool wrong, or is there a bug with it, or is this an issue with wdls?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:4916,adapt,adapted,4916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['adapt'],['adapted']
Modifiability,"ed, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of PGEN files and then merges them by chromosome. ### Part 1: PGEN-JNI; The PGEN-JNI library was written by Chris Norman of the GATK Engine Team and lives [here](https://github.com/broadinstitute/pgen-jni). It is written primarily in C++ for performance purposes and also compatibility with the pgenlib library (part of the [plink repo](https://github.com/chrchang/plink-ng/tree/master)). It builds on top of pgenlib to provide a writer for creating PGEN files and writing to them from HTSJDK VariantContext objects. PGEN-JNI is compatible with Linux and macOS. A build of this library is currently hosted on the Broad's artifactory repo, and that is being used as a dependency for GATK. Ownership of the PGEN-JNI library will stay with the GATK Engine Team and we will provide support for it if y'all encounter any issues with it. ### Part 2: ExtractCohortToPgen; ExtractCohortToPgen is a GATK tool that inherits from ExtractCohort and is based very closely on ExtractCohortToVcf. It produces 3-4 files:. 1. A `.pgen` file, which contains a mapping of samples and sites to variants,; 2. A `.psam` file, which contains a list of sample names,; 3. A `.pvar.zst` file, which is a zstd compressed list of sites with alleles, similar to a sites-only VCF, and; 4. Optionally (if specified by setting `write-mode` to `WRITE_SEPARATE_INDEX`), a `.pgi` file, which contains an index for the `.pgen` file. It has a few arguments that are specific to it that warrant explanation. #### pgen-chromosome-code; Plink defines a set of [chromosome codes](https://www.cog-genomics.org/plink/2.0/data#irreg_output) that correspond to different sets of contig names of chromosomes. This tool supports two of those chromosome code options: `chrM` and `MT`, which correspond to hg38 and hg19 contig naming, respectively. This argument is required. #### write-mode; The PGEN writer defined in PGEN-JNI defines two different writ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:2902,inherit,inherits,2902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['inherit'],['inherits']
Modifiability,ed-websocket\9.0.35\tomcat-embed-websocket-9.0.35.jar;E:\repository\org\springframework\spring-web\5.2.6.RELEASE\spring-web-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-webmvc\5.2.6.RELEASE\spring-webmvc-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-aop\5.2.6.RELEASE\spring-aop-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-context\5.2.6.RELEASE\spring-context-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-expression\5.2.6.RELEASE\spring-expression-5.2.6.RELEASE.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-starter\2.1.2\mybatis-spring-boot-starter-2.1.2.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-autoconfigure\2.1.2\mybatis-spring-boot-autoconfigure-2.1.2.jar;E:\repository\org\mybatis\mybatis\3.5.4\mybatis-3.5.4.jar;E:\repository\org\mybatis\mybatis-spring\2.0.4\mybatis-spring-2.0.4.jar;E:\repository\mysql\mysql-connector-java\8.0.20\mysql-connector-java-8.0.20.jar;E:\repository\org\springframework\boot\spring-boot-configuration-processor\2.3.0.RELEASE\spring-boot-configuration-processor-2.3.0.RELEASE.jar;E:\repository\org\springframework\spring-core\5.2.6.RELEASE\spring-core-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-jcl\5.2.6.RELEASE\spring-jcl-5.2.6.RELEASE.jar;E:\repository\com\google\firebase\firebase-admin\6.8.1\firebase-admin-6.8.1.jar;E:\repository\com\google\api-client\google-api-client\1.25.0\google-api-client-1.25.0.jar;E:\repository\com\google\oauth-client\google-oauth-client\1.25.0\google-oauth-client-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-jackson2\1.25.0\google-http-client-jackson2-1.25.0.jar;E:\repository\com\google\api-client\google-api-client-gson\1.25.0\google-api-client-gson-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-gson\1.25.0\google-http-client-gson-1.25.0.jar;E:\repository\com\google\code\gson\gson\2.8.6\gson-2.8.6.jar;E:\repository\com\google\http-client\google-http-client\1.25.0\google-http-,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:5458,config,configuration-processor,5458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration-processor']
Modifiability,"egion, modifying reads as a side effect; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(assemblyActiveRegion. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(MTAC.maxMnpDistance);. // when we trim on the originalAssemblyRegion, the trimmingResult takes its un-modified reads!; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(originalAssemblyRegion, allVariationEvents, referenceContext);. // now the assemblyResult gets the unmodified reads of the trimmingResult!; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. If we want things like `-dont-use-soft-clipped-bases` to work, we should call `trimmer.trim` on `untrimmedAssemblyResult`. I think that change alone may be all we need. Let's look at the corresponding code in `HaplotypeCallerEngine`:. ```; final AssemblyResultSet untrimmedAssemblyResult = AssemblyBasedCallerUtils.assembleReads(region. . .);. final SortedSet<VariantContext> allVariationEvents = untrimmedAssemblyResult.getVariationEvents(hcArgs.maxMnpDistance);. // same things as Mutect2 â€” we trim on the unmodified region; final AssemblyRegionTrimmer.Result trimmingResult = trimmer.trim(region, allVariationEvents, referenceContext);. // same as Mutect2; final AssemblyResultSet assemblyResult = untrimmedAssemblyResult.trimTo(trimmingResult.getVariantRegion());; ```. In addition to the proposed simple fix, this brings up a few code smells:. * One would expect assembly not to modify its input reads, but it does through the side effect of `finalizeRegion`.; * Assembly has both the permanent changes of finalize region and the temporary changes of read error correction.; * `AssemblyResultSet` stores the reads but so does `AssemblyRegion`. Without doing any serious refactoring, perhaps `finalizeRegion` could at least be split off from assembly so that the latter does not stealthily modify reads.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6686:2833,refactor,refactoring,2833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6686,1,['refactor'],['refactoring']
Modifiability,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8158:3679,Config,Config,3679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158,7,"['Config', 'config']","['Config', 'configuration']"
Modifiability,"ellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:805); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:789). the deletion that is causing the error is 141 base pairs, and I noticed the length of the contig Funcotator is trying to retrieve (895) is equal to the UTR length + deletion length + 1, 753 + 141 + 1. When I looked at the source code around where the error occurs, I see where the length of the retrieved interval is defined (line 738): . > final SimpleInterval transcriptInterval = new SimpleInterval(; > transcriptMapIdAndMetadata.mapKey,; > transcriptMapIdAndMetadata.fivePrimeUtrStart,; > transcriptMapIdAndMetadata.fivePrimeUtrEnd + extraBases; > );. and the logic for how large that extraBases should be (line 1566):. >final int numExtraTrailingBases = variant.getReference().length() < defaultNumTrail ingBasesForUtrAnnotationSequenceConstruction ? defaultNumTrailingBasesForUtrAnnotationSequenceConst ruction : variant.getReference().length() + 1;. I believe line 1566 is the source of the problem; there is no check that UTR-end + deletion length extends past the end of the transcript. #### Steps to reproduce. download funcotator_dataSources.v1.6.20190124s from Broad FTP server. run funcotator using:. `Funcotator -R /tmp/GRCh38.fa -V broken.vcf -O broken.out.vcf --data-sources-path funcotator_dataSources.v1.6.20190124s/ --output-file-format VCF --ref-version hg38`. on a vcf with a single variant:. >chr17 7241460 . ACTGCAAAAGATACAAGATGCAAGAAAGTCACAGAGGTCAAAAATGCCCTCAAAAGAACAGCTGCTAGGTGGAGCCTCCTCCCGCAGAGACTGCACTCCCACCCACAGGAAGCAAGCCTGAGTCTTGGATCAGGTTCCCAC A . #### Expected behavior; Funcotator should not attempt to retrieve a sequence that extends past the end of a transcript. #### Actual behavior; Funcotator crashes because it attempts to retrieve sequence past the end of a transcript",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6345:2961,extend,extends,2961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6345,2,['extend'],['extends']
Modifiability,"ellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:341); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:261); ... 18 more; ```. java version:; ```; java -version; openjdk version ""1.8.0_222""; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1~deb9u1-b10); OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode); ```; I added the cadd folder into data source folder like the structure mentioned in document:; ```; cadd; |- hg19; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; |; |- hg38; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; ```; The config file (cadd.config); ```; name = CADD; version = v1.4; src_file = InDels_inclAnno.tsv; origin_location =; preprocessing_script = UNKNOWN. Whether this data source is for the b37 reference.; Required and defaults to false.; isB37DataSource = false. Supported types:; simpleXSV -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript IDlocatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome locationgencode -- Custom datasource class for GENCODEcosmic -- Custom datasource class for COSMIC vcf -- Custom datasource class for Variant Call Format (VCF) files; type = locatableXSV; Required field for GENCODE files.Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path =. Required field for GENCODE files.; NCBI build version (either hg19 or hg38):; ncbi_build_version =. Required field for simpleXSV files.; Valid values:; GENE_NAME; TRANSCRIPT_ID; xsv_key = GENE_NAME. Required field for simpleXSV files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:4533,config,config,4533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4089,Config,ConfigFactory,4089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"enameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-10-01 02:53:01,20] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-10-01 02:53:01,31] [info] Running with database db.url = jdbc:hsqldb:mem:c4b3296a-4b73-4053-b6bf-d4eeb71c8956;shutdown=false;hsqldb.tx=mvcc; [2019-10-01 02:53:01,85] [info] Slf4jLogger started; [2019-10-01 02:53:02,22] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-876ccf5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-10-01 02:53:02,28] [info] Metadata summary refreshing every 1 second.; [2019-10-01 02:53:02,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,31] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-10-01 02:53:02,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-10-01 02:53:02,32] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-10-01 02:53:02,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-10-01 02:53:02,43] [info] SingleWorkflowRunnerActor: Version 46.1; [2019-10-01 02:53:02,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-10-01 02:53:02,49] [info] Unspecified type (Unspecified version) workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c submitted; [2019-10-01 02:53:02,51] [info] SingleWorkflowRunnerActor: Workflow submitted c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,51] [info] 1 new workflows fetched by cromid-876ccf5: c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,52] [info] WorkflowManagerActor Starting workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c; [2019-10-01 02:53:02,53] [info] WorkflowManage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:1474,config,configured,1474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,1,['config'],['configured']
Modifiability,"encing data. Given that a major source of coverage variance in targetted sequencing stems from the variance in bait efficiencies, the most reasonable read-depth calculation scheme is to associate **inserts to baits** rather than **single reads to targets**. No more arbitrary interval padding (which was a hacky way to get away not thinking about inserts). There is a subtle problem, though: inserts often overlap with more than one bait. In such cases, we need to have a model for estimating the probability that the insert is captured by either of the overlapping baits. The modeling can be done in the following semi-empirical fashion (thanks @yfarjoun), which needs to be done only once for each capture technology (Agilent, ICE):; - We locate isolated baits (i.e. those that are separated from one another by a few standard deviations of the average insert size); - We take a number of BAMs and calculate the empirical distribution of inserts around the isolated baits; - We fit a simple parametric distribution to the obtained empirical distributions, parameterized by bait length and insert length; we probably don't need to go all-in here, though the reference context of the bait is also likely to be an important covariate. Once these distributions are known, we can easily calculate the membership share of each bait in ambiguous cases and give each bait the appropriate share. Bonus:; -------. The empirical distribution of inserts around baits also allows us to associate a more reasonable GC content to each bait. Since GC bias is a property of the fragments that are pulled by the baits, a reasonable measure of ""GC content"" of each bait has to be calculated from the expected value of the GC content of the fragments that the bait pulls (not the GC content of the baits or targets), and this can be easily calculated from the previously obtained empirical distributions. ---. @mbabadi commented on [Fri Feb 17 2017](https://github.com/broadinstitute/gatk-protected/issues/914#issuecomm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2947:1447,parameteriz,parameterized,1447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947,1,['parameteriz'],['parameterized']
Modifiability,enhance ReadsPipelineSpark with metrics,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1237:0,enhance,enhance,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1237,1,['enhance'],['enhance']
Modifiability,"ent overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whether the in cases whether variants ar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1431,config,configuration,1431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['config'],['configuration']
Modifiability,"equester pays: disabled; 10:33:05.865 WARN SortSamSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: SortSamSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:33:05.865 INFO SortSamSpark - Initializing engine; 10:33:05.865 INFO SortSamSpark - Done initializing engine; 10:33:06.134 WARN Utils - Your hostname, gs2040t resolves to a loopback address: 127.0.1.1; using 172.20.19.130 instead (on interface bond0); 10:33:06.134 WARN Utils - Set SPARK_LOCAL_IP if you need to bind to another address; 10:33:06.242 INFO SparkContext - Running Spark version 3.3.0; 10:33:06.403 WARN SparkConf - Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).; 10:33:06.427 INFO ResourceUtils - ==============================================================; 10:33:06.427 INFO ResourceUtils - No custom resources configured for spark.driver.; 10:33:06.428 INFO ResourceUtils - ==============================================================; 10:33:06.428 INFO SparkContext - Submitted application: SortSamSpark; 10:33:06.446 INFO ResourceProfile - Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 600, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0); 10:33:06.454 INFO ResourceProfile - Limiting resource is cpu; 10:33:06.455 INFO ResourceProfileManager - Added ResourceProfile id: 0; 10:33:06.500 INFO SecurityManager - Changing view acls to: root; 10:33:06.501 INFO SecurityManager - Changing modify acls to: root; 10:33:06.501 INFO SecurityManager - Changing view acls groups to:; 10:33:06.502 INFO SecurityManager - Changing modify acls groups to:;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:41041,config,configured,41041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['config'],['configured']
Modifiability,"er and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job go",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1474,variab,variable,1474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,['variab'],['variable']
Modifiability,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1982,variab,variables,1982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,7,"['config', 'variab']","['configuration', 'variables']"
Modifiability,erArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWFkVGhyZWFkaW5nQXNzZW1ibGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `94.118% <Ã¸> (Ã¸)` | `1 <0> (Ã¸)` | :arrow_down: |; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `76.923% <Ã¸> (-0.946%)` | `34 <0> (-1)` | |; | [...walkers/haplotypecaller/HaplotypeCallerEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmUuamF2YQ==) | `78.425% <100%> (Ã¸)` | `76 <0> (Ã¸)` | :arrow_down: |; | [...rs/haplotypecaller/graphs/AdaptiveChainPruner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQWRhcHRpdmVDaGFpblBydW5lci5qYXZh) | `95.349% <100%> (+0.111%)` | `16 <0> (Ã¸)` | :arrow_down: |; | [...hellbender/tools/walkers/mutect/Mutect2Engine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRW5naW5lLmphdmE=) | `90.173% <100%> (Ã¸)` | `65 <0> (Ã¸)` | :arrow_down: |; | [...otypecaller/HaplotypeCallerArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5544/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <100%> (Ã¸)` | `3 <1> (+1)` | :arrow_up: |; | [...r/tools/walkers/mutect/Mutect2Integrati,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544#issuecomment-449424951:2272,Adapt,AdaptiveChainPruner,2272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544#issuecomment-449424951,1,['Adapt'],['AdaptiveChainPruner']
Modifiability,erators$ConcatenatedIterator.getTopMetaIterator(Iterators.java:1379); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$ConcatenatedIterator.hasNext(Iterators.java:1395); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.fillCache(PushToPullIterator.java:71); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.(PushToPullIterator.java:37); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiningIterator.(GVCFBlockCombiningIterator.java:14); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.lambda$writeVariantsSingle$516343c4$1(VariantsSparkSink.java:127); 	at org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitions$1(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858); 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:56); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:56); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:56); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93); 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166); 	at org.apache.spark.scheduler.Task.run(Task.scala:141); 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620); 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961:3230,adapt,adapted,3230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961,1,['adapt'],['adapted']
Modifiability,erce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40); 	at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:174); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar.copy(ShadowJar.java:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:46); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:39); 	at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:26); 	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:788); 	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:755); 	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:6739,plugin,plugins,6739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,ering/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTXV0ZWN0MkZpbHRlcmluZ0VuZ2luZS5qYXZh) | `97.115% <100%> (+0.057%)` | `43 <0> (Ã¸)` | :arrow_down: |; | [.../mutect/filtering/M2FiltersArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTTJGaWx0ZXJzQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `93.75% <100%> (+0.417%)` | `6 <0> (Ã¸)` | :arrow_down: |; | [...kers/mutect/filtering/MinAlleleFractionFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvTWluQWxsZWxlRnJhY3Rpb25GaWx0ZXIuamF2YQ==) | `100% <100%> (Ã¸)` | `7 <7> (?)` | |; | [...e/hellbender/utils/variant/GATKVCFHeaderLines.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `94.886% <100%> (+0.029%)` | `11 <0> (Ã¸)` | :arrow_down: |; | [...alkers/mutect/filtering/PolymorphicNuMTFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9maWx0ZXJpbmcvUG9seW1vcnBoaWNOdU1URmlsdGVyLmphdmE=) | `88.235% <88.235%> (Ã¸)` | `9 <9> (?)` | |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `87.309% <0%> (-0.306%)` | `244% <0%> (-2%)` | |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/5842/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477635851:3450,Polymorphi,PolymorphicNuMTFilter,3450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842#issuecomment-477635851,1,['Polymorphi'],['PolymorphicNuMTFilter']
Modifiability,"erval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkernel.tasks.inference_task_base - (denoising) starting...: 0%| | 0/1000 [00:00<?, ?it/s]; 15:10:03.410 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -1038.498 +/- 431.707, SNR: 71.3, T: 1.98: 8%|8 | 83/1000 [00:00<00:01, 826.53it/s]; 15:10:03.522 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -821.262 +/- 327.042, SNR: 38.9, T: 1.97: 17%|#6 | 166/1000 [00:00<00:01, 776.56it/s]; 15:10:03.636 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -727.432 +/- 277.971, SNR: 29.4, T: 1.95: 24%|##4 | 244/1000 [00:00<00:01, 732.12it/s]; 15:10:03.754 INFO gcnvkernel.tasks.inference_task_base - (deno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:7364,variab,variables,7364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variables']
Modifiability,"es can be specified as:; ./. .. completely missing (""."" or ""./."", depending on ploidy); ./x .. partially missing (e.g., ""./0"" or "".|1"" but not ""./.""); . .. partially or completely missing; a .. all genotypes; b .. heterozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascript or python plugins to extend some of the functionality where developers may not reach immediately. Personally I use htsjdk extensively (and sometimes p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:1030,Plugin,Plugin,1030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,1,['Plugin'],['Plugin']
Modifiability,"es/theano/__init__.py"", line 124, in <module>; from theano.scan_module import (scan, map, reduce, foldl, foldr, clone,; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4147,config,config,4147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['config']
Modifiability,"es:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4359,Config,ConfigFactory,4359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,eshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -15; --smith-waterman-read-to-haplotype-gap-open-penalty -30 --smith-waterman-read-to-haplotype-gap-extend-penalty -5 --flow-assembly-collapse-hmer-size 0 --flow-assembly-collapse-partial-mode; false --flow-filter-alleles false --flow-filter-alleles-qual-threshold 30.0 --flow-filter-alleles-sor-threshold 3.0 --flow-filter-lone-alleles false --flow-filter-alleles-debug-graphs fal; se --min-assembly-region-size 50 --max-assembly-region-size 300 --active-probability-threshold 0.002 --max-prob-propagation-distance 50 --force-active false --assembly-region-padding 100 -; -padding-around-indels 75 --padding-around-snps 20 --padding-around-strs 75 --max-extension-into-assembly-region-pa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7749,extend,extend-penalty,7749,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,3,['extend'],['extend-penalty']
Modifiability,est/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:61901,variab,variable,61901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['variab'],['variable']
Modifiability,expose DEFAULT_FEATURE_CACHE_LOOKAHEAD as a configurable option,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3489:44,config,configurable,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3489,1,['config'],['configurable']
Modifiability,"faults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4795,Config,ConfigFactory,4795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"fdbd7fc000-7ffdbd7fe000 r-xp 00000000 00:00 0 [vdso]; ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall]. VM Arguments:; jvm_args: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx16g ; java_command: /usr/bin/gatk-package-4.0.0.0-local.jar HaplotypeCaller -I CMT_Project04172015_20150062801_S_2.bam -R /beegfs/work/zxmai83/Reference/genome/b37/human_g1k_v37.fasta -O CMT_Project04172015_20150062801_S_2_variants.vcf -ERC GVCF --create-output-variant-index --annotation MappingQualityRankSumTest --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation FisherStrand --annotation Coverage --dbsnp /beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf --verbosity INFO; java_class_path (initial): /usr/bin/gatk-package-4.0.0.0-local.jar; Launcher Type: SUN_STANDARD. Environment Variables:; PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; LD_LIBRARY_PATH=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/.singularity.d/libs. Signal Handlers:; SIGSEGV: [libjvm.so+0x632f3c], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGBUS: [libjvm.so+0x632f3c], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGFPE: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGPIPE: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGXFSZ: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGILL: [libjvm.so+0x5622fa], sa_mask[0]=11111111011111111101111111111110, sa_flags=SA_RESTART|SA_SIGINFO; SIGUSR1: SIG_DFL, sa_mask[0]=00000000000000000000000000000000, sa_flags=none; SIGUSR2: [libjvm.so+0x562499], sa_mask[0]=000000000",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:39585,Variab,Variables,39585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Variab'],['Variables']
Modifiability,feCycle.doStart(ContainerLifeCycle.java:105); at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61); at org.eclipse.jetty.server.Server.doStart(Server.java:394); at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68); at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1155); at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:181); at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:885); at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:707); at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953); at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:926); at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1692); at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1314); at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1083); at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:958); at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:890); at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518); at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477); at org.broadinstitute.hellbender.testutils.MiniClusterUtils.getMiniCluster(MiniClusterUtils.java:30); at org.broadinstitute.hellbender.testutils.MiniClusterUtils.getMiniCluster(MiniClusterUtils.java:38); at org.broadinstitute.hellbender.metrics.MetricsUtilsTest.setupMiniCluster(MetricsUtilsTest.java:24). Caused by:; java.lang.IllegalArgumentException: Invalid Java version 11.0.16.1; at org.eclipse.jetty.util.JavaVersion.parseJDK9(JavaVersion.java:71); at org.eclipse.jetty.util.JavaVersion.parse(JavaVersion.java:49); at org.eclipse.jetty.util.JavaVersion.<clinit>(JavaVersion.java:[43](https://github.com/broadinstitute/gatk/action,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8098#issuecomment-1320505279:2385,config,configureNameService,2385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8098#issuecomment-1320505279,1,['config'],['configureNameService']
Modifiability,"find the string 'entrainScore=0.7203;HW=4.306476E-6' in the vcf file, and Line 104 is part of the header (I think Line 104 refers to tribble source though). Gatks ValidateVariants does not have any issues with the vcf file, and looking visually with bcftools I cannot see an issue either. . Can anyone suggest further diagnosis steps? Should I take this to GATK issue tracker?. The VCF file is here: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf. Here is the gatk command line: ; ```; gatk --java-options ""-Xmx100g -Xms100g"" \; VariantRecalibrator \; -V /share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SitesOnlyGatherVcf/execution/NA12878.sites_only.vcf.gz \; -O NA12878.snps.recal \; --tranches-file NA12878.snps.tranches \; -trust-all-polymorphic \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ -an SOR -an DP \; -mode SNP \; -sample-every 10 \; --output-model NA12878.snps.model.report \; --max-gaussians 6 \; -resource hapmap,known=false,training=true,truth=true,prior=15:/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/hapmap_3.3.b37.vcf \; -resource omni,known=false,training=true,truth=true,prior=12:/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/2ce78a09-433b-48eb-83d0-1fa1771d1181/call-SNPsVariantRecalibratorCreateModel/inputs/share/ClusterShare/biodata/contrib/evaben/hs37d5x/vcf/1000G_omni2.5.b37.vcf \; -resource 1000G,known=false,training=true,truth=fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4761:1583,polymorphi,polymorphic,1583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761,1,['polymorphi'],['polymorphic']
Modifiability,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1450:468,config,configurations,468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450,1,['config'],['configurations']
Modifiability,fixes #1484 ; I also refactored the code duplication between ExcessHet and InbreedingCoef . @droazen please have a look,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1487:21,refactor,refactored,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1487,1,['refactor'],['refactored']
Modifiability,"fixes #1653. ok, results are in, with asyncIO GATK4 out of the box beats GATK3 4.53x on PrintReads and 1.48x on BaseRecalibrator!. PrintReads; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1.32x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.68x baseline ; syncTribble, syncSamtools, JDKDeflater: 2.32x baseline. For comparison, GATK3 out of the box is 4.53x baseline on PrintReads. BaseRecalibrator; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1x baseline; asyncTribble, asyncSamtools, IntelDeflater: 2.14x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.04x baseline; syncTribble, syncSamtools, JDKDeflater: 1.03x baseline; asyncTribble, asyncSamtools, JDKDeflater: 2.12x baseline. For comparison, GATK3 out of the box is 1.48x baseline on BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1695:232,config,config,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1695,2,['config'],['config']
Modifiability,fixes and refactoring methods in SparkUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4765:10,refactor,refactoring,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765,1,['refactor'],['refactoring']
Modifiability,g.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:1299,Config,ConfigCache,1299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,1,['Config'],['ConfigCache']
Modifiability,"g.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; ^C; ####################### Ctrl-C after 16 hours ##############; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6298,variab,variable,6298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,2,['variab'],['variable']
Modifiability,"gAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (for ins, del, small dup, inversion only) by delegating to `ChimericAlignment.parseOneContig` and `NovelAdjacencyReferenceLocations(ChimericAlignment chimericAlignment, byte[] contigSequence, SAMSequenceDictionary)`; this should be deprecated; * exact variant type inference (delegated to `SvTypeInference.inferFromNovelAdjacency()`) and annotation (delegated to `AnnotatedVariantProducer.produceAnnotatedVcFromInferredTypeAndRefLocations()`); this should be deprecated; * imprecise variants detection; this should be kept and factored out. --------; --------. ## Planed steps. 1. repackaging & refactoring (no logic change, see #3934 ); 2. bring in some valuable changes made in PR #3668; 3. **more test coverage** (ticket #3431); 4. switch; make `StructuralVariationDiscoveryPipelineSpark` call into `SvDiscoverFromLocalAssemblyContigAlignmentsSpark` by default and optionally into `DiscoverVariantsFromContigAlignmentsSAMSpark`, i.e. opposite of what we currently do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:2665,refactor,refactoring,2665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,1,['refactor'],['refactoring']
Modifiability,gCNV in the CASE mode now fills in all hidden DenoisingModelConfig and CopyNumberCallingConfig arguments from the input model configuration. . This addresses issue #6994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7464:126,config,configuration,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7464,1,['config'],['configuration']
Modifiability,gCNV refactoring and code improvement,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2979:5,refactor,refactoring,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2979,1,['refactor'],['refactoring']
Modifiability,"gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1377,config,configparser,1377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4813,Config,ConfigFactory,4813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,ge Î” | Complexity Î” | |; |---|---|---|---|; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `80.743% <0%> (-4.581%)` | `89% <0%> (Ã¸)` | |; | [...ute/hellbender/utils/test/FuncotatorTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Z1bmNvdGF0b3JUZXN0VXRpbHMuamF2YQ==) | `95.161% <0%> (-3.084%)` | `7% <0%> (+1%)` | |; | [...GATKPlugin/GATKReadFilterPluginDescriptorTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yVGVzdC5qYXZh) | `88.62% <0%> (-1.76%)` | `48% <0%> (+1%)` | |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `88.235% <0%> (-1.43%)` | `58% <0%> (+1%)` | |; | [.../tools/walkers/haplotypecaller/RefVsAnyResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZWc0FueVJlc3VsdC5qYXZh) | `100% <0%> (Ã¸)` | `3% <0%> (+1%)` | :arrow_up: |; | [...ools/walkers/annotator/VariantAnnotatorEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5566/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9WYXJpYW50QW5ub3RhdG9yRW5naW5lLmphdmE=) | `91.304% <0%> (Ã¸)` | `70% <0%> (Ã¸)` | :arrow_down: |; | [...line/GATKPlugin/testpluggables/TestAnnotation.java](https:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5566#issuecomment-452843310:1862,Plugin,Plugin,1862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5566#issuecomment-452843310,1,['Plugin'],['Plugin']
Modifiability,"genome changes,; coding sequence changes, and protein changes.; This is due to how these fields are generated from the reference; sequence. - Fixed a bug (insertions on - strand):; Insertions on the - strand would have incorrect reference; sequences/alleles.; Now they are handled as a special case when computing the aligned; reference allele. - Fixed a bug in transcript selection for GencodeFuncotationFactory:; The LocusLevel / Curation Level was being incorrectly pulled from the; GENE features, rather than the TRANSCRIPT features that contain each; variant. As a result, the order in which representative transcripts; were chosen was wrong. The TRANSCRIPT feature is now being used to; determine the Locus/Curation Level. - `TranscriptType` now determined by transcript annotation, not gene annotation; - Start/stop codon overlapping now corrected for preceding indel bases (is now correct for more cases).; - Changed algorithm for how 5'UTRs are determined. - Refactored how frameshift indels have codon change strings created. - Added in helper some scripts for testing funcotator. - Fixed how codon change strings are rendered to be consistent and more; correct. - Fixed Protein Change strings to be consistent and more; correct. - Implemented tests for CreateProteinChangeInfo; - Implemented tests for RenderProteinChangeString; - Implemented tests for IsIndelBetweenCodons; - Implemented tests for GetCodonChangeString. - Added a unit test for; testCreateGencodeFuncotationBuilderWithTrivialFieldsPopulated. - Fixed a bug when variant ref allele doesn't match reference genome. - Fixed test cases for - strand indel cdna strings:; There is a bug in oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:2273,Refactor,Refactored,2273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,1,['Refactor'],['Refactored']
Modifiability,ging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:4275,config,config,4275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,ging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3412,plugin,plugins,3412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['plugin'],['plugins']
Modifiability,google-cloud-nio: implement configurable retries/reopens,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5306:28,config,configurable,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5306,1,['config'],['configurable']
Modifiability,"gradle uploadArchives will perform a maven release (currently a snapshot release). Unfortunately the maven plugin has an ""install"" task which installs to the local repo, so it's now necessary to specify `installApp` or `installDist` instead of just `gradle install`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/482:107,plugin,plugin,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/482,1,['plugin'],['plugin']
Modifiability,gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5197,plugin,plugins,5197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,hadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40); 	at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:174); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar.copy(ShadowJar.java:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73); 	at org.gradle.api.internal.project.taskfactory.StandardTask,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:6215,plugin,plugins,6215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,hc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `80.172% <Ã¸> (Ã¸)` | `19 <0> (Ã¸)` | :arrow_down: |; | [...tute/hellbender/tools/AnnotatePairOrientation.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9Bbm5vdGF0ZVBhaXJPcmllbnRhdGlvbi5qYXZh) | `96.429% <Ã¸> (Ã¸)` | `8 <0> (Ã¸)` | :arrow_down: |; | [...nder/tools/copynumber/utils/TagGermlineEvents.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL1RhZ0dlcm1saW5lRXZlbnRzLmphdmE=) | `100% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...t/java/org/broadinstitute/hellbender/MainTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluVGVzdC5qYXZh) | `85.714% <90.909%> (+2.787%)` | `15 <9> (+9)` | :arrow_up: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ender/tools/walkers/annotator/PolymorphicNuMT.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9Qb2x5bW9ycGhpY051TVQuamF2YQ==) | `92.593% <0%> (-3.704%)` | `8% <0%> (-1%)` | |; | [...r/tools/walkers/mutect/Mutect2IntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QySW50ZWdyYXRpb25UZXN0LmphdmE=) | `87.586% <0%> (-0.517%)` | `89% <0%> (-2%)` | |; | ... and [13 more](https://codecov.io/gh/broadinstitute/gatk/pull/5551/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5551#issuecomment-450184780:3447,Polymorphi,PolymorphicNuMT,3447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5551#issuecomment-450184780,1,['Polymorphi'],['PolymorphicNuMT']
Modifiability,"he 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz; ```. #### Steps to reproduce; gatk --java-options -Xms100g VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kageproj/gatk/bundle/Homo_sapiens_assembly38.dbsnp138.vcf.gz. The input VCF was generated with the dragen-gatk HaplotypeCaller with Allele Specific annotations. . #### Expected behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:11605,polymorphi,polymorphic,11605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['polymorphi'],['polymorphic']
Modifiability,"hila.sorted.bam \; -O hdfs://master2:9000/Drosophila/output/Drosophila.sorted.markdup.bam \; -M; hdfs://master2:9000/Drosophila/output/Drosophila.sorted.markdup_metrics.txt; \; -- \; --spark-runner SPARK --spark-master spark://master2:7077; ```; **error logs:**. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError:; scala/Product$class; at; org.bdgenomics.adam.serialization.InputStreamWithDecoder.<init>(ADAMKryoRegistrator.scala:35); at; org.bdgenomics.adam.serialization.AvroSerializer.<init>(ADAMKryoRegistrator.scala:45); at; org.bdgenomics.adam.models.VariantContextSerializer.<init>(VariantContext.scala:94); at; org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:179); at; org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:78); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$8(KryoSerializer.scala:170); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$8$adapted(KryoSerializer.scala:170); at scala.Option.foreach(Option.scala:407); at; org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:170); at; scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); at; org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:221); at; org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:161); at; org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102); at; com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48); at; org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109); at; org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:336); at; org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:256); at; org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:422); at; org.apache.spark.broadcast.TorrentBroadcast$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644:1308,adapt,adapted,1308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644,1,['adapt'],['adapted']
Modifiability,"his environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Succe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:2580,config,config,2580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,1,['config'],['config']
Modifiability,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2092:867,refactor,refactor,867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092,1,['refactor'],['refactor']
Modifiability,"https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/funcotator/dataSources/xsv/LocatableXsvFuncotationFactory.java#L245-L247. Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.; Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. Modify the variable â€˜supportedFieldNamesâ€™ with volatile to tackle the problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376:494,variab,variable,494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376,1,['variab'],['variable']
Modifiability,"icsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4935,Config,ConfigFactory,4935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ied to install in a virtual python environment the dependencies found in these two files:. gatk/scripts/gatkcondaenv.yml.template ; gatk/src/main/python/org/broadinstitute/hellbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in che",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1125,config,configparser,1125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"ies = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6149,Config,ConfigFactory,6149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Config'],['ConfigFactory']
Modifiability,"ies = 20; 17:54:55.320 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:54:55.320 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:54:55.320 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:54:55.320 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:54:55.320 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:54:55.320 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:54:55.320 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:6788,Config,ConfigFactory,6788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['Config'],['ConfigFactory']
Modifiability,"ies.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6123,config,config,6123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['config']
Modifiability,"if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:6301,refactor,refactored,6301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['refactor'],['refactored']
Modifiability,"igFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5130,Config,ConfigFactory,5130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"igPloidy - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4077,Config,ConfigFactory,4077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"igarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). This looks similar to issue #8232, but I've not been able to solve this using any of the fixes suggested on that page. I'm using Java version 8, the input BAMs were mapped using STAR via nfcore rnaseq without issue and I have plenty of space on my disk drive, even when specifying a temp directory. It also doesn't matter if I run the tool independently using the command above or as part of a pre-configured pipeline, and I get the same issue with SplitNCigarReads acting as if it has running out of space. . How do I fix this? I need this step to run variant calling on my rnaseq samples (I'm using the GATK best practices pipeline).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:10545,config,configured,10545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,1,['config'],['configured']
Modifiability,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356:3935,config,configuration,3935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356,2,"['Config', 'config']","['Configuration', 'configuration']"
Modifiability,"ility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/48570e53-024c-44b5-8835-3fd40b4c5866); ![image](https://github.com/broadinstitute/gatk/assets/11076296/99100e5d-05e2-4a5c-9d68-57db1b734029); ![image](https://github.com/broadinstitute/gatk/assets/11076296/abae09e1-70a5-4213-95a2-0cb10f9db192); ![image](https://github.com/broadinstitute/gatk/assets/11076296/ef68d0da-90df-4c4b-9802-97988a498280). 2) ... Will update more later!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1571,variab,variables,1571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,3,['variab'],['variables']
Modifiability,"inaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: 'FixVcfHead' is not a valid command.; Did you mean this?; FixVcfHeader; 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:341); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:172); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:192); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); ```. I expect something without the stack trace and the scary ""Exception"" message. For example:. ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run. ...skipped for brevity... VcfFormatConverter (Picard) Converts VCF to BCF or BCF to VCF.; VcfToIntervalList (Picard) Converts a VCF or BCF file to a Picard Interval List. --------------------------------------------------------------------------------------. Did you mean this?; FixVcfHeader; ```. The same happens with unknown commands. The code that should be changed for that is the following, where the `setupConfigAndExtractProgram` call should be also inside the try block:. https://github.com/broadinstitute/gatk/blob/8ac2f102b303f343c4787ad4e3359335641c5121/src/main/java/org/broadinstitute/hellbender/Main.java#L190-L212",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4256:1646,adapt,adapters,1646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4256,1,['adapt'],['adapters']
Modifiability,"ine.collect(ReferencePipeline.java:566); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:341); at org.broadinstitute.hellbender.utils.codecs.xsvLocatableTable.XsvLocatableTableCodec.readActualHeader(XsvLocatableTableCodec.java:64); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37); at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:261); ... 18 more; ```. java version:; ```; java -version; openjdk version ""1.8.0_222""; OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1~deb9u1-b10); OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode); ```; I added the cadd folder into data source folder like the structure mentioned in document:; ```; cadd; |- hg19; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; |; |- hg38; | |- cadd.config; | |- InDels_inclAnno.tsv; | |- InDels_inclAnno.tsv.gz.tbi; ```; The config file (cadd.config); ```; name = CADD; version = v1.4; src_file = InDels_inclAnno.tsv; origin_location =; preprocessing_script = UNKNOWN. Whether this data source is for the b37 reference.; Required and defaults to false.; isB37DataSource = false. Supported types:; simpleXSV -- Arbitrary separated value table (e.g. CSV), keyed off Gene Name OR Transcript IDlocatableXSV -- Arbitrary separated value table (e.g. CSV), keyed off a genome locationgencode -- Custom datasource class for GENCODEcosmic -- Custom datasource class for COSMIC vcf -- Custom datasource class for Variant Call Format (VCF) files; type = locatableXSV; Required field for GENCODE files.Path to the FASTA file from which to load the sequences for GENCODE transcripts:; gencode_fasta_path =. Required field for GENCODE files.; NCBI build version (either hg19 or hg38):; ncbi_build_version =. Required field for simpleXSV files.; Valid values:; GENE_NAME; TRANSCR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6223:4457,config,config,4457,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6223,1,['config'],['config']
Modifiability,"ineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4704,Config,ConfigFactory,4704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"ineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80). Â The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used:Â v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -oÂ  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1658,config,config,1658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['config'],['config']
Modifiability,"ingframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; 	at com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:131); 	at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); 	at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100); 	at com.luz.push.utils.GcmUtils.init(GcmUtils.java:31); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:389); 	at org.springf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:28788,variab,variable,28788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['variab'],['variable']
Modifiability,institute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:288); at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:225); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.getPileupsOverReference(AssemblyBasedCallerUtils.java:443); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.calculateRefConfidence(ReferenceConfidenceModel.java:195); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:645); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:212); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```; This hypothesis is further evidenced by the fact that one user at least claims that their input file validates and that they couldn't find the problem reads by looking at the input files manually. We probably will want to look at an example file in the debugger to catch what is happening at this site. We have refactored a bunch of code adjacent to this function recently so its possible this is a recent regression.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6490:3216,refactor,refactored,3216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6490,1,['refactor'],['refactored']
Modifiability,"involving ""zombie"" likelihoods from past removed evidences being assigned to new appended evidences in AlleleLikelihoods. Refactor and clean some code as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7154:122,Refactor,Refactor,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7154,1,['Refactor'],['Refactor']
Modifiability,"ional coverages, e.g. a value of 0.499 rounds to zero. The QC step does not transform data per se but does then throw out data points (targets, samples) from consideration and inclusion in the PoN. VR's debugging traces this QC step to HDF5PCACoveragePoNCreationUtils.java, line 212. It appears the current workaround in the official WDL scripts towards preventing data from being thrown out is to disable the QC step altogether, with the `--noQC true` option. However, this appears to me a hack that does not allow us to create a more refined PoN that the QC is meant to enable. **Can someone explain the impact of skipping QC?**. @mbabadi has said for me to go ahead and fix this line of code by changing the `roundToInteger` to true. I am placing this issue here for further discussion and to separate the discussion from the PR. ---; ### Information is lacking in our repo regards to certain settings; Our understanding of the CNV workflow was to use proportional coverage (PCOV) for CalculateTargetCoverage, whether for the PoN or samples. However, @LeeTL1220, there isn't any example INPUTS JSON or discussion of settings for such for the somatic WDL variables pertaining to `gatk/scripts/cnv_wdl/cnv_common_tasks.wdl`'s CalculateTargetCoverage task, where `transform` is the variable that defines whether counts ought to be raw or proportional. . If I search the repo for the WDL variable ""transform PCOV json"", then I get no hits. However, if I search the repo for ""transform RAW json"", then I get germline calling workflows that show '""transform = ""RAW""'. For example, '""transform = ""RAW""' is in `scripts/cnv_wdl/germline/cnv_germline_single_sample_calling_workflow.wdl` and `scripts/cnv_wdl/germline/cnv_germline_panel_creation_workflow.wdl`. Please correct me if I am wrong but It seems to me that this setting shouldn't be different between the binned targets of the germline workflow and a somatic WES workflow. What is the reason we use proportional (PCOV) counts instead of RAW counts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3163:11684,variab,variables,11684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3163,3,['variab'],"['variable', 'variables']"
Modifiability,"is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second attempt, now running, scattered the genome into 150 shards, and is running 12 jobs at a time with 8 cores each and the environment variables set. On the second attempt it looks like the jobs will finish the first denoising epoch in < 1 hour each. That's far faster than the 6x reduction in runtime you might expect if a) runtime is linear in the number of bins and b) runtime is proportional to 1/cpus used. Without doing a lot more experiments it's hard to tell whether the better runtime is due to less fighting over resources (I can imagine 24 jobs each running 96 threads could degrade performance) or because runtime is super-linear vs. number of bins. I'm not asking for total precision, but the current docs are not really enough for anyone outside the GATK team to get the CNV caller up and running in an efficient manner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:2133,variab,variables,2133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,1,['variab'],['variables']
Modifiability,"ission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3556,variab,variables,3556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['variab'],['variables']
Modifiability,"ist; Using GATK jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8432:1244,Config,Configure,1244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432,1,['Config'],['Configure']
Modifiability,"ite_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 4G --num-executors 4 --executor-cores 6 --executor-memory 16G --conf spark.dynamicAllocation.enabled=false /opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img -I hdfs:///user/sun/1982.unmapped.bam -R hdfs:///user/sun/ucsc.hg19.fasta -O hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam --sparkMaster yarn; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running spark-class from user-defined location.; 18:30:33.354 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:30:33.534 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [January 9, 2018 6:30:33 PM CST] BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage hdfs:///user/sun/ucsc.hg19.fasta.img --output hdfs:///user/sun/17F02897_17F02897M_WES_img.bwa.bam --reference hdfs:///user/sun/ucsc.hg19.fasta --input hdfs:///user/sun/1982.unmapped.bam --sparkMaster yarn --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater fal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:2806,variab,variables,2806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ite_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://973f3a3a3407:7077; 13:47:29.376 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:47:29.548 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDupl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:18403,variab,variables,18403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:4532,config,config,4532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,2,['config'],['config']
Modifiability,java:650); 	at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:458); 	at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:38); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:57); 	at org.gradle.api.internal.file.copy.CopyActionProcessingStream$process.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:4822,plugin,plugins,4822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,jdk1.8.0_121\jre\lib\ext\cldrdata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\dnsns.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jaccess.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\jfxrt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\localedata.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\nashorn.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunec.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunjce_provider.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunmscapi.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\sunpkcs11.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\ext\zipfs.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\javaws.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jce.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfr.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jfxswt.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\jsse.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\management-agent.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\plugin.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\resources.jar;D:\Program Files\Java\jdk1.8.0_121\jre\lib\rt.jar;C:\project\push\target\classes;E:\repository\org\springframework\boot\spring-boot-starter-jdbc\2.3.0.RELEASE\spring-boot-starter-jdbc-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter\2.3.0.RELEASE\spring-boot-starter-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot\2.3.0.RELEASE\spring-boot-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-autoconfigure\2.3.0.RELEASE\spring-boot-autoconfigure-2.3.0.RELEASE.jar;E:\repository\org\springframework\boot\spring-boot-starter-logging\2.3.0.RELEASE\spring-boot-starter-logging-2.3.0.RELEASE.jar;E:\repository\ch\qos\logback\logback-classic\1.2.3\logback-classic-1.2.3.jar;E:\repository\ch\qos\logback\logback-core\1.2.3\logback-core-1.2.3.jar;E:\repository\org\apache\logging\log4j\log4j-to-slf4j\2.13.2\log4j-to-slf4j-2.13.2.jar;E:\repository\org\apache\logging\log,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:1669,plugin,plugin,1669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['plugin'],['plugin']
Modifiability,"k - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-small-m.c.broad-dsde-methods.internal, exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:7126,config,configuration,7126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"k.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGermlineContigPloidy - Initializing engine; 08:48:45.931 DEBUG ScriptExecutor - Executing:; 08:48:45.931 DEBUG ScriptExecutor - python; 08:48:45.932 DEBUG ScriptExecutor - -c; 08:48:45.932 DEBUG ScriptExecutor - import gcnvkernel. WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.; /home/ec2-user/miniconda3/envs/gatk/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).ty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:5481,Config,ConfigFactory,5481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,k/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1467,config,configuration,1467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['config'],['configuration']
Modifiability,"k/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3536,Config,ConfigFactory,3536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"k/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkernel.tasks.inference_task_base - (denoising) starting...: 0%| | 0/1000 [00:00<?, ?it/s]; 15:10:03.410 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -1038.498 +/- 431.707, SNR: 71.3, T: 1.98: 8%|8 | 83/1000 [00:00<00:01, 826.53it/s]; 15:10:03.522 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -821.262 +/- 327.042, SNR: 38.9, T: 1.97: 17%|#6 | 166/1000 [00:00<00:01, 776.56it/s]; 15:10:03.636 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -727.432 +/- 277.971, SNR: 29.4, T: 1.95: 24%|##4 | 244/1000 [00:00<00:01, 732.12it/s]; 15:10:03.754 INFO gcnvkernel.tasks.inference_task_base - (denoising epoch 1) ELBO: -662.875 +/- 251.403, SNR: 23.4, T: 1.94: 32%|###1 | 318/1000 [00:00<00:00, 689.38it/s]; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:7492,variab,variables,7492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variables']
Modifiability,"l or two that use `PythonScriptExecutor` to call into a Python machine-learning library, and do an assessment of maintainability, etc. `PythonScriptExecutor` will come with an attached set of conditions for its use, intended to address the most serious issues raised by the engine and support teams with having Python code in the GATK. We should document these conditions in the docs for `PythonScriptExecutor` when it's implemented:. 1. All tools that use `PythonScriptExecutor` must have a Java-based front-end, with standard GATK (barclay-based) arguments. We put a lot of development effort into our arg parser and into striving for user-interface consistency across tools, and cannot afford to duplicate this effort in Python. Geraldine (CC'd) and the rest of the support team can back me up on this one!. 2. An honest effort should be made to minimize the amount of code written in Python -- as much of each tool's work as possible should be done in Java. In particular, reading/writing final inputs and outputs should happen in Java. This is important for a number of reasons, including the engine team's goal of ensuring universal GCS support, consistent Google authentication handling, etc. Again, we really don't want to have to duplicate that work in Python, or for the tools that call into Python to be inconsistent with the rest of the toolkit. 3. All dependencies (Python and native) of Python libraries used will be clearly documented, and included in the default GATK docker image. I don't think I need to explain why this one is important :) . 4. Before we go any further down this path, we prototype one or two tools using `PythonScriptExecutor`, and do a fair assessment of maintainability and other concerns of the engine/support teams, such as whether it will even be possible to package all dependencies without conflicts. 5. Engine team will continue to search for Java-based solutions while this evaluation is ongoing, but this proposal at least unblocks the CNV team for now.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501:1875,maintainab,maintainability,1875,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501,1,['maintainab'],['maintainability']
Modifiability,"lassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:13:11.050 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:13:11.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:13:12.999 INFO CountReadsSpark - ------------------------------------------------------------; 13:13:13.000 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:13:13.000 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:13:13.000 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:13:13.001 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:13:13.001 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:13:11 PM EST; 13:13:13.001 INFO CountReadsSpark - ----------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:1713,variab,variables,1713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,ldevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3209,Config,ConfigureActionsProjectEvaluator,3209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Config'],['ConfigureActionsProjectEvaluator']
Modifiability,le path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 15:16:41.540 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Familial_Cancer_Genes.no_dupes.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/familial/hg38/Familial_Cancer_Genes.no_dupes.tsv; > 15:16:41.545 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 15:16:41.556 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.575 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.config; > 15:16:41.707 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.709 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-17 15:16:41 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 15:16:41.717 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/dnaRepairGenes.20180524T145835.csv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/dna_repair_genes/hg38/dnaRepairGenes.20180524T145835.csv; > 15:16:41.723 INFO DataSourceUtils - Resolved data source file path: file://,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:10284,config,config,10284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['config'],['config']
Modifiability,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.u",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4769,config,config,4769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['config']
Modifiability,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6006,config,config,6006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['config']
Modifiability,"led: time = Wed Apr 14 11:52:33 2021; , filename = '/SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20210411.GRCh37.gatkcnv.brs/work/92/579e5a48aa9e52cd0; e1df603266809/B00HOTD.counts.hdf5', file descriptor = 250, errno = 121, error message = 'Remote I/O error', buf = ; 0x2b6ebddf38e8, total read size = 384, bytes this sub-read = 384, bytes actually read = 18446744073709551615, offs; et = 712120; major: Low-level I/O; minor: Read failed; HDF5-DIAG: Error detected in HDF5 (1.8.14) thread 0:; #000: /mnt/scr1/abyrne/HDFJava-platypus-2.11/native/HDF5-prefix/src/HDF5/src/H5D.c line 826 in H5Dvlen_reclaim(); : invalid dataspace; major: Invalid arguments to routine; minor: Inappropriate type; 11:52:33.796 INFO GermlineCNVCaller - Shutting down engine; [April 14, 2021 11:52:33 AM CEST] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed t; ime: 0.90 minutes.; Runtime.totalMemory()=2374500352; Exception in thread ""main"" java.lang.InternalError: H5DreadVL_str: failed to read variable length strings; 	at ncsa.hdf.hdf5lib.H5.H5DreadVL(Native Method); 	at org.broadinstitute.hdf5.HDF5File.lambda$readStringArray$0(HDF5File.java:161); 	at org.broadinstitute.hdf5.HDF5File.readDataset(HDF5File.java:349); 	at org.broadinstitute.hdf5.HDF5File.readStringArray(HDF5File.java:150); 	at org.broadinstitute.hellbender.tools.copynumber.utils.HDF5Utils.readIntervals(HDF5Utils.java:62); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.lambda$new; $2(HDF5SimpleCountCollection.java:76); 	at htsjdk.samtools.util.Lazy.get(Lazy.java:25); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.HDF5SimpleCountCollection.getInterva; ls(HDF5SimpleCountCollection.java:85); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readHDF5(Simpl; eCountCollection.java:119); 	at org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection.readAndSubset(; Simp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7202:3805,variab,variable,3805,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7202,1,['variab'],['variable']
Modifiability,lizer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1703,config,configuration,1703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['config'],['configuration']
Modifiability,llRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -I gs://fc-8268e82b-ed61-4e04-a8c9-a95a05c0952e/bda6f5ba-8928-45bf-a6b0-9fe67d8dd9a4/PreProcessingForVariantDiscovery_GATK4/cccdda67-56e1-4363-aa6c-46ce53ef8afd/call-GatherBamFiles/attempt-2/Abrams_cell.bam -tumor Abrams_1 --germline-resource gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/canid_wgs_ref.1.0.no_samples.vcf.gz -pon gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/b92f3c35-5813-455b-94dc-3de3b54f5f98/Mutect2_Panel/c9f21d8a-384e-4d17-a6f8-79a502698827/call-MergeVCFs/1-Mutect2_PON_2019-07-25T22-08-49.vcf -L gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/f2138b33-3918-4f8a-9b87-1823a0084ac3/Mutect2/c4844164-ecad-4878-9e5d-cd134a7fb40d/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:2782,variab,variable,2782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,1,['variab'],['variable']
Modifiability,"llbender/setup_gcnvkernel.py. I have installed gcnvkernel in my virtual environment. . ----. This is the error message I get when I try to import gcnvkernel: python -c ""import gcnvkernel"". Traceback (most recent call last):; File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1138, in _unify_values; sectiondict = self._sections[section]; KeyError: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1299,config,configparser,1299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"ller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4011,Config,ConfigFactory,4011,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at org.apache.logging.log4j.core.LoggerContext.<init>(LoggerContext.java:75); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:4167,config,config,4167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['config'],['config']
Modifiability,"lot of threading output writers through the codebase and perhaps this is better handled by the ""--debug"" argument like it used to? Thoughts? . Notes: ; - It should be noted that by design all of the added changes to HaplotypeCaller are opt-in, barring errors in implementation.; - This code is measurably slower than vanilla HaplotypeCaller. In particular FRD is a very expensive step that corresponds to ~5-7% of the runtime. This is in part because it has to duplicate many of the steps in the genotyper based on the number of unique mapping qualities present at a site as well as the fact that it performs an O(n^2) number of operations at sites with many possible alleles. There are options to cut down on the cost of this algorithm that moderately impact the results relative to DRAGEN. . This implementation is intended to produce results close to the results on DRAGEN 3.4.12 without stripping away the major improvements made in GATK4, as a result there are a number of areas in which we know we are producing different results: ; - In GATK4 variants that overlap with an upstream deletion will have added to their alleles list a sybmolic '*' deletion alleles which are genotyped as part of the allele array in the genotyeper. This is not the case in DRAGEN and it interacts with FRD in such a way as to produce a number of variants that in DRAGEN would have been called as 0/1 heterozygous calls with capped QUAL scores, in gatk they are called as 1/2 calls with uncapped quality scores.; - While we have added the option to use the legacy assembly region creation code, it is not part of the expected pipeline for running DRAGEN. This includes a number of arguments that were done away with in the recent refactoring pass. ; - I have added hooks to revert the assembly engine to approximately its state in gatk3. While I don't stand by that change I think we should bundle the minor assembly engine changes together with our other assembly engine work to try to make a more convincing case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:5372,refactor,refactoring,5372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['refactor'],['refactoring']
Modifiability,"ls = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5592,Config,ConfigFactory,5592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"ls = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.826 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4489,Config,ConfigFactory,4489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"ls by specifying bin_length = 0.; -I removed the separation between coverage and allelic packages to make the package structure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1219,refactor,refactoring,1219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['refactor'],['refactoring']
Modifiability,"ls=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1709,variab,variables,1709,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"lse --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:3838,variab,variable,3838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['variab'],['variable']
Modifiability,"lse --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:14:13.567 INFO PrintReadsSpark - Deflater: IntelDeflater; 09:14:13.567 INFO PrintReadsSpark - Inflater: IntelInflater; 09:14:13.567 INFO PrintReadsSpark - Initializing engine; 09:14:13.567 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system proper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:2947,variab,variable,2947,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['variab'],['variable']
Modifiability,"lse; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initializ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5049,Config,ConfigFactory,5049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"lse; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.CUSTOM_READER_FACTORY :; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3400,Config,ConfigFactory,3400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"lts.NON_ZERO_BUFFER_SIZE : 131072; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4327,Config,ConfigFactory,4327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"lts.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4043,Config,ConfigFactory,4043,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"ly braces can wait for a separate pass (we will want to do those in this PR though). If you're not sure what to include or not just ask. I like the idea of keeping the GATK3 tests working as we go along. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be delete",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1604,plugin,plugins,1604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,2,['plugin'],['plugins']
Modifiability,"ly. I am busy with graduation in recent days, and will work on visa application for the further postdoc position at Harvard Medical School. Thank you for the interests in our tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an opt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1033,enhance,enhancement,1033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349,1,['enhance'],['enhancement']
Modifiability,"ly. ```; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.FileAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.FileAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""file"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by ; log4j:ERROR [sun.misc.Launcher$AppClassLoader@7506e922] whereas object of type ; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@28c4711c].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```. By backtracking, the problem goes away at commit d827adc81266c788482c9cb4f119f2e3c1e152b8. Since spark-submmit was broken after 8af8bcc920ee5f393562e3e632d9ccd4acd9a638, the bug could be anywhere between commit 8af8bcc920ee5f393562e3e632d9ccd4acd9a638 and d25894b3bc80e450210cf8a9124c4171e65f3717. The log4j.property file is below:; ```; # Set ever",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2734:1121,variab,variable,1121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2734,1,['variab'],['variable']
Modifiability,"make a flexible coverage tool (DepthOfCoverage, DiagnoseTargets, ...)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/19:7,flexible,flexible,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/19,1,['flexible'],['flexible']
Modifiability,"makeBefore/makeAfter seems more flexible in that it makes before/after a property of the way the transformer is applied, rather than of the transformer itself. When we write the plugin descriptor, it can maintain two separate argument lists (for ""--preFilterTransformer ..."" and ""--postFilterTransformer ...""), and then merge them accordingly. It does complicate the tool structure, but its the price for flexibility, and the pattern is not that complex. I think we should be explicit about what ""pre"" and ""post"" are relative to in the method and argument names, i.e., makePreReadFilterTransformer and makePostReadFilterTransformer, or maybe even one makeReadFilterTransformer method that takes a pre/post argument.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246000255:32,flexible,flexible,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2085#issuecomment-246000255,2,"['flexible', 'plugin']","['flexible', 'plugin']"
Modifiability,making the version number depend on the git hash using a gradle git plugin from https://github.com/ajoberstar/gradle-git. It seems like the top gradle-git integration library. There are lots of pre-baked things in it to help with releases and such that we can grow into.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/196:68,plugin,plugin,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/196,1,['plugin'],['plugin']
Modifiability,"mark duplicates in dataflow - based on the code by garrickevans . The main work is done in; `private static final class MarkDuplicatesDataflowTransform extends PTransform<PCollection<Read>, PCollection<Read>>` - the sigrature conforms to the main read processing pipeline. Limitations:; - no optical duplicates; - only integration tests (would be good to have unit tests that check dup detection logic on very specific reads - ideally those from picard's tests). @droazen please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/541:152,extend,extends,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/541,1,['extend'],['extends']
Modifiability,"mbler.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.3891049Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3891593Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3892257Z symbol: class RangeMap; 2022-08-16T00:09:07.3892601Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3893126Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3893670Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3894352Z symbol: class Range; 2022-08-16T00:09:07.3894678Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3897711Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3902203Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3902980Z symbol: class RangeMap; 2022-08-16T00:09:07.3903340Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3903864Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3904505Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:6286,extend,extends,6286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,"mbler.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.7690890Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7738985Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7739852Z symbol: class RangeMap; 2022-08-16T22:45:53.7740332Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7740892Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7741707Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7743523Z symbol: class Range; 2022-08-16T22:45:53.7743866Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7747579Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7748444Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7776218Z symbol: class RangeMap; 2022-08-16T22:45:53.7776715Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7777389Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7778220Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:8324,extend,extends,8324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['extend'],['extends']
Modifiability,mbly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15849,Extend,Extended,15849,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,"md5/%s; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.REFERENCE_FASTA : null; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:3697,Config,ConfigFactory,3697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"mentations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been through many revisions from @meganshand, @jsotobroad, and @LeeTL1220, so hopefully there shouldn't be too much for you to find serious fault with. Note that I punted on adding MultidimensionalKernelSegmenterUnitTest and ModelSegmentsIntegrationTest. Filed #3916. Closes #2858. (FINALLY!); Closes #3825.; Closes #3661.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1717,refactor,refactor,1717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['refactor'],['refactor']
Modifiability,"moving settings from SparkCommandLineProgram to SparkContextFactory; the settings that were being applied to SparkContext.getConf were pointless since getConf is a copy of the configuration; instead they're applied to the SparkConfig in SparkContextFactory.getSparkContext. fixes #1096. <!-- Reviewable:start -->. [<img src=""https://reviewable.io/review_button.png"" height=40 alt=""Review on Reviewable""/>](https://reviewable.io/reviews/broadinstitute/gatk/1097). <!-- Reviewable:end -->",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1097:176,config,configuration,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1097,1,['config'],['configuration']
Modifiability,mpl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518); at org.apache.logging.log4j.core.config.DefaultConfiguration.<init>(DefaultConfiguration.java:49); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3346,Plugin,PluginRegistry,3346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['Plugin'],['PluginRegistry']
Modifiability,"ms that we will want to run the filter with more stringent parameters, as higher base error rates are causing homs to leak past the filter, which in turn affects the fit of the allele-fraction model (which only attempts to model hets) by biasing normal segments towards unbalanced, and 2) we now want to run ModelSegments separately on the normal to allow for the filtering of germline events. So we want to be more stringent with low-coverage normals without affecting our high-coverage tumors. For example, here's some hg38 NovaSeq FFPE WGS data from a ~40x normal:. ![download](https://user-images.githubusercontent.com/11076296/43977946-9bd0a1bc-9cb3-11e8-9d7f-016a99c1c173.png). Compare to an hg19 TCGA WGS ~40x normal:. ![download 1](https://user-images.githubusercontent.com/11076296/43978051-f8820770-9cb3-11e8-8e16-13b51792614f.png). The hom-ref tail in the first plot is much fatter and clearly leaks into the het cloud. Also curious is that the het cloud is far less binomial (or even beta-binomial---note also the absence of the tail extending to the origin). I am still not sure why the incoming data looks different. There are several confounding factors: NovaSeq vs. HiSeq, hg38 vs. hg19, AF > 2% gnomAD sites vs. AF > 10% 1000G sites, FFPE vs. frozen, etc. I have not seen enough examples/combinations to be able to say which are the most important factors. Changing the genotyping/filtering strategy can get around this change in the data without a corresponding change in the allele-fraction model for now, but getting the data to look as good as possible upstream would be even better. Another thought: would be nice if the strategy was easily compatible with an eventual implementation of multi-sample segmentation, which would require that the same sites are used in both the tumor and the normal. We would want to strike a balance between maximizing the number of sites and including questionable sites from the normal. Will add more details later. @davidbenjamin @LeeTL1220 @eit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218:1597,extend,extending,1597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-412189218,2,['extend'],['extending']
Modifiability,"n Tools; #; # Only update this environment if there is a *VERY* good reason to do so!; # If the build is broken but could be fixed by doing something else, then do that thing instead.; # Ensuring the correct environment for canonical (or otherwise reasonable) usage of our standard Docker takes precedence over edge cases.; # If you break the environment, you are responsible for fixing it and also owe the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely nec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1270,flexible,flexible,1270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['flexible'],['flexible']
Modifiability,"n occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 168, in fetch_val_for_key; return theano_cfg.get(section, option); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 781, in get; d = self._unify_values(section, vars); File ""/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx512/Core/python/3.6.10/lib/python3.6/configparser.py"", line 1141, in _unify_values; raise NoSectionError(section); configparser.NoSectionError: No section: 'blas'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 328, in __get__; delete_key=delete_key); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 172, in fetch_val_for_key; raise KeyError(key); KeyError: 'blas.ldflags'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1256, in check_mkl_openmp; import mkl; ModuleNotFoundError: No module named 'mkl'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/py",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:1793,config,configparser,1793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"n3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6227,config,configparser,6227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['configparser']
Modifiability,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:4653,variab,variable,4653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,2,"['Refactor', 'variab']","['Refactored', 'variable']"
Modifiability,"ncher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:170); 	at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:219); 	at java.util.ArrayList.forEach(ArrayList.java:1249); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:171); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:781); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```. It does not matter if I produce the pre-adapter metrics with the latest Picard jar v2.9.2. I get the same error. . I'm using a M2 callset from GATK3. Even so, I don't think I should get the above error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:3915,adapt,adapter,3915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,1,['adapt'],['adapter']
Modifiability,ncotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO Fe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8858,config,config,8858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['config'],['config']
Modifiability,"nd other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.3. # core R dependencies; these should only be used for plotting and do not take precedence over core python dependencies!; - r-base=3.6.2; - r-data.table=1.12.8; - r-dplyr=0.8.5; - r-getopt=1.20.3; - r-ggplot2=3.3.0; - r-gplots=3.0.3; - r-gsalib=2.1; - r-optparse=1.6.4. # other python dependencies; these should be removed after functionality is moved into Java code; - biopython=1.76; - pyvcf=0.6.8; - bioconda::pysam=0.15.3 # using older conda-installed versions may result in libcrypto / openssl bugs. # pip installs should be avoided, as pip may not respect the dependencies found by the conda solver; - pip:; - gatkPythonPackageArchive.zip; ```. It seems to successfully create the environment. I'd still recommend updating the information on your README.md and the file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:3013,variab,variable,3013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['variab'],['variable']
Modifiability,"nd the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge::keras=2.2.4 # updated from pip-installed 2.2.0, which caused various conflicts/clobbers of conda-installed packages; # conda-installed 2.2.4 appears to be the most recent version with a consistent API and without conflicts/clobbers; # if you wish to update, note that versions of conda-forge::keras after 2.2.5; # undesirably set the environment variable KERAS_BACKEND = theano by default; - defaults::intel-openmp=2019.4; - conda-forge::scikit-learn=0.22.2; - conda-forge::matplotlib=3.2.1; - conda-forge::pandas=1.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:2181,config,config,2181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['config'],['config']
Modifiability,"nd, so it cannot localize gs; paths. In other words, the WDL tests in travis need a local instance of; the file and to use that path in the json. On Wed, Apr 10, 2019 at 3:49 PM Jonn Smith <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220>; >; > This seems to be running into a cromwell / WDL error:; >; > java.lang.IllegalArgumentException: Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); > LinuxFileSystem: Cannot build a local path from gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; > 	Could not build the path ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: HTTP: gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt does not have an http or https scheme (IllegalArgumentException); >; > Isn't cromwell supposed to handle gs:// URLs for localizing files? Do you; > have any thoughts?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481836556>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7Wd-RgMx2g-UPLNrvjettNMf9ixks5vfkA3gaJpZM4clLLK>; > .; >. -- ; Lee Lichtenstein; Br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481853426:1069,config,configure,1069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5872#issuecomment-481853426,1,['config'],['configure']
Modifiability,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4845,adapt,adaptive-pruning,4845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['adapt'],"['adaptive-pruning', 'adaptive-pruning-initial-error-rate']"
Modifiability,nection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:158); 	at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489); 	at com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:206); 	at com.google.cloud.hadoop.util.CredentialConfiguration.getCredential(CredentialConfiguration.java:70); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1825); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1012); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:975); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2653); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:92); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2687); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2669); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:371); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500); 	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469); 	at org.apache.spark.SparkContext$$anonfun$newAPIHadoopFile$2.apply(SparkContext.scala:1084); 	at org.apache.spark.SparkContext$$anonfun$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369:7114,config,configure,7114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369,1,['config'],['configure']
Modifiability,"nfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5224,Config,ConfigFactory,5224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"nfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:38:14.590 DEBUG GenomeLocParser - Prepared reference sequence cont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5290,Config,ConfigFactory,5290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:4863,config,config,4863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['config'],['config']
Modifiability,"ng join() or detach(). This occurs when running JointGenotyping on 345 gvcfs created by GATK4 ExomeGermlineSingleSample; the workflow is running on an HPC cluster in Singularity (single node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80). Â The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used:Â v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -oÂ  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1550,config,config,1550,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['config'],['config']
Modifiability,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:6928,config,configuration,6928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['config'],['configuration']
Modifiability,"ngest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics fo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:1975,refactor,refactored,1975,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactored']
Modifiability,"nshot 2018-01-17 13 22 27"" src=""https://user-images.githubusercontent.com/11543866/35060379-501cb8c8-fb8c-11e7-845e-a146fc2ced94.png"">. ## Major wants; - The is_bamout Boolian appears to be hardcoded to `false` in the script. Users need to be able to understand that this option can be changed without ambiguity. So this should become a proper optional variable. @LeeTL1220 tells me this can be overwritten. However, why leave this as misinterpretable to newbie WDL-scriptors? Especially since `wdltools inputs` doesn't include it as a variable at all in the generated inputs list. Please can we make this a proper optional argument that `wdltools inputs` will generate a variable for.; - [ ""${variants_for_contamination}"" == *.vcf ] does not allow *.vcf.gz files. It should accept either.; - Outputs should allow either .vcf or .vcf.gz compression by user-specification. Alternatively, if we want to keep it simple and hardcode, then the preference is for compressed files. Some of us prefer to save on storage.; - Need to be able to specify optional string args for SplitIntervals. I would like to be able to use the BALANCING_WITHOUT_INTERVAL_SUBDIVISION mode. Furthermore, I'd like for the tool to automatically interpret this mode, when not given an -L intervals list, to not split reference contigs. I.e. a contig is an interval. (Perhaps already the tool behavior?); - The version of Oncotator is not compatible with GRCh38. Please, can we have an option to switch this out with Funcotator? . ## Minor wants; - The JSON template in the repo should show the optional variables.; - Script calls for a Picard jar. I don't mind specifying this because I like controlling for the Picard version I use. However, users may want to call the Picard version within the GATK jar. I cannot fathom a simple way to allow switching this out in the script, but perhaps something like the gatk_override option could work. The goal would be to call the Picard tool from a Docker. This better enables provenance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4188:1747,variab,variables,1747,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4188,1,['variab'],['variables']
Modifiability,nternal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2837,config,configuration,2837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['config'],['configuration']
Modifiability,"nternally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum valu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1221,variab,variables,1221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['variab'],['variables']
Modifiability,"ntervals of two jobs, and whether separating the jobs would impact calls. In this example, GenotypeGVCFs would run over 1:1050-1150. For example, if we had a multi-NT variant that spanned 1148-1052, we'd want that called correctly no matter what intervals were used for the jobs. I tried using running GenomicsDBImport with -L over a small region, or I ran SelectVariants on the gVCF first (which behaves a little differently), and then used that subset gVCF as input to GenomicsDBImport, where GenomicsDBImport is given the entire contig as the interval. The resulting workspaces will be slightly different, with the latter containing information over a wider region (GenomicsDBIport truncates start/end of the input records to just the target interval). . So if either of these workspaces is passed to GenotypeGVCFs, using --only-output-calls-starting-in-intervals and -L 1:1050-1150:. I think any upstream padding doesnt matter. If you have a multi-nucleotide polymorphism that starts upstream of 1050 but spans 1050, this job wouldnt be responsible for calling that. The prior job, which has an interval set upstream of this one should call it. I think GenomicsDbImport's behavior is fine here. If you have a multi-NT variant that starts within 1050-1150, but extends outside (i.e. deletion or insertion starting at 1148), this could be a problem. The GenomicsDB workspace created with the interval 1:1050-1150 lacks the information to score that, right? The workspace created using the more permissive SelectVariants->GenomicsDBImport contains that downstream information and presumably would make the same call as if GenotypeGVCFs was given the intact chromosome as input, right?. However, it seems that if I simply create the workspace with a reasonably padded interval (adding 1kb should be more than enough for Illumina, right?), and then run GenotypeGVCFs with the original, unpassed interval, then the resulting workspace should contain all available information and GenotypeGVCFs should be",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244:1317,polymorphi,polymorphism,1317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1221558244,2,['polymorphi'],['polymorphism']
Modifiability,"nvs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/confith.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.10--generic-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:240); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782:6496,config,configdefaults,6496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782,1,['config'],['configdefaults']
Modifiability,"o be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic workflows that call down to the lowest allele fraction variants. It seems this would simply be a matter of a tool or feature that replaces adapter sequence marked with the XT tag with base qualities of 2 and special handling by our callers of sequence with base quality of two.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:1252,adapt,adapter,1252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,5,['adapt'],['adapter']
Modifiability,"o go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2180,extend,extending,2180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,2,['extend'],['extending']
Modifiability,"o the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1548,inherit,inherit,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,2,['inherit'],['inherit']
Modifiability,"ocker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; de",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:1101,config,configurations,1101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['config'],['configurations']
Modifiability,"ode:. -`BaseRecalibratorSpark` is the standalone BQSR tool, and calls into the `BaseRecalibratorSparkFn` (which is also called from `ReadsPipelineSpark`). -`ApplyBQSRSpark` is the standalone ApplyBQSR tool, and calls into the `ApplyBQSRSparkFn` (also called from `ReadsPipelineSpark`). -Integration tests for the above are in `BaseRecalibratorSparkIntegrationTest` and `ApplyBQSRSparkIntegrationTest`. -Almost all other changes in the branch are related to the BQSR engine refactoring, which I summarize below:; - We pulled out the guts of the walker `BaseRecalibrator` tool, combined it with all of the code from the former `RecalibrationEngine` class (now deleted) to make a new `BaseRecalibrationEngine` class under `utils/recalibration`.; - We stripped out all copies of the code in `BaseRecalibrationEngine` from the walker, dataflow, and spark versions of BQSR, and modified them to call into `BaseRecalibrationEngine`.; - We moved all auxiliary classes needed by the `BaseRecalibrationEngine` (eg., the covariates, etc.) into `utils/recalibration`.; - We refactored the argument collections. Now there is a single shared `RecalibrationArgumentCollection` that contains **only** the parameters for the `BaseRecalibrationEngine` itself, and this argument collection is exposed by all 3 versions of the tool. Input/output arguments have been removed from this argument collection and put into the individual implementations of BQSR, since they vary between the walker, dataflow, and spark versions of the tool. This eliminates awkward problems such as having both a `knownSites` argument AND a `BQSRKnownVariants` exposed at the same time, with only 1 of them usable for a given version of a tool. The dataflow-only `BaseRecalibrationArgumentCollection` has been deleted completely as no longer needed.; - We tweaked the names of some tool arguments to enforce consistency between the 3 versions of the tool as well as the rest of hellbender (eg., output arg for BQSR is now a more standard `-O`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073:1113,refactor,refactored,1113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/911#issuecomment-142340073,1,['refactor'],['refactored']
Modifiability,odecov.io/gh/broadinstitute/gatk/pull/3447?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [.../broadinstitute/hellbender/utils/LoggingUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9Mb2dnaW5nVXRpbHMuamF2YQ==) | `82.222% <Ã¸> (Ã¸)` | `11 <0> (Ã¸)` | :arrow_down: |; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <Ã¸> (Ã¸)` | `0 <0> (Ã¸)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `65.926% <Ã¸> (Ã¸)` | `35 <0> (Ã¸)` | :arrow_down: |; | [...ellbender/utils/config/CustomBooleanConverter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ3VzdG9tQm9vbGVhbkNvbnZlcnRlci5qYXZh) | `100% <100%> (Ã¸)` | `2 <2> (?)` | |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `60.104% <100%> (+0.418%)` | `50 <2> (+1)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `86.408% <100%> (+0.408%)` | `29 <0> (Ã¸)` | :arrow_down: |; | [...oadinstitute/hellbender/engine/FeatureManager.java](https://codecov.io/gh/broadinstitute/gatk/pull/3447/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3447#issuecomment-323474032:1818,config,config,1818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3447#issuecomment-323474032,1,['config'],['config']
Modifiability,odehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.processFile(ShadowCopyAction.groovy:151); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:66); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyActionDecorator.java:60); 	at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.processFile(CopyFileVisitorImpl.java:62); 	at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.visitFile(CopyFileVisitorImpl.java:46); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker$1.visitFile(Jdk7DirectoryWalker.java:86); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker$1.visitFile(Jdk7DirectoryWalker.java:59); 	at java.nio.file.Files.walkFileTree(Files.java:2670); 	at org.gradle.api.internal.file.collections.jdk7.Jdk7DirectoryWalker.walkDir(Jdk7DirectoryWalker.java:59); 	at org.gradle.api.internal.file.collections.DirectoryFileTree,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:2008,plugin,plugins,2008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"ok, got it. sorry, i missed 'install' in that command. my initial impression is that VariantQC will be able to adapt fine to VariantEvalEngine. I wrote VariantEvalEngine with this is mind, but it's good to formally test it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-759645374:111,adapt,adapt,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-759645374,1,['adapt'],['adapt']
Modifiability,"olders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the check against sample_name.txt at the tool level would just be a formality. I can foresee headaches with globbing and funky sample names. I'm not sure I understand your point about extending PostprocessGermlineCNVCalls to run on all samples. The point of that tool is to take results from all genomic shards for a single sample and stitch them together, right? Even if we extend this to run on a batch of multiple samples (which would just be moving the loop over samples at the WDL level to some lower level, i.e., Java or python), we still need to see all shards for those samples. Perhaps I'm misunderstanding---can you clarify?. @mwalker174 can we once and for all clearly document the issue with the transpose? Perhaps by pointing to specific WGS runs that have issues with call caching? I think being able to pinpoint the exact issue will help us identify the right solution---whether that be choosing an appropriate bundling scheme, taking advantage of #5781 to reduce the number of shards, batching during the postprocessing step, removing unnecessary outputs, etc. Recall that we'd like to be able to use the same WDL locally (when you have easy access to all GermlineCNVCaller results from all genomic shards) and in the cloud, with minimal duplication of output from bundling when running locally, if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:2371,extend,extend,2371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,2,['extend'],['extend']
Modifiability,"olean readHasStarted = false;; boolean addedHardClips = false;. while (!cigarStack.empty()) {; final CigarElement cigarElement = cigarStack.pop();. if (!readHasStarted &&; cigarElement.getOperator() != CigarOperator.DELETION &&; cigarElement.getOperator() != CigarOperator.SKIPPED_REGION &&; cigarElement.getOperator() != CigarOperator.HARD_CLIP) {; readHasStarted = true;; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.HARD_CLIP) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.DELETION) {; totalHardClip += cigarElement.getLength();; } else if (!readHasStarted && cigarElement.getOperator() == CigarOperator.SKIPPED_REGION) {; totalHardClip += cigarElement.getLength();; }. if (readHasStarted) {; if (i == 1) {; if (!addedHardClips) {; if (totalHardClip > 0) {; inverseCigarStack.push(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; inverseCigarStack.push(cigarElement);; } else {; if (!addedHardClips) {; if (totalHardClip > 0) {; cleanCigar.add(new CigarElement(totalHardClip, CigarOperator.HARD_CLIP));; }; addedHardClips = true;; }; cleanCigar.add(cigarElement);; }; }; }; // first pass (i=1) is from end to start of the cigar elements; if (i == 1) {; shiftFromEnd = shift;; cigarStack = inverseCigarStack;; }; // second pass (i=2) is from start to end with the end already cleaned; else {; shiftFromStart = shift;; }; }; }; `. Notice that the variable _shift_ is initialized, but never assigned to again for the duration of the loop. Thus _shiftFromStart_ and _shiftFromEnd_ are always set to zero upon completion of the loop. These values are used by the _applyHardClipBases_ function, which is called in a number of places to hard clip bases from a read, but because of this error, they will always be zeroed out. The function containing the error is in the file ""src/main/java/org/broadinstitute/hellbender/utils/clipping/ClippingOp.java"" line 523",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6130:1892,variab,variable,1892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6130,1,['variab'],['variable']
Modifiability,"omicsDBImport - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 16:16:36.289 INFO GenomicsDBImport - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.REFERENCE_FASTA : null; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:4561,Config,ConfigFactory,4561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); 23/11/16 12:09:10 INFO SparkContext: Invoking stop() from shutdown hook; 23/11/16 12:09:10 INFO SparkContext: SparkContext is stopping with exitCode 0.; 23/11/16 12:09:10 INFO SparkUI: Stopped Spark web UI at http://SRINIVASiNDRARAVI:4040; 23/11/16 12:09:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 23/11/16 12:09:10 INFO MemoryStore: MemoryStore cleared; 23/11/16 12:09:10 INFO BlockManager: BlockManager stopped; 23/11/16 12:09:10 INFO BlockManagerMaster: BlockManagerMaster stopped; 23/11/16 12:09:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 23/11/16 12:09:10 INFO SparkContext: Successfully stopped SparkContext; 23/11/16 12:09:10 INFO ShutdownHookManager: Shutdown ho,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:12992,adapt,adapted,12992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['adapt'],['adapted']
Modifiability,"on.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81); 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79); 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133); 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856); 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387); 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360); 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239); 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789); 	at StudentAws$.delayedEndpoint$StudentAws$1(StudentAws.scala:36); 	at StudentAws$delayedInit$body.apply(StudentAws.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at StudentAws$.main(StudentAws.scala:8); 	at StudentAws.main(StudentAws.scala); Exception in thread ""main"" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z; 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method); 	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793); 	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249); 	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454); 	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972); 	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014); 	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761); 	at org.apache.hadoop.fs.FileSystem.listS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8587:7349,adapt,adapted,7349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8587,1,['adapt'],['adapted']
Modifiability,"on: class GVCFBlockCombiner; 2022-08-16T00:09:07.3893126Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T00:09:07.3893670Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T00:09:07.3894352Z symbol: class Range; 2022-08-16T00:09:07.3894678Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3897711Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3902203Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3902980Z symbol: class RangeMap; 2022-08-16T00:09:07.3903340Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3903864Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T00:09:07.3904505Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T00:09:07.3905250Z symbol: class Range; 2022-08-16T00:09:07.3905751Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3906273Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T00:09:07.3906908Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T00:09:07.3907793Z symbol: class Range; 2022-08-16T00:09:07.3908125Z location: class GVCFBlockCombiner; 2022-08-16T00:09:07.3910592Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3914013Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.3921838Z src/main/java/org/broadinstitute/hellbender/tools/walkers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:6678,extend,extends,6678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['extend'],['extends']
Modifiability,"on: class GVCFBlockCombiner; 2022-08-16T22:45:53.7740892Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:32: error: cannot find symbol; 2022-08-16T22:45:53.7741707Z final RangeMap<Integer, Range<Integer>> gqPartitions;; 2022-08-16T22:45:53.7743523Z symbol: class Range; 2022-08-16T22:45:53.7743866Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7747579Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7748444Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7776218Z symbol: class RangeMap; 2022-08-16T22:45:53.7776715Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7777389Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:62: error: cannot find symbol; 2022-08-16T22:45:53.7778220Z RangeMap<Integer,Range<Integer>> parsePartitions(final List<? extends Number> gqPartitions) ***; 2022-08-16T22:45:53.7779110Z symbol: class Range; 2022-08-16T22:45:53.7779574Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7780209Z src/main/java/org/broadinstitute/hellbender/utils/variant/writers/GVCFBlockCombiner.java:101: error: cannot find symbol; 2022-08-16T22:45:53.7780965Z static VCFHeaderLine rangeToVCFHeaderLine(Range<Integer> genotypeQualityBand) ***; 2022-08-16T22:45:53.7781896Z symbol: class Range; 2022-08-16T22:45:53.7782232Z location: class GVCFBlockCombiner; 2022-08-16T22:45:53.7785096Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPruner.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7789228Z src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/BaseVertex.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.7798240Z src/main/java/org/broadinstitute/hellbender/tools/walkers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:8716,extend,extends,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['extend'],['extends']
Modifiability,"onActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid \; -i \; \; --entrypoint /bin/bash \; -v /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:delegated \; broadinstitute/gatk@sha256:18146e79d06787483310e5de666502090a480e10ac0fad06a36a5e7a5c9bb1dc /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/script. # get the return code (working even if the container was detached); rc=$(docker wait cat /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid). # remove the container after waiting; docker rm cat /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid. # return exit code; exit $rc; [2020-07-14 05:09:45,29] [info] BackgroundConfigAsyncJobExecutionActor [968be8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:5347,config,configuration,5347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configuration']
Modifiability,once #3480 is in and #3447 is in @magicDGS requested that we should expose DEFAULT_FEATURE_CACHE_LOOKAHEAD as a configurable option,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3489:112,config,configurable,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3489,1,['config'],['configurable']
Modifiability,"onfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/23 20:41:38 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/23 20:41:39 INFO SecurityManager: Changing view acls to: zorzan; 18/04/23 20:41:39",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:6038,Config,ConfigFactory,6038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"ong. We should make a clear distinction between the old and new tests though. Ideally the GATK3 tests would be in a separate commit that we can just delete at the end, but that can get unwieldy if the files in the commit need to change as we go along. Alternatively you could isolate them into a separate directory. They should either be disabled or made dependent on a test method (see the `@Test` annotation properties `enabled` and `dependsOn`) that is easily toggled so they can be run locally, but don't run on the CI server. Otherwise the CI server build will always fail. In general, its really helpful to have the first commit in the PR contain the completely unmodified GATK3 source files. It makes it much easier for the reviewer to see what changed for the port. I noticed that you have 2 new plugins included in this. I'm not sure if that was suggested by someone on the GATK team (I'm wondering if we want to go down that path...) but I can tell you that the existing plugins required an enormous amount of test development and review iteration. If we do decide to make them plugins, I think it would be a good idea to do so in a separate PR. Also, if we choose to make an AbstractPlugin base class, we may want that to live in the Barclay repo. As @magicdgs points out, master already has your previous commits, so you should start by rebasing on that. Ideally, the branch would have the following commits before we start the first review cycle:. 1. A single commit containing the unmodified GATK3 source (unmodified with the exception that if a file is renamed for GATK4, its helpful to rename the GATK3 version in this commit so it's easy to compare in the next commit). This commit doesn't have to compile or run - its just to make the review process easier for us, and will be deleted at some point. I can help with how to get this into your branch if you like.; 2. Your modified GATK3 tests in a single commit. This will also be removed before merge.; 3. A single commit with all o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352:1781,plugin,plugins,1781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-407089352,2,['plugin'],['plugins']
Modifiability,onnection.java:966); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:176); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:270); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentialsUnsynchronized(DefaultCredentialsProvider.java:194); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:112); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:113); 	at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:86); 	at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:277); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:252); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:82); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:30); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:361); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```; I was able to fix the issue by setting the environment variable `NO_GCE_CHECK=true` in my shell though,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:6606,variab,variable,6606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['variab'],['variable']
Modifiability,"ool will have the exact same functionality as `CollectAllelicCounts`, to the point where I can re-use the integration tests. However, the integration tests fail. When I dig deeper in `CollectAllelicCountsSpark`, I see that only 8 RDDs (correct amount: 11) are being passed to processAlignments... Consider the following code:. ```; @Override; protected void processAlignments(JavaRDD<LocusWalkerContext> rdd, JavaSparkContext ctx) {; final String sampleName = SampleNameUtils.readSampleName(getHeaderForReads());; final SampleMetadata sampleMetadata = new SimpleSampleMetadata(sampleName);; final Broadcast<SampleMetadata> sampleMetadataBroadcast = ctx.broadcast(sampleMetadata);. final AllelicCountCollector finalAllelicCountCollector =; rdd.mapPartitions(distributedCount(sampleMetadataBroadcast.getValue(), minimumBaseQuality)); .reduce((a1, a2) -> combineAllelicCountCollectors(a1, a2, sampleMetadataBroadcast.getValue()));; final List<LocusWalkerContext> tmp = rdd.collect();; ....snip....; ```. In this case `tmp` will have a size of 8. However, the integration test would indicate a size of 11 is correct, since 11 intervals are being passed in. Note that `emitEmptyLoci()` returns `true`, so 11 is the correct number as seen in `CollectAllelicCountsSparkIntegrationTest` . . Additionally, in (at least) one result, the counts are wrong. `CollectAllelicCounts` (non-spark) passes the integration test. I have tried a couple of tests to gather more information:. - Is `emitEmptyLoci()` causing an issue? ; Does not appear to be causing the issue. I say this because when set to `false`, I get (essentially) the same error.; - The code uses `mapPartition` and not `map`, does this cause the issue? Why are you doing this?; This does not cause the issue. I refactored the code to use `map` and got the exact same issue. I use `mapPartition` in order to instantiate only one instance of `AllelicCountCollector` per partition, instead of per locus. Assigning to @tomwhite by request of @droazen ...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3823:1870,refactor,refactored,1870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3823,1,['refactor'],['refactored']
Modifiability,oppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:17128,plugin,plugin,17128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['plugin'],['plugin']
Modifiability,"or users (I downloaded Hadoop and built it from source, and then set gatk's java opts to load the native library). Two options might be: 1) distribute native libraries for supported architectures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:1396,variab,variables,1396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Fu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2199,config,config,2199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['config'],['config']
Modifiability,org\springframework\spring-web\5.2.6.RELEASE\spring-web-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-webmvc\5.2.6.RELEASE\spring-webmvc-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-aop\5.2.6.RELEASE\spring-aop-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-context\5.2.6.RELEASE\spring-context-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-expression\5.2.6.RELEASE\spring-expression-5.2.6.RELEASE.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-starter\2.1.2\mybatis-spring-boot-starter-2.1.2.jar;E:\repository\org\mybatis\spring\boot\mybatis-spring-boot-autoconfigure\2.1.2\mybatis-spring-boot-autoconfigure-2.1.2.jar;E:\repository\org\mybatis\mybatis\3.5.4\mybatis-3.5.4.jar;E:\repository\org\mybatis\mybatis-spring\2.0.4\mybatis-spring-2.0.4.jar;E:\repository\mysql\mysql-connector-java\8.0.20\mysql-connector-java-8.0.20.jar;E:\repository\org\springframework\boot\spring-boot-configuration-processor\2.3.0.RELEASE\spring-boot-configuration-processor-2.3.0.RELEASE.jar;E:\repository\org\springframework\spring-core\5.2.6.RELEASE\spring-core-5.2.6.RELEASE.jar;E:\repository\org\springframework\spring-jcl\5.2.6.RELEASE\spring-jcl-5.2.6.RELEASE.jar;E:\repository\com\google\firebase\firebase-admin\6.8.1\firebase-admin-6.8.1.jar;E:\repository\com\google\api-client\google-api-client\1.25.0\google-api-client-1.25.0.jar;E:\repository\com\google\oauth-client\google-oauth-client\1.25.0\google-oauth-client-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-jackson2\1.25.0\google-http-client-jackson2-1.25.0.jar;E:\repository\com\google\api-client\google-api-client-gson\1.25.0\google-api-client-gson-1.25.0.jar;E:\repository\com\google\http-client\google-http-client-gson\1.25.0\google-http-client-gson-1.25.0.jar;E:\repository\com\google\code\gson\gson\2.8.6\gson-2.8.6.jar;E:\repository\com\google\http-client\google-http-client\1.25.0\google-http-client-1.25.0.jar;E:\repository\com\google\code\findbugs\jsr305\3.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:5508,config,configuration-processor-,5508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration-processor-']
Modifiability,osmic_tissue.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/cosmic_tissue/hg19/cosmic_tissue.tsv; 15:41:49.599 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/dnaRepairGenes.20180524T145835.csv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dna_repair_genes/hg19/dnaRepairGenes.20180524T145835.csv; 15:41:49.600 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 15:41:49.604 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 15:41:49.604 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 15:41:49.659 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 15:41:49.659 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2020-08-19 15:41:49 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:49.663 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hgnc_download_Nov302017.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 15:41:49.851 INFO DataSourceUtils - Resol,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:12207,config,config,12207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['config'],['config']
Modifiability,"otator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:55:32.063 INFO Funcotator - Deflater: IntelDeflater; 02:55:32.063 INFO Funcotator - Inflater: IntelInflater; 02:55:32.063 INFO Funcotator - GCS max retries/reopens: 20; 02:55:32.063 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 02:55:32.063 WARN Funcotator - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 02:55:32.063 INFO Funcotator - Initializing engine; 02:55:32.318 INFO FeatureManager - Using codec VCFCodec to read file file:///export2/liuhw/wes_test/Mutect2_filter/K001137N_somatic_filtered.vcf.gz; 02:55:32.459 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 02:55:32.466 INFO Funcotator - Shutting down engine; [July 12, 2024 2:55:32 AM EDT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2148532224; ***********************************************************************. A USER ERROR has occurred: Bad input: ERROR in config file: file:///./software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/gnomAD_exome/hg38/gnomAD_exome.config - src_file does not exist: /./software/gatk_Funcotator/funcotator_dataSources.v1.8.hg38.20230908s/gnomAD_exome/hg38/gs:/broad-public-datasets/funcotator/gnomAD_2.1_VCF_INFO_AF_Only/hg38/gnomad.exomes.r2.1.sites.liftoverToHg38.INFO_ANNOTATIONS_FIXED.vcf.gz. ***********************************************************************; ```; How to solved it?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8913:3663,config,config,3663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8913,2,['config'],['config']
Modifiability,"ould like to address are similar to yours, with some inclussions. * Regarding NIO support, I would go to remove completely `File` support. If API users need to use the `File` abstraction, they should convert to a `java.nio.Path` using the `toPath` method.; * In addition, I would like that HTTP/S and FTP is handled also with NIO. For HTTP/S, I am working in a simple `FileSystemProvider` that should be good enough for using in combination with HTSJDK ([jsr203-http](https://github.com/magicDGS/jsr203-http)), and I can speed up the development there for needs in HTSJDK; for FTP, maybe [ftp-fs](https://github.com/robtimus/ftp-fs) can be used or a simple implementation can be derived from the HTTP/S implementation (without credentials). This will remove the special handling of HTTP/S and FTP paths in HTSJDK in favor of a consistent and pluggable manner.; * Interfaces for the data types are great, and maybe it will be good to have codec interfaces for both encoding and decoding. For example, I am missing encoders in tribble (an attempt in https://github.com/samtools/htsjdk/pull/822 for writing support).; * For VCF, I would like to have a less diploid-centric interface and design, or at least a way of configure the catching of genotype-related attributes. Currently there are methods for homozygotes/heterozygotes that aren't really useful for triploids or even VCFs without variation (for example, in Pool-Seq data).; * Modular design for artifacts: thus, a project with only SAM/BAM requirements will require only `htsjdk-sam`, and if they also want CRAM support, `htsjdk-cram`. See https://github.com/samtools/htsjdk/issues/896 for more info about it.; * Common license for all HTSJDK, or at least for each module. This will be good for taking into account legal concerns when including the library, because now there is a mixture depending on the files that are used. This is what is coming to my mind now. Maybe I added something else in https://github.com/samtools/htsjdk/issues/520",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940:1281,config,configure,1281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4340#issuecomment-363390940,2,['config'],['configure']
Modifiability,"ouldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, especially given the simplicity of writing a new walker type in GATK4. For all of these reasons we don't want `AssemblyRegionWalker` to inherit from a more abstract traversal type -- it's just going to be its own standalone thing, so that it can evolve freely without affecting anyone else. For `SlidingWindowWalker`, which we still want to merge, I recommend making the traversal do **exactly** what you want for your use case, as clearly and simply as possible, without worrying about serving as a base class for other traversals. Ping me once you're happy with it, and I'll re-review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:2073,inherit,inheritance,2073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,4,"['evolve', 'inherit']","['evolve', 'inherit', 'inheritance', 'inheriting']"
Modifiability,"output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2354,config,configuration,2354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['config'],['configuration']
Modifiability,"ozygous genotypes failing two-tailed binomial test (example below); q .. select genotypes using -i/-e options; and the new genotype can be one of:; . .. missing (""."" or ""./."", keeps ploidy); 0 .. reference allele (e.g. 0/0 or 0, keeps ploidy); c:GT .. custom genotype (e.g. 0/0, 0, 0/1, m/M, overrides ploidy); m .. minor (the second most common) allele (e.g. 1/1 or 1, keeps ploidy); M .. major allele (e.g. 1/1 or 1, keeps ploidy); p .. phase genotype (0/1 becomes 0|1); u .. unphase genotype and sort by allele (1|0 becomes 0/1); Usage: bcftools +setGT [General Options] -- [Plugin Options]; Options:; run ""bcftools plugin"" for a list of common options. Plugin options:; -e, --exclude <expr> Exclude a genotype if true (requires -t q); -i, --include <expr> include a genotype if true (requires -t q); -n, --new-gt <type> Genotypes to set, see above; -t, --target-gt <type> Genotypes to change, see above. Example:; # set missing genotypes (""./."") to phased ref genotypes (""0|0""); bcftools +setGT in.vcf -- -t . -n 0p. # set missing genotypes with DP>0 and GQ>20 to ref genotypes (""0/0""); bcftools +setGT in.vcf -- -t q -n 0 -i 'GT=""."" && FMT/DP>0 && GQ>20'. # set partially missing genotypes to completely missing; bcftools +setGT in.vcf -- -t ./x -n . # set heterozygous genotypes to 0/0 if binom.test(nAlt,nRef+nAlt,0.5)<1e-3; bcftools +setGT in.vcf -- -t ""b:AD<1e-3"" -n 0. # force unphased heterozygous genotype if binom.test(nAlt,nRef+nAlt,0.5)>0.1; bcftools +setGT in.vcf -- -t ./x -n c:'m/M'; ```; I was always wondering if GATK will have a plugin interface where people can code their own using groovy, kotlin, javascript or python plugins to extend some of the functionality where developers may not reach immediately. Personally I use htsjdk extensively (and sometimes pysam) to code a new personal tool each time I need something that I cannot find exactly what I look for. But a generic gatk plugin interface would be really useful and may provide means to extend the community support.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501:1923,plugin,plugin,1923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1556119501,5,"['extend', 'plugin']","['extend', 'plugin', 'plugins']"
Modifiability,packaging gatk-launch in our jar so that it's available to downstream projects; some build.gradle refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1772:98,refactor,refactoring,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1772,1,['refactor'],['refactoring']
Modifiability,"parameterize TTL with defaults, reduce memory allocation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7244:0,parameteriz,parameterize,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7244,1,['parameteriz'],['parameterize']
Modifiability,"peGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 21:13:04.223 INFO GenotypeGVCFs - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; > 21:13:04.224 DEBUG ConfigFactory - Configuration file values:; > 21:13:04.230 DEBUG ConfigFactory - gcsMaxRetries = 20; > 21:13:04.230 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Ini",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4067,Config,ConfigFactory,4067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"piped beta variables through to high-level beta workflow.; Also updated the gatk jar so it succeeds, as it didn't before. [Successful run of beta workflow on quickstart data](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/e9a1af96-8c1a-463a-8063-ae455d0ba6b3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8200:11,variab,variables,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8200,1,['variab'],['variables']
Modifiability,plBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7583:1949,Enhance,EnhancedBigQueryReadStub,1949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583,1,['Enhance'],['EnhancedBigQueryReadStub']
Modifiability,"practice?; - The error message when there is no supporting code does not tell what the problem is; whether the extension of the file (due to the the 1-to-1 name to type quasi-restriction above) or a more complex formatting issue in the file (e.g. required header missing, version not supported ... blah blah). ; - All codecs are tried out even when most won't ever apply. Even if the performance impact should in practice be minimal still may cause several file IO open operations as several Codec do actually peek into the file (e.g. BCF and VCF codecs). ; - Codec developers have to make sure their new codec does not collides with others; it would be better if codec development can be totally independent.; - General file extensions such as .tab , .tsv cannot be used by codecs due to possible collisions constraining users to name their files the way GATK needs them to; ""I don't like people telling what file names a have to use... I'm already placing the correct argument name before the file name. What else you need!"". Proposal:. An annotation to tell what codes to try out, the first one that canDecode returns true is used otherwise a configurable error message saying what the problem could be:. <pre>; @Codecs(BEDCodec.class); FeatureInput&lt;BEDFeature&gt; features;; </pre>. <pre>; @Codecs(value = BEDCodec.class, failureMessage = ""The file provided must be a BED formatted file with extension .bed""); FeatureInput&lt;BEDFeature&gt; features;; </pre> . <pre>; @Codecs(BCFCodec.class, VCFCodec.class); FeatureInput&lt;VariantContext&gt; variants;; </pre>. <pre>; // force = true, means that canDecode won't be called and instead we try to read the content directly,; // the codec's code is responsible to throw an appropriate UserException.BadInput indicating formatting issues; this should be the case already anyway.; @Codecs(value = TargetCodec.class, force = true); FeatureInput&lt;Target&gt; target;; </pre>. If the annotation is not present it can default to the current behavior.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184:2195,config,configurable,2195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184,1,['config'],['configurable']
Modifiability,"previously, tools that align reads required you to manually disable sequence dictionary validation; if you didn't, they would fail because the unaligned bam didn't have the required sequence dictionary. extracting out a SequenceDictionaryValidationArgumentCollection and providing a method for GATKSparkTools to configure it; ReadsPipeline couldn't easily make use of this, so instead it overrides the method that does validation. BwaSpark / BwaAndMarkDuplicatesPipelineSpark now do not require or allow dictionary validation; fixes #4131",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4308:312,config,configure,312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4308,1,['config'],['configure']
Modifiability,"provements that were discovered while reviewing the variants ; (https://github.com/SHuang-Broad/GATK-SV-callset-regressionTest/tree/master/Evaluation/Analysis/masterVSfeature/notes.xlsx); The implemented fixes are:; * for removing the hard-coded/explicit mentioning of ""chr"" in non-canonical versions, it is now fixed in 5eff782e4d582d516004fba2cee7535d984b1540; * for contigs whose alignments paint ambiguous picture, i.e. multiple alignment configurations offer equally good explanation:; 	1. if only one configuration has all alignment with MQ above a specified threshold, it is favored; this is implemented in ecc31f5fbec4e524b401fc9474a3a1b7ab08c561; 	2. if one configuration has alignment to non-canonical chromosome that explains the contig better than would-be-event-inducing mappings to canonical chromosomes, the canonical mappings are saved but the better non-canonical mappings are saved as SA tag as in SAM spec, and the VCF record produced is annotated accordingly; this is implemented in 65cdb523a2f9fa2026334713fed45381d76ffc82; * fixed a bug where sometimes an assembly contig as several alignments, only one of which has non-mediocre MQ but at the sametime this alignment contains a large gap, such contigs were previously incorrectly filtered away, they are now salvaged by commit b6b2f197b112981e00efd9d415f010c024d31b36. So, for the FN variants (FN in the sense that they are captured in the stable version of our interpretation tool but now goes missing in the experimental interpretation tool); that were curated in the above-mentioned review, only the following ones are not salvaged, with plans or comments attached. ```; asm012854:tig00000	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem as face by group represented by asm002398:tig00001); asm014580:tig00018	missing	classified as ""incomplete""; fixable by finishing the last TODO in AssemblyContigAlignmentSignatureClassifier (same problem ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522:863,config,configuration,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4326#issuecomment-370923522,2,['config'],['configuration']
Modifiability,"put); * positive (training with *.annot.hdf5) vs. positive-unlabeled (training with *.annot.hdf5 and *.unlabeled.annot.hdf5); * Java Bayesian Gaussian Mixture Model (BGMM) backend vs. python sklearn IsolationForest backend; (BGMM tests to be added once PR for the backend goes in.); - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs.; - [x] Parameter/mode validation.; - [x] Refactor main code block for model training; it's a bit monolithic and procedural now.; - [x] Decide on behavior for ill-behaved annotations. E.g., all missing, zero variance. Future work:. - [ ] We could allow subsetting of annotations here, which might allow for easier treatment of ill-behaved annotations. However, I'd say enabling workflows where the set of annotations is fixed is the priority.; - [ ] We could do positive-unlabeled training more rigorously or iteratively. Right now, we essentially do a single iteration to determine negative data. This could perhaps be preceded by a round of refactoring to clean up model training and make it less procedural.; - [ ] Automatic threshold tuning could be built into the tool, see #7711. We'd probably have to introduce a ""validation"" label. Perhaps it makes sense to keep this sort of thing at the workflow level?; - [ ] In the positive-negative framework enforced by the Java code in this tool, a ""model"" is anything that assigns a score, we fit two models to different subsets of the data, and then take the difference of the two scores. While the python backend does give some freedom to specify a model, future developers may want to go beyond the framework itself. For example, more traditional classification frameworks, etc. could be explored. As an intermediate step, one could perhaps use the positive/negative scores from the current framework in a more sophisticated way (e.g., using them as features), rather than just taking their difference. This sort of future work could be developed completely independently of the codebase associated wit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369:1411,refactor,refactoring,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948369,1,['refactor'],['refactoring']
Modifiability,"r example?. ```; * <h3>Output</h3>; * <p>; * A combined VCF with combined calls for each pair of samples specified and de-uniquified sample names.; * </p>; *; * <h3>Examples</h3>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * -o output.vcf; * </pre>; * <pre>; * java -jar GenomeAnalysisTK.jar \; * -R ref.fasta \; * -T CombineSampleData \; * --variant vcf1.vcf \; * --uniquified_sample_name NA12878.variant \; * --uniquified_sample_name NA12878.variant2; * -o output.vcf; * </pre>; ```. I don't get what's the difference between the first and second example. . In any case I'm not going to push this through now in light of all the TODOs:. ```; /*TODO: when this tool is moved into protected the following will have to be addressed:; * Do more robust error checking on sample name de-uniquification -- right now checks for pairs of <sampleName>.variantX and <sampleName>.variantY but should be extended to allow tagged VCF input into GenotypeGVCFs, which will produce names like <sampleName>.RODtagName; * Move sample name uniqufication/de-uniquification to SampleListUtils.java; * Check to make sure all genotype attributes are preserved after merge, e.g. allele phasing and genotype filters; * Generalize for all ploidies?; * Change GenotypeGVCFs --uniquifySamples argument from hidden (maybe still keep @advanced?); *; */; ```. ---. @ldgauthier commented on [Mon Nov 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-159059807). 1) Could be any two sets of data, I just did WGS + WEx for GTEx; 2) I think the first example probably should have two -V entries (that have the same sample names) compared with the second that has one input -V that has already uniquified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2485:2013,extend,extended,2013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485,1,['extend'],['extended']
Modifiability,r.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:31687,adapt,adapted,31687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,r/ports/biology/gatk/work/gatk-4.0.11.0/build/libs/gatk-package-1.0-SNAPSHOT-local.jar'.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:83); 	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.processFile(ShadowCopyAction.groovy:151); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:66); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyAction,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:1341,plugin,plugins,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['plugin'],['plugins']
Modifiability,"r_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_advi_iters=5000 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkern",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:6703,variab,variable,6703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['variab'],['variable']
Modifiability,"race_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4943,Config,ConfigFactory,4943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"ram/md5/%s; 23:43:52.471 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:43:52.472 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:43:52.472 DEBUG ConfigFactory - Configuration file values: ; 23:43:52.474 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3155,Config,ConfigFactory,3155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"ranch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ceb06); ![image](https://github.com/broadinstitute/gatk/assets/11076296/48570e53-024c-44b5-8835-3fd40b4c5866); ![image](https://github.com/broadinstitute/gatk/assets/11076296/99100e5d-05e2-4a5c-9d68-57db1b734029); ![image](https://github.com/broadinstitute/gatk/assets/11076296/abae09e1-70a5-4213-95a2-0cb10f9db192); ![image](https://github.com/broadinstitute/gatk/a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1391,variab,variables,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['variab'],['variables']
Modifiability,rapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15070,Plugin,PluginResolutionServiceClient,15070,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['Plugin'],['PluginResolutionServiceClient']
Modifiability,rebase Dgs configure plugin packages,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5573:11,config,configure,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5573,2,"['config', 'plugin']","['configure', 'plugin']"
Modifiability,ree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...bender/tools/spark/pathseq/PathSeqFilterSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFGaWx0ZXJTcGFyay5qYXZh) | `70.968% <Ã¸> (Ã¸)` | `7 <0> (Ã¸)` | :arrow_down: |; | [...itute/hellbender/tools/spark/pathseq/PSFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyLmphdmE=) | `92.617% <100%> (+0.531%)` | `33 <1> (+1)` | :arrow_up: |; | [...ools/spark/pathseq/PSFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `80% <100%> (+1.429%)` | `2 <0> (Ã¸)` | :arrow_down: |; | [...ellbender/transformers/AdapterTrimTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvQWRhcHRlclRyaW1UcmFuc2Zvcm1lci5qYXZh) | `92.857% <92.857%> (Ã¸)` | `12 <12> (?)` | |; | [...nder/transformers/SimpleRepeatMaskTransformer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90cmFuc2Zvcm1lcnMvU2ltcGxlUmVwZWF0TWFza1RyYW5zZm9ybWVyLmphdmE=) | `94.286% <94.286%> (Ã¸)` | `11 <11> (?)` | |; | [...nstitute/hellbender/utils/clipping/ClippingOp.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbGlwcGluZy9DbGlwcGluZ09wLmphdmE=) | `84.365% <0%> (+1.629%)` | `91% <0%> (+2%)` | :arrow_up: |; | [...stitute/hellbender/utils/clipping/ReadClipper.java](https://codecov.io/gh/broadinstitute/gatk/pull/3354?src=pr&el=tree#diff-c3JjL21haW4vamF,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354#issuecomment-317586310:1867,Adapt,AdapterTrimTransformer,1867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354#issuecomment-317586310,1,['Adapt'],['AdapterTrimTransformer']
Modifiability,refactor of VQSR filters,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6508:0,refactor,refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6508,1,['refactor'],['refactor']
Modifiability,refactor print sv evidence tool to use a single output stream,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7045:0,refactor,refactor,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7045,1,['refactor'],['refactor']
Modifiability,refactored table writing in BasicSomaticValidator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6136:0,refactor,refactored,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6136,1,['refactor'],['refactored']
Modifiability,refactoring code to remove confusion and unnecessary call to calculateChromosomeCounts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431,1,['refactor'],['refactoring']
Modifiability,refactoring for testablity,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7946:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7946,1,['refactor'],['refactoring']
Modifiability,refactoring in CalculateGenotypePosteriors to remove confusion,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4431:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4431,1,['refactor'],['refactoring']
Modifiability,"refactoring our SmithWaterman code to prepare us for using native code optimized aligners. * Adding new interfaces `SmithWatermanAligner` and `SmithWatermanAlignment`.; * Refactoring `SWPairwiseAlignment` to be a `SmithWatermanAligner`, renaming it to SmithWatermanJavaAligner to distinguish it from future native aligners.; * Refactoring and renaming`SWPairwiseAlignmentUnitTest` and abstracting a superclass `SmithWatermanAlignerAbstractUnitTest` ; * Creating `SWNativeAlignerWrapper` which can accept a `SWAlignerNativeBinding` and wrap it into a `SmithWatermanAligner` as well as a test for it; * adding an option to `AssemblyBasedCallerArgumentCollection` which allows the aligner to be specified, currently we only have 1 real option; * adding an aligner as a field to Mutect2 and HaplotypeCaller, updating all library calls that use alignment to accept an aligner as an argument",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3600:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3600,3,"['Refactor', 'refactor']","['Refactoring', 'refactoring']"
Modifiability,refactoring some stuff for clarity,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8598:0,refactor,refactoring,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8598,1,['refactor'],['refactoring']
Modifiability,removed unnecessary inheritance of M2 filtering arguments collection,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5498:20,inherit,inheritance,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5498,1,['inherit'],['inheritance']
Modifiability,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:23047,parameteriz,parameterized,23047,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['parameteriz'],['parameterized']
Modifiability,rg.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initial,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3251,Config,ConfigureActionsProjectEvaluator,3251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Config'],['ConfigureActionsProjectEvaluator']
Modifiability,"rg\apache\commons\commons-pool2\2.8.0\commons-pool2-2.8.0.jar;C:\Program Files\JetBrains\IntelliJ IDEA 2020.1\lib\idea_rt.jar"" com.luz.push.PushApplication; Connected to the target VM, address: '127.0.0.1:62530', transport: 'socket'. . ____ _ __ _ _; /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \; ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \; \\/ ___)| |_)| | | | | || (_| | ) ) ) ); ' |____| .__|_| |_|_| |_\__, | / / / /; =========|_|==============|___/=/_/_/_/; :: Spring Boot :: (v2.3.0.RELEASE). 2020-05-29 15:14:30.695 INFO 12904 --- [ main] com.luz.push.PushApplication : Starting PushApplication on DESKTOP-05L3FQL with PID 12904 (C:\project\push\target\classes started by Sweet in C:\project\push); 2020-05-29 15:14:30.712 INFO 12904 --- [ main] com.luz.push.PushApplication : No active profile set, falling back to default profiles: default; 2020-05-29 15:14:32.088 WARN 12904 --- [ main] o.m.s.mapper.ClassPathMapperScanner : No MyBatis mapper was found in '[com.luz.push]' package. Please check your configuration.; 2020-05-29 15:14:32.662 INFO 12904 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8282 (http); 2020-05-29 15:14:32.675 INFO 12904 --- [ main] o.a.coyote.http11.Http11NioProtocol : Initializing ProtocolHandler [""http-nio-8282""]; 2020-05-29 15:14:32.676 INFO 12904 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]; 2020-05-29 15:14:32.677 INFO 12904 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.35]; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1944 ms; 2020-05-29 15:14:32.899 INFO 12904 --- [ main] com.luz.push.utils.GcmUtils : start init gcm server; 2020-05-29 15:14:33.029 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredenti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:11692,config,configuration,11692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['config'],['configuration']
Modifiability,"ribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 20g --executor-cores 4 --executor-memory 8g /gatk/gatk-package-4.0.4.0-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://926a0516ccf6:7077; 11:01:48.445 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:01:48.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.0.4.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:01:49.333 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:01:49.334 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@926a0516ccf6 on Linux v4.4.0-127-generic amd64; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 28, 2018 11:01:48 AM UTC; 11:01:49.335 INFO BwaAndMarkDuplicatesPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:14666,variab,variables,14666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"ries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4360,Config,ConfigFactory,4360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"ries = 20; 23:43:52.474 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 23:43:52.474 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - impo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:3884,Config,ConfigFactory,3884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"rite_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --driver-memory 15g --executor-cores 2 --executor-memory 8g /gatk/build/libs/gatk-spark.jar BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 4000000 --input hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_fastqtosam.bam --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img --disable-sequence-dictionary-validation true --output hdfs://namenode:8020/PREPROCESSING/PFC_0028_SW_CGTACG_R_dedup_reads.bam --spark-master spark://680776067ebd:7077; 16:33:44.918 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:33:45.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 16:33:45.587 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.2.0-4-gb59d863-SNAPSHOT; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@680776067ebd on Linux v4.4.0-127-generic amd64; 16:33:45.588 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 16:33:45.589 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 26, 2018 4:33:45 PM UTC; 16:33:45.589 INFO BwaAndMarkDupli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:10926,variab,variables,10926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"rmlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.928 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 08:48:45.928 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 08:48:45.928 DEBUG ConfigFactory - createOutputBamIndex = true; 08:48:45.928 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 08:48:45.928 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 08:48:45.928 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 08:48:45.928 INFO DetermineGer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4921,Config,ConfigFactory,4921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,"roadinstitute/hellbender/engine/filters/CountingVariantFilter.java:197: error: cannot find symbol; 2022-08-16T00:09:07.4040311Z @VisibleForTesting; 2022-08-16T00:09:07.4040921Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4041294Z location: class CountingVariantFilter; 2022-08-16T00:09:07.4054361Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptor.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4060164Z src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilter.java:75: error: cannot find symbol; 2022-08-16T00:09:07.4060614Z @VisibleForTesting; 2022-08-16T00:09:07.4061233Z symbol: class VisibleForTesting; 2022-08-16T00:09:07.4061591Z location: class ReadFilter; 2022-08-16T00:09:07.4083439Z src/main/java/org/broadinstitute/hellbender/utils/config/ConfigFactory.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4092135Z src/main/java/org/broadinstitute/hellbender/utils/config/GATKConfig.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4107682Z src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.4116317Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4117746Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T00:09:07.4124264Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:32: error: cannot find symbol; 2022-08-16T00:09:07.4124816Z private static BiMap<Log.LogLevel, Level> loggingLevelNamespaceMap;; 2022-08-16T00:09:07.4125855Z symbol: class BiMap; 2022-08-16T00:09:07.4126189Z location: class LoggingUtils; 2022-08-16T00:09:07.4126674Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:12376,config,config,12376,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['config'],['config']
Modifiability,"roadinstitute/hellbender/engine/filters/CountingVariantFilter.java:197: error: cannot find symbol; 2022-08-16T22:45:53.8024772Z @VisibleForTesting; 2022-08-16T22:45:53.8025036Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.8025212Z location: class CountingVariantFilter; 2022-08-16T22:45:53.8032154Z src/main/java/org/broadinstitute/hellbender/cmdline/GATKPlugin/GATKReadFilterPluginDescriptor.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8035089Z src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilter.java:75: error: cannot find symbol; 2022-08-16T22:45:53.8035234Z @VisibleForTesting; 2022-08-16T22:45:53.8035505Z symbol: class VisibleForTesting; 2022-08-16T22:45:53.8035658Z location: class ReadFilter; 2022-08-16T22:45:53.8087327Z src/main/java/org/broadinstitute/hellbender/utils/config/ConfigFactory.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8103864Z src/main/java/org/broadinstitute/hellbender/utils/config/GATKConfig.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8113680Z src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVariantContextUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T22:45:53.8117654Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:3: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8118430Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:4: error: package com.google.common.collect does not exist; 2022-08-16T22:45:53.8124030Z src/main/java/org/broadinstitute/hellbender/utils/LoggingUtils.java:32: error: cannot find symbol; 2022-08-16T22:45:53.8124383Z private static BiMap<Log.LogLevel, Level> loggingLevelNamespaceMap;; 2022-08-16T22:45:53.8124657Z symbol: class BiMap; 2022-08-16T22:45:53.8124810Z location: class LoggingUtils; 2022-08-16T22:45:53.8125227Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:14414,config,config,14414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,1,['config'],['config']
Modifiability,"roblem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details. BUILD FAILED in 33s; 5 actionable tasks: 5 executed; ```; which does not seem related to any changes I made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:1832,plugin,plugins,1832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['plugin'],['plugins']
Modifiability,"rogram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: cngb-nas-f17-1: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1631); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:924); 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1504); 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:843); 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1494); 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1626); 	... 21 more; ```. #### Steps to reproduce; On a Linux machine without _Hadoop_, run `java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5` locally. #### Expected behavior; Produce *out.pon.hdf5*. #### Actual behavior; Exit with error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:4270,config,config,4270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,2,['config'],['config']
Modifiability,"root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:2704,inherit,inherits,2704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['inherit'],['inherits']
Modifiability,"roportional to number of samples, number of intervals, number of bias covariates and max copy number. What the docs don't say is what the default is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second attempt, now running, scattered the genome into 150 shards, and is running 12 jobs at a time with 8 cores each and the environment variables set. On the second attempt it looks like the jobs will finish the first denoising epoch in < 1 hour each. That's far faster than the 6x reduction in runtime you might expect if a) runtime is linear in the number of bins and b) runtime is proportional to 1/cpus used. Without doing a lot more experiments it's hard to tell whether the better runtime is due to less fighting over resources (I can imagine 24 jobs each running 96 threads could degrade performance) or because runtime is super-linear vs. number of bins. I'm not asking for total p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:1762,variab,variables,1762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,1,['variab'],['variables']
Modifiability,rpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:615); 	at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:261); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:360); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.access$1800(InstantiatingGrpcChannelProvider.java:81); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider$1.createSingleChannel(InstantiatingGrpcChannelProvider.java:231); 	at com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:72); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:241); 	at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:219); 	at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:199); 	at com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:89); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:129); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110); 	at com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:102); 	at org.broadinstitute.hellbender.utils.bigquery.StorageAPIAvroReader.<init>(StorageAPIAvroReader.java:60); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createSortedReferenceRangeCollectionFromBigQuery(ExtractCohortEngine.java:851); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.createVariantsFromUnsortedBigQueryRanges(ExtractCohortEngine.java:891); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohortEngine.traverse(ExtractCohortEngine.java:224); 	at org.broadinstitute.hellbender.tools.gvs.extract.ExtractCohort.traverse(ExtractCohort.java:335); 	at org.broadinstitute.hellbender.engin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7583:1917,Enhance,EnhancedBigQueryReadStub,1917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7583,1,['Enhance'],['EnhancedBigQueryReadStub']
Modifiability,"rsions, but also novel adjacencies (BND records whose meanings cannot be fully resolved solely from assembly alignment signatures) as well as complex variants that theoretically could be arbitrarily complex (`<CPX>`, as long as we have assembled across the full event). . ## Planed organization. the `discovery` package could be divided roughly now into. ### interface. `SvDiscoveryDataBundle`, `SvDiscoverFromLocalAssemblyContigAlignmentsSpark`, `SvType`, `AnnotatedVariantProducer`. ### alignment prep (sub package). `AlignmentInterval`, `AlignedContig` (refactor `AssemblyContigWithFineTunedAlignments` into `AlignedContig`), `AlignedContigGenerator`, `AlignedAssembly`, `ContigAlignmentsModifier` (refactor `AlnModType` into it), `GappedAlignmentSplitter`, `StrandSwitch`, `FilterLongReadAlignmentsSAMSpark` (factor out the major methods in the new alignment filter by score into a 1st level class). ### type & location inference (sub package). * imprecise: refactor out methods from to-be-deprecated `DiscoverVariantsFromContigAlignmentsSAMSpark`. * alignment classification: `ChimericAlignment` and `NovelAdjacencyReferenceLocations` (very tricky to decouple the functionalities because both have over 50 uses), `AssemblyContigAlignmentSignatureClassifier`, `VariantDetectorFromLocalAssemblyContigAlignments`. * simple: `SimpleSVType`, `SvTypeInference`, `InsDelVariantDetector`, `BreakpointComplications` (rename to `BreakpointComplicationsForSimpleTypes`). * complex: `BreakEndVariantType`, `SuspectedTransLocDetector`, `SimpleStrandSwitchVariantDetector`. ### deprecated. `DiscoverVariantsFromContigAlignmentsSAMSpark` . It currently provides 3 groups of functionalities:. * novel adjacency detection (for ins, del, small dup, inversion only) by delegating to `ChimericAlignment.parseOneContig` and `NovelAdjacencyReferenceLocations(ChimericAlignment chimericAlignment, byte[] contigSequence, SAMSequenceDictionary)`; this should be deprecated; * exact variant type inference (delegated to `",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111:1322,refactor,refactor,1322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111,1,['refactor'],['refactor']
Modifiability,"rt extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to samp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:1017,refactor,refactor,1017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactor']
Modifiability,"rt tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+comput",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4608,config,configdefaults,4608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configdefaults']
Modifiability,"ry - Configuration file values:; 20:41:37.626 DEBUG ConfigFactory - gcsMaxRetries = 20; 20:41:37.626 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 20:41:37.626 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 20:41:37.626 DEBUG ConfigFactory - samjdk.compression_level = 2; 20:41:37.626 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 20:41:37.626 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 20:41:37.626 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 20:41:37.626 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 20:41:37.626 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 20:41:37.626 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 20:41:37.627 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 20:41:37.627 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 20:41:37.627 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 20:41:37.627 DEBUG ConfigFactory - createOutputBamIndex = true; 20:41:37.627 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 20:41:37.627 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 20:41:37.627 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 20:41:37.627 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 20:41:37.627 INFO PathSeqPipelineSpark - Initializing engine; 20:41:37.627 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/23 20:41:38 INFO SparkContext: Running Spark version 2.2.0; 18/04/23 20:41:38 WARN NativeCodeLoader: Unable to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:5781,Config,ConfigFactory,5781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['Config'],['ConfigFactory']
Modifiability,"s VCF file to /staging/wes/1\_sample\_20210615/CNV\_calling/genotyped-intervals-case-A210066-vs-v7cohort.vcf.gz... ; ; 11:04:27.510 INFO PostprocessGermlineCNVCalls - Analyzing shard 1 / 1... ; ; 11:04:30.169 INFO PostprocessGermlineCNVCalls - Generating segments... ; ; 11:04:37.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:5176,variab,variable,5176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['variab'],['variable']
Modifiability,"s where results changed:. - For the snpeff test, since the behavior on this branch seems more correct to me than master, I tried running the GATK4 test case inputs with GATK3, and it produces exactly the same results as this branch does. So I think that issue was introduced by the original GATK4 port, and is fixed in this branch.; - The rest of the tests with changed results don't seem to hit your breakpoint, though. So I think we need to figure out why they changed, and maybe also compare them with GATK3 (which can be a pain because the output format is slightly different).; - As you mentioned, you changed the reference for testEvalTrackWithoutGenotypesWithSampleFields, which seems to have only affected the number of loci processed. So I'm unclear why that change was necessary. If the test truly should have been failing without this change, will it still fail if the change is reverted ? If not, can we fix it, and either way there should be a negative test for that case. A few other general comments:. - I changed this PR to `draft` mode for now, which just better categorizes it for our internal workflow purposes. When its ready for a detailed code review we can remove the `draft` status.; - The `HashMap<FeatureInput<VariantContext>, HashMap<String, Collection<VariantContext>>>` can be wrapped in a class with just a couple of methods, so we don't have to manifest that long type all over the place.; - I know this PR still in an interim state, but passing the VariantWalker in as an argument to the comp methods doesn't seem like a step forward to me. If we can't solve that problem completely in this PR (which is fine, I'm all for trying to contain this), are those changes necessary ? Perhaps that part should just wait for the next round.; - Any new classes/methods should use `final` for variables and parameters wherever applicable, and public classes and methods should have javadoc.; - Finally, I'm curious if you've tried any perf testing on this branch ? Is it better ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744689987:1888,variab,variables,1888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-744689987,1,['variab'],['variables']
Modifiability,"s(DefaultCredentialsProvider.java:124); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127); at shaded.cloud_nio.com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100); at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:304); at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:278); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:83); at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:31); at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:78); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:382); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:183); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. Produced by pulling the docker image, **shutting off the internet connection**, mounting [helloHaplotypeCaller](https://drive.google.com/file/d/0B7akc6CTmxIHdy11R1M3ZjJJdUU/view), and running:. ```shell; docker run \; --rm \; -v /Users/kshakir/Downloads/helloHaplotypeCaller:/data \; broadinstitute/gatk:4.0.11.0 \; gatk \; HaplotypeCaller \; -R /data/ref/human_g1k_b37_20.fasta \; -I /data/inputs/NA12878_wgs_20.bam \; -O test.vcf; ```. Adding in a `GOOGLE_APPLICATION_CREDENTIALS` environment variable short circuits the above stack trace. ```shell; docker run \; -e GOOGLE_APPLICATION_CREDENTIALS=whatever; --rm \; -v /Users/kshakir/Downloads/helloHaplotypeCaller:/data \; broadinstitute/gatk:4.0.11.0 \; gatk \; HaplotypeCaller \; -R /data/ref/human_g1k_b37_20.fasta \; -I /data/inputs/NA12878_wgs_20.bam \; -O test.vcf; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843:4199,variab,variable,4199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843,1,['variab'],['variable']
Modifiability,"s.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3692,Config,ConfigFactory,3692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,"s.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:4596,Config,ConfigFactory,4596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"s/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5007,config,configparser,5007,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configparser']
Modifiability,"s; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.REFERENCE_FASTA : null; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 11:35:40.189 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:35:40.190 INFO Mutect2 - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 11:35:40.190 DEBUG ConfigFactory - Configuration file values: ; 11:35:40.196 DEBUG ConfigFactory - 	gcsMaxRetries = 20; 11:35:40.196 DEBUG ConfigFactory - 	gcsProjectForRequesterPays = ; 11:35:40.196 DEBUG ConfigFactory - 	gatk_stacktrace_on_user_exception = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_read_samtools = false; 11:35:40.196 DEBUG ConfigFactory - 	samjdk.use_async_io_write_samtools = true; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG Conf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:3710,Config,ConfigFactory,3710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:5163,Config,ConfigFactory,5163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['Config'],['ConfigFactory']
Modifiability,"samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4554,Config,ConfigFactory,4554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2281); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83); ... 27 more; Caused by,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:32354,adapt,adapted,32354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"scenario: someone wants to use gatk4 as a framework and add new tools. They need show on the list of tools etc. They package names are the user's, ie not org.broadinstitute.hellbender*. The way to do this is to extend Main but we have no example and not documentation of this (other that in the Main class, which is not the right place - I think it should be in README or some such).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1397:211,extend,extend,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1397,1,['extend'],['extend']
Modifiability,scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:13052,adapt,adapted,13052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,"se 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30,55] [info] Unspecified type (Unspecified version) workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 submitted; [2020-07-14 05:09:30,66] [info] SingleWorkflowRunnerActor: Workflow submitted 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,68] [info] WorkflowManagerActor Starting workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] WorkflowManagerActor Successfully started WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2020-07-14 05:09:30,72] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2020-07-14 05:09:30,83] [info] MaterializeWorkflowDescriptorActor [968be82c]: Parsing workflow as WDL 1.0; [2020-07-14 05:09:31,60] [info] MaterializeWorkflowDescriptorActor [968be82c]: Call-to-Backend assignments: ValidateBamsWf.ValidateBAM -> Local; [2020-07-14 05:09:31,82] [warn] Local [968be82c]: Key/s [memory, disks] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2020-07-14 05:09:35,38] [info] Not triggering log of token queue status. Effective log interval = ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:3664,config,configured,3664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configured']
Modifiability,"se; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG Genoty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4491,Config,ConfigFactory,4491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2663,refactor,refactor,2663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactor']
Modifiability,see if PerReadAlleleLikelihoodMap can be / should be refactored out,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1467:53,refactor,refactored,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1467,1,['refactor'],['refactored']
Modifiability,seems like a good candidate to be moved into the config files...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3552#issuecomment-327585040:49,config,config,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3552#issuecomment-327585040,1,['config'],['config']
Modifiability,"ser-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:1858,config,configuration,1858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['config'],['configuration']
Modifiability,"serException.BadArgumentValue(""ERC/gt_mode"",""you cannot request reference confidence output and GENOTYPE_GIVEN_ALLELES at the same time"");; ; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FOR_EMITTING = -0.0;; SCAC.genotypeArgs.STANDARD_CONFIDENCE_FOR_CALLING = -0.0;; ; -; // also, we don't need to output several of the annotations; annotationsToExclude.add(""ChromosomeCounts"");; annotationsToExclude.add(""FisherStrand"");; @@ -651,6 +651,9 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if (!SCAC.annotateAllSitesWithPLs); logger.info(""All sites annotated with PLs forced to true for reference-model confidence output"");; SCAC.annotateAllSitesWithPLs = true;; + } else if ( ! doNotRunPhysicalPhasing ) {; + doNotRunPhysicalPhasing = true;; + logger.info(""Disabling physical phasing, which is supported only for reference-model confidence output"");; }; ; if ( SCAC.AFmodel == AFCalcFactory.Calculation.EXACT_GENERAL_PLOIDY ); @@ -678,7 +681,7 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if( SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES && consensusMode ); throw new UserException(""HaplotypeCaller cannot be run in both GENOTYPE_GIVEN_ALLELES mode and in consensus mode. Please choose one or the other."");; ; - genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, tryPhysicalPhasing);; + genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, !doNotRunPhysicalPhasing);; // initialize the output VCF header; final VariantAnnotatorEngine annotationEngine = new VariantAnnotatorEngine(Arrays.asList(annotationClassesToUse), annotationsToUse, annotationsToExclude, this, getToolkit());; ; @@ -699,8 +702,10 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; VCFConstants.DEPTH_KEY,; VCFConstants.GENOTYPE_PL_KEY);; ; - if ( tryPhysicalPhasing ); - headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_KEY, VCFHeaderL",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:3579,extend,extends,3579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,1,['extend'],['extends']
Modifiability,"ses many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2094,flexible,flexible,2094,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['flexible'],['flexible']
Modifiability,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11261,refactor,refactoring,11261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,8,['refactor'],['refactoring']
Modifiability,"share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:1652,Config,Config,1652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['Config'],['Config']
Modifiability,"sing ) {; + doNotRunPhysicalPhasing = true;; + logger.info(""Disabling physical phasing, which is supported only for reference-model confidence output"");; }; ; if ( SCAC.AFmodel == AFCalcFactory.Calculation.EXACT_GENERAL_PLOIDY ); @@ -678,7 +681,7 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; if( SCAC.genotypingOutputMode == GenotypingOutputMode.GENOTYPE_GIVEN_ALLELES && consensusMode ); throw new UserException(""HaplotypeCaller cannot be run in both GENOTYPE_GIVEN_ALLELES mode and in consensus mode. Please choose one or the other."");; ; - genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, tryPhysicalPhasing);; + genotypingEngine = new HaplotypeCallerGenotypingEngine( getToolkit(), SCAC, !doNotRunPhysicalPhasing);; // initialize the output VCF header; final VariantAnnotatorEngine annotationEngine = new VariantAnnotatorEngine(Arrays.asList(annotationClassesToUse), annotationsToUse, annotationsToExclude, this, getToolkit());; ; @@ -699,8 +702,10 @@ public class HaplotypeCaller extends ActiveRegionWalker<List<VariantContext>, In; VCFConstants.DEPTH_KEY,; VCFConstants.GENOTYPE_PL_KEY);; ; - if ( tryPhysicalPhasing ); - headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_KEY, VCFHeaderLineCount.UNBOUNDED, VCFHeaderLineType.String, ""Physical phasing information, each unique ID within a given sample (but not across samples) connects alternate alleles as occurring on the same haplotype""));; + if ( ! doNotRunPhysicalPhasing ) {; + headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_ID_KEY, 1, VCFHeaderLineType.String, ""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group""));; + headerInfo.add(new VCFFormatHeaderLine(HAPLOTYPE_CALLER_PHASING_GT_KEY, 1, VCFHeaderLineType.String, ""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another""));; + }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237:4352,extend,extends,4352,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-470679237,1,['extend'],['extends']
Modifiability,"st and there's no batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never released one publicly because it seemed like it might cause confusion, but it's a reasonable possibility. . If you have any secret methods to reduce the size of python or R installations we're ha",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:1203,layers,layers,1203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,1,['layers'],['layers']
Modifiability,"stTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, column 50:. <title>gatk/mutect2_multi_sample.wdl at master ? broadinstitute/gatk ? GitHub</title>; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:1614,adapt,adapted,1614,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['adapt'],['adapted']
Modifiability,"stTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for pon; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 151, column 69:. gatk GenomicsDBImport --genomicsdb-workspace-path pon_db -R ~{ref_fasta} -V ~{sep=' -V ' input_vcfs} -L ~{intervals}; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:3254,adapt,adapted,3254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['adapt'],['adapted']
Modifiability,"stributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4873,config,configparser,4873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['config'],['configparser']
Modifiability,"sync_io_write_samtools = true; > 21:13:04.230 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; > 21:13:04.230 DEBUG ConfigFactory - samjdk.compression_level = 1; > 21:13:04.230 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4622,Config,ConfigFactory,4622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['Config'],['ConfigFactory']
Modifiability,"t convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:2178,config,configuration,2178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['config'],['configuration']
Modifiability,"t scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+compute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4433,config,configdefaults,4433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configdefaults']
Modifiability,"t$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:530); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:354); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:337); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:155); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:259); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:306); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; ```. one happened while working on chr21, the other on chr9",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-936771625:2971,variab,variable,2971,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7494#issuecomment-936771625,1,['variab'],['variable']
Modifiability,t&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9kcmFnc3RyL0NhbGlicmF0ZURyYWdzdHJNb2RlbC5qYXZh) | `70.345% <Ã¸> (Ã¸)` | |; | [...r/utils/fasta/CachingIndexedFastaSequenceFile.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9mYXN0YS9DYWNoaW5nSW5kZXhlZEZhc3RhU2VxdWVuY2VGaWxlLmphdmE=) | `70.330% <Ã¸> (-1.099%)` | :arrow_down: |; | [...t/java/org/broadinstitute/hellbender/MainTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9NYWluVGVzdC5qYXZh) | `2.564% <Ã¸> (-82.182%)` | :arrow_down: |; | [...Plugin/GATKAnnotationPluginDescriptorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS0Fubm90YXRpb25QbHVnaW5EZXNjcmlwdG9yVW5pdFRlc3QuamF2YQ==) | `7.219% <Ã¸> (-81.016%)` | :arrow_down: |; | [...GATKPlugin/GATKReadFilterPluginDescriptorTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vR0FUS1JlYWRGaWx0ZXJQbHVnaW5EZXNjcmlwdG9yVGVzdC5qYXZh) | `0.484% <Ã¸> (-88.136%)` | :arrow_down: |; | [...lbender/engine/AssemblyRegionIteratorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7920/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7920#issuecomment-1239413884:2777,Plugin,Plugin,2777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7920#issuecomment-1239413884,1,['Plugin'],['Plugin']
Modifiability,"tCoverage simply counts the number of overlapping reads with each target. Optionally, low quality calls are hard filtered. Here, we propose a probabilistic approach that avoids the usage of hard filters and fits well with the new probabilistic target coverage modeler. By definition, mapping quality MAPQ = -10 \log_10{mapping position is wrong} (see http://samtools.github.io/hts-specs/SAMv1.pdf, pg 5, item 5). It is defined in the range [0, 2^8-1]. The specific value 255 is reserved for when MAPQ is not available. Most MAPQs are well below 255. We consider the following process for assigning reads to each target. Pick a read ""k"" aligned to target ""t"" with a given MAPQ_k. By definition, it maps to the genomic position ""x"" with p_x = 1 - 10^{-MAPQ_k/10}, and to some other position with probability 1 - p_x. We refer to the alignment genomic position of read k as x_k, and the exome target(s) it overlaps with T_k. Let's assume we have T exome targets, and let z_{kt} be a 1-of-#T indicator variable for a read where t is a target and #T is the number of all exome targets. \pi_{kq} = P(z_{kq} = 1) =. p_k x O_{kq} if q \in T_k; (1 - p_k) / (#T - #T_k) if q \notin T_k. Here, O_{kq} is the fractional overlap of the read to an exome target q. Note that since we don't have the information about the next best alignment position, we take a flat prior. Finally, the number of reads belonging to target t, n_t, reads as:. n_t = \sum_k z_{kt}. Since there are many reads, n_t will be approximately Gaussian. It is an elementary calculation to calculate coverage mean E[n_t] and coverage variance var[n_t] in terms of \pi_{kq}. In the probabilistic target coverage model, var[n_t] will be added to the statistical noise. So, the read count collection will have two entries for each target: coverage mean, and coverage variance. ---. @mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/748#issuecomment-254812286). @samuelklee @asmirnov239 ; The outcome of",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2908:1129,variab,variable,1129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2908,1,['variab'],['variable']
Modifiability,taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; Using GATK jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --conf [jars=~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar] --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 20:41:36.853 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:41:37.246 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/zorzan/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:41:37.277 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/zorzan/libgkl_compression6179723182683465083.so; 20:41:37.613 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 20:41:37.613 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 20:41:37.613 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:41:37.614 INFO PathSeqPipelineSpark - Executing as zorzan@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 20:41:37.614 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 20:41:37.614 INFO PathSeqPipelineSpark - Start ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:2074,variab,variables,2074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"te). Strands of the intervals indicate whether the distal target intervals are; * upstream or downstream of their proposed breakpoints: true indicates that the breakpoint is upstream of the interval; * start position; false indicates that the breakpoint is downstream of the interval end position; */; ```. What else would you like to see documented there? . - The use of the word strand in this case is largely driven by a mapping of these data structures to the BEDPE format, which is the older format for representing breakpoints implied by paired-end mapping data without assembly. If you only consider read pair mappings, strand has the natural interpretation of being the strand to which reads aligned. For example, a deletion's two intervals have strands `+` and `-` because the `+` reads align at left breakpoint and `-` reads align near the right breakpoint. Extending the concept to supplementary mappings of split reads muddies the concept a bit, which made me change the definition of strand to the existing one: whether the evidence suggests a breakpoint upstream of the interval start or downstream of the interval end. . - I created `StrandedInterval` mostly just as a data container since I was often passing around an interval and an associated strand, and using them in conjunction with the `PairedStrandedIntervalTree` data structure. My goal with those was to have them be utility classes that could be used by anyone without regards to the particular mechanics of imprecise evidence clustering I've implemented here. I'd prefer to put the definition of how we're interpreting the interval and strand in our logic classes (`BreakpointEvidence`, `EvidenceTargetLink`, and EvidenceTargetLinkClusterer`). Does that make sense?. - A ""distal target region"" can be represented by a `StrandedInterval`. So can the original, proximal (non-distal) location of the breakpoint evidence. An `EvidenceTargetLink` has the two `StrandedInterval` objects representing the proximal and distal loca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471:1758,Extend,Extending,1758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3628#issuecomment-333857471,1,['Extend'],['Extending']
Modifiability,"team would potentially be willing to accept a PR to add a feature to GenotypeGVCFs. The general problem is this:. 1) When running GenotypeGVCFs, the default is to output variant sites, and this will therefore vary based on the set of samples. While there is an argument to include every site, calling against every position of the genome takes a very long time.; 2) As you know, a VCF file generally only includes variable sites in the current samples. Therefore, this doesnt differentiate between the situation where all samples have no data and when all samples are wild-type.; 3) We want to merge VCFs with data from different cohorts, including WGS and WES. It's just not practical to call 1000s of samples as one unit through GenotypeGVCFs (we're constantly adding new data and would need to keep re-calling). When merging these VCFs, we see a problem that is especially acute at sites with relatively rare variants. If the variant is only present in one or a few input VCFs, the other VCFs frequently lack that site (they are all wild-type). On merge, this is interpreted as no-data, which can be misleading. . The best solution I can devise is to force the input VCFs to output at a whitelist of sites, including if all samples are non-variant. While GenotypeGVCFs can be made to output at every genomic position, outputting everything is a huge leap in computational time. . I would propose to make a PR to augment GenotypeGVCFs to support an ""--always-output-calls-whitelist"" argument. The user can provide a FeatureInput. If provided, GenotypeGVCFs would output all variable sites (existing behavior), and also output any position spanning the intervals of this file, even if all samples at wild-type. . I only just started to look at how to implement this - i can some back with a more specific proposal. However, my initial thought is that we need to hook into drivingVariants or LocusWalker.traverse. Does your team have any thoughts on this, before we spend too much time on it? Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6239:1608,variab,variable,1608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6239,1,['variab'],['variable']
Modifiability,tentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.Composit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:16967,plugin,plugin,16967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['plugin'],['plugin']
Modifiability,ter yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:1963,config,configuration,1963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['config'],['configuration']
Modifiability,"termineGermlineContigPloidy - HTSJDK Defaults.REFERENCE_FASTA : null; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:48:45.921 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:48:45.922 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 08:48:45.922 DEBUG ConfigFactory - Configuration file values:; 08:48:45.927 DEBUG ConfigFactory - gcsMaxRetries = 20; 08:48:45.927 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 08:48:45.927 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 08:48:45.927 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 08:48:45.927 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 08:48:45.927 DEBUG ConfigFactory - samjdk.compression_level = 2; 08:48:45.927 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 08:48:45.927 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 08:48:45.927 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 08:48:45.927 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 08:48:45.927 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 08:48:45.927 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 08:48:45.928 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 08:48:45.928 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 08:48:45.92",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6217:4405,Config,ConfigFactory,4405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6217,1,['Config'],['ConfigFactory']
Modifiability,ternal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:17284,plugin,plugin,17284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['plugin'],['plugin']
Modifiability,tests for CigarUtils + refactored methods in CigarUtils. ; Did not add tests to isValid because pull req #380 is addressing this method. There's a potential issue in countRefBasesBasedOnCigar - it's not clear why the implementation does what it does. @amilev can you comment on the intended semantics of this method and whether it can/should use `CigarOperator.consumesReferenceBases`?. addresses #153 and #450 ; @vruano please review.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/455:23,refactor,refactored,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/455,1,['refactor'],['refactored']
Modifiability,thanks for the review @kcibul. I made some changes accordingly. re: PrepareCallset file of sample names. That would be nice! It would make this workflow simpler and it also simplifies the access requirements for PrepareCallset. re: Dockstore. We actually ruled this out because Terra says that the definition of a method configuration can change automatically if its updated in dockstore. Which can be useful but it adds a security risk since a compromised Dockstore can change the definition of the production AoU extraction WDL which runs with highly elevated permissions. We already have a script that creates method configurations from github so I can probably add something a little hacky to resolve relative imports to the raw github file that it refers to.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686:321,config,configuration,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7242#issuecomment-846494686,4,['config'],"['configuration', 'configurations']"
Modifiability,"the Main class is nominally extensible and our doc claims "" If you want your own single command line program, extend this class and give instanceMain a new list of java packages in which to search for classes that extend CommandLineProgram."". we need a test + example of this (test can serve as and example tool)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1396:110,extend,extend,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396,2,['extend'],['extend']
Modifiability,"the deletion more or less the same and call it (assigning B to the variant and A/C to reference) ; - At the second position:; -- DRAGEN (and GATK with the `--disable-spanning-event-genotyping` argument enabled) follow the GATK3 approach of assigning haplotype C to the variant and the A and B haplotypes to the reference. The B haplotype is assigned as such because the deletion does not START at position 224905964 thus its reference according to the old way of assigning likelihoods. This means that all of the likelihoods from the true deletion at this site are weighted towards the reference which will end up drowning out the SNP call resulting in no SNP being called at this site.; -- GATK assigns C to the variant, A to to the reference, and B to a third option â€œspanning deletionâ€ which prevents the deletion from outweighing the likelihoods assigned to the SNP resulting in better performance at many sites. This pattern even extends to SNP sites where a deletion was not called, since we still assign the haplotype to ""spanning deletion"" if there was a deletion at that site. . For indels however this can cause some extra false positives at sites like this one (the left variant under the deletion in the gatk track):; <img width=""1616"" alt=""Screen Shot 2020-07-14 at 4 09 47 PM"" src=""https://user-images.githubusercontent.com/16102845/87471543-86a2ee80-c5ec-11ea-9cdd-8acf1beb8c14.png"">; <img width=""178"" alt=""Screen Shot 2020-07-14 at 4 10 45 PM"" src=""https://user-images.githubusercontent.com/16102845/87471596-9de1dc00-c5ec-11ea-9d4c-786e114d57d3.png"">; This is a messy site that is perhaps complicated by representation issues but we can see that GATK emitted an extra insertion underlying the longer event (which was marked as homozygous in the truth set). Following the same logic as above we can see DRAGEN did not make the call because it assigned all of the likelihoods for the longer deletion to the reference when compared against the shorter insertion underlying it which outwe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6707:1993,extend,extends,1993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6707,1,['extend'],['extends']
Modifiability,"the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global genomics community and provide you with the latest technology for managing your genomic data. We have lots of other projects on the way, and look forward to s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:1150,extend,extend,1150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,2,['extend'],['extend']
Modifiability,"the implementation of posterior sampling, 3) some shape/dimshuffle operations, and other things along these lines. Using a single test shard of 20 1kGP WES samples x 1000 intervals, I have verified determinism/reproducibility for DetermineGermlineContigPloidy COHORT/CASE modes, GermlineCNVCaller COHORT/CASE modes, and PostprocessGermlineCNVCalls. Numerical results are also relatively close to those from 4.4.0.0 for all identifiable call and model quantities (albeit far outside any reasonable exact-match thresholds, most likely due to differences in RNG, sampling, and the aforementioned priors). Some remaining TODOs:. - [x] Rebuild and push the base Docker. EDIT: Mostly covered by #8610, but this also includes an addition of `libblas-dev`.; - [x] Update expected results for integration tests, perhaps add any that might be missing. EDIT: These were generated on WSL Ubuntu 20.04.2, we'll see if things pass on 22.04. Note that changing the ARD priors does change the *names* of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:2232,variab,variable,2232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['variab'],['variable']
Modifiability,"the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1436,variab,variables,1436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['variab'],['variables']
Modifiability,the requirement is to make MD fully work in a tested way (all Picard integration tests must work - perhaps by comparing the sets of reads that got marked as 'duplicate'). Note: we'll migrate this code from genomics-pipeline and adapt it to our needs and style.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/488:228,adapt,adapt,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/488,1,['adapt'],['adapt']
Modifiability,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1211:670,adapt,adapted,670,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211,1,['adapt'],['adapted']
Modifiability,this is our chance to rewrite gatk-protected history and un-commit the large files that were added by mistake,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2777:22,rewrite,rewrite,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2777,1,['rewrite'],['rewrite']
Modifiability,"tionally, this PR adds branch filters to the dockstore.yml file that will help with development. The filter for each workflow indicates which branch(es) will show up for that workflow in dockstore. If we don't include these filters, dockstore will run checks of ALL workflows on ALL branches, which causes timeouts. We could remove these filters later (before merging to master) or not, but for now this could help us develop on ah_var_store. Note that we'll need to add feature branches to that file as we work on them. This workflow was tested in Terra and the upload succeeded. Also confirmed that if one file fails, the entire process throws an error code (i.e. -m flag will not cause failures to silently pass) - in example below, `test_file_list.txt` was a list of 6 files, including 1 file that did not exist.; ```; âžœ cat test_file_list.txt | gsutil cp -I gs://dsp-fieldeng-dev/test_cp/; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; CommandException: No URLs matched: test4.txt; âžœ cat test_file_list.txt | gsutil -m cp -I gs://dsp-fieldeng-dev/test_cp/; If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o ""GSUtil:parallel_process_count=1""`. Note that multithreading is still available even if you disable multiprocessing. CommandException: No URLs matched: test4.txt; Copying file://test1.txt [Content-Type=text/plain]...; Copying file://test5.txt [Content-Type=text/plain]...; Copying file://test2.txt [Content-Type=text/plain]...; Copying file://test3.txt [Content-Type=text/plain]...; Copying file://test6.txt [Content-Type=text/plain]...; - [5/5 files][ 37.0 B/ 37.0 B] 100% Done; Operation completed over 5 objects/37.0 B.; CommandException: 1 file/object could not be transferred.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7104:1458,config,config,1458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7104,1,['config'],['config']
Modifiability,"titute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.456 INFO ComposeSTRTableFile - ----------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:3549,variab,variable,3549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['variab'],['variable']
Modifiability,"toHDFSSpark - Defaults.REFERENCE_FASTA : null; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:1834,variab,variable,1834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['variab'],['variable']
Modifiability,"tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/test/staging/lb_staging/gatk-package-4.beta.6-37-g0a135f8-SNAPSHOT-spark_7002d0551e84ddef0d74adf95dfee104.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/test/staging/lb_staging/756f43e6-4663-49ce-8a8c-bf717b07a8c7.vcf --sparkMaster yarn; Job [dfac787d-19aa-4296-8078-c033cd9f440d] submitted.; Waiting for job output...; 19:43:09.678 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:43:09.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/dfac787d-19aa-4296-8078-c033cd9f440d/gatk-package-4.beta.6-37-g0a135f8-SNAPSHOT-spark_7002d0551e84ddef0d74adf95dfee104.jar!/com/intel/gkl/native/libgkl_compression.so; [November 15, 2017 7:43:09 PM UTC] PrintVariantsSpark --output gs://hellbender-test-logs/test/staging/lb_staging/756f43e6-4663-49ce-8a8c-bf717b07a8c7.vcf --variant gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --sparkMaster yarn --variantShardSize 10000 --variantShardPadding 1000 --shuffle false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --Q",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:1336,variab,variables,1336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2348,variab,variables,2348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,4,"['config', 'variab']","['configured', 'variables']"
Modifiability,"tor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:33:26.273 INFO CountReadsSpark - Start Date/Time: January 7, 2019 11:33:24 AM EST; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.273 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2355,variab,variables,2355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,4,"['config', 'variab']","['configured', 'variables']"
Modifiability,"tory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4775,Config,ConfigFactory,4775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"tory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (thea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4299,Config,ConfigFactory,4299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"tory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4842,Config,ConfigFactory,4842,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"tory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4366,Config,ConfigFactory,4366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"tory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (198295559 bp); 16:16:37.373 DEBUG GenomeLocParser - chr4 (190214555 bp); 16:16:37.373 DEBUG GenomeLocParser - chr5 (181",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5706,Config,ConfigFactory,5706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,"tory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 21:05:38.395 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 21:05:38.395 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 21:05:38.395 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 21:05:38.395 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 21:05:38.395 DEBUG ConfigFactory - createOutputBamIndex = true; 21:05:38.396 INFO GermlineCNVCaller - Deflater: IntelDeflater; 21:05:38.396 INFO GermlineCNVCaller - Inflater: IntelInflater; 21:05:38.396 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 21:05:38.396 INFO GermlineCNVCaller - Requester pays: disabled; 21:05:38.396 INFO GermlineCNVCaller - Initializing engine; 21:05:38.399 DEBUG ScriptExecutor - Executing:; 21:05:38.399 DEBUG ScriptExecutor - python; 21:05:38.399 DEBUG ScriptExecutor - -c; 21:05:38.399 DEBUG ScriptExecutor - import gcnvkernel; 21:06:10.792 DEBUG ScriptExecutor - Result: 0; 21:06:10.792 INFO GermlineCNVCaller - Done initializing engine; 21:06:10.826 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:4603,Config,ConfigFactory,4603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Config'],['ConfigFactory']
Modifiability,tractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:102); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3500m -jar /root/gatk.jar Funcotator --data-sources-path /cromwell_root/datasources_dir --ref-version hg38 --output-file-format VCF -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/94e769a1-28e1-4bd7-b09f-9e47fb7d8352/omics_mutect2/14fe5685-740c-4e09-9d1a-8c8d14c0ae5b/call-mutect2/Mutect2/2de52f4f-eea0-4ec7-acc1-f47b1a2d1e6c/call-Filter/attempt-2/CDS-2jucw0.hg38-filtered.vcf.gz -O CDS-2jucw0.hg38-filtered.vcf.gz.annotated.vcf.gz -L /cromwell_root/ccleparams/region_file_wgs.list --annotation-default normal_barcode: --annotation-default tumor_barcode:NP5 --annotation-default Center:DEPMAP --annotation-default source:Unknown; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:7041,variab,variable,7041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['variab'],['variable']
Modifiability,"tre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/__init__.py"", line 41, in <module>; from theano.scan_module import scan_opt; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/scan_module/scan_opt.py"", line 60, in <module>; from theano import tensor, scalar; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/__init__.py"", line 17, in <module>; from theano.tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4268,config,configparser,4268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['config'],['configparser']
Modifiability,"ts header (either proactively or on demand), but transparently to ; application code that is running against the SAMRecord API.; This would allow SAM headers to be transmitted out-of-band in a way that ; depends on the execution environment. Depending on the environment, ; this might be done by proactive broadcast, or you could think of the ; header tag as a promise to retrieve the header if/when it is needed. ; The size and complexity of the header tag might also depend on the ; execution environment. If the execution environment only supports a ; small finite number of headers, the header tag could be a small integer, ; or in a different execution environment it could be; a unique hash of the header or something like that. Memory footprint in ; the receiver is minimized because many SAMRecords can all share the same ; header object.; This requires more work to support in each execution environment, but it ; seems like it could be efficient and allows application code written to ; operate on SAMRecords to be portable; across different execution environments without having to contend with ; the possible presence of headerless SAMRecords. -Bob. On 9/17/15 4:28 PM, droazen wrote:. > @davidadamsphd https://github.com/davidadamsphd, @lbergelson ; > https://github.com/lbergelson, and myself met for an hour or two ; > just now to discuss this issue, and after reviewing all the options I ; > think we were convinced by the following argument:; > ; > The |SAMRecord| class currently allows its header to be set to null, ; > so if there are cases where the class won't function properly or can ; > enter into an inconsistent state when a header is not present these ; > should be treated as bugs and patched, and we should add unit tests to ; > htsjdk to prove that headerless |SAMRecords| function properly. Then ; > in hellbender we can freely use headerless |SAMRecords| everywhere, ; > only restoring the header to the record when writing out the final bam ; > (since our bam writers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518:2594,portab,portable,2594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141451518,1,['portab'],['portable']
Modifiability,tureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -O 22.hg38-filtered.vcf; 2020/07/25 01:46:01 Starting delocalization.; 2020/07/25 01:46:02 Delocalization script execution started...; 2020/07/25 01:46:02 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-ac4624cb-a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5507,variab,variable,5507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['variab'],['variable']
Modifiability,"ty to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:2187,polymorphi,polymorphic,2187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['polymorphi'],['polymorphic']
Modifiability,typingEngineUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyR2Vub3R5cGluZ0VuZ2luZVVuaXRUZXN0LmphdmE=) | `100% <100%> (Ã¸)` | `31 <0> (Ã¸)` | :arrow_down: |; | [...ypecaller/AssemblyBasedCallerGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyR2Vub3R5cGluZ0VuZ2luZS5qYXZh) | `89.45% <100%> (+0.049%)` | `89 <0> (+1)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0NvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `83.117% <0%> (+1.948%)` | `43% <0%> (+1%)` | :arrow_up: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `76.398% <0%> (+3.727%)` | `45% <0%> (+2%)` | :arrow_up: |; | [...r/arguments/CopyNumberArgumentValidationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2FyZ3VtZW50cy9Db3B5TnVtYmVyQXJndW1lbnRWYWxpZGF0aW9uVXRpbHMuamF2YQ==) | `77.778% <0%> (+6.173%)` | `20% <0%> (+1%)` | :arrow_up: |; | [...tute/hellbender/utils/runtime/ProcessSettings.java](https://codecov.io/gh/broadinstitute/gatk/pull/5365/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1Byb2Nlc3NTZXR0aW5ncy5qYXZh) | `93.75% <0%> (+6.25%)` | `18% <0%> (+2%)` | :arrow_up: |; | [...te/hellbender/utils/python/PythonExecutorBase.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5365#issuecomment-433471265:1961,config,config,1961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5365#issuecomment-433471265,2,"['Config', 'config']","['ConfigFactory', 'config']"
Modifiability,"ub.com> wrote:; > ; > @SHuang-Broad commented on this pull request.; > ; > In src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/prototype/CpxVariantDetector.java:; > ; > > + this.tigWithInsMappings = new AssemblyContigWithFineTunedAlignments(contig, tigWithInsMappings.insertionMappings);; > +; > + this.basicInfo = new BasicInfo(contig);; > +; > + annotate(refSequenceDictionary);; > + }; > +; > + private static List<AlignmentInterval> deOverlapAlignments(final List<AlignmentInterval> originalAlignments,; > + final SAMSequenceDictionary refSequenceDictionary) {; > + final List<AlignmentInterval> result = new ArrayList<>(originalAlignments.size());; > + final Iterator<AlignmentInterval> iterator = originalAlignments.iterator();; > + AlignmentInterval one = iterator.next();; > + while (iterator.hasNext()) {; > + final AlignmentInterval two = iterator.next();; > + // TODO: 11/5/17 an edge case is possible where the best configuration contains two alignments,; > + // one of which contains a large gap, and since the gap split happens after the configuration scoring,; > I agree it is backwards. But...; > ; > The reason was that the (naive) alignment configuration scoring module rightnow uses MQ and AS (aligner score) for picking the ""best"" configuration (i.e. sub-list of the alignments given by aligner), which would be technically wrong if we were to split the gap and to simply grab the originating alignment's values.; > ; > This is especially true for AS, whose recomputing takes more time, and code, and forces us to know how AS are computed in the aligner so that there's no bias in computing the scores of naive alignments vs gap-split alignments (may not matter in practice, but still takes more code to compute).; > ; > Lots of the code in the discovery stage was devoted actually to alignment related acrobatics and edge cases so that the breakpoints we could resolve are as accurate as possible.; > I've kept in mind your wisdom that different aligners may ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009:1509,config,configuration,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805#issuecomment-350618009,4,['config'],['configuration']
Modifiability,"ults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:16:36.290 INFO GenomicsDBImport - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:16:36.290 DEBUG ConfigFactory - Configuration file values:; 16:16:36.295 DEBUG ConfigFactory - gcsMaxRetries = 20; 16:16:36.295 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 16:16:36.295 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 16:16:36.296 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 16:16:36.296 DEBUG ConfigFactory - samjdk.compression_level = 2; 16:16:36.296 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 16:16:36.296 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 16:16:36.296 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 16:16:36.296 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 16:16:36.296 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 16:16:36.297 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:5146,Config,ConfigFactory,5146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['Config'],['ConfigFactory']
Modifiability,umReads: 0; 12:13:56.984 DEBUG Mutect2 - Processing assembly region at chrM:14845-15144 isActive: false numReads: 0; 12:13:56.995 DEBUG Mutect2 - Processing assembly region at chrM:15145-15444 isActive: false numReads: 0; 12:13:57.009 DEBUG Mutect2 - Processing assembly region at chrM:15445-15744 isActive: false numReads: 0; 12:13:57.027 INFO ProgressMeter - chrM:15445 38.3 60 1.6; 12:13:57.035 DEBUG Mutect2 - Processing assembly region at chrM:15745-15960 isActive: false numReads: 14; 12:13:57.047 DEBUG Mutect2 - Processing assembly region at chrM:15961-16230 isActive: true numReads: 30; 12:13:57.055 DEBUG ReadThreadingGraph - Recovered 1 of 1 dangling tails; 12:13:57.063 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 12:13:57.096 DEBUG ReadThreadingGraph - Recovered 3 of 3 dangling tails; 12:13:57.106 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling heads; 12:13:57.464 DEBUG Mutect2Engine - Active Region chrM:15961-16230; 12:13:57.469 DEBUG Mutect2Engine - Extended Act Region chrM:15861-16299; 12:13:57.472 DEBUG Mutect2Engine - Ref haplotype coords chrM:15861-16299; 12:13:57.476 DEBUG Mutect2Engine - Haplotype count 111; 12:13:57.479 DEBUG Mutect2Engine - Kmer sizes count 0; 12:13:57.482 DEBUG Mutect2Engine - Kmer sizes values []; 12:13:58.821 DEBUG Mutect2 - Processing assembly region at chrM:16231-16299 isActive: false numReads: 15; 12:13:58.938 INFO Mutect2 - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityNotZeroReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered by: WellformedReadFilter ; 0 total reads filtered; 12:13:58.943 INFO ProgressMeter - chrM:15445 38.3 63 1.6; 12:13:58.946 INFO ProgressM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:22555,Extend,Extended,22555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Extend'],['Extended']
Modifiability,update the artifactory url to point to the new artifactory; update the travis build with another environment variable called UPLOAD which determines if that build should upload a snapshot or not; fixes #3068,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3075:109,variab,variable,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3075,1,['variab'],['variable']
Modifiability,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1478:64,plugin,plugin,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478,3,['plugin'],['plugin']
Modifiability,updating dataflow and htsjdk to newest versions; adding gradle versions plugin to help with identifying dependencies that need updates. This broke one of our spark related tests so I've excluded it for now. See #581. It should be reeneabled when https://github.com/cloudera/spark-dataflow/issues/49 is complete.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/582:72,plugin,plugin,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/582,1,['plugin'],['plugin']
Modifiability,urces.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.125 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 12:28:18.424 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:10036,config,config,10036,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['config'],['config']
Modifiability,"ureInput;. public CodecWrapper(FeatureCodec<FEATURE_TYPE, SOURCE> childCodec, FeatureInput<FEATURE_TYPE> featureInput); {; this.childCodec = childCodec;; this.featureInput = featureInput;; }. @Override; public Feature decodeLoc(SOURCE source) throws IOException {; return childCodec.decodeLoc(source);; }. @Override; public FEATURE_TYPE decode(SOURCE source) throws IOException {; FEATURE_TYPE feature = childCodec.decode(source);. //Either look for marker class or otherwise poke in FeatureInput here:; if (feature instanceof VariantContext); {; feature = new FeatureInputAwareVariantContext(feature, featureInput);; }. return feature;; }. @Override; public FeatureCodecHeader readHeader(SOURCE source) throws IOException {; return childCodec.readHeader(source);; }. @Override; public Class<FEATURE_TYPE> getFeatureType() {; return childCodec.getFeatureType();; }. @Override; public SOURCE makeSourceFromStream(InputStream bufferedInputStream) {; return childCodec.makeSourceFromStream(bufferedInputStream);; }. @Override; public LocationAware makeIndexableSourceFromStream(InputStream inputStream) {; return childCodec.makeIndexableSourceFromStream(inputStream);; }. @Override; public boolean isDone(SOURCE source) {; return childCodec.isDone(source);; }. @Override; public void close(SOURCE source) {; childCodec.close(source);; }. @Override; public boolean canDecode(String path) {; return childCodec.canDecode(path);; }; }. public static interface FeatureInputAware<FEATURE_TYPE extends Feature>; {; public FeatureInput<FEATURE_TYPE> getFeatureInput();; }. public static class FeatureInputAwareVariantContext extends VariantContext implements FeatureInputAware<VariantContext>; {; private FeatureInput<VariantContext> featureInput;. public FeatureInputAwareVariantContext(VariantContext parent, FeatureInput<VariantContext> featureInput); {; super(parent);; this.featureInput = featureInput;; }. @Override; public FeatureInput<VariantContext> getFeatureInput() {; return featureInput;; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766:2219,extend,extends,2219,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-823546766,2,['extend'],['extends']
Modifiability,"use_async_io_write_tribble = false; 11:35:40.197 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 11:35:40.197 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 11:35:40.197 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 11:35:40.197 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 11:35:40.197 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 11:35:40.198 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 11:35:40.198 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 11:35:40.198 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 11:35:40.198 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 11:35:40.198 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 11:35:40.198 DEBUG ConfigFactory - 	createOutputBamIndex = true; 11:35:40.200 INFO Mutect2 - Deflater: JdkDeflater; 11:35:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-packa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:4713,Config,ConfigFactory,4713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Config'],['ConfigFactory']
Modifiability,"use_async_io_write_tribble = false; 23:43:52.474 DEBUG ConfigFactory - 	samjdk.compression_level = 2; 23:43:52.474 DEBUG ConfigFactory - 	spark.kryoserializer.buffer.max = 512m; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.maxResultSize = 0; 23:43:52.474 DEBUG ConfigFactory - 	spark.driver.userClassPathFirst = true; 23:43:52.474 DEBUG ConfigFactory - 	spark.io.compression.codec = lzf; 23:43:52.474 DEBUG ConfigFactory - 	spark.executor.memoryOverhead = 600; 23:43:52.475 DEBUG ConfigFactory - 	spark.driver.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	spark.executor.extraJavaOptions = ; 23:43:52.475 DEBUG ConfigFactory - 	codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:43:52.475 DEBUG ConfigFactory - 	read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:4237,Config,ConfigFactory,4237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Config'],['ConfigFactory']
Modifiability,"using --conf passes args into the spark configuration. these args will take precendence over spark args specified in any other way. moved --sparkMaster and --conf to their own SparkCommandLineArgumentCollection. passing spark options through to gatk with DIRECT, this will only work with --sparkMaster and --conf. fixes #1339",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1356:40,config,configuration,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1356,1,['config'],['configuration']
Modifiability,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:14093,parameteriz,parameterize,14093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['parameteriz'],['parameterize']
Modifiability,"uted data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2056,refactor,refactor,2056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['refactor'],['refactor']
Modifiability,"utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.la",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:7773,adapt,adapted,7773,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-pack,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:12097,adapt,adapted,12097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collecti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:30816,adapt,adapted,30816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,utputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 11:00:54.334 INFO ShutdownHookManager - Shutdown hook called; 11:00:54.335 INFO ShutdownHookManager - Deleting directory /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/spark-f9c7c336-4e98-4fcc-855b-ba8a5a29e074; ```. The first lines of the log file:; ```; vm.max_map_count = 2147483642; Using GATK jar /Public/Everythings/misc/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:36687,adapt,adapted,36687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['adapt'],['adapted']
Modifiability,va:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsSequencial(CalibrateDragstrModel.java:459); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel --tmp-dir tmp -R /restricte; d/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.STR.table -O gvcf.STR/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui; .Dragstr.model -I ../pop/Brahui/HGDP00001/alignment/HGDP00001.alt_bwamem_GRCh38DH.20181023.Brahui.cram; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:6783,variab,variable,6783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['variab'],['variable']
Modifiability,"ver VM v1.8.0_181-b13; 14:35:47.080 INFO SelectVariants - Start Date/Time: September 24, 2018 2:35:45 PM EET; 14:35:47.080 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.081 INFO SelectVariants - ------------------------------------------------------------; 14:35:47.082 INFO SelectVariants - HTSJDK Version: 2.16.1; 14:35:47.082 INFO SelectVariants - Picard Version: 2.18.13; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:47.082 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:47.082 INFO SelectVariants - Deflater: IntelDeflater; 14:35:47.082 INFO SelectVariants - Inflater: IntelInflater; ```. From @jean-philippe-martin . > This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM).; > ; > As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via gcloud auth, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on.; > ; > The message is useful, for if we were running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.; > . We should tone down the error message if possible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5220:5855,variab,variable,5855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5220,1,['variab'],['variable']
Modifiability,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:2858,adapt,adaptive-pruning,2858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,1,['adapt'],['adaptive-pruning']
Modifiability,version: gatk 4.0.2.1; I use the pipeline :BwaAndMarkDuplicatesPipelineSpark-ã€‹BQSRPipelineSpark-ã€‹HaplotypeCallerSparkï¼Œand I get the bad resultã€‚by testingï¼ŒHaplotypeCallerSpark lose a lot of variable sites and HaplotypeCallerSpark 'result jitter to the same input bamã€‚,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4488:189,variab,variable,189,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488,1,['variab'],['variable']
Modifiability,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1392:193,config,config,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392,1,['config'],['config']
Modifiability,"willing to give it a try but I need minimal info to do so. * can I add `library(""ggplot2"")` to the code or will this new library not be supported by the package?. * what value (are these defaults?) do. ```; targetTITV = as.numeric(args[2]); targetSensitivity = as.numeric(args[3]) ; ```. take for a command like this. > cmd=""java ${javaopts} -jar $GATK/gatk.jar \; > 	VariantRecalibrator \; > 	-R ${reference_fa} \; > 	-V ${outfolder}/gatk_variants_excesshet_sitesonly.vcf.gz \; > 	-O ${outfolder}/gatk_variants_recalibrate_SNP.recal.vcf.gz \; > 	${intervals} \; > 	--resource:1001Gsnp,known=true,training=true,truth=true,prior=12.0 ${knownsnps} \; > 	--trust-all-polymorphic \; > 	--use-annotation DP \; > 	--use-annotation QD \; > 	--use-annotation FS \; > 	--use-annotation SOR \; > 	--use-annotation MQ \; > 	--use-annotation MQRankSum \; > 	--use-annotation ReadPosRankSum \; > 	--mode SNP \; > 	--max-gaussians ${maxSNPgaussians} \; > 	--tranches-file ${outfolder}/gatk_variants_recalibrate_snp.tranches \; > 	--tranche 100.0 \; > 	--tranche 99.95 \; > 	--tranche 99.9 \; > 	--tranche 99.8 \; > 	--tranche 99.6 \; > 	--tranche 99.5 \; > 	--tranche 99.4 \; > 	--tranche 99.3 \; > 	--tranche 99.0 \; > 	--tranche 98.0 \; > 	--tranche 97.0 \; > 	--tranche 90.0 \; > 	--rscript-file ${outfolder}/gatk_variantsgatk_variants_recalibrate_snp_plots.R \; > 	--tmp-dir ${basedir}/tmpfiles/"". Sorry but I do not know where to look in the java code for this. Thanks in advance",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6585#issuecomment-624680466:664,polymorphi,polymorphic,664,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6585#issuecomment-624680466,1,['polymorphi'],['polymorphic']
Modifiability,"windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract methods for the three parameters (`getWindowSize()`, `getWindowStep()` and `getWindowPadding()`, and implement `ReadWindowWalker` as a extension of this interface setting `getWindowStep()` to return `getWindowSize()` and `requiresReads()` to true. I can do this once the PR #1567 is accepted and generate the two interfaces (to be sure that the integration with the HC engine is working as expected with the changes), or just implement the `SlidingWindowWalker` and you can include it in the HC PR, or update afterwards to avoid redundancy in the code. What do you thi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:1629,extend,extend,1629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['extend'],['extend']
Modifiability,"y 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 21:47:48.270 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 21:47:48.270 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:5242,variab,variables,5242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"y and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is manda",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:6772,refactor,refactored,6772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['refactor'],['refactored']
Modifiability,"y increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7653:1623,config,configuration,1623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653,1,['config'],['configuration']
Modifiability,"y we are making it through much of the pipeline, but failing on `GetPileupSummaries`. There's a thread about it on the discussion board [here] (https://gatk.broadinstitute.org/hc/en-us/community/posts/6179012337819-No-Pileup-Tables). . We are specifying a file for `variants_for_contamination`, and a file for `variants_for_contamination_idx` in the workflow, but the index is never passed to `GetPileupSummaries`, and it fails with this enigmatic error message:. ```; A USER ERROR has occurred: An index is required but was not found for file gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_reference_files/small_exac_common_3.hg38.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.; ```. If you check out the source code in [mutect2.wdl](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl), you can see that that input variable `variants_for_contamination_idx`, which we have thoughtfully set and passed into the workflow, is never actually used in `GetPileupSummaries`. I'm not even sure there is an option to pass the index, from reading the [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360037593451-GetPileupSummaries). Here is an example of how the command is being called within our workflow:. ```; gatk --java-options ""-Xmx149500m"" GetPileupSummaries -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/cfce2061-efd6-449e-bdc9-a7ff2b633644/PreProcessingForVariantDiscovery_GATK4/b4adf777-4f97-425c-b3e2-b37c9d927667/call-GatherBamFiles/SRR7588418.hg38.bam --interval-set-rule INTERSECTION -L gs://fc-d31bc4e7-6d10-4dc4-a585-5895ab2346f3/81583498-648e-4e70-8452-80509b626927/Mutect2/dbb6ef96-ea07-4cfe-9e85-3b133c6d89ea/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0030-scattered.interval_list \; -V gs://bruce-processed-data/Prins_Cloughesy_Neoadjuvant/terra_re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7935:1237,variab,variable,1237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7935,1,['variab'],['variable']
Modifiability,"yf_documentation_update we; can use that for initial testing. On Tue, Dec 5, 2017 at 1:56 PM, sooheelee <notifications@github.com> wrote:. > @samuelklee <https://github.com/samuelklee>, thanks for the update and; > suggestion. I moved CollectAllelicCounts to the Coverage Analysis; > category. CollectFragmentCounts isn't on the list currently so I added it; > to the same. I hope I'm not missing a bunch of other new tools given I; > missed this one.; >; > @yfarjoun <https://github.com/yfarjoun>; >; > - You are now in charge of deciding whether we should include; > authorship in code. What the Comms team wants is for authorship to NOT show; > up in the gatkDoc/javaDoc. If you want to keep them, author lines should be; > at the bottom and formatted so they do not show up in the documentation.; > Geraldine is fine with completely removing them if you prefer that. There; > is a format trick that has javaDoc skip the author line and I can get that; > to you if you decide to keep some of these and @vdauwera; > <https://github.com/vdauwera> would know this or I can get you what I; > see in other docs. Let either of us know.; > - I can help you test your changes. I think the categories are good to; > go now so I will need to put these into both Picard and GATK; > HelpConstants.java, with the latter being a placeholder until the new; > Picard release is incorporated into the next GATK release, with variables; > that then must be included in each tool doc. I will find an example in a; > bit. Which tool do you want to test? @cmnbroad; > <https://github.com/cmnbroad> can explain the engineering details in; > engineering lingo if you need more information.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349404645>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0jIdprE580XBgq1jL-EIV1hFOcDyks5s9ZHAgaJpZM4QitCF>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349407253:1482,variab,variables,1482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349407253,1,['variab'],['variables']
Modifiability,"â€¦bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684:105,refactor,refactoring,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684,1,['refactor'],['refactoring']
Performance,"	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, column 50:. <title>gatk/mutect2_multi_sample.wdl at master ? broadinstitute/gatk ? GitHub</title>; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:2442,load,load,2442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['load'],['load']
Performance, 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:330); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: htsjdk.variant.variantcontext.LazyGenotypesContext; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826); 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000); 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:6486,concurren,concurrent,6486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['concurren'],['concurrent']
Performance, 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:362); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.invalidateSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_pro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:7675,concurren,concurrent,7675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['concurren'],['concurrent']
Performance, 	at com.github.discvrseq.walkers.BackportLiftedVcf.apply(BackportLiftedVcf.java:156); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at com.github.discvrseq.Main.main(Main.java:51); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 26 more,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182:2635,Load,LoadSnappy,2635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance, 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:15 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000006 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:15 INFO storage.BlockManagerMaster: Removal of executor 4 requested; 18/01/09 18:31:15 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 4; 18/01/09 18:31:15 INFO storage.BlockManagerMaster: Removal of executor 5 requested; 18/01/09 18:31:15 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 5; 18/01/09 18:31:15 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.; 18/01/09 18:31:15 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.; 18/01/09 18:31:18 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Cont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:22620,concurren,concurrent,22620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance, 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 ERROR Executor: Exception in task 518.0 in stage 0.0 (TID 518); java.io.FileNotFoundException: /home/data/WGS/F002/F002.sort.bam (Too many open files); 	at java.io.FileInputStream.open0(Native Method); 	at java.io.FileInputStream.open(FileInputStream.java:195); 	at java.io.FileInputStream.<init>(FileInputStream.java:138); 	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.java:106); 	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:202); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:349); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.WrapSeekable.openPath(WrapSeekable.java:60); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:147); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316:4635,concurren,concurrent,4635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316,1,['concurren'],['concurrent']
Performance," 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 18/10/17 19:23:59 WARN TaskSetManager: Lost task 517.0 in stage 0.0 (TID 517, localhost, executor driver): org.broadinstitute.hellbender.exceptions.UserException$NoSuitableCodecs: Cannot read /dev/shm/gatktmp/spark-30e238e4-b1b7-41f9-b31e-844f16879051/userFiles-4621c82d-5f86-4b51-9321-ccc84ab49979/dbsnp_138.hg19.vcf because no suitable codecs found; 	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:462); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:320); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316:7979,concurren,concurrent,7979,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316,1,['concurren'],['concurrent']
Performance, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385); 	at hdfs.jsr203.HadoopFileSystem$3.read(HadoopFileSystem.java:478); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:3669,concurren,concurrent,3669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['concurren'],['concurrent']
Performance," ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFControlSample/Benchmark/0c99102a-bca1-4426-97c6-5a311ace93c1/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFTestSample/Benchmark/a3925c8a-7e0a-4fec-8507-f885061b69c3/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CreateHTMLReport/cacheCopy/report.html""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:14465,cache,cacheCopy,14465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,2,['cache'],['cacheCopy']
Performance," ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); 4.1.8.1. ### Description ; A user is getting a java.lang.NullPointerException when running Mutect2. As discussed at GATK Office Hours 09/28/20, it seems to be an issue where the BAM contigs are not present in the reference sequence dictionary. We discussed an improvement with either the filter for this problem or the error message. Complete stack trace:; ```; Using GATK jar GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar Mutect2 -R ref/Homo_sapiens_assembly38.fasta -I SRR_MM10_2pass_recal.bam --germline-resource /af-only-gnomad.hg38.vcf.gz --panel-of-normals ref/1000g_pon.hg38.vcf.gz -O SRR_somatic_mutect.vcf.gz; 13:24:08.400 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 25, 2020 1:24:08 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:24:08.556 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 13:24:08.557 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:24:08.557 INFO Mutect2 - Executing as xxx on Linux v3.10.0-1127.18.2.el7.x86_64 amd64; 13:24:08.557 INFO Mutect2 - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_192-b12; 13:24:08.557 INFO Mutect2 - Start Date/Time: September 25, 2020 1:24:08 PM BST; 13:24:08.557 INFO Mutect2 - ------------------------------------------------------------; 13:24:08.557 INFO Mutect2 - -----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:1016,Load,Loading,1016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['Load'],['Loading']
Performance," (FÃ©vrier in french) August (AoÃ»t) or December (DÃ©cembre) have ISO-8859-1 encoding instead of UTF-8 encoding. Indeed, the output files have this line:. `##GATKCommandLine=<ID=GenotypeGVCFs,CommandLine=""GenotypeGVCFs --output /volumes/vol002/COVID/GenomicDB/vcf/COVID.05022021.int00.vcf.gz --variant gendb://dbtot/int00 --reference /volumes/vol002/reference/human_g1k_v37.fasta --tmp-dir /volumes/vol002/COVID/GenomicDB/tmp/tmpint00 --include-non-variant-sites false --merge-input-intervals false --input-is-somatic false --tumor-lod-to-emit 3.5 --allele-fraction-error 0.001 --keep-combined-raw-annotations false --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-heterozygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samples-if-no-call 0 --genomicsdb-use-bcf-codec false --genomicsdb-shared-posixfs-optimizations false --only-output-calls-starting-in-intervals false --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false --disable-tool-default-annotations false --enable-all-annotations false --allow-old-rms-mapping-quality-annotation-data false"",Version=""4.1.9.0"",Date=""5 f<E9>vrier 2021 10:42:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7081:1246,optimiz,optimizations,1246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7081,1,['optimiz'],['optimizations']
Performance," (PathSeqBuildKmers) and filtering reads that are low-quality, low-complexity, or come from the host (PathSeqFilterSpark). Sorry for the especially large size on this PR. **PathSeqBuildKmers tool**. Note this has been renamed from PathSeqKmerSpark. Input:; 1) Host reference FASTA; 2) False positive probability (0 create a hash set, >0 to create a Bloom filter); 3) Kmer length (1-31); 4) Kmer base indices to mask (optional). Output:; 1) Serialized kmer Hopscotch set (.hss) or Bloom filter (.bfi) file. For each reference record, the tool generates a list of long's containing the canonicalized/masked kmers. The result is a Collection<long[]> variable, which is then converted to either a PSKmerSet (Hopscotch set) or PSKmerBloomFilter, depending on the desired false positive probability. . The PSKmerSet/BloomFilter classes are basically wrappers for LargeLongHopscotchSet and LongBloomFilter, respectively. They both inherit PSKmerCollection, which provides a contains() function for querying new kmers for set membership and makes loading the kmers for filtering more convenient. These classes also store the kmer size, mask, and false positive probability. They also handle canonicalization/masking on queried kmers. **PathSeqFilterSpark tool**. Input:; 1) Input BAM; 2) Host kmer set file (optional); 3) Host reference bwa image (optional). Output:; 1) BAM containing paired reads that still have mates; 2) BAM containing unpaired reads / reads whose mates were filtered out; 3) Metrics file containing read counts and elapsed wall time at each step (optional). Filtering steps performed on each read:; - If the user sets the --isHostAligned, the read will first be filtered if it is aligned sufficiently well ; - Alignment info is stripped; - A series of quality filters (same as in the previous version of this tool); - Kmerized and filtered out if at least a threshold number of kmers are in the host set (default 1); - Aligned to the host reference and filtered if it maps sufficiently ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:1098,load,loading,1098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['load'],['loading']
Performance," - 1:210675831 4.2 5438000 1282169.2; 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6103,concurren,concurrent,6103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance," - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:4438,concurren,concurrent,4438,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance," - Picard Version: 2.21.9; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:45.791 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:45.792 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:45.792 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:40:45.792 INFO HaplotypeCaller - Inflater: IntelInflater; 14:40:45.792 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:40:45.792 INFO HaplotypeCaller - Requester pays: disabled; 14:40:45.792 INFO HaplotypeCaller - Initializing engine; 14:40:47.694 INFO IntervalArgumentCollection - Processing 50818468 bp from intervals; 14:40:47.714 INFO HaplotypeCaller - Done initializing engine; 14:40:47.826 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:40:47.864 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:40:47.868 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:40:47.921 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:40:47.922 INFO IntelPairHmm - Available threads: 1; 14:40:47.922 INFO IntelPairHmm - Requested threads: 4; 14:40:47.922 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:40:47.922 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:40:48.005 INFO ProgressMeter - Starting traversal; 14:40:48.006 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:40:51.792 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:8200,Load,Loading,8200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['Load'],['Loading']
Performance," - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 02:07:53.598 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.49244E-4; 02:07:53.598 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.007888748000000001; 02:07:53.598 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 02:07:53.598 INFO HaplotypeCaller - Shutting down engine; [28 November 2019 at 2:07:53 AM IST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=8220835840; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.read.AlignmentUtils.needsConsolidation(AlignmentUtils.java:758); at org.broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:3602,multi-thread,multi-threaded,3602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['multi-thread'],['multi-threaded']
Performance, - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/oreganno.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2020-08-19 15:41:49 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:49.663 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hgnc_download_Nov302017.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 15:41:49.851 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.851 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 15:41:49.852 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.938 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.021 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.092 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 15:41:50.093 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:13492,cache,cache,13492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['cache'],['cache']
Performance," - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 10:20:18.549 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 10:20:24.995 INFO gcnvkernel.tasks.task_case_denoising_calling - Loading the model and updating the instantiated model and workspace...; 10:20:25.005 INFO gcnvkernel.io.io_commons - Reading model parameter values for ""log_mean_bias_t""... Stderr: Traceback (most recent call last):; File ""/media/Data/tmp/case_denoising_calling.3564509013495540802.py"", line 201, in <module>; shared_workspace, initial_params_supplier, args.input_model_path); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/tasks/task_case_denoising_calling.py"", line 128, in __init__; self.continuous_model_approx, input_model_path)(); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_denoising_calling.py"", line 93, in __call__; self.input_path, self.denoising_model_approx, self.denoising_model); File ""/usr/BioinfSoftware/Anaconda/3-2020.11/envs/gatk4.3.0.0/lib/python3.6/site-packages/gcnvkernel/io/io_commons.py"", line 471, in read_mean_field_global_params; ""expected: {2}"".format(var_name, var_mu.sha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:7758,Load,Loading,7758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Load'],['Loading']
Performance," --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.3.0/install/bin/gatk-package-4.0.3.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 13:48:31.261 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:48:31.426 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.3.0/install/bin/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:48:31.693 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.693 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 13:48:31.693 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:48:31.694 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:48:31.694 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:48:31.694 INFO CountReadsSpark - Start Date/Time: December 21, 2018 1:48:31 PM EST; 13:48:31.694 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.694 INFO CountReadsSpark - ------------------------------------------------------------; 13:48:31.695 INFO CountReadsSpark - HTSJDK Vers",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:1878,Load,Loading,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['Load'],['Loading']
Performance," -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx130g -jar /gatk/gatk-package-4.1.8.1-local.jar Mutect2 -R /ucsc.hg19.fasta -I my.bam -L /test.bed --f1r2-tar-gz DD.f1r2.tar.gz --force-active --genotype-germline-sites --kmer-size 10 --kmer-size 20 --recover-all-dangling-branches --max-reads-per-alignment-start 0 --native-pair-hmm-threads 33 -O DD.vcf.gz; `. ### Affected version(s); Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar. ### Description ; When bed is created with a reference genome that is not the same as the bam file, an null pointer can occurs. The error is not catched by GATK, and the error is difficult to understand. Here a discussion about it.; https://gatk.broadinstitute.org/hc/en-us/community/posts/360077477391-Haplotype-caller-fails-to-run-GATK-4-1-8-0-and-GATK-4-2-0-0-. The case below occurs when provided bed has been made with the wrong genome reference.; `; 14:25:55.254 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 07, 2021 2:25:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:25:55.525 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.525 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.1; 14:25:55.525 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:25:55.525 INFO Mutect2 - Executing as toto on Linux v5.4.123-1.el7.elrepo.x86_64 amd64; 14:25:55.525 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 14:25:55.526 INFO Mutect2 - Start Date/Time: October 7, 2021 2:25:55 PM GMT; 14:25:55.526 INFO Mutect2 - ------------------------------------------------------------; 14:25:55.526 INFO Mutect2 - ----------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7496:1110,Load,Loading,1110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7496,1,['Load'],['Loading']
Performance, 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:28:26.332 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:30:24.296 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:4->5; > 21:30:24.299 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > . Here's standard error:. > WARNING: No valid combination operation found for INFO field DS -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:6201,cache,cache,6201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance," 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be d; isabled with 'optimizer=None'.; HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:500); at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(Postproc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:12415,optimiz,optimizations,12415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,2,['optimiz'],"['optimizations', 'optimizer']"
Performance," 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it to run GATK tools? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1991,concurren,concurrent,1991,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,1,['concurren'],['concurrent']
Performance," 02:53:12,34] [info] Aborting all running workflows.; [2019-10-01 02:53:12,34] [info] JobExecutionTokenDispenser stopped; [2019-10-01 02:53:12,35] [info] WorkflowStoreActor stopped; [2019-10-01 02:53:12,35] [info] WorkflowLogCopyRouter stopped; [2019-10-01 02:53:12,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-10-01 02:53:12,35] [info] WorkflowManagerActor All workflows finished; [2019-10-01 02:53:12,35] [info] WorkflowManagerActor stopped; [2019-10-01 02:53:12,65] [info] Connection pools shut down; [2019-10-01 02:53:12,65] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] SubWorkflowStoreActor stopped; [2019-10-01 02:53:12,65] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-10-01 02:53:12,65] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-10-01 02:53:12,66] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-10-01 02:53:12,66] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-10-01 02:53:12,66] [info] JobStoreActor stopped; [2019-10-01 02:53:12,66] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-10-01 02:53:12,66] [info] CallCacheWriteActor stopped; [2019-10-01 02:53:12,66] [info] IoProxy stopped; [2019-10-01 02:53:12,66] [info] ServiceRegistryActor stopped; [2019-10-01 02:53:12,67] [info] DockerHashActor stopped; [2019-10-01 02:53:12,69] [info] Database closed; [2019-10-01 02:53:12,69] [info] Stream materializer shut down; [2019-10-01 02:53:12,69] [info] WDL HTTP import resolver closed; Workflow c55a06f3-abc1-4db1-8e0f-ea0303caab2c transitioned to state Failed; ```. Any help will be much appreciated. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6189:10800,queue,queued,10800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6189,3,['queue'],['queued']
Performance," 07:32:25 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 107) in 279 ms on localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:9236,concurren,concurrent,9236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance, 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparing$ea9a8b3a$1(Ljava/util/Comparator;Ljava/util/function/Function;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b20f3e0) thrown at [/home/buildozer/aports/community/openjdk8/s; Event: 3.490 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparingInt$7b0bb60$1(Ljava/util/function/ToIntFunction;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b219168) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.; Event: 3.491 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29085,load,loading,29085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance," 1.6.1-hadoop2; 17/10/11 14:19:12 INFO yarn.Client: Uploading resource file:/tmp/hdfs/spark-8c88439f-dcb0-48b2-86f3-fc82cef4c438/__spark_conf__8945422067005652415.zip -> hdfs://mg:8020/user/hdfs/.sparkStaging/application_1507683879816_0006/__spark_conf__8945422067005652415.zip; 17/10/11 14:19:13 INFO spark.SecurityManager: Changing view acls to: hdfs; 17/10/11 14:19:13 INFO spark.SecurityManager: Changing modify acls to: hdfs; 17/10/11 14:19:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs); 17/10/11 14:19:13 INFO yarn.Client: Submitting application 6 to ResourceManager; 17/10/11 14:19:13 INFO impl.YarnClientImpl: Submitted application application_1507683879816_0006; 17/10/11 14:19:14 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:14 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507702753100; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507683879816_0006/; 	 user: hdfs; 17/10/11 14:19:15 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:15 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null); 17/10/11 14:19:15 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507683879816_0006), /proxy/application_1507683879816_0006; 17/10/11 14:19:15 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/11 14:19:16 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:17 INFO yarn.Client: Applicatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:6898,queue,queue,6898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['queue'],['queue']
Performance," 11 hours to get to the point where the error occurs it has been difficult to trouble shoot, I am hoping that I can fix this without rebuilding everything which is why I decided to write. Thanks for any information or suggestions you may have. . Dan; ; Using GATK jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx1600g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar GenotypeGVCFs -R /home/dan_vanderpool/Wolf_raw_reads/Wolf_genome/GCA_905319855.2_mCanLor1.2_genomic.fa -V gendb://Wolf_Genome_Variantsdb -O All_Wolf_Samples_Joint_Genotypes_Raw.vcf.gz -L /scratch/dan/Wolf_reads_raw/Wolf_GenCov300_Q20_Merged.interval_list -imr ALL --genomicsdb-max-alternate-alleles 10 --max-alternate-alleles 6; 17:49:29.781 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/dan_vanderpool/src/gatk-4.2.5.0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2022 5:49:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:49:30.164 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:30.165 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 17:49:30.165 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:30.165 INFO GenotypeGVCFs - Executing as dan_vanderpool@0e07622619ad on Linux v4.4.0-210-generic amd64; 17:49:30.165 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13; 17:49:30.166 INFO GenotypeGVCFs - Start Date/Time: February 23, 2022 at 5:49:29 PM UTC; 17:49:30.166 INFO GenotypeGVCFs - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454:4188,Load,Loading,4188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1049112454,1,['Load'],['Loading']
Performance," 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6619,concurren,concurrent,6619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance, 2017 16:51:57 BST] Executing as ameyner2@node2c15.ecdf.ed.ac.uk on Linux 3.10.0-327.36.3.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.2; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:57.770 INFO SparkGenomeReadCounts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:57.770 INFO SparkGenomeReadCounts - Deflater: IntelDeflater; 16:51:57.770 INFO SparkGenomeReadCounts - Inflater: IntelInflater; 16:51:57.770 INFO SparkGenomeReadCounts - Initializing engine; 16:51:57.770 INFO SparkGenomeReadCounts - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 17/07/21 16:51:58 INFO SparkContext: Running Spark version 2.0.2; 17/07/21 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls to: ameyner2; 17/07/21 16:51:58 INFO SecurityManager: Changing view acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: Changing modify acls groups to: ; 17/07/21 16:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ameyner2); groups with view permissions: Set(); users with modify permissions: Set(ameyner2); groups with modify permissions: Set(); 17/07/21 16:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43815.; 17/07/21 16:51:58 INFO SparkEnv: Registering MapOutputTracker; 17/07/21 16:51:58 INFO SparkEnv: Registering BlockManagerMaster; 17/07/21 16:51:58 INFO DiskBlockManager: Created local directory at /tmp/ameyner2/blockmgr-d8bbd2bc-8366-4b98-a238-2d51da1689d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360:3010,load,load,3010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360,1,['load'],['load']
Performance," 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3688baab{/,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fe2dd02{/api,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@726a8729{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a2724d3{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:33:27 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:46828/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1546878807984; 2019-01-07 11:33:28 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-07 11:33:29 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-07 11:33:30 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-07 11:33:30 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-07 11:33:30 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-07 11:33:30 INFO Client:54 - Setting up container launch context for our AM; 2019-01-07 11:33:30 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-07 11:33:30 INFO Client:54 - Preparing resources for our AM container; 2019-01-07 11:33:30 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1883879239_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-07 11:33:30 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11334 for farrell on ha-hdfs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:10619,load,loaded,10619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['load'],['loaded']
Performance," 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16b64a03{/,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1584c019{/api,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5817f1ca{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b395581{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:12 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:42689/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1547058912934; 2019-01-09 13:35:13 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-09 13:35:13 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-09 13:35:14 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-09 13:35:14 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-09 13:35:14 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-09 13:35:14 INFO Client:54 - Setting up container launch context for our AM; 2019-01-09 13:35:14 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-09 13:35:14 INFO Client:54 - Preparing resources for our AM container; 2019-01-09 13:35:14 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-682487019_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-09 13:35:14 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11353 for farrell on ha-hdfs:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:10359,load,loaded,10359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['load'],['loaded']
Performance," 2021] Executing on Linux 2.6.32-696.el6.x86_64 amd64; INFO 14:49:42,884 HelpFormatter - Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12; INFO 14:49:42,889 HelpFormatter - Program Args: -T HaplotypeCaller -R /share/Onc_Soft_DB/database/capsmart/hg19/hg19_20210805/hg19.rm_CRLF2_P2RY8.fix_PRSS1_MUC16.fasta -I /share/Onc_RD_Pipeline/OncDir/zhuangll/210927-commercial-tissue-zhangaiyuan/germline/Z19W06700-F1WA/2.Realign/Z19W06700-F1WA.bam -L /share/Onc_Soft_DB/database/capsmart/bed/gene102.snpindel.capsmart.bed -U -o /share/Onc_RD_Pipeline/OncDir/yanhs/test/GATK/Z19W06700-F1WA.HaplotypeCaller.raw.vcf -stand_call_conf 50 -A RMSMappingQuality -A BaseCounts; INFO 14:49:42,892 HelpFormatter - Executing as yanhs3941@compute-0-76 on Linux 2.6.32-696.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_11-b12.; INFO 14:49:42,892 HelpFormatter - Date/Time: 2021/10/09 14:49:42; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,892 HelpFormatter - ------------------------------------------------------------------------------------; INFO 14:49:42,922 NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/Onc_Soft_DB/software/GATK3.8/GenomeAnalysisTK.jar!/com/intel/gkl/native/libgkl_compression.so; INFO 14:49:42,957 GenomeAnalysisEngine - Deflater: IntelDeflater; INFO 14:49:42,958 GenomeAnalysisEngine - Inflater: IntelInflater; INFO 14:49:42,958 GenomeAnalysisEngine - Strictness is SILENT; INFO 14:49:43,125 GenomeAnalysisEngine - Downsampling Se. the error is :; maxAltAlleles (6), the following will be dropped: TAAC.; WARN 14:59:10,944 HaplotypeCallerGenotypingEngine - location chr12:21623284-21623286: too many alternative alleles found (8) larger than the maximum requested with -maxAltAlleles (6), the following will be dropped: C, CA.; INFO 14:59:13,453 ProgressMeter - chr12:21624342 1.0358131E7 9.5 m 55.0 s 49.5% 19.2 m 9.7 m; WARN 14:59:37,613 HaplotypeCallerGenotypingEngine -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7499:1683,Load,Loading,1683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7499,1,['Load'],['Loading']
Performance," 26|2, 2], DP=94, ECNT=1, GERMQ=93, MBQ=[31, 20], MFRL=[288, 110], MMQ=[60, 60], MPOS=56, NALOD=1.37, NLOD=6.17, POPAF=4.6, ROQ=93, TLOD=10.97} GT=GT:AD:AF:DP:F1R2:F2R1:SB 0/1:46,4:0.07:50:14,3:10,0:28,18,2,2 0/0:23,0:0.041:23:8,0:5,0:15,8,0,0 filters=; 11:43:25.661 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000441716.2 for problem variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A); 11:44:04.904 INFO ProgressMeter - chr8:677091 4.5 3000 666.0; 11:45:35.226 INFO ProgressMeter - chr11:62279639 6.0 4000 665.6; 11:46:54.284 INFO ProgressMeter - chr15:19905537 7.3 5000 682.4; 11:48:12.767 WARN FuncotatorUtils - createAminoAcidSequence given a coding sequence of length not divisible by 3. Dropping bases from the end: 2 (size=293, ref allele: G); 11:48:16.949 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000379751.5 for variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T): Cannot yet handle indels starting outside an exon and ending within an exon.; 11:48:16.949 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000379751.5 for problem variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T); 11:48:31.506 INFO ProgressMeter - chr21:18282114 8.9 6000 670.6; 11:49:08.210 INFO ProgressMeter - chr21:18282114 9.6 6888 720.6; 11:49:08.210 INFO ProgressMeter - Traversal complete. Processed 6888 total variants in 9.6 minutes.; 11:49:08.210 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2; 11:49:08.211 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/4781; 11:49:08.230 INFO Funcotator - Shutting down engine; [July 7, 2021 11:49:08 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 9.72 minutes.; Runtime.totalMemory()=4879548416; Tool returned:; true",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422:2306,cache,cache,2306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422,2,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 5; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 10; initial apicid	: 10; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:48865,cache,cache,48865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:50038,cache,cache,50038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:51211,cache,cache,51211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:52384,cache,cache,52384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance," 4 --bam-output tumor.recalibrated.realigned.bam --add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!. at htsjdk.samtools.SAMFileHeader.addProgramRecord(SAMFileHeader.java:202). at htsjdk.samtools.SAMTextHeaderCodec.parsePGLine(SAMTextHeaderCodec.java:158). at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:107). at htsjdk.samtools.SAMFileHeader.clone(SAMFileHeader.java:398). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriterFromFactory(ReadUtils.java:1215). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriter(ReadUtils.java:1163). at org.broadinstitute.hellbender.utils.haplotype.SAMFileDestination.(SAMFileDestinat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:1698,Load,Loading,1698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['Load'],['Loading']
Performance, 7	CombineReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/3e6142ad4eb23d4d9227fafd8e52b498263b4369/src/main/java/org/broadinstitute/hellbender/tools/exome/CombineReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1072	yes	; 13	CreatePanelOfNormals		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/CreatePanelOfNormals.java	scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1073	yes	; 20	NormalizeSomaticReadCounts		19-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/NormalizeSomaticReadCounts.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1074	yes	; 25	PerformSegmentation		19-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/PerformSegmentation.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1075	yes	; 27	PlotSegmentedCopyRatio		21-May	https://github.com/broadinstitute/gatk-protected/blob/e1ffbff498db40c894105c06a41b443859b58a04/src/main/java/org/broadinstitute/hellbender/tools/exome/plotting/PlotSegmentedCopyRatio.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1077	yes	; 5	CallSegments		21-May	https://github.com/broadinstitute/gatk-protected/blob/465d9fe90cd26d55cca440e14cbcf5dd9fd50566/src/main/java/org/broadinstitute/hellbender/tools/exome/CallSegments.java	scripts/cnv_wdl/somatic/cnv_somatic_copy_ratio_bam_workflow.wdl	https://github.com/broadinstitute/gatk-protected/pull/1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3055:2030,Perform,PerformSegmentation,2030,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055,1,['Perform'],['PerformSegmentation']
Performance," 7, 2017 12:48:13 AM UTC] Executing as tianj@ip-xxx-xx-xx-xxx on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.alpha.2-1100-g04dbeb2-SNAPSHOT; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 00:48:13.680 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:48:13.680 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 00:48:13.680 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 00:48:13.680 INFO MarkDuplicatesSpark - Initializing engine; 00:48:13.680 INFO MarkDuplicatesSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4aa298b7] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@37574691].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 00:48:19.247 INFO MarkDuplicatesSpark - Shutting down engine; [June 7, 2017 12:48:19 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=1029701632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 0.0 failed 4 times, most recent failure: Lost task 15.3 in stage 0.0 (TID 59, 172.31.77.139, ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:3903,load,loaded,3903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['load'],['loaded']
Performance, : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:18:22.403 INFO ProgressMeter - chr1:75065650 0.2 250240 1501440.0; 12:18:29.713 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.009098343; 12:18:29.713 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.121747383; 12:18:29.713 INFO SmithWatermanAligner - Total compute ti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:2963,multi-thread,multi-threaded,2963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['multi-thread'],['multi-threaded']
Performance, ; at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:119) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:156) ; ; at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27) ; ; at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.loadNextRead(ReadFilteringIterator.java:53) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:47) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadFilteringIterator.next(ReadFilteringIterator.java:13) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadTransformingIterator.next(ReadTransformingIterator.java:14) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.fillCache(PushToPullIterator.java:72) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58) ; ; at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.next(PushToPullIterator.java:52) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadCachingIterator.next(ReadCachingIterator.java:42) ; ; at org.broadinstitute.hellbender.utils.iterators.ReadCachingIterator.ne,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:11124,load,loadNextRead,11124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['load'],['loadNextRead']
Performance," > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6689,concurren,concurrent,6689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance, Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:12354,cache,cache,12354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance," BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Åuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Åuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Åuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO Â NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN Â NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO Â NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN Â NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO Â BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO Â BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO Â BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO Â BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO Â BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:11:11.813 INFO Â BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:11:11 AM CST ; ; 00:11:11.8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:9463,Load,Loading,9463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['Load'],['Loading']
Performance, Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:28:26.332 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:30:24.296 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:4->5; > 21:30:24.299 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > . Here's,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:6117,cache,cache,6117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance, Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Ref haplotype coords chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engine - Haplotype count 1; 11:35:45.413 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:45.414 DEBUG Mutect2Engine - Kmer sizes values []; 11:35:45.737 DEBUG Mutect2 - Processing assembly region at chrM:1398-1697 isActive: false numReads: 2722; 11:35:45.837 DEBUG Mutect2 - Processing assembly region at chrM:1698-1997 isActive: false numReads: 0; 11:35:45.999 DEBUG Mutect2 - Processing assembly region at chrM:1998-2297 isActive: false numReads: 0; 11:35:46.219 DEBUG Mutect2 - Processing assembly region at chrM:2298-2543 isActive: false numReads: 2555; 11:35:46.674 DEBUG Mutect2 - Processing assembly region at chrM:2544-2841 isActive: true numReads: 5108; 11:35:48.094 DEBUG ReadThreadingGraph - Recovered 17 of 20 dangling tails; 11:35:48.198 DEBUG ReadThreadingGraph - Recovered 16 of 50 dangling heads; 11:35:48.511 DEBUG IntToDoubleFunctionCache - cache miss 2389 > 10 expanding to 2399; 11:35:48.874 DEBUG Mutect2Engine - Active Region chrM:2544-2841; 11:35:48.874 DEBUG Mutect2Engine - Extended Act Region chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Ref haplotype coords chrM:2444-2941; 11:35:48.875 DEBUG Mutect2Engine - Haplotype count 128; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes count 0; 11:35:48.875 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:08.907 INFO ProgressMeter - chrM:2544 0.4 10 22.3; 11:36:08.954 DEBUG Mutect2 - Processing assembly region at chrM:2842-2920 isActive: false numReads: 4726; 11:36:09.094 DEBUG Mutect2 - Processing assembly region at chrM:2921-3202 isActive: true numReads: 4600; 11:36:09.663 DEBUG ReadThreadingGraph - Recovered 1 of 2 dangling tails; 11:36:09.671 DEBUG ReadThreadingGraph - Recovered 4 of 7 dangling heads; 11:36:09.750 DEBUG Mutect2Engine - Active Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:9348,cache,cache,9348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance," FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT in gatk.public.repo.local (file:../..",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:2048,Queue,Queue,2048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance," HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3311,perform,performance,3311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance," HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 16:17:05.844 INFO HaplotypeCaller - Deflater: JdkDeflater. 16:17:05.844 INFO HaplotypeCaller - Inflater: JdkInflater. 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20. 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled. 16:17:05.845 INFO HaplotypeCaller - Initializing engine. 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf. 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals. 16:17:06.551 INFO HaplotypeCaller - Done initializing engine. 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:4051,Load,Loading,4051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['Load'],['Loading']
Performance," I am working on GATK VariantsToTable tool and my VCF file consists of 12 chromosomes but the output shows only one chromosome. Could you please help me out.; OS:-Ubuntu 20.04; GATK version:-4.1.9.0; Java:-open jdk version 11.0.8; Command:-gatk VariantsToTable -R '/home/india/Downloads/Reference.fasta' -V '/home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf' -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table. Using GATK jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar VariantsToTable -R /home/india/Downloads/Reference.fasta -V /home/india/Downloads/Galaxy57-[Merged_file.vcf].vcf -F CHROM -F POS -F REF -F ALT -GF AD -GF DP -GF GQ -GF PL -O bothbulks_new.table; 16:46:03.294 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/india/Downloads/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 16, 2020 4:46:04 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:46:04.315 INFO VariantsToTable - ------------------------------------------------------------; 16:46:04.316 INFO VariantsToTable - The Genome Analysis Toolkit (GATK) v4.1.9.0; 16:46:04.316 INFO VariantsToTable - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:46:04.317 INFO VariantsToTable - Executing as india@india-HP-ProBook-445-G1 on Linux v5.4.0-26-generic amd64; 16:46:04.317 INFO VariantsToTable - Java runtime: OpenJDK 64-Bit Server VM v11.0.8+10-post-Ubuntu-0ubuntu120.04; 16:46:04.317 INFO VariantsToTable - Start Date/Time: 16 October 2020 at 4:46:02 PM IST; 16:46:04.318 INFO VariantsToTable - -------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6897:1006,Load,Loading,1006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6897,1,['Load'],['Loading']
Performance," I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purposes where you need to run M2 a lot and don't need perfection:. * making an M2 panel of normals; * making true positives + false positives training data sets for artifact classifiers; * testing changes. ---. @ldgauthier commented on [Mon Mar 06 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284502170). ""Quick and dirty"" would be useful for testing changes, but the PoN and training sets shouldn't be recreated very often so there's less savings. I hate to leverage the fact that we break phasing to optimize things because I dream of a future where HaplotypeCaller actually calls haplotypes (as you've already added an issue for).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945:3348,optimiz,optimize,3348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945,1,['optimiz'],['optimize']
Performance," INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 3, 2017 5:27:51 AM UTC] Executing as centos@master.novalocal on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11; Version: 4.beta.5; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 05:27:52.642 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 05:27:52.642 INFO PrintReadsSpark - Deflater: IntelDeflater; 05:27:52.642 INFO PrintReadsSpark - Inflater: IntelInflater; 05:27:52.643 INFO PrintReadsSpark - GCS max retries/reopens: 20; 05:27:52.643 INFO PrintReadsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 05:27:52.643 INFO PrintReadsSpark - Initializing engine; 05:27:52.643 INFO PrintReadsSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@dcf3e99] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@61df66b6].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```; I can run command using the spark-shell but somehow GATK4 fails. Any idea?. thank you very much",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3651:3601,load,loaded,3601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3651,2,['load'],['loaded']
Performance, INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 15:16:41.066 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 15:16:41.083 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 15:16:41.540 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Familial_Cancer_Genes.no_dupes.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/familial/hg38/Familial_Cancer_Genes.no_dupes.tsv; > 15:16:41.545 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 15:16:41.556 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.575 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.config; > 15:16:41.707 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > 15:16:41.709 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-17 15:16:41 AsciiLineReader Creating an indexable source for an ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:9824,cache,cache,9824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['cache'],['cache']
Performance," INFO HaplotypeCaller - Inflater: IntelInflater; 12:18:42.093 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:18:42.093 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /beegfs/work/iiipe01/Exome-Test/work/1e/fc972c6b14c8006857230849630a49/hs_err_pid85482.log; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:3597,Load,Loading,3597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Load'],['Loading']
Performance," INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2568,Load,Loading,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1985:1613,load,load,1613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985,1,['load'],['load']
Performance," JdkInflater. 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20. 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled. 16:17:05.845 INFO HaplotypeCaller - Initializing engine. 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater. 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf. 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals. 16:17:06.551 INFO HaplotypeCaller - Done initializing engine. 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.590 **WARN** IntelPairHmm - Intel G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:4239,load,load,4239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['load'],['load']
Performance," Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING imple",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:2288,load,load,2288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,1,['load'],['load']
Performance," ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.051 INFO GenomicsDBImport - Done importing batch 1/1; 05:39:42.060 INFO ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.061 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 2531.4 minutes.; 05:39:42.061 INFO GenomicsDBImport - Import completed!; 05:39:42.061 INFO GenomicsDBImport - Shutting down engine; [January 16, 2021 5:39:42 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2237,optimiz,optimizations,2237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['optimiz'],['optimizations']
Performance," ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:551); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:202); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:6172,concurren,concurrent,6172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['concurren'],['concurrent']
Performance," QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle 15 TB /table /day import limit (#7167); - #260 filter out AS_QD, SOR, FS from cohort extract VCF (#7173); - Full scientific validation via end to end comparison of filtered results between WARP and BQ (#7179); - Cherry pick of commits to fix GATK tests from master (#7183); - ExtractCohort supports -XL exclusion and follows intervals, other optimizations (#7181); - ExtractFeatures supports -XL exclusion and follows intervals, other optimizations (#7184); - change 0/0 GQ0 sites to nocalls (#7190); - updated (#7195); - Rename ""metadata"" table to ""sample_info"" table, fix vet schema (#7196); - Allow users to specify VQSLOD sensitivity and apply threshold in ExtractCohort (#7194); - Calculate and Store site-level QCs (#7197); - Filter Failing QC Sites from Extract (#7201); - WDLize GvsPrepareCallset (briefly known as CreateCohortTable) (#7200); - default drop_state to 60, but allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:12928,optimiz,optimizations,12928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['optimiz'],['optimizations']
Performance, Region chrM:2921-3202; 11:36:09.750 DEBUG Mutect2Engine - Extended Act Region chrM:2821-3302; 11:36:09.750 DEBUG Mutect2Engine - Ref haplotype coords chrM:2821-3302; 11:36:09.751 DEBUG Mutect2Engine - Haplotype count 32; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:09.751 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:14.909 DEBUG Mutect2 - Processing assembly region at chrM:3203-3502 isActive: false numReads: 2398; 11:36:15.137 DEBUG Mutect2 - Processing assembly region at chrM:3503-3702 isActive: false numReads: 2587; 11:36:15.184 DEBUG Mutect2 - Processing assembly region at chrM:3703-3943 isActive: true numReads: 5164; 11:36:15.511 DEBUG ReadThreadingGraph - Recovered 3 of 5 dangling tails; 11:36:15.517 DEBUG ReadThreadingGraph - Recovered 1 of 5 dangling heads; 11:36:15.911 DEBUG ReadThreadingGraph - Recovered 34 of 41 dangling tails; 11:36:15.932 DEBUG ReadThreadingGraph - Recovered 13 of 31 dangling heads; 11:36:15.995 DEBUG IntToDoubleFunctionCache - cache miss 2401 > 2399 expanding to 4800; 11:36:16.347 DEBUG Mutect2Engine - Active Region chrM:3703-3943; 11:36:16.348 DEBUG Mutect2Engine - Extended Act Region chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Ref haplotype coords chrM:3603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:11212,cache,cache,11212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance, [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMCompute,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2568,cache,cachemanager,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance," \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || | ; 12:11:32.828 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:17932,cache,cache,17932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache']
Performance," `HaplotypeCaller` and `Mutect2`. Probably HTSJDK level issue, but popped up exception is kind of misleading. #### Stacktrace:; ```; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:863); at htsjdk.samtools.MemoryMappedFileBuffer.<init>(MemoryMappedFileBuffer.java:23); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:931); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:550); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:417); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:130); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:69); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.prepareIteratorsForTraversal(ReadsPathDataSource.java:412); at org.broadinstitute.hellbender.engine.ReadsPathDataSource.iterator(ReadsPathDataSource.java:336); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1176); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:384); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:97); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7579:1437,load,loadNextIterator,1437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7579,1,['load'],['loadNextIterator']
Performance, a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.077 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f410396038; 18:15:23.361 INFO FeatureManager - Using codec VCFCodec to read file file:///SCRATCH-BIRD/users/lindenbaum-p/work/NEXTFLOW/20221123.hs38me.NTS299.ultrares/work/34/f4[0/1667]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 IN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:4647,Load,Loading,4647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['Load'],['Loading']
Performance," accounting for 1) sample-specific depth (which determines the means of the negative-binomial distributions), 2) multiplicative contig-specific bias (which is mild, at least for WGS), and 3) additive sample-contig-specific mosaicism or bias (note that the above genotype priors imply that mosaicism/bias on top of a baseline of CN = 2 is the only deviation allowed for the autosomes, which is somewhat restrictive but greatly aids convergence). I put together a pure PyMC3 prototype that seems to work relatively well. Here are the per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:2495,optimiz,optimizer,2495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,2,['optimiz'],['optimizer']
Performance," acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:39 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:11718,queue,queue,11718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,2,['queue'],['queue']
Performance," advance. ```. Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants --version; Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants -R /home/fmbuga/tools/hg38/hg38.fa -V /home/fmbuga/gatk4_gcp_wgs/06_vcf_raw/SRR16299720_dedup_AORRG_recal_raw.vcf -O ./08_vcf_1dCNN/SRR16299720_dedup_AORRG_recal_raw_1dCNN_scored.vcf; 05:39:39.149 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:39:39.304 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.305 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.6.1; 05:39:39.305 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:39:39.305 INFO CNNScoreVariants - Executing as fmbuga@node05.cluster on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 05:39:39.305 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_332-b09; 05:39:39.305 INFO CNNScoreVariants - Start Date/Time: October 9, 2022 5:39:39 AM PDT; 05:39:39.305 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490:1114,Load,Loading,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490,1,['Load'],['Loading']
Performance," array, etc.). This is (we think) done in `VariantContext.fullyDecode().`. This turned out not to be possible for the following reasons. First, there are roughly four types of genotype subsetting you could do:. a) By the sample names (`--sample-name NA12878`); b) JEXL (`--select GQ > 0`); c) JEXL by accessing the variant context object (`--select vc.getGenotype('NA12878').getGQ() > 1`); d) Others (e.g. `--remove-fraction-genotype`). a) does not need ""fully-decode."" It turns out b) was never supported (GATK currently removes all variants and succeed.) And from my experiments, c) does not seem to ever trigger calling `VariantContext.fullyDecode().` In fact the only code path I can see that calls fullyDecode() is by setting the `fully-decode` SelectVariants argument, which seems to just call fullyDecode at the beginning just for the sake of calling it (or so it appears to me. The utility of this command line argument is highly dubious.) . It's possible that apache code does something similar to fully decoding that could affect performance. All that is to say that we cannot achieve performance improvement with our original blueprint simply because this expensive ""fullyDecode"" operation seems to be a mythical operation that is never used in reality. So while I could not speed up SelectVariants, I cleaned up the code and added the following new arguments:. * `--select-genotype`: with this new genotype-specific JEXL argument, we support filtering by genotype fields like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. I have not added the ability to do 'GQ > 0 for all samples' but it should be a simple (but not easyâ€¦) exercise in boolean operations.; * `applyJexlFiltersBeforeFilteringGenotypes`: if set to true, we do the JEXL checking before we subset by samples. In my tests, performance improvement from this option was very modest. Subsetting a ~3k 1kg SV vcf to a single sample was about 30 seconds faster (out of ~20 min total run t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092:1564,perform,performance,1564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092,1,['perform'],['performance']
Performance," as the reference genotype for any site not present in the source workspace. In contrast, if you run a similar command with GenotypeGVCFs + --force-output + gVCF, the resulting VCF reports the correct reference base. In both cases we are passing the FASTA as -R to the tool. Here are two example outputs:. This is from GenotypeGVCFs with a GenomicsDB workspace as input. Note: 1565827 and 1565828 are wild-type in all samples, but are included b/c of --force-output. It reports N as REF:; #CHROM	POS	ID	REF	ALT; 1	1565827	.	N	.; 1	1565828	.	N	.; 1	1565829	.	T	TGATGGTGGC. This is from a similar command with a gVCF as input. The REF base is correct. Note, the ALT is different b/c of different samples in the inputs:; #CHROM	POS	ID	REF	ALT; 1	1565827	.	G	.; 1	1565828	.	A	.; 1	1565829	.	T	. But the key behavior difference is that if this is a site that would only get output b/c of --force-output, then when a gVCF is the source it gets the REF right, and when GenomicsDB is the source it reports N. I am happy to do some legwork digging into the code and proposing a fix, but it would be helpful to know if there's a known issue around this, and/or get any pointers into where in the GATK/HTSJDK layer I might start looking. An example command is something like:. ```; java8 -jar <JAR> GenotypeGVCFs \; 	-R $REF \; 	-O $OUTPUT \; 	--variant $GENOMICS_DB_WORKSPACE \; 	-L 1:1565827-1565829 \; 	-L 1:1699262-1699298 \; 	-L 6:14972856-14972872 \; 	-L X:135349386-135349395 \; 	-L Y:3491100-3491102 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations \; 	--force-output-intervals <BED_FILE>. ```. In this example, those -L positions were selected b/c they elicit the behavior. Those sites are present in the BED file used for --force-output-intervals. At least in our hands, the combination of calling from a GenomicsDB workspace and using --force-output-intervals to output some site that would not normally get output will result in the 'N' REFs.; Thanks,; Ben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005:1900,optimiz,optimizations,1900,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005,1,['optimiz'],['optimizations']
Performance, at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.inter,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9165,Cache,CacheStep,9165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance," cases where the haplotype has multiple SNPs and the phasing is poor, this could artificially inflate the likelihoods. Although we've seen that the graph traversal frequently breaks phasing then generating haplotypes anyway, so maybe I overestimate our current likelihood accuracy. Anyway, take my advice with a grain of salt. It's just some musings from a bored and somewhat sleep-deprived mom with a sleeping baby on her lap. ---. @davidbenjamin commented on [Fri Mar 03 2017](https://github.com/broadinstitute/gatk-protected/issues/909#issuecomment-284024760). > I think the main blocker in implementing it would be the complexity of the existing code, though I don't know how much you cleaned up the GATK4 version. We refactored all the engine stuff shared with `HaplotypeCaller` to be very distinct from the somatic genotyping logic, so the only complexity would be in local assembly and PairHMM. Which could be significant, of course. > We would also want to make sure that we only implement this optimization if that SNP is the only SNP on the haplotype. . .Although we've seen that the graph traversal frequently breaks phasing. The specific case I had in mind is when you have a bubble or something more complex in the graph, followed by a stretch of reference (i.e. all haplotypes have nothing going on here), followed (or not) by more activity. It seems reasonable in that case to chop each active area into its own haplotype(s), which is equivalent to pinning the ref-only area to be ref-only in PairHMM. I believe but could be wrong that in a case like this our assembly would not respect phasing between the two active areas anyway, so we lose nothing. By the way, I should clarify that the idea is not to truncate the `ActiveRegion`, but rather to break it into a few small haplotypes semi-intelligently *after* building the whole deBruijn graph. It could well be that my optimism is ill-founded. Nonetheless, having a quick-and-dirty mode would be very useful for the following purpose",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945:1820,optimiz,optimization,1820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945,1,['optimiz'],['optimization']
Performance, chrM:7772-8071 isActive: false numReads: 359; 11:39:12.636 INFO ProgressMeter - chrM:7772 3.5 30 8.5; 11:39:12.638 DEBUG Mutect2 - Processing assembly region at chrM:8072-8371 isActive: false numReads: 0; 11:39:27.522 DEBUG IntToDoubleFunctionCache - cache miss 9173 > 5354 expanding to 10710; 11:39:31.241 DEBUG Mutect2 - Processing assembly region at chrM:8372-8671 isActive: false numReads: 0; 11:39:43.892 DEBUG Mutect2 - Processing assembly region at chrM:8672-8829 isActive: false numReads: 148658; 11:39:47.277 DEBUG IntToDoubleFunctionCache - cache miss 92836 > 47638 expanding to 95278; 11:40:02.830 DEBUG Mutect2 - Processing assembly region at chrM:8830-9129 isActive: true numReads: 296990; 11:41:56.997 DEBUG ReadThreadingGraph - Recovered 7 of 8 dangling tails; 11:41:57.047 DEBUG ReadThreadingGraph - Recovered 2 of 24 dangling heads; 11:41:57.286 DEBUG IntToDoubleFunctionCache - cache miss 136737 > 53234 expanding to 136747; 11:41:57.301 DEBUG IntToDoubleFunctionCache - cache miss 136976 > 136747 expanding to 273496; 11:41:57.935 DEBUG Mutect2Engine - Active Region chrM:8830-9129; 11:41:57.937 DEBUG Mutect2Engine - Extended Act Region chrM:8730-9229; 11:41:57.939 DEBUG Mutect2Engine - Ref haplotype coords chrM:8730-9229; 11:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotyp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:15701,cache,cache,15701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance," class(es). All Spark tools that takes parameter `-L`. ### Affected version(s); - [x] Latest public release version [4.0.4.0]; - [x] Latest master branch as of [2018-06-30]. ### Description . When running a Spark tool and passing in interval arguments via the standard `-L` argument, if the interval file (only BED file is tested) is stored in HDFS, we see errors like below. ```; org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: Badly formed genome unclippedLoc: Query interval ""hdfs://shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-m:8020/data/merged_commonFPDel.bed"" is not valid for this input.; 	at org.broadinstitute.hellbender.utils.GenomeLocParser.getUnambiguousInterval(GenomeLocParser.java:350); 	at org.broadinstitute.hellbender.utils.GenomeLocParser.parseGenomeLoc(GenomeLocParser.java:309); 	at org.broadinstitute.hellbender.utils.IntervalUtils.parseIntervalArguments(IntervalUtils.java:300); 	at org.broadinstitute.hellbender.utils.IntervalUtils.loadIntervals(IntervalUtils.java:226); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.parseIntervals(IntervalArgumentCollection.java:174); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getTraversalParameters(IntervalArgumentCollection.java:155); 	at org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection.getIntervals(IntervalArgumentCollection.java:111); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeIntervals(GATKSparkTool.java:514); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4852:1024,load,loadIntervals,1024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852,1,['load'],['loadIntervals']
Performance," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:759,optimiz,optimized,759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,1,['optimiz'],['optimized']
Performance, configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:1733,cache,cache,1733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance," do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils348417925139",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:1926,multi-thread,multi-threaded,1926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,1,['multi-thread'],['multi-threaded']
Performance," for any reference. This allows us (i) to call on ALT-aware mappings if data is such and (ii) call on SNPs and indels generated by putative structural variants that go _across contigs_. I know that this filter is active in the GATK4.beta.3-Mutect2 (see last line):; ```; WMCF9-CB5:align shlee$ gatk-launch Mutect2 -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -I hcc1143_N_subset500.bam -tumor HCC1143_normal -O 1_normalforpon.vcf.gz; Using GATK jar /Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar Mutect2 -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -I hcc1143_N_subset500.bam -tumor HCC1143_normal -O 1_normalforpon.vcf.gz; 19:26:43.105 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [August 24, 2017 7:26:43 PM EDT] Mutect2 --tumorSampleName HCC1143_normal --output 1_normalforpon.vcf.gz --input hcc1143_N_subset500.bam --reference /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta --genotypePonSites false --af_of_alleles_not_in_resource 0.001 --log_somatic_prior -6.0 --tumor_lod_to_emit 3.0 --initial_tumor_lod 2.0 --max_population_af 0.01 --normal_lod 2.2 --annotation Coverage --annotation DepthPerAlleleBySample --annotation TandemRepeat --annotation OxoGReadCounts --annotation ClippedBases --annotation ReadPosition --annotation BaseQuality --annotation MappingQuality --annotation FragmentLength --annotation StrandArtifact --dontTrimActiveRegions false --maxDiscARExtension 25 --maxGGAARExtension 300 --paddingAroundIndels 150 --paddingAroundSNPs 20 --kmerSize 10 --kmerSize 25 --dontI",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3514:1770,Load,Loading,1770,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3514,1,['Load'],['Loading']
Performance," from TumorNormalizeSomaticReadCounts contained NaNs; 2. TumorPerformSeg threw the following error:. ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp; [February 16, 2017 3:23:02 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation --tangentNormalized /cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output small_NA12878.seg --log2Input true --alpha 0.01 --nperm 10000 --pmethod hybrid --minWidth 2 --kmax 25 --nmin 200 --eta 0.05 --trim 0.025 --undoSplits none --undoPrune 0.05 --undoSD 3 --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [February 16, 2017 3:23:02 PM UTC] Executing as root@3addd2d7b373 on Linux 3.16.0-0.bpo.4-amd64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: Version:c17c8ed-SNAPSHOT; [February 16, 2017 3:23:04 PM UTC] org.broadinstitute.hellbender.tools.exome.PerformSegmentation done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=185597952; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/cromwell_root/tmp/root/Rlib.5210694187065743072';source('/cromwell_root/tmp/root/CBS.8616708738798684646.R'); --args --sample_name=NA12878 --targets_file=/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:1326,Perform,PerformSegmentation,1326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,1,['Perform'],['PerformSegmentation']
Performance," go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2h5MhZ7nXrNgo6MrFpMD-TGiAE8ks5tt8gjgaJpZM4TtVZZ>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:2359,optimiz,optimization,2359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['optimiz'],['optimization']
Performance," implemented in sl_purity_ploidy_mcmc branch. Could stand some refactoring and code cleanup before it is PR ready and needs tests.; - [x] Algorithm improvements; - Currently, the model is initialized assuming a 50-50 normal-tumor split and only a clonal population. This is run for ~100 MCMC iterations, and the result is used to initialize a second run that expands the number of populations. This tends to work reasonably well, but there are situations where the model can get stuck in incorrect, degenerate solutions. Going to try adding some MH steps that will swap populations to see if these can help get the model unstuck.; - Need to add outlier absorption to the model, which appears to be critical for inference of subclonal populations from real data (i.e., ACNV output), which may have spurious segments, oversegmentation, etc. Simple clonal models appear to work reasonably well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:1618,Queue,Queue,1618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['Queue'],['Queue']
Performance," in stage 0.0 (TID 0, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 1]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on xx.xx.xx.xx, executor 0: java.lang.IllegalStateException (unread block data) [duplicate 2]; 18/04/23 20:42:02 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0, partition 0, PROCESS_LOCAL, 4956 bytes); 18/04/23 20:42:02 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on xx.xx.xx.xx, executor 0: java.lang.IllegalStat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:13807,concurren,concurrent,13807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['concurren'],['concurrent']
Performance," in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/23 20:42:02 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 11.814317 s; 18/04/23 20:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/23 20:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/23 20:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/23 20:42:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/23 20:42:03 INFO MemoryStore: MemoryStore cleared; 18/04/23 20:42:03 INFO BlockManager: BlockManager stopped; 18/04/23 20:42:03 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/23 20:42:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/23 20:42:03 INFO SparkContext: Successfully stopped SparkContext; 20:42:03.045 INFO PathSeqPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:16330,concurren,concurrent,16330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['concurren'],['concurrent']
Performance," in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:18669,concurren,concurrent,18669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['concurren'],['concurrent']
Performance," in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before inference optimizations, assuming we address #5716 to minimize disk costs. @asmirnov239 can you review? And maybe you can address dCR output in PostprocessGermlineCNVCalls and expose the number of samples in a separate PR? We can make some further changes to the dCR format there if we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:1475,scalab,scalability,1475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,6,"['optimiz', 'perform', 'scalab']","['optimization', 'optimizations', 'optimized', 'performance', 'scalability']"
Performance," increased dictionary size, because they are more repetitive than the DNA data. And shorter BAMs would be different because they are less repetitive (usually less coverage), so their compression relies more on CPU-expensive crunching of the ""2bit nature"" of the DNA.; So they might logically suffer more from a lower compression level.; It might be instructive to compare compression sizes of raw sequence data of two BAM files with output of faToTwoBit / 2.bit files.; 2bit files are compressed by a factor of around four, which gzip often does not reach (because it doesn't know ahead of time that DNA has only four letters).; Use reference genome fasta as proxy for nearly no repetition at all. It doesn't compress much beyond 2bit. Tweaking of the Huffmann coding etc. might have influenced the compression level much in this case, by ""giving the compressor a subtle hint about the four letters"".; Paradoxically, Intel might have optimized for average data and thus brought a disadvantage for the four letter nature of DNA (and also the few letters used in quality data encoding compared to text). 3. BQSR:; When I did interleaving compression experiments, I noticed that the BQSR step decreases compressiblity considerably.; In this example I had the same BAM file in different versions that were aligned to hs38DH, hs38, hs37d5 and could compress them to nearly the size of one, by putting similar pieces of the files after one another.; Adding the same BAM with BQSR increased final file size more than several pre-BQSR versions together.; Note: This piece-meal packing might be useful for different BAMs mostly only with many BAMs where similar regions accumulate. 4. Even faster:; In my experience, level 0 (no compression) (with samtools view -u) increases speed even more, if files are on a lz4 encrypted disk (such as with ZFS).; The speed-up of lz4 over even level 1 of any gzip-like compression is substantial.; With data on SSDs or similarly fast storage, that can make a huge differenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673:2700,optimiz,optimized,2700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673,1,['optimiz'],['optimized']
Performance," is specific to human data. Funcotator could be useful for users with non-human data if there is a workaround for these errors. ### GATK Information; GATK 4.1.9.0; gatk IndexFeatureFile -I gencode.vM25.annotation.gtf; This request was created from a contribution made by T. Li on January 25, 2021 04:37 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076815852-Error-Running-IndexFeatureFile-on-Ensembl-Mouse-GTF-file-](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076815852-Error-Running-IndexFeatureFile-on-Ensembl-Mouse-GTF-file-). #### Error Log. ```; Using GATK jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar IndexFeatureFile -I gencode/mm10/gencode.vM25.annotation.gtf ; ; 04:33:13.081 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Jan 25, 2021 4:33:13 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 04:33:13.195 INFO IndexFeatureFile - ------------------------------------------------------------ ; ; 04:33:13.195 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT ; ; 04:33:13.195 INFO IndexFeatureFile - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 04:33:13.195 INFO IndexFeatureFile - Executing as root@b4c480938d0d on Linux v5.4.0-1029-aws amd64 ; ; 04:33:13.195 INFO IndexFeatureFile - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08 ; ; 04:33:13.195 INFO IndexFeatureFile - Start Date/Time: January 25, 2021 4:33:13 AM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7054:1133,Load,Loading,1133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7054,1,['Load'],['Loading']
Performance," is that it is a ppc64le system. When I use HaplotypeCaller, I see the following messages on the screen:. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100. 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so. 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so. 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------. 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1. 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/. 16:17:05.843 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:1182,Load,Loading,1182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['Load'],['Loading']
Performance, java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:298); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.jav,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4625,concurren,concurrent,4625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance," localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:9320,concurren,concurrent,9320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance," long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it's own. . If we decided to do this it would make sense to make `modifyProviders` use the same synchronization conditions as the actual `FileSystemProvider` loading, in order to not have any race condition",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312:2185,load,loadInstalledProviders,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312,1,['load'],['loadInstalledProviders']
Performance," max retries/reopens: 20; 09:01:25.951 INFO HaplotypeCaller - Requester pays: disabled; 09:01:25.952 INFO HaplotypeCaller - Initializing engine; 09:01:26.059 INFO HaplotypeCaller - Done initializing engine; 09:01:26.060 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:01:26.067 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output; 09:01:26.067 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:01:26.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:01:26.078 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ywt/anaconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:01:26.089 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:01:26.089 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:01:26.090 INFO IntelPairHmm - Available threads: 36; 09:01:26.090 INFO IntelPairHmm - Requested threads: 4; 09:01:26.090 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:01:26.121 INFO ProgressMeter - Starting traversal; 09:01:26.121 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:01:26.406 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 1A:145 and possibly subsequent; at least 10 samples must have called genotypes; 09:01:33.373 WARN DepthPerSampleHC - Annotation will not be calculated at position 1A:1702502 and possibly subsequent; genotype for sample",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192:3658,Load,Loading,3658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192,1,['Load'],['Loading']
Performance," may occur.; 06:42:41.665 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.annotation.REORDERED.gtf; 06:42:41.666 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 28) (given: 34): ##description: evidence-based annotation of the human genome (GRCh38), version 34 (Ensembl 100) Continuing, but errors may occur.; 06:42:41.691 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.pc_transcripts.fa -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencode/hg38/gencode.v34.pc_transcripts.fa; 06:42:46.805 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.805 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 06:42:46.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:46.951 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/clinvar_20180429_hg38.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.023 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:47.098 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:47.107 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:7685,cache,cache,7685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['cache'],['cache']
Performance, more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15765,cache,cache,15765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance," normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actuall",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1105,perform,performed,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['performed']
Performance, org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:4489,concurren,concurrent,4489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,2,['concurren'],['concurrent']
Performance, org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15653,cache,cache,15653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance," phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5381890 1.0 22460 22033.3; 14:51:31.097 INFO ProgressMeter - chr17:6474462 1.2 27070 22821.4; 14:51:41.535 INFO ProgressMeter - chr17:7455949 1.4 31150 22902.4; 14:51:51.542 INFO ProgressMeter - chr17:8073825 1.5 33820 22149.2; 14:52:01.549 INFO ProgressMeter - chr17:9138632 1.7 38220 22566.0; 14:52:11.962 INFO ProgressMeter - chr17:10514361 1.9 43840 23478.4; 14:52:21.975 INFO ProgressMeter - chr17:11679575 2.0 48560 23872.6; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:7233,multi-thread,multi-threaded,7233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['multi-thread'],['multi-threaded']
Performance," physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5872,load,load,5872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance," reads (with reads below 10 bases in length being removed) have their base qualities farther modified in `PairHMMLikelihoodCalculationEngine.createQualityModifiedRead()` in various ways. This modification does not stick however since the base qualities are all modified on a clean partial copy of the read.; 4. Following this the reads (the ones from step 2) are realigned to the reference according to their best haplotypes. Sometimes this means as few as 11 bases of ""read"" are being realigned at this stage. . It is these realigned reads that are used for genotyping, where the only reads that are actually used to contribute likelihoods for calls are reads that overlap the variant event within 2 bases of overlap on either side. In DRAGEN they do something different that we had to replicate to achieve concordance. Dragen still performs equivalent modifications for steps 1-3 as they apply to the reads but rather than performing step 4 and using those reads for genotype assignment it instead for genotyping reaches back for each read (that has survived filtering) to its original BAM alignment (before being unclipped/hardclipped) and uses those reads for FRD/BQD calling. When running GATK with the new argument `--use-original-alignments-for-genotyping-overlap` this is what happens as well (step 4 is skipped entirely in addition). The results were somewhat surprising (listed below): . ![RealignmentPlotIndels](https://user-images.githubusercontent.com/16102845/87588690-13fc4680-c6b2-11ea-98e9-4c69259c2869.png); ![RealignmentPlotSNPs](https://user-images.githubusercontent.com/16102845/87588692-1494dd00-c6b2-11ea-96dc-ba06f45357c2.png). This says that running GATK in DRAGEN mode without realigning reads performs slightly better for low complexity region SNPs than it does with realignment. This could perhaps be a side effect of the BQD algorithm as it cares about the specific bases that are applied for SNPs. I have theorized that perhaps the explanation for this behavior has to d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6706:1887,perform,performs,1887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6706,2,['perform'],"['performing', 'performs']"
Performance," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2269,perform,performed,2269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['performed']
Performance," reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:1019,concurren,concurrent,1019,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance," sc.setLogLevel(newLevel).; Failed to created SparkJLineReader: java.io.IOException: Permission denied; Falling back to SimpleReader.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command â€./gradlew bundleâ€ï¼Œ it appeared the error in the last ï¼Œdid this matterï¼Ÿ**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2049,Load,Loaded,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['Load'],['Loaded']
Performance, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5145:2527,concurren,concurrent,2527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145,2,['concurren'],['concurrent']
Performance," scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, com2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.; 17/10/11 14:19:28 INFO storage.BlockManagerMaster: Removal of executor 1 requested; 17/10/11 14:19:28 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 1; 17/10/11 14:19:28 INFO spark.ExecutorAllocationManager: Existing executor 1 has been removed (new total is 0); 17/10/11 14:19:35 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (com2:35590) with ID 2; 17/10/11 14:19:35 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 1.0 (TID 3, com2, executor 2, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:35",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:21092,concurren,concurrent,21092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance," scheduler.TaskSetManager: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:28105,concurren,concurrent,28105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance, shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5992,concurren,concurrent,5992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1977,optimiz,optimization,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['optimiz'],['optimization']
Performance," should proceed here, if at all? @ldgauthier reminded me that this story was unfinished and is getting a little stale. @fleharty take note if we want to report progress on this front to our MalariaGEN collaborators. On my end, there are a couple of things to do:; - [x] rebase and resolve conflicts; - [x] change TSV input as discussed above; - [x] add doc strings for new arguments; - [x] add integration tests to make absolutely sure exposure was done correctly, perhaps? I'm open to discussion about how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/present",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:934,optimiz,optimizations,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizations']
Performance," spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.730 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1896,Load,Loading,1896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['Load'],['Loading']
Performance," src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x80) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; src/main/java/org/broadinstitute/hellbender/tools/walkers/groundtruth/GroundTruthScorer.java:68: error: unmappable character (0x99) for encoding US-ASCII; * <li>Score : A flow-based alignment score. Since the alignment is per-flow, in the case that there???s a cycle skip, the read and reference flow signals will not be aligned, and therefore the score will be inaccurate.</li>; ^; ```. This test is skipped without any apparent reason:; ```; Running Test: Test method loadIndex(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > loadIndex FAILED; java.lang.UnsatisfiedLinkError: 'boolean org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(java.lang.String, java.lang.String, java.lang.String)'; at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createReferenceIndex(Native Method); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:227); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:196); at org.broadinstitute.hellbender.BwaMemIntegrationTest.loadIndex(BwaMemIntegrationTest.java:49); Running Test: Test method testChimericUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testChimericUnpairedMapping SKIPPED; Running Test: Test method testPerfectUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:2137,load,loadIndex,2137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['load'],['loadIndex']
Performance," stage 0.0 (TID 59, 172.31.77.139, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:5870,concurren,concurrent,5870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['concurren'],['concurrent']
Performance," still generating the error which it does not (even on my side). However, my few tests made today resulted in interesting observations. ## Traces. Below the command and trace produced from my real case. I annonymized it but the number of characters in path were kept. ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 16:48:58 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; D",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:1199,Load,Loading,1199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['Load'],['Loading']
Performance," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:11392,load,loading,11392,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,12,"['Load', 'Perform', 'load']","['LoadBigQueryData', 'Perform', 'load', 'loading']"
Performance," the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1632,perform,performance,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['perform'],['performance']
Performance," the log file: ```*** Error in `javaâ€™: munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***```. The respective backtraces: . ```; *** Error in `java': double free or corruption (out): 0x00007f6364699340 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f636ba307e5]; /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f636ba3937a]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f636ba3d53c]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f63123c8fa8]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f63123c8bf8]; [0x7f6355bff192]; ```. ```; *** Error in `java': munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f68634c37e5]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7f68634d0698]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f6830cf2fa8]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f6830cf2bf8]; [0x7f684dc31f92]; ```. In each of these occurrences, the filtered vcf file was produced, but the vcf.idx file was missing. Although the java errors occur, the last line of the log denotes the step as a success: (This might be true, but only when the option --create-output-variant-index is set to false.; `SetOperationStatus(copied 0 file(s) to <destinations_folder> succeeded""`. I also performed a test based on machine type. (outside of the full workflow, starting the steps on my own on a separate instance & replicating the steps of the workflow); - Using an instance with 2 vCPU's, 7.5 GB of ram, just ran out of memory.; - Using an instance with 8 vCPU's, 30 GB of ram finished successfully, producing both the filtered vcf & vcf.idx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262:2481,perform,performed,2481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262,1,['perform'],['performed']
Performance," this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['performance']
Performance," to me that the issue is due to memory management. Then I realized several test, by increasing both Java machine memory and docker job memory (10G) wich seems enough for mutect2 and my job was still failling. I also try to change CPU parameters but nothing change.; We did more test and try to run the same command with 2 others version of gatk (4.1.9.0 and 4.1.0.0). The job failed in 4.1.9.0 with the same log than 4.1.4.0 but the version 4.1.0.0 ran successfully. #### Steps to reproduce; you will find the command below. I'am not aware of the confidentiality about my input. If i can i will send it to you if needed.; `java -Xmx4000m -Xms4000m -XX:ParallelGCThreads=1 -XX:+AggressiveHeap -jar /usr/share/java/gatk-package-4.1.4.1-local.jar Mutect2 --smith-waterman FASTEST_AVAILABLE -I WES-T_S7_chr_1_bqsr.bam -I WGS-C_S12_chr_1_bqsr.bam -normal WGS-C -L 1 -O OUTPUT -R GRCh38.92.fa`; #### Expected behavior; _Tell us what should happen_. ### Description. > 10:29:22.302 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 10:29:22 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:29:22.407 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.408 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.1; 10:29:22.408 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:29:22.408 INFO Mutect2 - Executing as spim@992fbecc5b50 on Linux v3.10.0-693.el7.x86_64 amd64; 10:29:22.408 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v11.0.6+10-post-Debian-1deb10u1; 10:29:22.408 INFO Mutect2 - Start Date/Time: January 6, 2021 at 10:29:22 AM UTC; 10:29:22.408 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.408 INFO Mutect2 - --------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:1419,Load,Loading,1419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['Load'],['Loading']
Performance," to messages in stdout. Includes # total records, number of records that were trimmed, # variant records skipped due to ref allele being too long and finally the max-indel-length value that needs to be set to include these in the leftalignandtrim. This is an improvement to previous stdout messaging. Upping max-indel-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 14:03:44.358 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 14:03:44.358 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 14:03:44.359 INFO LeftAlignAndTrimVariants - For support and documentation go ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:6768,Load,Loading,6768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Load'],['Loading']
Performance," to port this code and hook up to our tools via the standardized output writer creation methods. The VCF header lines look like this:. ```; ##GATKCommandLine.SelectVariants=<ID=SelectVariants,Version=3.4-46-gbc02625,Date=""Wed Aug 19 10:29:53 EDT 2015"",Epoch=1439994593766,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=[20:10000000-10250000, 21:10000000-10250000] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=dbsnp_138.b37.vcf) discordance=(RodBind",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2269:1117,perform,performanceLog,1117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2269,1,['perform'],['performanceLog']
Performance," user: hdfs; 17/10/11 14:19:15 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:15 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null); 17/10/11 14:19:15 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507683879816_0006), /proxy/application_1507683879816_0006; 17/10/11 14:19:15 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/11 14:19:16 INFO yarn.Client: Application report for application_1507683879816_0006 (state: ACCEPTED); 17/10/11 14:19:17 INFO yarn.Client: Application report for application_1507683879816_0006 (state: RUNNING); 17/10/11 14:19:17 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.159; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507702753100; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507683879816_0006/; 	 user: hdfs; 17/10/11 14:19:17 INFO cluster.YarnClientSchedulerBackend: Application application_1507683879816_0006 has started running.; 17/10/11 14:19:17 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34044.; 17/10/11 14:19:17 INFO netty.NettyBlockTransferService: Server created on 34044; 17/10/11 14:19:17 INFO storage.BlockManager: external shuffle service port = 7337; 17/10/11 14:19:17 INFO storage.BlockManagerMaster: Trying to register BlockManager; 17/10/11 14:19:17 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:34044 with 530.0 MB RAM, BlockManagerId(driver, 10.131.101.159, 34044); 17/10/11 14:19:17 INFO storage.BlockManagerMaster: Registered BlockManager; 17/10/11 14:19:17 INFO scheduler.EventLoggingListener: Logging ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:8063,queue,queue,8063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['queue'],['queue']
Performance," via `-L PATH_TO_BED_FILE`, error is reported; * Scenario 2: run with the WGS bam and give intervals via `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 3: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L PATH_TO_BED_FILE`, no error; * Scenario 4: run with bam that is shrunk from the WGS bam by including reads only in the union of intervals, then with `-L chrX:[0-9]+-[0-9]+`, no error; * Scenario 5: download the shrunken bam to local machine and run `PrintReadsSpark` with `-L PATH_TO_BED_FILE`, no error. Stack trace from scenario 1:; ```; ./gatk PrintReadsSpark \; -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam \; -O hdfs://shuang-small-m:8020/results/temp.bam \; -L hdfs://shuang-small-m:8020/data/intervals.bed \; -- \; --spark-runner GCS \; --cluster shuang-small \; --project broad-dsde-methods. Using GATK jar /Users/shuang/GATK/gatk/build/libs/gatk-spark.jar; found cached jar: gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar. Replacing spark-submit style args with dataproc style args. --cluster shuang-small --project broad-dsde-methods -> --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2. Running:; gcloud dataproc jobs submit spark --cluster shuang-small --project broad-dsde-methods --properties spark.kryoserializ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:1501,cache,cached,1501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['cache'],['cached']
Performance," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:32136,queue,queued,32136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,3,['queue'],['queued']
Performance, | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoT2JqZWN0QXNzZXJ0cy5qYXZh) | `63.636% <84.615%> (+15.249%)` | `9 <3> (+4)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:3220,cache,cachemanager,3220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 01:39:03.364 INFO FilterAlignmentArtifacts - Initializing engine; 01:39:07.644 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5205,multi-thread,multi-threaded,5205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['multi-thread'],['multi-threaded']
Performance,"!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman1809483713436863458.so+0x177e] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x60e; #; # Core dump written. Default location: /cromwell_root/core or core.24; #; # An error report file with more information is saved as:; # /cromwell_root/hs_err_pid24.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:1341,multi-thread,multi-threaded,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['multi-thread'],['multi-threaded']
Performance,!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 11:24:09.944 INFO FilterAlignmentArtifacts - Initializing engine; 11:24:10.534 INFO FeatureManager - Using codec VCFCodec to read file file:///raid/tmp/82/68cd46b704bab21cb8661465e5c2b8/WGS-NA12878.filtered.vcf; 11:24:10.814 INFO FilterAlignmentArtifacts - Done initializing engine; 11:24:10.816 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:24:10.817 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 11:24:10.818 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 11:24:10.818 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 11:24:10.957 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:24:10.980 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:24:10.980 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:24:10.981 INFO IntelPairHmm - Available threads: 80; 11:24:10.981 INFO IntelPairHmm - Requested threads: 4; 11:24:10.981 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:24:10.981 INFO ProgressMeter - Starting traversal; 11:24:10.981 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 11:25:26.222 INFO ProgressMeter - chr1:32527418 1.3 1000 797.5; 11:26:14.235 INFO ProgressMeter - chr1:103944651 2.1 2000 973.6; 11:26:59.367 INFO ProgressMeter - chr1:121884881 2.8 3000 1069.0; 11:28:22.595 INFO ProgressMeter - chr1:124412677 4.2 4000 953.8; 11:30:27.936 INFO ProgressMeter - chr1:146326436 6.3 5000 795.9; 11:31:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8221:3967,Load,Loading,3967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221,1,['Load'],['Loading']
Performance,"![weighted](https://user-images.githubusercontent.com/11076296/97032266-8ab9a300-152f-11eb-8d73-148ff99963be.png). Here is the result of optimizing for sensitivity in the high-confidence, low-compexity region of chr22 in CHM, allowing haplotype-to-reference and read-to-haplotype (match, mismatch, gap open) to range over ([1, 20], [-20, -1], [-20, -1]) and fixing gap extend penalties to -1. The optimal (match, mismatch, gap open) parameters found in this run appear to be:. haplotype-to-reference: 2, -8, -19; read-to-haplotype: 1, -4, -3. I wouldn't put much stock in interpreting these parameters or their exact values for now, but it does appear that the match values and the haplotype-to-reference gap-open penalty might be saturating the bounds of the search. Plots of the type suggested by @dalessioluca might be more illuminating. Compare with default performance:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 9.000 4003 4019 494 1036 0.8905 0.7944 0.8397; None 4009 4025 511 1030 0.8873 0.7956 0.8390; ````. That the corresponding curve with a precision/sensitivity endpoint of (0.8873, 0.7956) above isn't at the top of the pack means that we could squeeze out some extra calls by varying the SW parameters. Of course, this doesn't account for negative impact elsewhere. One could imagine writing a loss where this sensitivity is optimized while putting minimum constraints on precision, sensitivity, and/or F1 in the high-confidence, high-complexity regions (the assumption being the truth set is complete in those regions), or some weightings/variations thereof. EDIT: Actually, looks like overall performance in the high-confidence region improves:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692:137,optimiz,optimizing,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 898, in __call__; storage_map=getattr(self.fn, 'storage_map', None)); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 884, in __call__; self.fn() if output_subset is None else\; File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 989, in rval; r = p(n, [x[0] for x in i], o); File ""/home/shlee/anaconda3/envs/gatk/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 978, in p; self, node); File ""theano/scan_module/scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform (/home/shlee/.theano/compiledir_Linux-4.13--gcp-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/scan_perform/mod.cpp:2628); NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 97; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. HINT: Re-running with most Theano optimiza",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4840:11226,perform,perform,11226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840,1,['perform'],['perform']
Performance,"# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output-file-format MAF --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --disable-sequence-dictionary-validation true --remove-filtered-variants false --five-prime-flank-size 5000 --three-prime-flank-size 0 --force-b37-to-hg19-reference-contig-conversion false --transcript-selection-mode CANONICAL --lookahead-cache-bp 100000 --min-num-bases-for-segment-funcotation 150 --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.2.0.0"",Date=""March 24, 2021 12:11:32 PM GMT"">; ## Funcotator 4.2.0.0 | Date 20211124T121132 | Gencode 34 CANONICAL | Achilles 110303 | CGC full_2012_03-15 | ClinVar 12.03.20 | C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:19223,cache,cache-bp,19223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache-bp']
Performance,"# Actual behavior. > (base) [pkus@master1 mutect_test]$ ~/programs/gatk-4.1.8.0/gatk Funcotator --variant filtered_variants/P1.vcf.gz --reference ~/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path ~/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 15:16:39.460 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 17, 2020 3:16:39 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 15:16:39.785 INFO Funcotator - ------------------------------------------------------------; > 15:16:39.786 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 15:16:39.786 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 15:16:39.787 INFO Funcotator - Executing as xxx on Linux v3.10.0-957.5.1.el7.x86_64 amd64; > 15:16:39.787 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 15:16:39.787 INFO Funcotator - Start Date/Time: July 17, 2020 3:16:39 PM CEST; > 15:16:39.787 INFO Funcotator - ------------------------------------------------------------; > 15",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:1885,Load,Loading,1885,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['Load'],['Loading']
Performance,"# Extracting PGEN from GVS. ## The PGEN format; PGEN is a format written for and used by version 2 of [PLINK](https://www.cog-genomics.org/plink/2.0/). ***IT IS VERY IMPORTANT TO NOTE THAT VERSION 2 OF PLINK IS STILL IN ALPHA AND THE PGEN FORMAT IS STILL SUBJECT TO CHANGE.*** The format comprises 3 file types (or actually sometimes 4):; 1. A `.pgen` file. This is a binary file that stores a mapping of samples and sites to genotypes in a very cleverly compressed way that I can't explain super well because it's complicated.; 2. A `.pvar` file. This is essentially a sites-only VCF. It has information for each site referenced in the `.pgen` file. PLINK also provides an option to produce/use a zstd compressed version of the file (`.pvar.zst`), and we have opted to write that for performance purposes.; 3. A `.psam` file. This is a plaintext file that contains a list of samples referenced in the `.pgen` file. It also optionally includes some phenotype data.; 4. Optionally, a `.pgi` file. Typically, a `.pgen` file has an index at the top. Optionally, PLINK supports using a `.pgen` file with an index in a separate `.pgi` file. The PGEN format does not store all of the information that a VCF has. It leaves out a lot of the fields and annotations you can store in a VCF. As a result of this and the clever compression in the `.pgen` file, these files are typically much smaller than equivalent VCFs. For more information on the PGEN file format, see the official spec [here](https://github.com/chrchang/plink-ng/blob/master/pgen_spec/pgen_spec.pdf). ## The code; The code for the PGEN extract can be divided into 3 parts:; 1. The PGEN-JNI, a C++/JNI library that handles writing HTSJDK VariantContext objects to PGEN files,; 2. ExtractCohortToPgen, a GATK tool based on ExtractCohortToVcf that processes VariantContexts and passes them to PGEN-JNI for writing, and; 3. GvsExtractCallsetPgenMerged, a WDL workflow based on GvsExtractCallset that uses ExtractCohortToPgen to write a series of P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:785,perform,performance,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,1,['perform'],['performance']
Performance,"# Steps to reproduce; Try to do a PostprocessGermlineCNVCalls with not all the autosomal chromosomes. #### Command. ```/home/tintest/miniconda2/bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar PostprocessGermlineCNVCalls -L Refseq_GrCh38_1-3-9-17-Y-M.bed -R hg38_1-3-9-17-Y-M.fasta --calls-shard-path GermlineCNVCaller/GermlineCNVCaller-calls/ --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller-model --sample-index 274 --autosomal-ref-copy-number 2 --allosomal-contig Y --output-genotyped-intervals intervals/SAMPLE_274_PostprocessGermlineCNVCalls_interval.vcf --output-genotyped-segments segments/SAMPLE_274_PostprocessGermlineCNVCalls_segments.vcf```. #### Output; ```14:21:35.446 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:21:35.534 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.536 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.0.5.1; 14:21:35.537 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:21:35.538 INFO PostprocessGermlineCNVCalls - Executing as tintest@dahu63 on Linux v4.9.0-6-amd64 amd64; 14:21:35.539 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 14:21:35.541 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 1, 2018 2:21:35 PM CEST; 14:21:35.542 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.543 INFO PostprocessGermlineCNVCalls - ------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231:1968,Load,Loading,1968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231,1,['Load'],['Loading']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=h1) Report; > Merging [#2811](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8?src=pr&el=desc) will **increase** coverage by `0.022%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2811 +/- ##; =============================================; + Coverage 79.978% 80% +0.022% ; - Complexity 16726 16795 +69 ; =============================================; Files 1139 1139 ; Lines 60894 61155 +261 ; Branches 9436 9497 +61 ; =============================================; + Hits 48702 48924 +222 ; - Misses 8396 8422 +26 ; - Partials 3796 3809 +13; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <Ã¸> (Ã¸)` | `2 <0> (Ã¸)` | :arrow_down: |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `100% <0%> (Ã¸)` | `15% <0%> (+7%)` | :arrow_up: |; | [...nstitute/hellbender/utils/help/GATKHelpDoclet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oZWxwL0dBVEtIZWxwRG9jbGV0LmphdmE=) | `100% <0%> (Ã¸)` | `9% <0%> (+3%)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGU,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892:929,Perform,PerformAlleleFractionSegmentation,929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=h1) Report; > Merging [#3036](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/84fbda69bf9528059777496a415be8eb6db63e61?src=pr&el=desc) will **increase** coverage by `0.331%`.; > The diff coverage is `88.554%`. ```diff; @@ Coverage Diff @@; ## master #3036 +/- ##; ===============================================; + Coverage 79.973% 80.304% +0.331% ; - Complexity 16727 17771 +1044 ; ===============================================; Files 1139 1152 +13 ; Lines 60902 65165 +4263 ; Branches 9437 10284 +847 ; ===============================================; + Hits 48705 52330 +3625 ; - Misses 8401 8900 +499 ; - Partials 3796 3935 +139; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ome/segmentation/PerformCopyRatioSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUNvcHlSYXRpb1NlZ21lbnRhdGlvbi5qYXZh) | `86.667% <Ã¸> (+6.667%)` | `4 <0> (+2)` | :arrow_up: |; | [...institute/hellbender/tools/exome/ACNVModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9BQ05WTW9kZWxsZXIuamF2YQ==) | `97.143% <Ã¸> (-0.079%)` | `17 <0> (Ã¸)` | |; | [...ellbender/tools/exome/copyratio/CopyRatioData.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9jb3B5cmF0aW8vQ29weVJhdGlvRGF0YS5qYXZh) | `95.349% <0%> (-2.27%)` | `14 <1> (+1)` | |; | [...nder/tools/exome/segmentation/AFCRHiddenState.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201:960,Perform,PerformCopyRatioSegmentation,960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201,1,['Perform'],['PerformCopyRatioSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=h1) Report; > Merging [#3157](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/eab8761cbdfdaf24a5bf7551172b9f262d26d8cf?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3157 +/- ##; ===========================================; Coverage 80.132% 80.132% ; Complexity 16993 16993 ; ===========================================; Files 1145 1145 ; Lines 61641 61641 ; Branches 9606 9606 ; ===========================================; Hits 49394 49394 ; Misses 8419 8419 ; Partials 3828 3828; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <Ã¸> (Ã¸)` | `6 <0> (Ã¸)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829:887,Perform,PerformSegmentation,887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829,1,['Perform'],['PerformSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=h1) Report; > Merging [#3183](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/64eba53c96ea739638d34222f0f2c61c39153a64?src=pr&el=desc) will **increase** coverage by `0.05%`.; > The diff coverage is `89.931%`. ```diff; @@ Coverage Diff @@; ## master #3183 +/- ##; ==============================================; + Coverage 80.415% 80.465% +0.05% ; - Complexity 17294 17368 +74 ; ==============================================; Files 1165 1165 ; Lines 62573 62785 +212 ; Branches 9763 9789 +26 ; ==============================================; + Hits 50318 50520 +202 ; - Misses 8350 8353 +3 ; - Partials 3905 3912 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <Ã¸> (Ã¸)` | `63 <0> (Ã¸)` | :arrow_down: |; | [...nder/cmdline/ExomeStandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0V4b21lU3RhbmRhcmRBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `0% <Ã¸> (Ã¸)` | `0 <0> (Ã¸)` | :arrow_down: |; | [...der/tools/coveragemodel/nd4jutils/Nd4jIOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL25kNGp1dGlscy9OZDRqSU9VdGlscy5qYXZh) | `81.731% <Ã¸> (Ã¸)` | `19 <0> (Ã¸)` | :arrow_down: |; | [...bender/tools/exome/TargetAnnotationCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643:932,cache,cachemanager,932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643,1,['cache'],['cachemanager']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=h1) Report; > Merging [#3515](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/a218b6bbf6b3f243bce34a30f2458308319aadf3?src=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `84.946%`. ```diff; @@ Coverage Diff @@; ## master #3515 +/- ##; ===============================================; + Coverage 79.905% 79.917% +0.012% ; - Complexity 17918 17945 +27 ; ===============================================; Files 1199 1200 +1 ; Lines 65102 65195 +93 ; Branches 10142 10160 +18 ; ===============================================; + Hits 52020 52102 +82 ; - Misses 9042 9049 +7 ; - Partials 4040 4044 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <84.946%> (Ã¸)` | `27 <27> (?)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `78.571% <0%> (+0.649%)` | `39% <0%> (Ã¸)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `73.973% <0%> (+2.74%)` | `11% <0%> (Ã¸)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094:944,optimiz,optimization,944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094,1,['optimiz'],['optimization']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=h1) Report; > Merging [#3590](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/58108d0f3f1a760884201a62469105bc55c09a29?src=pr&el=desc) will **increase** coverage by `0.358%`.; > The diff coverage is `93.939%`. ```diff; @@ Coverage Diff @@; ## master #3590 +/- ##; ===============================================; + Coverage 79.736% 80.094% +0.358% ; - Complexity 18148 18799 +651 ; ===============================================; Files 1217 1226 +9 ; Lines 66602 69015 +2413 ; Branches 10429 11073 +644 ; ===============================================; + Hits 53106 55277 +2171 ; - Misses 9289 9415 +126 ; - Partials 4207 4323 +116; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <Ã¸> (Ã¸)` | `27 <0> (Ã¸)` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `93.939% <93.939%> (Ã¸)` | `44 <44> (?)` | |; | [.../tools/spark/sv/evidence/QNamesForKmersFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9RTmFtZXNGb3JLbWVyc0ZpbmRlci5qYXZh) | `83.333% <0%> (-16.667%)` | `7% <0%> (Ã¸)` | |; | [...nder/tools/spark/pathseq/PSPathogenTaxonScore.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077:954,optimiz,optimization,954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077,1,['optimiz'],['optimization']
Performance,"## Bug Report. ### Affected tool(s) or class(es). CNNScoreVariants. ### Affected version(s); - [x] Latest master branch as of [12/05/2021]. Same issue for an older master branch in v4.1.9. ### Description. **Issue with vqsr_cnn package in Conda environment.; AttributeError: module 'keras.backend' has no attribute 'clear_session'**. > Using GATK jar /lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /lustre/home/regmova/tools/gatk/build ; >/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar CNNScoreVariants -V TR017.GL.vcf.gz -R /home/regmova; >/Scratch/RefGenome/hs37d5.fa -O TR017.CNNscored.vcf; 10:46:04.904 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2021 10:46:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.0.0-19-ge60cdf8-SNAPSHOT; 10:46:05.152 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:46:05.152 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_92-b14; 10:46:05.152 INFO CNNScoreVariants - Start Date/Time: 12 May 2021 10:46:04 BST; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.152 INFO CNNScoreVariants - ------------------------------------------------------------; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Version: 2.24.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250:876,Load,Loading,876,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250,1,['Load'],['Loading']
Performance,## Bug Report. ### Affected tool(s) or class(es). HC java.lang.IllegalStateException: Padded span must contain active span. ### Affected version(s); - [X] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description. ```; Runtime.totalMemory()=2494038016; java.lang.IllegalStateException: Padded span must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7289:931,load,loadNextAssemblyRegion,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289,1,['load'],['loadNextAssemblyRegion']
Performance,"## Bug Report. ### Affected tool(s) or class(es); AnalyzeCovariates . ### Affected version(s); - [x] Latest public release version [v4.1.4.0] [hash:cec850f20311f0686fcf88510bc44e529590d78bec7076a603132115943c09e6]. ### Description ; AnalyzeCovariates fails with ; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.4.0-local.jar AnalyzeCovariates -bqsr /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/IN-PM01004_rmd.recal.bam.recalTable -plots /researchers/sebastian.hollizeck/lowcWGS/IN-PM01004/Bam/AnalyzeCovariates.pdf; 23:15:29.581 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 19, 2020 11:15:30 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:30.435 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.437 INFO AnalyzeCovariates - The Genome Analysis Toolkit (GATK) v4.1.4.0; 23:15:30.437 INFO AnalyzeCovariates - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:30.438 INFO AnalyzeCovariates - Executing as shollizeck@papr-res-compute204.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 23:15:30.438 INFO AnalyzeCovariates - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-8u212-b03-0ubuntu1.16.04.1-b03; 23:15:30.438 INFO AnalyzeCovariates - Start Date/Time: January 19, 2020 11:15:29 PM UTC; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - ------------------------------------------------------------; 23:15:30.439 INFO AnalyzeCovariates - HTSJDK Version: 2.20.3; 23:15:30.439 INFO AnalyzeCovariates - Picard Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6393:723,Load,Loading,723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6393,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Any tool that uses log4j2 and is compiled in java11. ### Affected version(s); - [ X ] Latest master branch as of 20210707. ### Description ; Runing almost everything (anything that makes use of log4j2) I get an UnsuportedOperationException. For example:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx120g -XX:ParallelGCThreads=20 -jar /opt/gatk-4.2.0.0-42-g2fc3b65-SNAPSHOT/gatk-package-4.2.0.0-42-g2fc3b65-SNAPSHOT-local.jar HaplotypeCaller -R Reference/Cork_oak_ref.fasta -I 03-Mapping/05-Deduped-Sorted/S_B_10-deduped-sorted.bam -O 04-SNPcalling/01-HaplotypeCaller/S_B_10.raw.g.vcf --emit-ref-confidence GVCF; WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:304); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.UnsupportedOperationException: No class provided, and an appropriate one cannot be found.; at org.apache.logging.log4j.LogManager.callerClass(LogManager.java:576); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:601); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:588); at org.broadinstitute.barclay.argparser.ClassFinder.<clinit>(ClassFinder.java:29); ... 4 more; ```. #### Steps to reproduce; Compile master with java11 and run ; `gatk CheckIlluminaDirectory --help`. #### Expected behavior; Normal execution. #### Actual behavior; UnsupportedOperationException",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7338:869,perform,performance,869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7338,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); CollectMultipleMetrics. ### Affected version(s); 4.1.2.0. ### Description ; CollectMultipleMetrics performs Percent-encoding of input paths. When running this tool as a step of a packed CWL workflow with Cromwell, this causes a `No such file or directory` error. The input file; ```; /cromwell-executions/transform_pack.cwl#main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; becomes; ```; file:///cromwell-executions/transform_pack.cwl%23main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; and can not be found. #### Expected behavior; The tool should collect metrics without error. #### Actual behavior; `CollectMultipleMetrics`; ```; Job main.metrics.metrics.cwl.gatk_collectmultiplemetrics:NA:1 exited with return code 3 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /mnt/scratch/runpack/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/execution/stderr.; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/tmp.a2640a46; 20:19:59.771 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/bin/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu May 09 20:20:00 UTC 2019] CollectMultipleMetrics --INPUT /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5931:149,perform,performs,149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931,1,['perform'],['performs']
Performance,"## Bug Report. ### Affected tool(s) or class(es); CollectWgsMetrics. ### Affected version(s); - 4.1.3.0. ### Description ; CollectWgsMetrics crashes on input that has worked for previous versions (including 4.1.2.0). #### Steps to reproduce; ```bash; gatk CollectWgsMetrics \; --java-options ""-Xms6G -Xmx10G -Djava.io.tmpdir=."" \; --INPUT=example.bam \; --OUTPUT=example.seq_metrics.txt \; --REFERENCE_SEQUENCE=ucsc.hg19.fasta \; --USE_FAST_ALGORITHM=true \; --LOCUS_ACCUMULATION_CAP 25000 \; --COVERAGE_CAP=100; ```; #### Expected behavior; CollectWgsMetrics should not crash. #### Actual behavior; ```terminal; 02:33:12.885 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/base-v1.4.1/share/gatk4-4.1.3.0-0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Sep 16 02:33:13 UTC 2019] CollectWgsMetrics --INPUT example.bam --OUTPUT example.seq_metrics.txt --COVERAGE_CAP 100 --LOCUS_ACCUMULATION_CAP 25000 --USE_FAST_ALGORITHM true --REFERENCE_SEQUENCE ucsc.hg19.fasta --MINIMUM_MAPPING_QUALITY 20 --MINIMUM_BASE_QUALITY 20 --STOP_AFTER -1 --INCLUDE_BQ_HISTOGRAM false --COUNT_UNPAIRED false --SAMPLE_SIZE 10000 --ALLELE_FRACTION 0.001 --ALLELE_FRACTION 0.005 --ALLELE_FRACTION 0.01 --ALLELE_FRACTION 0.02 --ALLELE_FRACTION 0.05 --ALLELE_FRACTION 0.1 --ALLELE_FRACTION 0.2 --ALLELE_FRACTION 0.3 --ALLELE_FRACTION 0.5 --READ_LENGTH 150 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 8 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Sep 16, 2019 2:33:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Sep 16 02:33:15 UTC 2019] Executing as user@server on Linux 3.10.0-693.21.1.el7.x86_64 amd64; OpenJDK 64-Bi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163:653,Load,Loading,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5686:610,Load,Loading,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); GATK 4.1.0.0. ### Description ; Funcotator does not perform any annotation on a minimal VCF with canonical cancer variants and returns the following error:. ```; 23:28:30.519 INFO Funcotator - Initializing Funcotator Engine...; 23:28:30.523 INFO Funcotator - Creating a VCF file for output: file:xxx/sandbox/idh.funcotated.vcf; 23:28:30.541 INFO ProgressMeter - Starting traversal; 23:28:30.541 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:28:30.652 INFO ProgressMeter - unmapped 0.0 15 8108.1; 23:28:30.652 INFO ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.; 23:28:30.652 WARN Funcotator - ================================================================================; 23:28:30.652 WARN Funcotator - _ _ _ __ __ _ _ _ _; 23:28:30.652 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || |; 23:28:30.652 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5777:139,perform,perform,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777,1,['perform'],['perform']
Performance,"## Bug Report. ### Affected tool(s) or class(es); GATK PostprocessGermlineCNVCalls. ### Affected version(s); v4.4.0.0. ### Description ; Run GTAK on a batch of WES samples with `PostprocessGermlineCNVCalls` encountered: ""Records were not strictly sorted in dictionary order.""; I tried to detect germline CNV in cohort mode on 25 WES samples by the official tutorial. At first, I didn't perform scatter and the step `PostprocessGermlineCNVCalls` was very time-consuming but eventually worked. So I split the reference genome into 45 parts to save time. It's OK for the first sample but there was an error ""Records were not strictly sorted in dictionary order."" from the second sample. I was really annoyed by it. `03:12:39.275 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xiangxd/project/software/callers/gatk_4.4/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 03:12:39.467 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.473 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.4.0.0; 03:12:39.474 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Executing as xiangxd@cu07 on Linux v3.10.0-327.el7.x86_64 amd64; 03:12:39.475 INFO PostprocessGermlineCNVCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v20.0.2+9-78; 03:12:39.477 INFO PostprocessGermlineCNVCalls - Start Date/Time: April 15, 2024, 3:12:39â€¯AM CST; 03:12:39.477 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.478 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 03:12:39.495 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 3.0.5; 03:12:39.496 INFO PostprocessGermlineCNVCalls - Picard Version: 3.0.0; 03:12:39.497 INFO PostprocessGermlineCNVCalls - Built for Spark Vers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8776:386,perform,perform,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8776,2,"['Load', 'perform']","['Loading', 'perform']"
Performance,"## Bug Report. ### Affected tool(s) or class(es); GATK v4.1.4.0 using FilterMutectCalls. ### Affected version(s); - [x] Latest public release version `4.1.4.0` installed from conda release `gatk4-4.1.4.0-1`; - [ ] Latest master branch as of [date of test?]. ### Description ; This issue reports the same error that is reported in #6237, but on the latest release, and in a mitochondrial calling setting. My command is:; ```bash; gatk FilterMutectCalls -V MT.vcf.gz\; -R human_g1k_v37.main.fasta\; -O MT.filtered.vcf.gz\; --stats MT.vcf.gz.stats\; --mitochondria-mode; ```. I get the following output to STDERR:; ```; 11:15:57.152 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 07, 2019 11:15:57 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:15:57.328 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.328 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.0; 11:15:57.328 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:15:57.328 INFO FilterMutectCalls - Executing as warkre@fuji on Linux v4.9.0-9-amd64 amd64; 11:15:57.328 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 11:15:57.329 INFO FilterMutectCalls - Start Date/Time: November 7, 2019 11:15:57 AM CET; 11:15:57.329 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.329 INFO FilterMutectCalls - ------------------------------------------------------------; 11:15:57.329 INFO FilterMutectCalls - HTSJDK Version: 2.20.3; 11:15:57.329 INFO FilterMutectCalls - Picard Version: 2.21.1; 11:15:57.329 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:657,Load,Loading,657,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:685,Load,Loading,685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6059:866,optimiz,optimizations,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059,1,['optimiz'],['optimizations']
Performance,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller GVCF mode. ### Affected version(s); GATK 4.1.8.0 . ### Description ; Discussed on the GATK forum: https://gatk.broadinstitute.org/hc/en-us/community/posts/360072760032-HaplotypeCaller-NullPointerException-Error. Command: ; `gatk --java-options ""-Xmx4g"" HaplotypeCaller -R hg19.fa.gz -I test.bam -O test.g.vcf.gz -ERC GVCF`. #### Stack Trace. ```; 17:08:11.229 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zepengmu/tools/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 27, 2020 5:08:12 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:08:12.021 INFO HaplotypeCaller - ------------------------------------------------------------; 17:08:12.028 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.0; 17:08:12.028 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:08:12.038 INFO HaplotypeCaller - Executing as zepengmu@midway2-0243.rcc.local on Linux v3.10.0-1127.8.2.el7.x86_64 amd64; 17:08:12.038 INFO HaplotypeCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 17:08:12.039 INFO HaplotypeCaller - Start Date/Time: August 27, 2020 5:08:11 PM CDT; 17:08:12.039 INFO HaplotypeCaller - ------------------------------------------------------------; 17:08:12.039 INFO HaplotypeCaller - ------------------------------------------------------------; 17:08:12.039 INFO HaplotypeCaller - HTSJDK Version: 2.22.0; 17:08:12.039 INFO HaplotypeCaller - Picard Version: 2.22.8; 17:08:12.039 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:08:12.040 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:08:12.040 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:08:12.040 INFO HaplotypeCal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6783:453,Load,Loading,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6783,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCallerSpark. ### Affected version(s); - [ 4.6.0.0] . ### Description ; spark task failed, here is the stack trace:; ```shell; java.lang.NullPointerException: Cannot invoke ""java.util.List.size()"" because ""cache"" is null; 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypesCache.ensureCapacity(GenotypesCache.java:84); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypesCache.get(GenotypesCache.java:43); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.makeGenotypeCall(GATKVariantContextUtils.java:341); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.subsetAlleles(AlleleSubsettingUtils.java:133); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.subsetAlleles(AlleleSubsettingUtils.java:48); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:191); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:263); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:979); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$0(HaplotypeCallerSpark.java:179); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197); 	at java.base/java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1856); 	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:292); 	at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.base/java.util.stream.StreamSp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961:264,cache,cache,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961,1,['cache'],['cache']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Haplotypecaller. ### Affected version(s); 4.1.1 or later. ### Description ; if you look at haplotypecaller.java::callRegion(), you will see these lines:; ```; if (trimmingResult.hasLeftFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantLeftFlankRegion(), false, VCpriors));; }; // output variant containing region.; result.addAll(referenceConfidenceModel.calculateRefConfidence(assemblyResult.getReferenceHaplotype(),; calledHaplotypes.getCalledHaplotypes(), assemblyResult.getPaddedReferenceLoc(), regionForGenotyping,; readLikelihoods, genotypingEngine.getPloidyModel(), calledHaplotypes.getCalls(), hcArgs.standardArgs.genotypeArgs.supportVariants != null,; VCpriors));; // output right-flanking non-variant section:; if (trimmingResult.hasRightFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantRightFlankRegion(), false, VCpriors));; }. ```; We trim the region left and right and calculate the Reference confidence. The problem is when we call calculateRefConfidence , we might trim reads for the left region and calculate TransientAttributes and put them in the cache. Next time, when we call calculateRefConfidence, we trim the reads for the right region but we still look at the cache which is prepopulated with data for the left region and we use the same data which might result in the wrong output. . if you turn USE_CACHED_READ_INDEL_INFORMATIVENESS_VALUES off, you might get different results which I do not think is the expected outcome. Let me know if you want more information. Thank you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5908:1194,cache,cache,1194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5908,2,['cache'],['cache']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); - [x] 4.1.1.0..4.1.9.0. ### Description ; I am evaluating Mutect2 variant calling performance in GiaB mixtures (target capture, no UMI, 2000x avg coverage). In particular, I am comparing 4.0.12.0 against 4.1.9.0 with default parameters. Below, I am providing data from a representative sample.; 4.1.9.0 misses variants that 4.0.12.0 was able to call. When feeding a reference VCF with option `--alleles` the variants are detected with decent quality scores. It is unclear why 4.1.9.0 does not make these variant calls and if this could be changed by modifying input parameters. Unlike in this issue https://github.com/broadinstitute/gatk/issues/6724 the variants were not called with the option `--force-active`. . These are the variants that are only called by 4.1.9.0 when the reference VCF is fed as input:. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	Sample; 2	25458546	.	C	T	.	.	AS_SB_TABLE=723,503|25,14;DP=1302;ECNT=1;MBQ=20,20;MFRL=189,190;MMQ=60,60;MPOS=36;POPAF=7.3;TLOD=61.58	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:1226,39:0.033:1265:576,19:554,19:723,503,25,14; 4	55152040	.	C	T	.	.	AS_SB_TABLE=1102,1078|15,13;DP=2349;ECNT=2;MBQ=20,20;MFRL=180,164;MMQ=60,60;MPOS=35;POPAF=7.3;TLOD=31.85	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:2180,28:0.012:2208:1003,16:1104,10:1102,1078,15,13; 5	170833472	.	AAT	A	.	.	AS_SB_TABLE=201,501|7,17;DP=750;ECNT=1;MBQ=20,26;MFRL=203,209;MMQ=60,60;MPOS=20;POPAF=7.3;RPA=2,1;RU=AT;STR;TLOD=45.4	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:702,24:0.035:726:329,12:311,12:201,501,7,17; 7	101844851	.	A	G	.	.	AS_SB_TABLE=1022,1178|25,25;DP=2406;ECNT=1;MBQ=20,20;MFRL=189,198;MMQ=60,60;MPOS=46;POPAF=7.3;TLOD=65.52	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:2200,50:0.021:2250:854,29:906,19:1022,1178,25,25; 7	101916798	.	C	A	.	.	AS_SB_TABLE=91,916|1,37;DP=1060;ECNT=1;MBQ=32,32;MFRL=213,195;MMQ=60,60;MPOS=26;POPAF=7.3;TLOD=54.92	GT:AD:AF:DP:F1R2:F2R1:SB	0/1:1007,38:0.033:1045:438,17:511,18:91,916,1,37; 7	148506396	.	A	C	.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7015:166,perform,performance,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7015,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); Mutect2. ### Affected version(s); I test this problem in two versions, V4.1.4.1 and V4.3.0.0.They all have this problem. ### Description ; Following the recommendations of the 'Best Practice Workflows', I run mutect2 in the following command. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${all_chrome_bed}; --bam-output ${bam_output} ; -O ${vcf_output}. To improve parallelism, I try to split my all chrome bed to 25 files.Parallel running the flowing command brings me signficient performance improvement. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr1_bed}; --bam-output ${chr1_bam_output} ; -O ${chr1_vcf_output}. java -jar -Djava.io.tmpdir=${tmpDir} -Xms2g -Xmx16g ; /mnt/bin/gatk-4.1.4.1/gatk-package-4.1.4.1-SNAPSHOT-local.jar Mutect2 ; --native-pair-hmm-threads 32 ; -R ${Fasta} ; -I ${cancer_bam} ; -I ${normal_bam}; --tumor-sample cancer --normal-sample normal ; -L ${chr2_bed}; --bam-output ${chr2_bam_output} ; -O ${chr2_vcf_output}. But when I examined the vcf results produced by both modes of operation, I found consistency issues. #### Expected behavior; Let's focus on chromosome 2.I expect 100% consistency between the following two runs.; 1. The vcf file is obtained using a bed file containing only chromosome 2.; 2. Use bed file with all chromosomes to get all calling results, then filter to get chromesome 2 calling result. #### Actual behavior; 1. The first method above gives one more result than the second.; 2. There are 168 vcf results inconsistent, out of 1247 total.One of the inconsistencie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8152:745,perform,performance,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8152,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); The docker image: `broadinstitute/gatk`. ### Affected version(s); `latest`. ### Description ; - The current GATK image has 44 layers; - In [the Azure Container Registry standard service tier](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling), ""ReadOps per minute"" is limited to 3000; - ""A `docker pull` translates to multiple read operations based on the number of layers in the image, plus the manifest retrieval.""; - 3000 / 45 = 66. That means that the image can only be pulled 66 times per minute. This is problematic for running many concurrent workflows that also have many shards. Once that limit is exceeded, the task can fail, which can cause the entire workflow to fail. ; - Layers can be viewed here: `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`; [gatk-image-layers.txt](https://github.com/broadinstitute/gatk/files/14212774/gatk-image-layers.txt). #### Steps to reproduce; `docker history --no-trunc broadinstitute/gatk > gatk-image-layers.txt`. #### Expected behavior; `--squash` shall be added to `build_docker_base_cloud.sh`, like has been added to `build_docker_base_locally.sh` already: https://github.com/broadinstitute/gatk/blob/a353e49f218e675f331abf629f0bb46df1d5151d/scripts/docker/gatkbase/build_docker_base_locally.sh#L24. #### Workaround; Users can pull the existing image, and [use `docker-squash` to squash the image to a single layer](https://github.com/goldmann/docker-squash), then push it into their private ACR, then update their WDLs to reference the new image.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684:334,throughput,throughput-and-throttling,334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684,2,"['concurren', 'throughput']","['concurrent', 'throughput-and-throttling']"
Performance,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); GATK 4.2.0.0 . ### Description . When running VariantRecalibrator on a joint-called gVCF with 2000 samples, the following java.lang.IllegalStateException occurs: **Gaussian mean vector does not have the same size as the list of annotations**. ```; 17:56:38.072 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 28, 2021 5:56:38 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:56:38.485 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.487 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 17:56:38.487 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:56:38.488 INFO VariantRecalibrator - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 17:56:38.488 INFO VariantRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 17:56:38.488 INFO VariantRecalibrator - Start Date/Time: July 28, 2021 5:56:38 PM EDT; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.489 INFO VariantRecalibrator - ------------------------------------------------------------; 17:56:38.490 INFO VariantRecalibrator - HTSJDK Version: 2.24.0; 17:56:38.491 INFO VariantRecalibrator - Picard Version: 2.25.0; 17:56:38.491 INFO VariantRecalibrator - Built for Spark Version: 2.4.5; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:56:38.491 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRIT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:384,Load,Loading,384,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _FilterMutectCalls_. ### Affected version(s); - gatk-4.1.0.0 (_latest_). ### Description . Hi,. I am using _Mutect2_ (v4.1.0.0) and similar to a previous bug reported on `AF=.`, _FilterMutectCalls_ seems to complain about MPOS fields having a value of `.`. No intermediate processing was done between _Mutect2_ and _FilterMutectCalls_. Below the error stack trace :. ```; 17:13:28.491 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data-ddn/home/anthony/sbx/mutect2/work/conda/gatk4-mutect2-nf-bcf605d6af4c0524a368d3d105898641/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:13:30.503 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.503 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 17:13:30.504 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:13:30.504 INFO FilterMutectCalls - Executing as anthony@node063 on Linux v2.6.32-220.el6.x86_64 amd64; 17:13:30.504 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 17:13:30.504 INFO FilterMutectCalls - Start Date/Time: February 17, 2019 5:13:28 PM CET; 17:13:30.504 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 17:13:30.505 INFO FilterMutectCalls - Picard Version: 2.18.25; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5684:462,Load,Loading,462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _Mutect2_. ### Affected version(s); - GATK 4.1.4.1. ### Description ; When running Mutect2 (from GATK v4.1.4.1) using the following command:. `gatk Mutect2 -R [path to grch37-1kg.fa] -I testcase.bam -O pon.vcf`. to create a PoN on NovaSeq WGS-data processed through the best practice pipeline (with the BQSR-steps run through the Spark-enabled tools, and bwa mem with -Y flag) I get the following error in multiple regions:. [Stacktrace](https://www.dropbox.com/s/d2n5zflj9u11oj8/stacktrace.png?dl=0). AFAIK this is related to the new code path introduced in #6240 and seem to be triggered when there are more than 2 reads supporting a fragment but all of them are either duplicate reads or supplemntary/secondary alignments. Any input is greatly appreciated. I guess a temporary fix is to use the --independent-mates flag (although haven't tried it yet -- how much worse mutation calling performance do one incur when using that flag?). #### Steps to reproduce; Use the following small test case .bam-file as input to the command specified above:. [Testcase](https://www.dropbox.com/s/hilcj3aj0jnjdmh/testcase.bam?dl=0). #### Expected behavior; Completion of mutect2 without Exception. #### Actual behavior; Early termination of the mutect2 run due to raising an exception when trying to create a fragment with no read data to back it up. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6310:939,perform,performance,939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6310,1,['perform'],['performance']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. FilterAlignmentArtifacts. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. 4.3.0.0. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. ```; Using GATK jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UseNUMA -jar /gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar FilterAlignmentArtifacts -R /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.gz -O WGS-NA12878.FilterAlignmentArtifacts.vcf --tmp-dir . -V WGS-NA12878.filtered.vcf -I WGS-NA12878.sorted.dedup.recal.bam --bwa-mem-index-image /raid/bundle/hg38/Homo_sapiens_assembly38.fasta.img; 11:24:09.761 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:24:09.942 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.942 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.3.0.0; 11:24:09.943 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:24:09.943 INFO FilterAlignmentArtifacts - Executing as root@D52BV-2U on Linux v4.15.0-202-generic amd64; 11:24:09.943 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_352-8u352-ga-1~18.04-b08; 11:24:09.943 INFO FilterAlignmentArtifacts - Start Date/Time: February 24, 2023 11:24:09 AM CST; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 11:24:09.943 INFO Filter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8221:986,Load,Loading,986,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8221,1,['Load'],['Loading']
Performance,"## Bug Report. ### Affected tool(s) or class(es); _womtool-47, grpah_; or; _version 1.0 wdls_. ### Description . I've been trying to graph mutect2 wdls using the latest womtool, but each time the tool throws an exception. Command-line:; `java -jar womtool-47.jar graph mutect2.wdl`. Exception:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 445, column 42:. samtools view -h -T ~{ref_fasta} ~{cram} |; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, colu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:693,load,load,693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,4,['load'],"['load', 'loadUsingSource']"
Performance,"## Bug Report. ### Affected tool(s) or class(es); gatk MarkDuplicatesSpark. ### Affected version(s); - GATK 4.2.6.1; - Spark 3.2.1. ### Description ; File sizes are different between MarkDuplicates and MarkDuplicatesSpark (run locally). file sizes:; input cram: 1094584927; output bam (MarkDuplicates): 2839215419; output bam (MarkDuplicatesSpark): 3536690732. #### Steps to reproduce; command:. `java -Xmx200G -jar /opt/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar MarkDuplicatesSpark \; -I file.cram \; -O file_sorted_markduplicates.bam \; -M file_markduplicates_metrics.txt \; -R homo_sapiens.fa`. #### Expected behavior; output bam should be the same size (or very similar) between MarkDuplicates and MarkDuplicatesSpark. Note: this is when using the local version of the gatk package, using the spark version I get the following error:. `Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Partitioner; 	at java.lang.Class.getDeclaredConstructors0(Native Method); 	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671); 	at java.lang.Class.getConstructors(Class.java:1651); 	at org.broadinstitute.hellbender.utils.ClassUtils.canMakeInstances(ClassUtils.java:31); 	at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:319); 	at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:180); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Partitioner; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:387); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 8 more`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8007:1659,load,loadClass,1659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8007,3,['load'],['loadClass']
Performance,"## Bug Report. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7914:300,load,load,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914,2,['load'],['load']
Performance,"## Bug Report. ### Affected tool(s); GenotypeGVCFs . ### Affected version(s); 4.0.2.0. ### Description ; After running GenomicsDBImport which takes a short time, GenotypeGVCFs takes a really long time to genotype a short interval. It should not take so long. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11471/performance-troubleshooting-tips-for-genotypegvcfs/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4512:375,perform,performance-troubleshooting-tips-for-genotypegvcfs,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4512,1,['perform'],['performance-troubleshooting-tips-for-genotypegvcfs']
Performance,"## Bug Report. Dear developers,. I tried to update the GENCODE database and used the getGencode.sh scripts to get the data. However, I was not able to index the feature-file: Do you have any idea why that happens and how to get it done?. Code:; /home/robby/Tools/NGS/gatk-4.2.0.0/gatk IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; Using GATK jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar IndexFeatureFile -I /home/robby/Tools/NGS/gatk-master4_2_src/scripts/funcotator/data_sources/gencode/hg19/gencode.v37lift37.annotation.REORDERED.gtf; 18:53:59.113 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robby/Tools/NGS/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 08, 2021 6:53:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 18:53:59.283 INFO IndexFeatureFile - ------------------------------------------------------------; 18:53:59.283 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.2.0.0; 18:53:59.284 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:53:59.290 INFO IndexFeatureFile - Initializing engine; 18:53:59.290 INFO IndexFeatureFile - Done initializing engine; 18:53:59.417 WARN GencodeGtfCodec - GENCODE GTF Header line 1 has a version number that is above maximum tested version (v 34) (given: 37): ##description: evidence-based annotation of the human genome (GRCh38), version 37 (Ensembl 103), mapped to GRCh37 with gencode-backmap Continuing, but err",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7134:948,Load,Loading,948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7134,1,['Load'],['Loading']
Performance,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:506,load,load,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,11,"['Load', 'load']","['LoadType', 'LoadersManager', 'load', 'loaders']"
Performance,"## Bug Report; GenotypeGVCFs stuck indefinitely at ""Initializing engine"" step. ### Affected tool(s) or class(es); gatk GenotypeGVCFs; ### Affected version(s); GATK v4.1.4.1 (installed in a `conda` convironment from the bioconda channel), on a RHEL server 7.6 (Maipo). ### Description ; Following the recommended pipeline of HaplotypeCaller, GenomicsDBImport and then GenotypeGVCFs, the last command hangs indefinitely and from the log file, it seems like it doesn't get past the ""Initialize engine"" step. This is an example of the standard error stream (after the `GenotypeGVCFs` job reached 20 hours wall time and was killed) :; ```; 22:28:44.293 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/export/user/home/miniconda3/envs/aDNA/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.; jar!/com/intel/gkl/native/libgkl_compression.so; Dec 17, 2020 10:28:44 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:28:44.639 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.4.1; 22:28:44.640 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:28:44.640 INFO GenotypeGVCFs - Executing as user@gc-prd-hpcn002 on Linux v3.10.0-957.27.2.el7.x86_64 amd64; 22:28:44.640 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:28:44.640 INFO GenotypeGVCFs - Start Date/Time: December 17, 2020 10:28:44 PM AEST; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Version: 2.21.0; 22:28:44.640 INFO GenotypeGVCFs - Picard Version: 2.21.2; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007:675,Load,Loading,675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007,1,['Load'],['Loading']
Performance,"## Bug Report; when I run the MarkDuplicatesSpark, it throws me an error: basically it shows the spark engine stopped when run this function. ; the part of the error log is here:; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/rnaseq_pipeline_app/Apps/GATK/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 21/01/12 15:50:31 INFO SparkContext: Running Spark version 2.4.5; 21/01/12 15:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 21/01/12 15:50:31 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls to: root; 21/01/12 15:50:31 INFO SecurityManager: Changing view acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: Changing modify acls groups to: ; 21/01/12 15:50:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); groups with view permissions: Set(); users with modify permissions: Set(root); groups with modify permissions: Set(); 21/01/12 15:50:31 INFO Utils: Successfully started service 'sparkDriver' on port 36657.; 21/01/12 15:50:31 INFO SparkEnv: Registering MapOutputTracker; 21/01/12 15:50:31 INFO SparkEnv: Registering BlockManagerMaster; 21/01/12 15:50:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 21/01/12 15:50:31 INFO B",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7035:904,load,load,904,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7035,1,['load'],['load']
Performance,"## Feature request. ### Tool(s) or class(es) involved. SV pipeline, Funcotator, etc. ### Description. In trying to build test data for SV, time and time again we face the problem of not being able to find actual desired events on the two chromosomes 20 and 21, hence end up having to painfully perform all kinds of coordinate hacks in order to have enough test coverage. It seems that the Funcotator team is also facing a similar issue. Therefore it will be great if the whole reference genome for HG38, and maybe HG19 as well, can be included in the tests, so that tool developers spend less time worrying about hassles in moving real events to chr20 and chr21. One of the potential downside is obvious: it increases the repo size and time for running tests (downloading a bigger file) on Travis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5111:294,perform,perform,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111,1,['perform'],['perform']
Performance,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description. Currently, there is a Caching mechanism in VcfFuncotationFactory. This could be refactored into a separate component and shared with other funcotation factories. Not all funcotation factories would benefit from this cache, but certainly LocatableXsvFuncotationFactory would. If the refactoring is to put the cache into DataSourceFuncotationFactory, then there needs to be a mechanism for subclasses to disable it. Additionally, the cache should be made into a separate class that can be used from DataSourceFuncotationFactory. Just to encapsulate the cache functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4974:300,cache,cache,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4974,4,['cache'],['cache']
Performance,"## Feature request. ### Tool(s) or class(es) involved; GenomicsDBImport. ### Description; Users get confused by this error message: `A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader`; `Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 1$1$249250621 at workspace: ...; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed`. In one of our [docs](https://gatk.broadinstitute.org/hc/en-us/articles/360035889971--How-to-Consolidate-GVCFs-for-joint-calling-with-GenotypeGVCFs), we offer this advice, but this is not a proper argument in the GATK tool docs yet:; _If youâ€™re working on a POSIX filesystem (e.g. Lustre, NFS, xfs, ext4 etc), you must set the environment variable TILEDB_DISABLE_FILE_LOCKING=1 before running any GenomicsDB tool. If you donâ€™t, you will likely see an error like Could not open array genomicsdb_array at workspace:[...]_. **This request is to add a proper argument to deal with this scenario in GenomicsDBImport and to document it in the tool docs.**",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519:403,load,load,403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519,1,['load'],['load']
Performance,"## Feature request. ### Tool(s) or class(es) involved; VariantRecalibrator. ### Description; Currently VariantRecalibrator only accepts vcf as input. Previously when performing Joint Genoptying using GenotypeGVCFs, the outputs were vcf hence this behavior made sense. . Now that we have GenomicsDB import (which is quite fast) we still have to use GenotypeGVCFs to extract a vcf for Variant Recalibrator. So, let's just skip a step and let VariantRecalibrator use GenomicsDB as an input!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7169:166,perform,performing,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7169,1,['perform'],['performing']
Performance,"## Feature request. ### Tool(s) or class(es) involved; _FindBreakpointEvidenceSpark_, _StructuralVariationDiscoveryPipelineSpark_, when using _XGBoostEvidenceFilter.java_. ### Description; The SV pipeline filters BreakpointEvidence based on BreakpointDensityFilter, or optionally XGBoostEvidenceFilter. The XGBoostEvidenceFilter uses a saved classifier model trained with Python code external to the GATK. This poses two main problems:; 1) The external Python code was designed for proof-of-principle and method development, not maintainability or ease of use. Additionally, GATK users and developers are assumed to be familiar with Java, not necessarily Python.; 2) The external Python code must share heterogeneous data with Java for unit/integration tests (supplying test BreakpointEvidence, expected classifier features, and expected classifier probabilities). Currently this is done via JSON files organized to (invertibly) store Pandas or Numpy objects. The resulting code to load these JSON files in on the Java side is complex.; These problems can be resolved by; 1) Replacing external python code by porting to an **experimental** tool in the GATK.; 2) Replacing JSON files with a serialization strategy currently supported by the GATK (e.g. Kryo). Additional benefits can be obtained by ensuring that the classifier-training subroutines are sufficiently general to speed development for other projects that may want to use boosted decision trees for classification.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4922:982,load,load,982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4922,1,['load'],['load']
Performance,"## Feature request. ### Tool(s) or class(es) involved; _StructuralVariationDiscoveryPipelineSpark, XGBoostEvidenceFilter.java_. ### Description; Currently there are two unresolved large structural decisions about features for the XGBoostEvidenceFilter classifier. At the moment these decisions are switched by static member booleans, however that results in bad software engineering with one active code path and one inactive code path. The decisions to be made are:; 1. Whether to merge templateSize and readCount; * Yes: avoid NaN properties and decrease the number of columns by 1; * No: properties are easier to understand; 2. Whether to merge overlapping mappability k-mers for the mappability score; * Yes: the property is much easier to understand and explain. This seems like a no-brainer.; * No: unfortunately the currently best-performing classifier was trained on unmerged k-mers. Resolving this issue requires training new classifiers (altering feature design and training approach) in order to come to a definitive decision on these decisions (with the strong hope that decision 2 is to merge overlapping k-mers). Then inactive code paths and boolean switches can be removed. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5041:838,perform,performing,838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5041,1,['perform'],['performing']
Performance,"## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_; [DepthOfCoverage](https://gatk.broadinstitute.org/hc/en-us/articles/360041851491-DepthOfCoverage-BETA-). ### Description. Are there plans to make DepthOfCoverage multi-threaded? If not, would it be possible to require such improvements?. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7890:260,multi-thread,multi-threaded,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7890,1,['multi-thread'],['multi-threaded']
Performance,"## Feature request. I used gatk4 docker file to perform the germline CNV cohort analysis. I got the individual vcf file for each sample. But I cannot find the tool to combine the CNV, just like combining gvcf in SNV analysis. Is there any such tool that I missed? And, is there any visualization scripts I can use? Thank you so much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5373:48,perform,perform,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5373,1,['perform'],['perform']
Performance,"## Feature request. Related to #6239. I'm interested in performing joint genotyping on a set of given alleles in order to avoid deflating rare variants (previously genotype given alleles, now force call filtered alleles). In particular, I'd like to regenotype on all of the alleles in the input gVCF output by CombineGVCFs, which I'll define in Proposal 1. To be more in line with HaplotypeCaller/Mutect, I've also provided Proposal 2. If this is something you'd be open to having in the GATK, I'd be happy to submit a PR. ### Tool(s) or class(es) involved. #### Proposal 1. If we move the `force-call-filtered-alleles` argument from `AssemblyBasedCallerArgumentCollection` to `GenotypeCalculationArgumentCollection`, this will expose it from `GenotypeGVCFs`. If this argument is true, we use the input alleles for regenotyping in `GenotypeGVCFsEngine` in `genotypingEngine. calculateGenotypes`. #### Proposal 2. If we move the `force-call-filtered-alleles` and `alleles` arguments from `AssemblyBasedCallerArgumentCollection` to `GenotypeCalculationArgumentCollection`, this will expose them from `GenotypeGVCFs`. If provided, the features can then be used for regenotyping in `GenotypeGVCFsEngine` in `genotypingEngine. calculateGenotypes`. ### Description. When performing joint genotyping, the user could tell GenotypeGVCFs to regenotype on a given set of alleles, similar to how they would for HaplotypeCaller. #### Proposal 1. ```; GenotypeGVCFs; --force-call-filtered-alleles true; --input combined.g.vcf; ```. #### Proposal 2. ```; GenotypeGVCFs; --force-call-filtered-alleles true; --alleles rare.combined.g.vcf; --input combined.g.vcf; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6550:56,perform,performing,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6550,2,['perform'],['performing']
Performance,"## Tool(s) or class(es) involved; GenomicsDBImport(v4.1.8.1). ### Description; Hello, I was construct to genomicdb using GenomicDBImport and import gvcf file for update genomicdb(Command). but since my server was shut down, during GenomicDBImport process, process is broke down. After I tried to GenomicDBImport same command for overwrite genomicdb and encounter to error message(error message).; ; In this case, should genomicdb be recreated from scratch? How to update the gvcf file I want to update? what should I do?. ### Command. java -jar $GATK GenomicsDBImport \; -L chr${1} \; -V $File_PATH/4762/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4763/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4764/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4765/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; -V $File_PATH/4767/bwa-gatk4/VARIANT/ForEachChr/chr22.g.vcf \; --genomicsdb-update-workspace-path $DB_PATH/test_database \; --genomicsdb-shared-posixfs-optimizations true \; --max-num-intervals-to-import-in-parallel 5 \; --reader-threads 10 \; --tmp-dir $Script_PATH/tmp. ### Error message. 10:49:12.018 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/mone/OMICS/Tools/Programs/gatk/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 18, 2021 10:49:12 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:49:12.231 INFO GenomicsDBImport - ------------------------------------------------------------; 10:49:12.232 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:49:12.232 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:49:12.232 INFO GenomicsDBImport - Executing as chowoo1023@bdcm04 on Linux v3.10.0-514.2.2.el7.x86_64 amd64; 10:49:12.232 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Ser",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7324:1010,optimiz,optimizations,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7324,1,['optimiz'],['optimizations']
Performance,"## Update numpy\scipy\pymc3 python package. ### Tool(s) or class(es) involved. python\gcnvkernel; python\vqsr_cnn. ### Description; want to use the newer numpy 1.19.4, but I found that gatk uses conda-force to install the older numpy 1.17.5, and it is not allowed to upgrade numpy because of scipy version restrictions. And scipy cannot be upgraded because of the version limitation of pymc3. I think we should use the new version of the software (in the new version, some bugs are fixed, the performance is better), we need to deal with the difficulties and help the software upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978:493,perform,performance,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978,1,['perform'],['performance']
Performance,"### Affected tool(s) or class(es); HaplotypeCallerSpark. gatk HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I GatherBamFiles.bam -O g.vcf.gz. The HaplotypeCaller works, but not HaplotypeCallerSpark.; Tried to use the docker image, and different server; tried to build the newest gatk, same error message. ### Affected version(s); - [ x ] Latest public release version 4.1.8.1; java version ; ```; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode); ```. ### Description ; keep get the error message like below. ```; Using GATK jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Dsamjdk.compression_level=5 -Xms10G -jar /cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCallerSpark -R GRCh38_full_analysis_set_plus_decoy_hla.fa -I SRR1573206.GatherBamFiles.bam -O SRR1573206.g.vcf.gz -G StandardAnnotation -G StandardHCAnnotation -G AS_StandardAnnotation -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -ERC GVCF; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 09:38:05.617 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 09:38:05.655 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cache/home/xc278/p/GATK/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 15, 2020 9:38:05 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:38:05.911 INFO HaplotypeCallerSpark -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6750:660,cache,cache,660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6750,2,['cache'],['cache']
Performance,### Affected tool(s) or class(es); _EstimateDragstrParameters_. ### Affected version(s); - [ ] Latest public release version [version?]; - [X] Latest master branch as of [after PR 6634 has been merged in]. ### Description . Look for usages of ```Utils.runInParallel```. Change those to use Spark instead. There is a possibility of removing multi-threading all together if we change the way we decimate and filter sites.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6876:340,multi-thread,multi-threading,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6876,1,['multi-thread'],['multi-threading']
Performance,"### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### ate of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; **GenotypeGVCFs stuck at Starting traversal for coulple of days:**. Using GATK jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR4_gvcf_database -G StandardAnnotation -O fat_ALL_MATERIALS_chr4.g.vcf.gz; 11:58:13.194 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:58:14.522 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/software/apps/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 20, 2022 11:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:58:19.059 INFO GenotypeGVCFs - ------------------------------------------------------------; 11:58:19.060 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:58:19.060 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:58:19.060 INFO GenotypeGVCFs - Executing as gaoshibin@fat1 on Linux v3.10.0-693.el7.x86_64 amd64; 11:58:19.060 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 11:58:19.060 INFO GenotypeGVCFs - Start Date/Time: 2022å¹´5æœˆ20æ—¥ ä¸Šåˆ11æ—¶58åˆ†13ç§’; 11:58:19.060 INFO GenotypeGVCFs - -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866:2185,Load,Loading,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866,1,['Load'],['Loading']
Performance,"### Affected tool(s) or class(es); docker version GATK:4.1.1.0. ### Affected version(s); ; latest release. ### Description ; Funcotator shuts down part way through job. A configuration problem @ google?; [funcotator_crash.txt](https://github.com/broadinstitute/gatk/files/3652568/funcotator_crash.txt). RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; ; ### Description . 04:13:19.667 INFO ProgressMeter - 15:85753672 1834.2 199000 108.5; 04:17:42.593 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 04:17:42.593 INFO VcfFuncotationFactory - gnomAD_exome 2.1 cache hits/total: 0/1402; 04:17:42.593 INFO VcfFuncotationFactory - gnomAD_genome 2.1 cache hits/total: 0/162233; 04:17:42.665 INFO Funcotator - Shutting down engine; [September 25, 2019 4:17:42 AM UTC] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1,845.78 minutes.; Runtime.totalMemory()=4523032576; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:318); at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6182:331,concurren,concurrent,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6182,5,"['cache', 'concurren']","['cache', 'concurrent']"
Performance,"### Affected tool(s) or class(es); gatk PrintReads. ### Affected version(s); v4.1.4.1. ### Description ; Command like; ```; java -Xms2g -Xmx3g -jar gatk.jar PrintReads --gcs-project-for-requester-pays my-project -R hg38.fa -I gs://some-bucket/data.cram -L loci.interval_list -L UNMAPPED -O data.loci.bam; ```; crashes near the end with this error:; ```. 05:03:04.672 INFO PrintReads - Shutting down engine; [March 1, 2020 5:03:04 AM EST] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 18.22 minutes.; Runtime.totalMemory()=3094872064; htsjdk.samtools.util.RuntimeEOFException: java.nio.channels.ClosedChannelException; 	at htsjdk.samtools.CRAMFileReader.queryUnmapped(CRAMFileReader.java:413); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryUnmapped(SamReader.java:543); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:129); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:111); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6475:892,load,loadNextIterator,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6475,1,['load'],['loadNextIterator']
Performance,"### Bug Report. Hi, after installing the conda environment and running `conda activate gatk` without errors, I seem to still have a problem importing the gcnvkernel module. Is there a way I can install it through pip or what is something I may have done wrong? I already went over the README and standard documentation, and don't think I missed a step. ### Affected tool(s) or class(es); gvnvkernel, other expected modules. #### Expected behavior; Generate output file from my VCF. #### Actual behavior; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GermlineCNVCaller --input var.vcf --run-mode CASE --contig-ploidy-calls X/prefix-calls --output-prefix regular.vcf --output testfile.vcf; 21:21:12.277 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/gamer456148/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 23, 2020 9:21:12 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:21:12.543 INFO GermlineCNVCaller - ------------------------------------------------------------; 21:21:12.544 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.4.1; 21:21:12.544 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:21:12.544 INFO GermlineCNVCaller - Executing as gamer456148@gamer456148-Inspiron-15-7579 on Linux v4.15.0-88-generic amd64; 21:21:12.544 INFO GermlineCNVCaller - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_191-b12; 21:21:12.544 INFO GermlineCNVCaller - Start Date/Time: February 23, 2020 9:21:12 PM EST; 21:21:12.544 INFO GermlineCNVCaller - -------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6467:965,Load,Loading,965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6467,1,['Load'],['Loading']
Performance,"### Bug Report; When running the StructuralVariationDiscoveryPipelineSpark, I am getting the following error:. ```; java.lang.UnsatisfiedLinkError:`/tmp/jp102/libfml.833188020007107749.jnilib: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jp102/libfml.833188020007107749.jnilib); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.loadNativeLibrary(FermiLiteAssembler.java:157); at org.broadinstitute.hellbender.utils.fermi.FermiLiteAssembler.<init>(FermiLiteAssembler.java:24); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:72); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FermiLiteAssemblyHandler.apply(FermiLiteAssemblyHandler.java:23); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5145:340,load,load,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145,4,['load'],"['load', 'loadLibrary', 'loadNativeLibrary']"
Performance,"### Instructions. Dear GATK team,. I am encountering an error when using the mutect2 function in ""gatk-4.2.6.1"". Whenever I enable the ""--disable-tool-default-read-filters"" option, I receive a java.lang.ArrayIndexOutOfBoundsException error. Since I need to call SNVs for RNA-seq data, I first split the bam file by chromosome, and then perform markduplicate and splitNcigar in two steps. I found that when I use the bam file obtained after using the splitNcigar function to run mutect2, the same error still occurs, even if I don't disable the -read-filters. Therefore, I suspect that the error may be introduced by splitNcigar. Thus, I tried running mutect2 directly on the bam file after markduplicate. If the --disable-tool-default-read-filters option is not set, the command runs successfully. However, once it is set, the same error occurs. ----. ## Bug Report; [February 28, 2023 10:46:30 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2394947584; java.lang.ArrayIndexOutOfBoundsException; at java.lang.System.arraycopy(Native Method); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.applyHardClipBases(ClippingOp.java:216); at org.broadinstitute.hellbender.utils.clipping.ClippingOp.apply(ClippingOp.java:69); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipRead(ReadClipper.java:142); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.clipLowQualEnds(ReadClipper.java:251); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:255); at org.broadinstitute.hellbender.utils.clipping.ReadClipper.hardClipLowQualEnds(ReadClipper.java:263); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.finalizeRegion(AssemblyBasedCallerUtils.java:132); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:270); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8224:336,perform,perform,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8224,1,['perform'],['perform']
Performance,### Instructions; I have run java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx64g -jar /rawdata/software-wes/software/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar MarkDuplicates -I /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --REMOVE_DUPLICATES TRUE --VALIDATION_STRINGENCY SILENT -O /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam -M /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat. ----. ## Bug Report; it just returns ; 01:41:21.972 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/rawdata/software-wes/software/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Sep 14 01:41:21 PDT 2023] MarkDuplicates --INPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.bam --OUTPUT /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.bam --METRICS_FILE /rawdata/2023result/0914/chip5868/R5868.2023.09.10.161.PT50493WR2/1_BAM/R5868.2023.09.10.161.PT50493WR2.sorted.rmdup.stat --REMOVE_DUPLICATES true --VALIDATION_STRINGENCY SILENT --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --DUPLEX_UMI false --ADD_PG_TAG_TO_READS true --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDuplicates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8520:795,Load,Loading,795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8520,1,['Load'],['Loading']
Performance,"### Summary; This user was able to access the GenomicsDB workspace but is having performance issues with SelectVariants. They tried the same command locally and it took less than a minute. Are there any changes with how the user is running SelectVariants to improve the performance?. ### GATK Info; GATK 4.1.9.0; . This request was created from a contribution made by Lucas Taniguti on February 01, 2021 22:41 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community\_comment\_360014183291](https://gatk.broadinstitute.org/hc/en-us/community/posts/360076845511-How-do-I-SelectVariants-from-GenomicsDB-stored-in-GCS-#community_comment_360014183291). \--. Thank you, it has started to work with gendb.gs://. But now I think it does not run. I have only one sample stored into the database and I'm selecting only chr20:1-1000000 and it is running for more than 30 minutes. Is it expected?. I'm using a VM from GCE, in the same region as the GCS bucket. Using GATK jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar ; ; ```; Running: ; ; Â Â Â java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx10g -Xms5g - ; ; jar /home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar SelectVariants -R Homo\_sapiens\_assembly38.fasta -V gendb.gs://mybucket/genomicsdb -L chr20:1-1000000 -O teste. ; ; vcf.gz ; ; 23:01:23.595 INFO Â NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/taniguti/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl\_compres ; ; sion.so ; ; 23:01:23.914 INFO Â SelectVariants - ------------------------------------------------------------ ; ; 23:01:23.915 INFO Â SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.9.0 ; ; 23:01:23.915 INFO Â SelectVariants - For support and documentation go to [https://software.bro",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7070:81,perform,performance,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7070,2,['perform'],['performance']
Performance,"### code; docker run -it -v /media/sj/14t1:/data2 broadinstitute/gatk-dev:NVSCOREVARIANTS-PREVIEW-SNAPSHOT /bin/bash; ####result; 08:37:42.884 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-NVSCOREVARIANTS-PREVIEW-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.995 INFO NVScoreVariants - The Genome Analysis Toolkit (GATK) vNVSCOREVARIANTS-PREVIEW-SNAPSHOT; 08:37:42.995 INFO NVScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:37:42.995 INFO NVScoreVariants - Executing as root@0e48fe56d3ce on Linux v5.15.0-79-generic amd64; 08:37:42.995 INFO NVScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:37:42.995 INFO NVScoreVariants - Start Date/Time: August 29, 2023 8:37:42 AM GMT; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.995 INFO NVScoreVariants - ------------------------------------------------------------; 08:37:42.996 INFO NVScoreVariants - HTSJDK Version: 3.0.1; 08:37:42.996 INFO NVScoreVariants - Picard Version: 2.27.5; 08:37:42.996 INFO NVScoreVariants - Built for Spark Version: 2.4.5; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:42.996 INFO NVScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:42.996 INFO NVScoreVariants - Deflater: IntelDeflater; 08:37:42.996 INFO NVScoreVariants - Inflater: IntelInflater; 08:37:42.996 INFO NVScoreVariants - GCS max retries/reopens: 20; 08:37:42.996 INFO NVScoreVariants - Requester pays: disabled; 08:37:42.996 WARN NVScoreVariants - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8501:170,Load,Loading,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8501,1,['Load'],['Loading']
Performance,"###### | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a string.; Multidimensional indexing with anything but a tuple is deprecated. This means that the index list in ind = [slice(None), 0]; arr[ind] should be changed to a tuple, e.g., ind = [slice(None), 0]; arr[tuple(ind)] or arr[(slice(None), 0)]. That change is necessary to avoid ambiguity in expressions such as arr[[[0, 1], [0, 1]]], currently interpreted as arr[array([0, 1]), array([0, 1])], that will be interpreted as arr[array([[0, 1], [0, 1]])] in the future.; Imports from the following sub-modules are deprecated, they will be removed at some future date.; numpy.testing.utils; numpy.testing.decorators; numpy.testing.nosetester; numpy.testing.noseclasses; numpy.core.umath_tests; ````. regards. Eric",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:1818,load,loads,1818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,5,['load'],"['load', 'loads']"
Performance,"########code:; docker run -it -v /media/sj/14t1:/data2 broadinstitute/gatk:latest. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar CNNVariantWriteTensors --output-tensor-dir /data2/example/results --reference /data2/example/1/hg19.fa --truth-bed /data2/example/hg19.hybrid.bed --truth-vcf /data2/example/hg19.hybrid.vcf.gz --variant /data2/example/NA12877.vcf.gz. ##########result:; 02:02:31.316 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 02:02:31.342 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.345 INFO CNNVariantWriteTensors - The Genome Analysis Toolkit (GATK) v4.4.0.0; 02:02:31.345 INFO CNNVariantWriteTensors - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:02:31.345 INFO CNNVariantWriteTensors - Executing as root@d768be9a3fc5 on Linux v5.15.0-79-generic amd64; 02:02:31.345 INFO CNNVariantWriteTensors - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1; 02:02:31.345 INFO CNNVariantWriteTensors - Start Date/Time: August 30, 2023 at 2:02:31 AM GMT; 02:02:31.345 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.345 INFO CNNVariantWriteTensors - ------------------------------------------------------------; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Version: 3.0.5; 02:02:31.346 INFO CNNVariantWriteTensors - Picard Version: 3.0.0; 02:02:31.346 INFO CNNVariantWriteTensors - Built for Spark Version: 3.3.1; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:02:31.346 INFO CNNVariantWriteTensors - HTSJDK Defaults.USE_A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8506:577,Load,Loading,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8506,1,['Load'],['Loading']
Performance,"##The CalculateContamination Bug Report. Hello, I have a problem to ask you:. I running this command in the gatk4-4.2.3.0-0:; `gatk CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table`. the following information is displayed:. Using GATK jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar CalculateContamination -I gewb.tumor.pileups.table -matched gewb.normal.pileups.table -O gewb.contamination.table; 19:10:31.163 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cluster/home/jialu/miniconda3/envs/wes2/share/gatk4-4.2.3.0-0/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 06, 2022 7:10:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:10:31.437 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.437 INFO CalculateContamination - The Genome Analysis Toolkit (GATK) v4.2.3.0; 19:10:31.437 INFO CalculateContamination - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:10:31.438 INFO CalculateContamination - Executing as haojie@node1 on Linux v3.10.0-957.el7.x86_64 amd64; 19:10:31.438 INFO CalculateContamination - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 19:10:31.438 INFO CalculateContamination - Start Date/Time: March 6, 2022 7:10:31 PM CST; 19:10:31.438 INFO CalculateContamination - ------------------------------------------------------------; 19:10:31.438 INFO CalculateContamination - --------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7707:823,Load,Loading,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707,1,['Load'],['Loading']
Performance,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1677:198,perform,performance,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677,1,['perform'],['performance']
Performance,"#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#7355); - no longer loading sample info table in this wdl (#7407); - divide up creation/population of temp pet table [VS-48] (#7395); - Sample QC metrics (#7396); - update import for is_loaded (#7416); - fix partition end, add 1 (#7420); - Fixes to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:16949,load,loading,16949,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['load'],['loading']
Performance,"#Bug Report. ### Affected tool(s) or class(es); CNNScoreVariant. ### Affected version(s); 4.0.7.0. ### Description ; For 1,297,033 variant sites, the CNN_1D=-16.118. . #### Steps to reproduce; /run_cnn.sh adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; Using GATK jar /share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /share/pkf/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O cnn/adsp-5k.hg38.GATK.aws-batch_SNP_INDEL.chr22.4794samples.g.vcf.gz; 11:29:43.339 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.7.0/install/bin/gatk-package-4.0.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:29:43.467 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.467 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.0.7.0; 11:29:43.467 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:29:43.468 INFO CNNScoreVariants - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-696.28.1.el6.x86_64 amd64; 11:29:43.468 INFO CNNScoreVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 11:29:43.469 INFO CNNScoreVariants - Start Date/Time: August 13, 2018 11:29:43 AM UTC; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - ------------------------------------------------------------; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Version: 2.16.0; 11:29:43.469 INFO CNNScoreVariants - Picard Version: 2.18.7; 11:29:43.469 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:29:43",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5101:836,Load,Loading,836,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5101,1,['Load'],['Loading']
Performance,"$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7330,concurren,concurrent,7330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:5639,concurren,concurrent,5639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['concurren'],['concurrent']
Performance,$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:3959,concurren,concurrent,3959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,2,['concurren'],['concurrent']
Performance,"$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). 00:11:09.634 ERROR TaskSetManager:70 - Task 15 in stage 1.0 failed 1 times; aborting job; 00:11:09.810 WARN TaskSetManager:66 - Lost task 33.0 in stage 1.0 (TID 528, localhost): TaskKilled (killed intentionally); 00:11:24.786 INFO HaplotypeCallerSpark - Shutting down engine; [May 26, 2017 12:11:24 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 10.58 minutes.; Runtime.totalMemory()=16622026752; org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 1 times, most recent failure: Lost task 15.0 in stage 1.0 (TID 519; , localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:8984,concurren,concurrent,8984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['concurren'],['concurrent']
Performance,$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Opti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:13425,concurren,concurrent,13425,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['concurren'],['concurrent']
Performance,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:11119,concurren,concurrent,11119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,2,['concurren'],['concurrent']
Performance,% +0.051% ; - Complexity 38899 38963 +64 ; ===============================================; Files 2336 2336 ; Lines 182709 182730 +21 ; Branches 20060 20066 +6 ; ===============================================; + Hits 158213 158325 +112 ; + Misses 17441 17365 -76 ; + Partials 7055 7040 -15 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8074?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_sourc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:1878,scalab,scalable,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 4 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details.; ```; The warnings show up in at least these tasks: gatkTabComplete, installDist, gatkDoc, shadowJar, sparkJar. Seems like it should be easy to fix, I'm not sure how we didn't see them when doing the upgrade.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:2092,optimiz,optimizations,2092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,1,['optimiz'],['optimizations']
Performance,'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparingInt$7b0bb60$1(Ljava/util/function/ToIntFunction;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b219168) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.; Event: 3.491 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl-x86_64.so.1; 2b5f56d5e000-2b5f56d60000 ---p 00000000 00:00 0 ; 2b5f56d60000-2b5f56d63000 ---p 00000000 00:00 0 ; 2b5f56d63000-2b5f56e61000 rw-p 00000000 00:00 0 [stack:85483]; 2b5f56e61000-2b5f56e62000 r--p 00000000 00:00 0 ; 2b5f56e62000-2b5f56e63000 rw-p 00000000 00:00 0 ; 2b5f56e63000-2b5f56e6b000 rw-s 00000000 08:01 69704,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29404,load,loading,29404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance,'rm --help' for more information.; [INFO] ; [INFO] --- maven-failsafe-plugin:2.16:integration-test (integration-tests) @ gatk-aggregator ---; ```. I have no idea whether it breaks something downstream but provided building fails for me later with. ```; [INFO] Reactor Summary:; [INFO] ; [INFO] GATK Root .......................................... SUCCESS [ 16.744 s]; [INFO] GATK Aggregator .................................... SUCCESS [ 4.647 s]; [INFO] GATK GSALib ........................................ SUCCESS [ 6.040 s]; [INFO] GATK Utils ......................................... SUCCESS [ 39.733 s]; [INFO] GATK Engine ........................................ SUCCESS [ 7.557 s]; [INFO] GATK Tools Public .................................. SUCCESS [ 7.689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] -----------------------------------------,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1220,Queue,Queue,1220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance,"(1) I am studying GMS mappability scores. To the best of my knowledge, it is the only such analysis that considers both paired-end reads and base calling error rate characteristic of Illumina machines. We could feed the GMS score as a feature file to the coverage collector tool for filtering. (2) I am also working on the ""optimal strategy"" for different SV types. (3) @samuelklee, do we get the same wavy pattern in other samples in the same region? in other words, it is sample-specific or region-specific?. (4) While fragment-based GC correction is difficult (and probably unnecessary) to perform without keeping a full index of aligned reads (like GS), it might be worthwhile to at least collect per-sample per-interval fragment-based average GC content (perhaps along with other summaries such as average fragment length, MQ, etc). It is easy to show that that the difference between full fragment-based GC correction and correction only using the observed average fragment GC content for the pile-up is of the order of the curvature of the GC curve, which is presumably small. We could collect these statistics either on-the-go during coverage collection, or from the sparse counts table as you suggested before (most sensible approach, once we figure out a way to represent sparse tensors).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152:593,perform,perform,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152,1,['perform'],['perform']
Performance,"(90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Conne",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:1014,load,loadHeaderFromVCFUri,1014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['load'],['loadHeaderFromVCFUri']
Performance,"(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=505937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:46078,concurren,concurrent,46078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['concurren'],['concurrent']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.intern,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5748,concurren,concurrent,5748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.interna,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4622,concurren,concurrent,4622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4175,concurren,concurrent,4175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['concurren'],['concurrent']
Performance,(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); 17:43:23.161 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b72d4821-5e36-4d36-aa79-aa6263768669/1000G_phase1.indels.hg19.sites.vcf; 20/01/05 17:43:23 INFO NewHadoopRDD: Input split: file:/panfs/roc/groups/6/clinicalmdl/shared/wgs_exome_v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam:167436615680+33554432; 20/01/05 17:43:23 ERROR Executor: Exception in task 4990.0 in stage 0.0 (TID 4990); java.io.FileNotFoundException: /panfs/roc/groups/6/clinicalmdl/shared/v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam (Too many open files); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.j,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:4867,concurren,concurrent,4867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,1,['concurren'],['concurrent']
Performance,(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:1788,concurren,concurrent,1788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['concurren'],['concurrent']
Performance,"(added here for easier tracking). In bwajni.c; - [ ] lift `(*env)->GetArrayLength(env,baseArray)` out of loops; - [ ] pass pointers directly not wrapped in classes, eg. BwaIndex could be passed as a pointer; - [ ] cache all fieldIDs, methodIDs and classes; - [ ] pass in data directly without the ShortRead; - [ ] batch multiple calls to align (1 read) ; - [ ] pass in a struct for the native code to fill rather then allocate a new AlnRgn everytime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1857:214,cache,cache,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857,1,['cache'],['cache']
Performance,") get rid of the warnings about missing .so files. As an aside, I'm curious whether PowerPC architecture has an instruction; set similar to AVX. This is something I might actually be able to; contribute to the project so I'm excited by the prospect!. -Dan. On Fri, Sep 4, 2020, 11:53 AM R-obert <notifications@github.com> wrote:. > Hello,; >; > I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1427,Load,Loading,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Shutdown hook called** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e0e0327-45a3-46e8-872a-f5a63c3c7a98** ; **Using GATK jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar** ; **Running:** ; **java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx200G -jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar PathSeqPipelineSpark --input /clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam --filter-bwa-image /clinix1/Analysis/mongol/phenomata/04.GC\_CC/PathSeq/hg19\_custom/ucsc.hg19.fasta.img --kmer-file /clinix1/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:51522,concurren,concurrent,51522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,")** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, PROCESS\_LOCAL, 7972 bytes)** ; **20/03/05 09:28:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)** ; **20/03/05 09:28:58 WARN TaskSetManager: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:519)** ; **",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:29090,concurren,concurrent,29090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,")** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **20/03/05 09:28:58 ERROR TaskSetManager: Task 34 in stage 0.0 failed 1 times; aborting job** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Cancelling stage 0** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 30.0 in stage 0.0 (TID 30), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 9.0 in stage 0.0 (TID 9), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 31.0 in stage 0.0 (TID 31), reason: Stage cancelled** ; **20/03/05 0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:32538,concurren,concurrent,32538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,")** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!** ; **20/03/05 09:28:58 INFO SparkContext: Successfully stopped SparkContext** ; **09:28:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:41152,concurren,concurrent,41152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)**,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45579,concurren,concurrent,45579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,"); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ------------------------------------------------------------------------------------------; ```. ---; - Original discussion wi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:11633,concurren,concurrent,11633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,1,['concurren'],['concurrent']
Performance,); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:2683,concurren,concurrent,2683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['concurren'],['concurrent']
Performance,"); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf: Too many open files, for input source: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:263); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:102); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:127); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:121); at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578:5982,concurren,concurrent,5982,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578,1,['concurren'],['concurrent']
Performance,"* Added MinGqVariantFilterBase; * * loads VCF, pedigree, UCSC genome tract, and truth data; * * calculates variant overlap with genome tracts; * * forms matrices, tensors, and other helping data for machine learning; * * provides for TRAIN and FILTER modes; * * provides functions for calculating loss given assigned min GQ values; * * computes best estimate of truth data used for training xgboost model; * Added XGBoostMinGqVariantFilter; * * calculates new GQ based on gradient boosting; * Added PropertiesTable for loading VCF properties into tensors; * Added TractOverlapDetector for computing overlap properties with; UCSC genome tracts. Training loss is based on weighted combination of heredity and truth; data, broken down by variant category.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7705:36,load,loads,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7705,2,['load'],"['loading', 'loads']"
Performance,"* Added the possibility of requesting a gridded output intervals set in ProcessIntervals.; * Also added a min-interval-length argument in case we want to skip smallish intervals (e.g.; at the end of contig).; * Some possible performance improvements in filtering bins that only contain Ns (was using a Stream<Byte>, a bit abusive). * IntervalUtils has now a rutine to write a interval file out of an stream of intervals. Works with NIO.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5701:225,perform,performance,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5701,1,['perform'],['performance']
Performance,* Cache the result of getBestAvailableSequenceDictionary instead of calling it on every variant.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6672:2,Cache,Cache,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6672,1,['Cache'],['Cache']
Performance,"* Currently things are in a weird state, picard style interval lists are handled either as tribble files if they are named correctly as .interval_list; If they are named .intervals, .picard, or .list they are loaded with a different code path.; This unifies it so that picard files are only loaded as .interval_list and .intervals is always considered a Gatk style list. * This removes the work around for broken 0 length intervals that was put in place a long time ago. However, the workaround was effectively removed; for all .interval_list files in 4.1.3.0 when we started reading those through the tribble plugin. Either the broken files no longer are used or they; are misnamed as .intervals. * fix tests to deal correctly with .inverval_list vs .intervals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6465:209,load,loaded,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6465,2,['load'],['loaded']
Performance,"* Fixing a non-deterministic point in HaplotypeCaller's KBestHaplotypeFinder; * It uses a priority queue to compare scores, if there are ties the tie breaking is arbitrary and seems to be different depending on circumstances of the run.; * For some as of yet unknown reason reading from a gs path vs a local path can cause this to be triggered.; * Adding a tie breaker which uses the entirety of the bases in the Path in cases where the score is tied, this is unique per path.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6104:99,queue,queue,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6104,1,['queue'],['queue']
Performance,"* Modifies gCNV WDLs to improve Cromwell performance when running on a large number of intervals, as in WGS; * Adds optional `disabled_read_filters` input to `CollectCounts`; * Enables GCS streaming for `CollectCounts` and `CollectAllelicCounts`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607,1,['perform'],['performance']
Performance,"* Should be runnable on-demand using a convenient mechanism (eg., reviewer types a command on a github PR). * Should be robust enough to provide confidence that a substantial change to a stable variant-calling tool is safe to merge. * Should cover performance as well as correctness. * Output may be a report that a human has to read (do not need automated pass/fail). * Implement for `HaplotypeCaller` and CNV tools first (with help of @LeeTL1220), then work with other teams to get test coverage for their tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4630:248,perform,performance,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4630,1,['perform'],['performance']
Performance,* Successful run in PMI land that finds nothing to clean up [here](https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20Echo%20First%20Look/job_history/ba861882-96ee-4635-b522-2fe9489b0076).; * Successful run in Integration land that finds loads to clean up [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/01667ae7-fd85-4a12-abcb-69e892500fa3).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8644:253,load,loads,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8644,1,['load'],['loads']
Performance,"* The performance should be fine - TileDB/GenomicsDB stores each field in a separate file (columnar storage) and so adding MIN_DP file to the list of queried fields (~5-10 INFO fields) should be fine.; * One possible source of performance improvement - I was querying the PL field in the sites only query (not producing it in the output VariantContext objects). I think it can be dropped from the query. I was assuming that the PL field would be needed to correctly handle spanning deletions (spanning deletion corresponds to deletion allele with min PL). However, for spanning deletions, all INFO fields are dropped. Hence, any INFO fields that depend on the allele order (allele specific annotations) would be dropped for the spanning deletion. Hence, the exact deletion allele corresponding to the spanning deletion is irrelevant making it possible to drop the PL field from the query as well. Is that correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568:6,perform,performance,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568,2,['perform'],['performance']
Performance,"* There was an ""optimization"" put in place in SelectVariants which accidentally added a quadraticly scaling check on the genotypes.; * This keeps the optimization but makes it linear instead of quadratic on the number of samples.; * On one example with several thousand samples there was a speed up from ~5 minutes to .1 minutes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6729:16,optimiz,optimization,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6729,2,['optimiz'],['optimization']
Performance,"* _Large number of open file handles_: this was an issue in TileDB which got fixed as part of the restructuring that @nalinigans did for supporting HDFS/S3/GCS (#5017). I was too lazy to fix this again. If it's going to take some time for PR #5017 to be merged, I can submit a separate fix for this. This would fix any crashes/termination issues.; * _Performance of a single import process with a large number of intervals_; * Restating the obvious, but this is a single process (and by default, a single thread) with many intervals to import. As you increase the number of samples, this will become a performance pain point.; * More important than the number of intervals is the amount of data imported per interval. Each interval import involves opening the VCF files (loading index structures while creating FeatureReader objects), writing to TileDB/GenomicsDB. and closing the VCF file handles (destroying FeatureReader objects). If the amount of data written for each interval is sufficiently large, the cost of opening/closing the VCF files (creating/destroying FeatureReaders) is small relative to the total time taken.; * In the test cases I and Chris were trying, the amount of data written per interval was small (or 0 in many cases). The time taken in opening/closing the VCF files (and loading/destroying the index) dominates the total time.; * For a single import process (single thread), creating a large interval is better (or no worse) than passing several small intervals. TileDB/GenomicsDB has 0 overhead for regions with no data (for example, WES gVCFs). Having larger intervals will likely avoid issues described above. Hence, an advisory message will be beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757:602,perform,performance,602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757,3,"['load', 'perform']","['loading', 'performance']"
Performance,"* added a reference parameter to FeatureData source and FeatureManager methods; * genomicsDB requires a reference, previously this was being passed; through by hardcoding it in the required json files; * json files are now autogenerated by the importer tool, but the; reference wasn't being handled correctly. * updated the various walkers to pass the reference through if available. * gendb:// paths now point to the workspace directory instead of a; directory of jsons. * removed the ability to specify array, vidmap.json, and; callset.json paths in the importer tool since we now rely on the; structure and naming of the files when loading; moved some constants to GenomicsDBConstants. * updated GenomicsDBIntegration tests to use the new importer instead of a; prepackaged and very brittle set of json files. fixed a bug in GenomicsDBImporterIntegrationTests that made both tests; write to the same workspace",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2626:635,load,loading,635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626,1,['load'],['loading']
Performance,"************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:27:54.144 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:27:53 PM CST; 16:27:54.144 INFO GenotypeGVCFs - -------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4325,Load,Loading,4325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['Load'],['Loading']
Performance,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1935:553,perform,performance,553,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935,1,['perform'],['performance']
Performance,"**The following work has been done:**; - We performed a round of evaluations against XHMM and cn.MOPS on a cohort of 160 samples from SFARI project (which is described in our ASHG poster). For ground truth we used a callset generated from Talkowski lab SV pipeline on matched whole genome samples. Unfortunately, SFARI cohort is not public and cannot be used for public facing evaluations.; - Some hyperparameter tweaking was necessary to achieve good performance. Hyperparameters changed were contained mostly only to `psi_t` parameter.; - We developed a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:44,perform,performed,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,5,"['optimiz', 'perform']","['optimization', 'optimizations', 'performance', 'performed']"
Performance,"**changes in this PR:**; - resolves specops issue #247 - ImportGenomes.wdl takes Array[File] from data table as vcf input; - refactor LoadBigQueryData.wdl back into ImportGenomes; - returns an error if the `bq load` step fails (workflow was silently succeeding when this step failed); - checks existence of tables using `bq show` rather than the csv file - this should still be safe against a race condition because of @ericsong 's refactoring to prevent the `CreateTables` step from being scattered; - run CreateTables at the start (don't wait for CreateImportTsvs); - does NOT use a preemptible VM for the LoadTables step, to minimize (though not eliminate) the possibility of loading a duplicate set of data (see specops issue #248 for further discussion). **testing:**; - these changes were tested in Terra, BQ outputs checked and verified",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7112:134,Load,LoadBigQueryData,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112,5,"['Load', 'load', 'race condition']","['LoadBigQueryData', 'LoadTables', 'load', 'loading', 'race condition']"
Performance,"+00 --active_class_padding_hybrid_mode=50000 --enable_bias_factors=True --disable_bias_factors_in_active_class=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=100 --max_advi_iter_subsequent_epochs=100 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=2.000000e+00 --num_thermal_epochs=20 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:35:09.182 INFO cohort_denoising_calling - Loading 4 read counts file(s)...; 10:35:12.176 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; sample_names. Stderr: Traceback (most recent call last):; File ""/tmp/wujh/cohort_denoising_calling.7794651839449939395.py"", line 114, in <module>; n_st, sample_names, sample_metadata_collection); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 379, in __init__; sample_metadata_collection, sample_names, self.contig_list); File ""/opt/NfsDir/BioDir/Anaconda3/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 586, in _get_baseline_copy_number_and_read_depth; ""Some samples do not have read depth metadata""; AssertionError: Some samples do not have read depth metadata. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCura",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4457:1972,Load,Loading,1972,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457,1,['Load'],['Loading']
Performance,+32 ; Lines 146768 147415 +647 ; Branches 16223 16225 +2 ; ================================================; - Hits 127666 55100 -72566 ; - Misses 13189 87388 +74199 ; + Partials 5913 4927 -986; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5732?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...s/copynumber/models/AlleleFractionInitializer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvbkluaXRpYWxpemVyLmphdmE=) | `89.063% <Ã¸> (Ã¸)` | `17 <0> (Ã¸)` | :arrow_down: |; | [...r/tools/copynumber/models/AlleleFractionState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvblN0YXRlLmphdmE=) | `100% <Ã¸> (Ã¸)` | `7 <0> (Ã¸)` | :arrow_down: |; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `77.419% <Ã¸> (-10.753%)` | `24 <0> (-4)` | |; | [...bender/tools/copynumber/models/CopyRatioState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9Db3B5UmF0aW9TdGF0ZS5qYXZh) | `100% <Ã¸> (Ã¸)` | `5 <0> (Ã¸)` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `95.671% <Ã¸> (-2.164%)` | `45 <0> (-3)` | |; | [...s/copynumber/models/AlleleFractionLikelihoods.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496:1595,optimiz,optimization,1595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496,1,['optimiz'],['optimization']
Performance,+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (Ã¸)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (Ã¸)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (Ã¸)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (Ã¸)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `72.222% <72.222%> (Ã¸)` | |; | ... and [20 more](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:4733,scalab,scalable,4733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,", et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. However, it is clear that GATK's present reading of the full tbi is not scalable given the memory requirements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:2214,scalab,scalable,2214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,2,['scalab'],['scalable']
Performance,", which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:1100,cache,caches,1100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['caches']
Performance,", you'll see it's running with Xmx178g. We added 60G to the cluster memory request to leave buffer for the C layer. We're on v4.2.5.0. Does this error look familiar, and/or do you have any troubleshooting suggestions? Thanks in advance. ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	-Djava.io.tmpdir=<path> \; 	-Xmx178g -Xms178g \; 	-Xss2m \; 	-jar GenomeAnalysisTK4.jar \; 	GenotypeGVCFs \; 	-R 128_Mmul_10.fasta \; 	--variant gendb:///home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb \; 	-O /home/exacloud/gscratch/prime-seq/workDir/1bb5295c-6ec5-103a-8692-f8f3fc86cd3f/Job1.work/WGS_pre-mGAPv2.3_1852.vcf.gz \; 	--annotate-with-num-discovered-alleles \; 	-stand-call-conf 30 \; 	--max-alternate-alleles 6 \; 	--force-output-intervals mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed \; 	-L 1:1-3714165 \; 	--only-output-calls-starting-in-intervals \; 	--genomicsdb-shared-posixfs-optimizations. 12:31:14.647 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 15, 2022 12:31:14 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:31:14.783 INFO GenotypeGVCFs - ------------------------------------------------------------; 12:31:14.783 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.5.0; 12:31:14.783 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:31:14.784 INFO GenotypeGVCFs - Executing as labkey_submit@exanode-6-25 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 12:31:14.784 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_60-b27; 12:31:14.784 INFO GenotypeGVCFs - Start Date/Time: February 15, 2022 12:31:14 PM PST; 12:31:14.784 INFO GenotypeGVCFs - --------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:1421,Load,Loading,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,1,['Load'],['Loading']
Performance,",67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,68] [info] WorkflowManagerActor Starting workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] WorkflowManagerActor Successfully started WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,69] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2020-07-14 05:09:30,72] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2020-07-14 05:09:30,83] [info] MaterializeWorkflowDescriptorActor [968be82c]: Parsing workflow as WDL 1.0; [2020-07-14 05:09:31,60] [info] MaterializeWorkflowDescriptorActor [968be82c]: Call-to-Backend assignments: ValidateBamsWf.ValidateBAM -> Local; [2020-07-14 05:09:31,82] [warn] Local [968be82c]: Key/s [memory, disks] is/are not supported by backend. Unsupported attributes will not be part of job executions.; [2020-07-14 05:09:35,38] [info] Not triggering log of token queue status. Effective log interval = None; [2020-07-14 05:09:37,15] [info] WorkflowExecutionActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 [968be82c]: Starting ValidateBamsWf.ValidateBAM; [2020-07-14 05:09:37,39] [info] Assigned new job execution tokens to the following groups: 968be82c: 1; [2020-07-14 05:09:41,61] [warn] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:4213,queue,queue,4213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['queue'],['queue']
Performance,"- Built for Spark Version: 2.4.5; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:55:36.414 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:55:36.415 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:55:36.415 INFO HaplotypeCaller - Inflater: IntelInflater; 12:55:36.415 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:55:36.415 INFO HaplotypeCaller - Requester pays: disabled; 12:55:36.415 INFO HaplotypeCaller - Initializing engine; 12:55:36.508 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 12:55:36.511 INFO HaplotypeCaller - Done initializing engine; 12:55:36.515 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 12:55:36.523 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:55:36.524 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/linux/Downloads/SNP/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:55:36.552 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:55:36.553 INFO IntelPairHmm - Available threads: 12; 12:55:36.553 INFO IntelPairHmm - Requested threads: 4; 12:55:36.553 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 12:55:36.569 INFO ProgressMeter - Starting traversal; 12:55:36.569 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 12:55:36.587 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7229:2112,Load,Loading,2112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7229,1,['Load'],['Loading']
Performance,"- Create `PairHMMNativeArguments` in HaplotypeCaller and pass to `VectorLoglessPairHMM` in `PairHMM.java`.; - Supply GATK temp directory in `VectorLoglessPairHMM.java`. Currently passing `null`, which uses the system temp directory. ```; final boolean isSupported = new IntelPairHmm().load(null);; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1946:285,load,load,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1946,1,['load'],['load']
Performance,"- Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:2344,load,load,2344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['load'],['load']
Performance,"- Fixed a NPE in the `problem` variant case, which is now resolved.; This was due to not filling out the dataSourceName field.; - Tested with b37 gnomAD matching against b37 variants with hg19 data; sources.; - Fixed some issues with the default problem variant annotations.; - Adding in per-data source cache settings.; - Fixing logger in DataSourceUtils. Fixes #5456",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5491:304,cache,cache,304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5491,1,['cache'],['cache']
Performance,"- I sneaked in another change where I pass in a single file containing a list of input_vcfs instead of an array of input_vcfs. I made this because Terra couldn't save my inputs when I passed in 700 samples.; - Most of the logic was moved into `CreateTables`, including the determination for what files to load. It would have been cleaner to move all of the file loading logic into `LoadTable` but the current approach cuts down the on the number of `gsutil ls` calls made and more importantly, only spins up a shard if there are files to load.; - I pushed the logic into a separate workflow because I wanted to refactor it as two tasks and I couldn't find a way to get a Task to call another Task without wrapping it in a workflow.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7056:305,load,load,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7056,4,"['Load', 'load']","['LoadTable', 'load', 'loading']"
Performance,"- Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.8392900000000002E-4; > 38 15:07:54.412 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.03020143; > 39 15:07:54.412 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.05 sec; > 40 15:07:54.413 INFO Mutect2 - Shutting down engine; > 41 [June 19, 2023 at 3:07:54 PM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; > 42 Runtime.totalMemory()=285212672; > 43 java.lang.IndexOutOfBoundsException: Index -1 out of bounds for length 1; > 4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3628,multi-thread,multi-threaded,3628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['multi-thread'],['multi-threaded']
Performance,- Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:06:12.479 INFO FeatureManager - Using codec IntervalListCodec to read file file:///paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGuts/-947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list; 21:06:12.640 DEBUG FeatureDataSource - Cache statistics for FeatureInput /paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohort8/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermli947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:; 21:06:12.640 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 21:06:12.645 INFO IntervalArgumentCollection - Processing 4999155 bp from intervals; 21:06:12.656 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 21:06:18.914 WARN GermlineCNVCaller - Sequence dictionary in annotated-intervals file does not match the master sequence dictionary.; 21:06:19.130 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 21:06:19.200 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 21:06:19.200 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:07:11.897 DEBUG ScriptExecutor - Executing:; 21:07:11.897 DEBUG ScriptExecutor - python; 21:07:11.897 DEBUG ScriptExecutor - /paedyl01/disk1/louisshe/tmp/gatk/cohort_denoising_calling.418897092082188314.py; 21:07:11.897 DEBUG ScriptExecutor - --ploidy_calls_path=/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:6427,Cache,Cache,6427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Cache'],['Cache']
Performance,- Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; WARNING 2020-08-19 15:41:50 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 15:41:50.159 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hg19_All_20180423.vcf.gz -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.159 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; 15:41:50.163 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.277 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/hg19_All_20180423.vcf.gz -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.375 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 15:41:50.490 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/gencode_xhgnc_v75_37.hg19.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg19/gencode_xhgnc_v75_37.hg19.tsv; 15:41:51.07,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:15794,cache,cache,15794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['cache'],['cache']
Performance,"- Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; 13:38:54.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:38:55.869 INFO CountReads - ------------------------------------------------------------; 13:38:55.870 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:38:55.870 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:38:55.871 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:38:55.871 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:38:55.871 INFO CountReads - Start Date/Time: January 9, 2019 1:38:54 PM EST; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.872 INFO CountReads - HTSJDK Version: 2.18.1; 13:38:55.873 INFO CountReads - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:44003,Load,Loading,44003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['Load'],['Loading']
Performance,"- When querying VCFs, the VcfFuncotationFactory will consider the allele (i.e. Number=""R"" and ""A"") in the output. This allows single-allele queries hitting datasource multiallelic variant contexts to be rendered properly. Closes #4957 ; - Added very simple caching to VCF FuncotationFactory; - VCF Funcotation factory can recognize when alleles are not exactly the same, but equivalent. ; - Fixed small speed bottleneck where Set.equals(...) could be used instead of more complex method. This was happening in validation of funcotation metadata.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4977:409,bottleneck,bottleneck,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977,1,['bottleneck'],['bottleneck']
Performance,- [ ] Sample-specific unexplained variance for all samples + quantile in the cohort; - [ ] HMM log likelihood + quantile in the cohort; - [ ] Bias factor loading quantiles in the cohort. This issues requires calculating and saving a number of summary statistics in the COHORT mode in conjunction with the coverage model parameters bundle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4060:154,load,loading,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4060,1,['load'],['loading']
Performance,- [ ] make a call to `segment_gcnv_calls.py`; - [ ] generate VCF file from .tsv files generated by `segment_gcnv_calls.py`. The python CLI scripts takes 3 arguments:; - ploidy calls; - model shards; - calls shards. Note:; - gcnvkernel 0.6.0 (PR #4335) now writes the baseline copy number to `baseline_copy_number_t.tsv` in the calls output path (for each sample). `PostprocessGermlineCNVCalls` can simply load this table (without needing to load ploidy calls) to determine the baseline copy-number state. `PostprocessGermlineCNVCalls` could take an additional argument `--allosomal-contigs` to specify sex chromosomes. `PostprocessGermlineCNVCalls` would then set the `REF` copy-number state on sex chromosomes appropriately. It could further take `--ref-autosomal-copy-number` (optional) to allow the users set the appropriate REF autosomal copy-number (for non-homo-sapiens species).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4336:405,load,load,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4336,2,['load'],['load']
Performance,"- `GvsImportGenomes.wdl` - renamed WDL, fixed issue with poorly returned bq load string; - `GvsCreateFilterSet.wdl` - renamed WDL, added SA support, fixed a lot of issues in ExtractFeatures tool surrounding permissions - BQ projectIDs are now being properly passed through, freq_table UDF defined in repo rather than in BQ; - `GvsPrepareCallset.wdl` (done in previous PR); - `GvsExtractCallset.wdl` - renamed WDL, added SA support; - SA testing README added. all 4 tested with SA in this Terra workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_sa_testing; testing also without SA in this workspace: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/gvs_testing_no_sa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7205:76,load,load,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7205,1,['load'],['load']
Performance,- `localization_optional` for indexes localization in `CreateImportTsvs`; - check to make sure there are more than 0 samples to load before going forward in `GetSampleIds`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7574:128,load,load,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7574,1,['load'],['load']
Performance,"- adds a scatter to the `SNPsVariantRecalibrator` call when the number of samples is over a certain threshold; - separates out ""generate tsv/csv files"" and ""load tsv/csv files into BigQuery"" steps of `UploadFilterSetToBQ` into two different tasks, `CreateFilterSetFiles` (scattered) and `UploadFilterSetFilesToBQ` (not). Closes https://github.com/broadinstitute/dsp-spec-ops/issues/326. - [ ] To do before merging: remove change to `.dockstore.yml`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7320:157,load,load,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7320,1,['load'],['load']
Performance,"- adds three new tables: vcf_header_lines_temp, vcf_header_lines, and sample_vcf_header; - populates vcf_header_lines_temp table during CreateVariantIngestFiles (LoadData WDL task); - parses data in vcf_header_lines_temp table in python script and populates vcf_header_lines and sample_vcf_header tables, then cleans up (ProcessVCFHeaders WDL task); - fixed ""bug"" where BigQueryUtils.doRowsExistFor() assumed value was a String that was really an Long, did not work for actual String values; - all behind feature flag (set to false by default) so to not break Beta. Successful run of `GvsJointVariantCalling`: https://job-manager.dsde-prod.broadinstitute.org/jobs/7d5e7b30-7b7c-475c-bf4e-86d6d38cfc8d; Successful run of `GvsQuickstartVcfIntegration`: https://job-manager.dsde-prod.broadinstitute.org/jobs/a17e171e-f8a7-465b-8214-52630f9ec9d1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8321:162,Load,LoadData,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8321,1,['Load'],['LoadData']
Performance,- also added it to the PGEN export WDL to analyze performance easier. test run of filter correction: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/f7de418a-05c7-4720-8dc9-a997b1cfd456; test run of PGEN extract: https://app.terra.bio/#workspaces/gvs-dev/RSA%20-%20GVS%20Quickstart%20V2%20/job_history/e2b2c875-4cfd-4cbe-879a-18fb91c1518e,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8954:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8954,1,['perform'],['performance']
Performance,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - CacheNode keys and tags have their own classes now for code clarity",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3034:124,cache,caches,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3034,3,"['Cache', 'cache']","['CacheNode', 'cache', 'caches']"
Performance,"- reviewed and restricted the access modifiers of all ICG-related classes; - got rid of the functionality to hold on to old caches: whenever a cache goes out of date, the reference is immediately made null in the new ICG; - completely rewrote ComputableGraphStructure in a functional style; - got rid of unused and unnecessary methods; - Created an ImmutableComputableGraphUtils and factored out the builder and other common methods; - math equality asserts for NDArray; - unit tests for ComputableGraphStructure; - unit tests for ImmutableComputableGraph; - unit tests for ImmutableComputableGraphUtils; - keys and tags have their own classes now",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035:124,cache,caches,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035,2,['cache'],"['cache', 'caches']"
Performance,- use aou service account to access gvcf and to load to BQ; - do manual localization with aou service account in create ingest tsv task to minimize vm spin up,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7133:48,load,load,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7133,1,['load'],['load']
Performance,-------------------------------------------; 10:29:22.408 INFO Mutect2 - ------------------------------------------------------------; 10:29:22.409 INFO Mutect2 - HTSJDK Version: 2.21.0; 10:29:22.409 INFO Mutect2 - Picard Version: 2.21.2; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:29:22.409 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:29:22.409 INFO Mutect2 - Deflater: IntelDeflater; 10:29:22.409 INFO Mutect2 - Inflater: IntelInflater; 10:29:22.409 INFO Mutect2 - GCS max retries/reopens: 20; 10:29:22.409 INFO Mutect2 - Requester pays: disabled; 10:29:22.409 INFO Mutect2 - Initializing engine; 10:29:22.609 INFO IntervalArgumentCollection - Processing 170805979 bp from intervals; 10:29:22.613 INFO Mutect2 - Done initializing engine; 10:29:22.622 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:29:22.624 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 10:29:22.625 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 10:29:22.625 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 10:29:22.631 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/share/java/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 10:29:22.660 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 10:29:22.660 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 10:29:22.660 INFO IntelPairHmm - Available threads: 40; 10:29:22.660 INFO IntelPairHmm - Requested threads: 4; 10:29:22.660 INFO PairHMM - Using the OpenMP multi-threaded AVX-accel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7032:3341,Load,Loading,3341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7032,1,['Load'],['Loading']
Performance,"-------------------------------------------; 16:26:35.422 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:26:35.423 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:26:35.427 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:26:35.427 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:26:35.427 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:26:35.427 INFO GenotypeGVCFs - Requester pays: disabled; 16:26:35.427 INFO GenotypeGVCFs - Initializing engine; 16:26:37.201 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:26:39.459 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:26:39 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2303197184; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:3044,load,load,3044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:27:54.145 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:54.146 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:54.146 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:27:54.146 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:27:54.146 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:27:54.146 INFO GenotypeGVCFs - Requester pays: disabled; 16:27:54.146 INFO GenotypeGVCFs - Initializing engine; 16:27:55.873 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:27:58.483 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:27:58 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2231894016; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:6399,load,load,6399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 21:16:35.498 INFO GenotypeGVCFs - Picard Version: 2.22.8; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:16:35.498 INFO GenotypeGVCFs - Deflater: IntelDeflater; 21:16:35.499 INFO GenotypeGVCFs - Inflater: IntelInflater; 21:16:35.499 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 21:16:35.499 INFO GenotypeGVCFs - Requester pays: disabled; 21:16:35.499 INFO GenotypeGVCFs - Initializing engine; 21:16:36.737 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 21:16:38.472 INFO GenotypeGVCFs - Shutting down engine; [January 17, 2021 9:16:38 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2551709696; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:4512,load,load,4512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['load'],['load']
Performance,"---------------------------------------; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 13:56:52.187 INFO GenotypeGVCFs - Picard Version: 2.22.8; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:56:52.187 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:56:52.187 INFO GenotypeGVCFs - Deflater: IntelDeflater; 13:56:52.188 INFO GenotypeGVCFs - Inflater: IntelInflater; 13:56:52.188 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 13:56:52.188 INFO GenotypeGVCFs - Requester pays: disabled; 13:56:52.188 INFO GenotypeGVCFs - Initializing engine; 13:56:53.115 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed.; 13:57:15.762 INFO GenotypeGVCFs - Shutting down engine; [December 21, 2020 1:57:15 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=2119696384; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012:3212,load,load,3212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012,1,['load'],['load']
Performance,"-----------------------------------. funcotator output:. (gatk) root@75181703d894:/gatk# ./gatk Funcotator \; > --variant ./my_data/test_b37.vcf \; > --reference ./my_data/human_g1k_v37.fasta \; > --ref-version hg19 \; > --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s \; > --output ./my_data/variants.funcotated.maf \; > --output-file-format MAF \; > --disable-sequence-dictionary-validation; Using GATK jar /gatk/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.0.0-local.jar Funcotator --variant ./my_data/test_b37.vcf --reference ./my_data/human_g1k_v37.fasta --ref-version hg19 --data-sources-path ./my_data/funcotator_dataSources.v1.7.20200521s --output ./my_data/variants.funcotated.maf --output-file-format MAF --disable-sequence-dictionary-validation; 12:11:19.732 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 24, 2021 12:11:19 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 12:11:19.904 INFO Funcotator - ------------------------------------------------------------; 12:11:19.904 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.2.0.0; 12:11:19.904 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:11:19.905 INFO Funcotator - Executing as root@75181703d894 on Linux v4.15.0-132-generic amd64; 12:11:19.905 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 12:11:19.905 INFO Funcotator - Start Date/Time: March 24, 2021 12:11:19 PM GMT; 12:11:19.905 INFO Funcotator - ------------------------------------------------------------; 12:11:19.905 INFO Funcot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:2642,Load,Loading,2642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['Load'],['Loading']
Performance,"------------------------------; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Version: 2.24.1; 10:49:50.814 INFO PathSeqBuildKmers - Picard Version: 2.25.4; 10:49:50.814 INFO PathSeqBuildKmers - Built for Spark Version: 2.4.5; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:49:50.814 INFO PathSeqBuildKmers - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:49:50.814 INFO PathSeqBuildKmers - Deflater: IntelDeflater; 10:49:50.814 INFO PathSeqBuildKmers - Inflater: IntelInflater; 10:49:50.815 INFO PathSeqBuildKmers - GCS max retries/reopens: 20; 10:49:50.815 INFO PathSeqBuildKmers - Requester pays: disabled; 10:49:50.815 INFO PathSeqBuildKmers - Initializing engine; 10:49:50.815 INFO PathSeqBuildKmers - Done initializing engine; 10:49:50.816 INFO PathSeqBuildKmers - Loading reference kmers...; 10:50:49.423 INFO PSKmerUtils - Generating kmers from 3713370968 bases in 412468 records...; 10:51:03.515 INFO PSKmerUtils - 6.7% complete - 249.0 Mbp at 1060.1 Mbp/min, 3.27 min remaining; 10:51:19.888 INFO PSKmerUtils - 13.2% complete - 491.1 Mbp at 967.3 Mbp/min, 3.33 min remaining; 10:51:35.575 INFO PSKmerUtils - 18.6% complete - 689.4 Mbp at 896.3 Mbp/min, 3.37 min remaining; 10:51:47.982 INFO PSKmerUtils - 23.7% complete - 879.7 Mbp at 901.3 Mbp/min, 3.14 min remaining; 10:52:01.291 INFO PSKmerUtils - 28.6% complete - 1061.2 Mbp at 886.0 Mbp/min, 2.99 min remaining; 10:52:10.127 INFO PSKmerUtils - 33.2% complete - 1232.0 Mbp at 916.0 Mbp/min, 2.71 min remaining; 10:52:21.475 INFO PSKmerUtils - 37.5% complete - 1391.4 Mbp at 906.9 Mbp/min, 2.56 min remaining; 10:52:28.848 INFO PSKmerUtils - 41.4% complete - 1536.5 Mbp at 927.2 Mbp/min, 2.35 min remaining; 10:52:34.246 INFO PSKmerUtils - 45.1% complete - 1674.9 Mbp at 958.7 Mbp/min, 2.13 min remaining;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8204:2573,Load,Loading,2573,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8204,1,['Load'],['Loading']
Performance,---------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1460850 1.3 5240 4105.1; 09:50:34.165 INFO ProgressMeter - 1:1960541 1.5 7060 4860.1; 09:50:46.537 INFO ProgressMeter - 1:2489135 1.7 8930 5383.3; 09:50:56.541 INFO ProgressMeter - 1:3195743 1.8 11330 6206,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1985,multi-thread,multi-threaded,1985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['multi-thread'],['multi-threaded']
Performance,------------; 12:18:11.388 INFO Mutect2 - ------------------------------------------------------------; 12:18:11.388 INFO Mutect2 - HTSJDK Version: 2.14.3; 12:18:11.388 INFO Mutect2 - Picard Version: 2.17.2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:18:11.388 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:11.388 INFO Mutect2 - Deflater: IntelDeflater; 12:18:11.388 INFO Mutect2 - Inflater: IntelInflater; 12:18:11.389 INFO Mutect2 - GCS max retries/reopens: 20; 12:18:11.389 INFO Mutect2 - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:11.389 INFO Mutect2 - Initializing engine; 12:18:11.724 INFO Mutect2 - Done initializing engine; 12:18:12.288 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 12:18:12.290 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 12:18:12.290 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 12:18:12.290 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/loeblabm11/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 12:18:12.368 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 12:18:12.368 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 12:18:12.369 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 12:18:12.403 INFO ProgressMeter - Starting traversal; 12:18:12.403 INFO ProgressMeter - Current Locus Elapsed Minute,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:2650,Load,Loading,2650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['Load'],['Loading']
Performance,---------; 13:24:08.557 INFO Mutect2 - HTSJDK Version: 2.23.0; 13:24:08.557 INFO Mutect2 - Picard Version: 2.22.8; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:24:08.558 INFO Mutect2 - Deflater: IntelDeflater; 13:24:08.558 INFO Mutect2 - Inflater: IntelInflater; 13:24:08.558 INFO Mutect2 - GCS max retries/reopens: 20; 13:24:08.558 INFO Mutect2 - Requester pays: disabled; 13:24:08.558 INFO Mutect2 - Initializing engine; 13:24:09.048 INFO FeatureManager - Using codec VCFCodec to read file file://ref/1000g_pon.hg38.vcf.gz; 13:24:09.207 INFO FeatureManager - Using codec VCFCodec to read file file://ref/af-only-gnomad.hg38.vcf.gz; 13:24:09.374 INFO Mutect2 - Done initializing engine; 13:24:09.435 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 13:24:09.438 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 13:24:09.472 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 13:24:09.472 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 13:24:09.473 INFO IntelPairHmm - Available threads: 24; 13:24:09.473 INFO IntelPairHmm - Requested threads: 4; 13:24:09.473 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 13:24:09.501 INFO ProgressMeter - Starting traversal; 13:24:09.502 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 13:24:19.721 INFO ProgressMeter - chr1:634040 0.2 2460 14443.7; 13:24:29.736 INFO ProgressMeter - chr1:1564703 0.3 7220 21,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:3065,Load,Loading,3065,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['Load'],['Loading']
Performance,"----. ## Bug Report. ### Affected tool(s) or class(es); - Tool/class name(s), special parameters: GenomicsDBImport. ### Affected version(s); - Version: gatk4-4.4.0.0-0. ### Description ; Hello,. I have been having an issue come up when utilizing `GenomicsDBImport`. This issue has happened when using a range of samples and shard counts (8 - 1000 samples, shard count of up to 2000). My current example is an attempt to joint call 1000 samples together. I will submit the jobs and 1-2 of the shards (of the ~100 concurrently running) will throw a `malloc(): unaligned tcache chunk detected`. When I resubmit that shard, it will usually rerun without a problem. Or if I kill all jobs and resubmit, a different shard will throw the malloc error. . I have run approximately 20 tests and I seem to get this failure 2/3 times. However, it only arises on the initial submission and not when additional jobs are submitted as previous shards complete. Please note that the 1000 samples have successfully been imported into the GenomicsDB but this error seems to persist somewhat randomly across multiple machines. . Thank you for your assistance! . #### Steps to reproduce. - Command used (omitting paths to 1000 samples for brevity) for one of the failed shards. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8g -jar /gpfs/gpfs_de6000/home/dalegre/miniconda3/envs/GOASTv4.0/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport -V [samples 1-1002] --genomicsdb-workspace-path results/jointcalling/genomicsDB/temp_0882_of_2000_DB --merge-input-intervals false --bypass-feature-reader --tmp-dir temp --max-num-intervals-to-import-in-parallel 10 --batch-size 50 --intervals results/germline/interval/temp_0882_of_2000/scattered.interval_list --genomicsdb-shared-posixfs-optimizations true; ```. #### Expected behavior; All shards are imported into the GenomicsDB successfu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683:512,concurren,concurrently,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683,1,['concurren'],['concurrently']
Performance,----. ## Bug Report. ### Affected tool(s) or class(es); GATK LiftoverVcf. ### Affected version(s); gatk/4.1.7.0. ### Description . The LiftoverVcf generates the following error. The error occurs with SVs where the INFO/END is not also lifted over. This results in INFO/END before the site start position which triggers the error.; ```; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar LiftoverVcf -I b37/HG002_SVs_Tier1_v0.6.vcf.gz -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz -CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 10:20:35.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Sun Jul 26 10:20:35 EDT 2020] LiftoverVcf --INPUT b37/HG002_SVs_Tier1_v0.6.vcf.gz --OUTPUT b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz --CHAIN grch37_to_grch38.over.chain.gz --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz --REFERENCE_SEQUENCE /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --WARN_ON_MISSING_CONTIG false --LOG_FAILED_INTERVALS true --WRITE_ORIGINAL_POSITION false --WRITE_ORIGINAL_ALLELES false --LIFTOVER_MIN_MATCH 1.0 --ALLOW_MISSING_FIELDS_IN_HEADER false --RECOVER_SWAPPED_REF_ALT false --TAGS_TO_REVERSE AF --TAGS_TO_DROP MAX_AF --DISABLE_SORT false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:958,Load,Loading,958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['Load'],['Loading']
Performance,"--master spark://ip-xxx-xx-xx-xxx:xxxx --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true /curr/tianj/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I /curr/tianj/data/sortedbam/xx_sort.bam -M xx.m -O xx_markduplicatespark.bam --TMP_DIR tmp --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx; 00:48:13.577 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 7, 2017 12:48:13 AM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark --output xx_markduplicatespark.bam --METRICS_FILE xx.m --input /curr/tianj/data/sortedbam/xx_sort.bam --sparkMaster spark://ip-xxx-xx-xx-xxx:xxxx --TMP_DIR tmp --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --READ_NAME_REGEX [a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 7, 2017 12:48:13 AM UTC] Executing as tian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:1955,Load,Loading,1955,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['Load'],['Loading']
Performance,"-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Using GATK jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms454m -Xmx3181m -XX:+UseSerialGC -jar /mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar GenomicsDBImport --reader-threads 1 --genomicsdb-workspace-path CDL-164-04P-1_0_249250621_genomicsdb -L 1:1-249250621 --variant /mnt/isilon/cbmi/variome/rathik/mendelian_rnaseq/gatk_output/CDL-164-04P/variation/rnaseq/gatk-haplotype/Sample_1__CDL-164-04P-gatk-haplotype-annotated-rnaedit-annotated-gemini.vcf.gz; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/mnt/isilon/cbmi/variome/tmp/rathik; 11:49:24.784 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/isilon/cbmi/variome/bin/bcbio-nextgen/bcbio/anaconda/share/gatk4-4.0.6.0-0/gatk-package-4.0.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:49:25.130 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.130 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.6.0; 11:49:25.131 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:49:25.134 INFO GenomicsDBImport - Executing as rathik@reslnrefo01.research.chop.edu on Linux v3.10.0-862.el7.x86_64 amd64; 11:49:25.134 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 11:49:25.134 INFO GenomicsDBImport - Start Date/Time: July 20, 2018 11:49:24 AM EDT; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; 11:49:25.134 INFO GenomicsDBImport - ------------------------------------------------------------; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045:1697,Load,Loading,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045,1,['Load'],['Loading']
Performance,"-23590f0ab31f/call-PathSeqAlign/MMRF_2072_2_BM.microbe_aligned.paired.bam:33554432+33554432 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5) java.util.NoSuchElementException: next on empty iterator at scala.collection.Iterator$$anon$2.next(Iterator.scala:39) at scala.collection.Iterator$$anon$2.next(Iterator.scala:37) at scala.collection.Iterator$$anon$13.next(Iterator.scala:469) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155) at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); `. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709:2024,concurren,concurrent,2024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709,2,['concurren'],['concurrent']
Performance,"-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.171 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.172 INFO GenomicsDBImport - Starting batch inp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:5297,concurren,concurrent,5297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance,"-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time: July 25, 2020 1:39:03 AM GMT; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.363 INFO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2358,Load,Loading,2358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFControlSample/Benchmark/06cbfab4-17a7-4415-9118-d0ebbe156bfd/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFTestSample/Benchmark/362a3e75-6a39-4bde-bb79-e6562dc66dd9/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:18366,cache,cacheCopy,18366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"-CompareSAMs not ported because ReadWalker traversal is not suited; for it. -SplitNCigarReads not ported because of the way it uses the reference; (could be ported to ReadWalker with some refactoring, however). There were a few engine changes as well to accomodate the new ReadWalker tools:. -Method to allow walkers to access the SAM header from the reads data source. -No longer require an index for BAM/SAM files when no intervals are; provided and no queries are performed. -onTraversalDone() now allows tools to return a value, which is printed; out by the engine. Resolves #113",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/122:467,perform,performed,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/122,1,['perform'],['performed']
Performance,"-Created a new base class for Spark tools, GATKSparkTool, that centrally manages; and validates standard tool inputs (reads, reference, and intervals). This allows; us to enforce consistency across tools, delete duplicated boilerplate code from tools; to load inputs, and perform standard kinds of validation (eg., sequence dictionary; validation) in one place. -Tools that don't fit into the pattern established by GATKSparkTool can still extend; SparkCommandLineProgram directly. -This is just a first step -- there is still much work to be done to unify our data source; classes and transparently handle inputs from different sources (GCS, hdfs, files), but; having inputs centrally managed should make the remaining tasks much easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/955:255,load,load,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/955,2,"['load', 'perform']","['load', 'perform']"
Performance,"-Created a new class of tool, IntervalWalker, that processes a single interval at a time,; with the ability to query optional overlapping sources of reads, reference data, and/or; features/variants. Current implementation is simple/naive with no special caching;; performance issues will be addressed once we port this traversal type to dataflow. -Added the ability for VariantWalkers to access contextual reads/reference/feature data. -To enable the above changes, migrated most of the engine to use SimpleIntervals rather; than GenomeLocs. This allows for the creation of Context objects in traversals where there; is not necessarily a sequence dictionary available (eg., VariantWalker). -Moved shared arguments/code from Walker classes up into GATKTool. Still some issues; related to marking engine-wide arguments as optional/required on a per-traversal or; per-tool basis, but tickets have been created for these. -Since there isn't yet an htsjdk release that contains SimpleInterval, temporarily; checked a copy of it into our repo, which we can remove the next time we; rev htsjdk. TODOs:. -We currently still require a sequence dictionary to actually parse intervals in; IntervalArgumentCollection. This is due entirely to our support of intervals without; specific stop positions (eg., ""chr1"" and ""chr1:1+"") -- for these intervals we must; look up the stop position in a sequence dictionary. This means that IntervalWalkers; currently require at least one input that contains a sequence dictionary (although; VariantWalkers do not). We should look into ways of relaxing this restriction. Resolves #109",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/297:264,perform,performance,264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/297,1,['perform'],['performance']
Performance,"-Dev, Python, libgomp) and tried running HaplotypeCaller on a sample of mine to test everything. . Unfortunately, it always breaks when running the steps with the JNI libraries. If I start running things on native Java (without the C++ libraries), things work well. . I copied my log file and the command + hs_err file in here. . Anything I missed here?. ``` ; Using GATK jar /usr/bin/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx16g -jar /usr/bin/gatk-package-4.0.0.0-local.jar HaplotypeCaller -I file.bam -R human_g1k_v37.fasta -O test.vcf -ERC GVCF --create-output-variant-index --annotation MappingQualityRankSumTest --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation FisherStrand --annotation Coverage --dbsnp dbsnp_138.b37.vcf --verbosity INFO; 12:18:41.830 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.092 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 12:18:42.092 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:18:42.092 INFO HaplotypeCaller - Executing as iiipe01@node050 on Linux v3.10.0-693.2.2.el7.x86_64 amd64; 12:18:42.092 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_151-b12; 12:18:42.092 INFO HaplotypeCaller - Start Date/Time: January 15, 2018 12:18:41 PM GMT; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.092 INFO HaplotypeCaller - ------------------------------------------------------------; 12:18:42.093 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 12:18:42.093 INFO HaplotypeCaller",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:1146,Load,Loading,1146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Load'],['Loading']
Performance,"-Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar FilterAlignmentArtifacts --reference /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.gz --variant /data/filteredVCF/in2510-8.orientationFilter.vcf --input /data/rawVCF/mutectBAM/in2510-8.mutect2.bam --bwa-mem-index-image /home/gatk/references/Sars_cov_2.ASM985889v3.dna_sm.toplevel.fa.img --output /data/alignmentArtifactFilteredVCF/in2510-8.orientationFilter.alignmentArtifactFilter.vcf; 08:33:36.572 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 08:33:36.591 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 08:33:36.592 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 08:33:36.826 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 25, 2021 8:33:37 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 08:33:37.130 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 08:33:37.130 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.9.0-SNAPSHOT; 08:33:37.130 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 08:33:37.131 INFO FilterAlignmentArtifacts - Executing as gatk@1ff04a9b2ba9 on Linux v5.4.72-microsoft-standard-WSL2 amd64; 08:33:37.131 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 08:33:37.131 INFO FilterAlignmentArtifacts - Start Date/Time: March 25, 2021 8:33:36 AM GMT; 08:33",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7162:2291,Load,Loading,2291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7162,1,['Load'],['Loading']
Performance,"-Ported the remaining pieces of the HaplotypeCaller and assembled them; into a runnable tool. Fixed many bugs in our ported code in the process; of doing so. -Extracted a separate ""engine"" that does all the work of the HC and is; separate from the runnable walker. -Added a new walker class, ReadWindowWalker, as a prospective replacement; for active region traversal. -Hooked up the native VECTOR_LOGLESS_CACHING PairHmm to the HaplotypeCaller,; and activated it by default (this speeds up performance by ~3x in my tests).; Also added a fallback mode when AVX is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1567:491,perform,performance,491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567,1,['perform'],['performance']
Performance,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1847:209,load,load,209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847,4,['load'],"['load', 'loaded', 'loading', 'loads']"
Performance,"-Reduce memory usage of AssemblyRegion traversal by an order of magnitude; by loading the reads for each shard more lazily. -Add a sharding mode that creates one shard per user interval (or per contig,; if there are no explicit intervals), and make it the default for both HaplotypeCaller; and Mutect2. -When determining active regions, only consider loci within the user's intervals (but; still include surrounding reads in the final region). This mimics GATK3.x behavior. -Serve up empty pileup objects for uncovered loci (this also mimics GATK3.x behavior).; The fact that we weren't doing this before was responsible for much of the remaining; difference vs. the GATK 3.x HaplotypeCaller. -Ported GATK 3 PR 1389 (use median rather than the second-best likelihood for the; NON_REF allele). -Ported a change to the ReferenceConfidenceModel from GATK3. -Fixed a bug in ReadLikelihoods that was causing ArrayIndexOutOfBoundsException. -Added special handling of RawMQ to HaplotypeCaller (mirrors the handling of RawMQ; from GenotypeGVCFs). -Added updated concordance test data generated with HaplotypeCaller 3.8-4-g7b0250253f. Resolves #1950; Resolves #3516; Resolves #3517; Resolves #3518; Resolves #3233; Resolves #2848",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3519:78,load,loading,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3519,1,['load'],['loading']
Performance,"-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1679,optimiz,optimized,1679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimized']
Performance,"-dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that are problematic, so we could have our cake and eat it too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5791:2884,perform,performance,2884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791,1,['perform'],['performance']
Performance,"-driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:175); at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentB",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:1417,load,loadClass,1417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,1,['load'],['loadClass']
Performance,"-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Na",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1865,load,load,1865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"-initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:6318,Load,Loading,6318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Load'],['Loading']
Performance,"-output-prefix {params.prefix} -imr OVERLAPPING_ONLY -O {output.ploidy_calls}`. When solved, {params.files} creates something like ""-I sample1.hdf5 -I sample2.hdf5"" etc... Involved **software versions**:; **gcnvkernel** = 0.7; **gatk** = 4.2.0.0; **Python** = 3.6.10. **Complete log**: . ```; Using GATK jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar DetermineGermlineContigPloidy -L results/cnv/targets.preprocessed.interval_list -I results/cnv/hdf5/MGM20-0848_S4.hdf5 -I results/cnv/hdf5/MGM20-0872_S2.hdf5 -I results/cnv/hdf5/MGM20-1121_S4.hdf5 -I results/cnv/hdf5/MGM20-1543_S10.hdf5 --contig-ploidy-priors resources/contig_ploidy_priors.tsv --output-prefix ploidy -imr OVERLAPPING_ONLY -O results/cnv/ploidy; 15:09:27.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 18, 2021 3:09:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:09:27.686 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.686 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.2.0.0; 15:09:27.687 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:09:27.687 INFO DetermineGermlineContigPloidy - Executing as n.liorni@hpc001 on Linux v3.10.0-1127.el7.x86_64 amd64; 15:09:27.687 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 15:09:27.687 INFO DetermineGermlineContigPloidy - Start Date/Time: 18 ottobre 2021 15.09.27 CEST; 15:09:27.68",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:1303,Load,Loading,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['Load'],['Loading']
Performance,". After running the given below code, I am not able to find the output file (summary file). This is the link, where the code is given \[[https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary\](/hc/en-us/articles/4405451404699-Concordance#--summary)](https://gatk.broadinstitute.org/hc/en-us/articles/4405451404699-Concordance#--summary](/hc/en-us/articles/4405451404699-Concordance#--summary)). Please also find the log file below. Is the summary file required as input file to run the below script? Please advice. Â gatk Concordance \\ ; ; Â Â  -R /scicore/home/cichon/GROUP/memory\_optimization/data/reference/gch38.fa \\ ; ; Â Â  -eval /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/sample1\_affect.filtered.vcf \\ ; ; Â Â  --truth /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/NA12878.vcf.gz \\ ; ; Â Â  --summary /scicore/home/cichon/GROUP/memory\_optimization/variants/filtered/summary.tsvÂ Â  ; 11:26:21.545 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/scicore/soft/apps/GATK/4.2.2.0-foss-2018b-Java-1.8/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 11, 2021 11:26:21 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:26:21.681 INFO Concordance - ------------------------------------------------------------ ; ; 11:26:21.682 INFO Concordance - The Genome Analysis Toolkit (GATK) v4.2.2.0 ; ; 11:26:21.682 INFO Concordance - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:26:21.682 INFO Concordance - Executing as [thirun0000@shi85.cluster.bc2.ch](mailto:thirun0000@shi85.cluster.bc2.ch) on Linux v3.10.0-1062.18.1.el7.x86\_64 amd64 ; ; 11:26:21.682 INFO Concordance - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_212-b03 ; ; 11:26:21.682 INFO Concordance - S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7562:1601,Load,Loading,1601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7562,1,['Load'],['Loading']
Performance,". CombineGVCFs truns to a error ""Exception in thread ""main"" java.lang.OutOfMemoryError"" .; then I chose to use GenomicsDBImport to do this job. It still doesn't work. First error is ""read_one_line_fully && ""Buffer did not have space to hold a line fully - increase buffer size""; I add ""--genomicsdb-vcf-buffer-size 16384000"" , it causes different error ""Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space"". This is my command and work log.; My java version is ; openjdk version ""1.8.0_152-release""; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12). GATK is very helpful in my research, and I really need some help to get it work. gatk --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - H",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:1232,Load,Loading,1232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,1,['Load'],['Loading']
Performance,. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2260,concurren,concurrent,2260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,. The exception I get is:; org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (20) exceeded: evaluations; 	at org.apache.commons.math3.optim.BaseOptimizer$MaxEvalCallback.trigger(BaseOptimizer.java:242); 	at org.apache.commons.math3.util.Incrementor.incrementCount(Incrementor.java:155); 	at org.apache.commons.math3.optim.BaseOptimizer.incrementEvaluationCount(BaseOptimizer.java:191); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.computeObjectiveValue(UnivariateOptimizer.java:148); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:225); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:43); 	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:153); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.optimize(UnivariateOptimizer.java:70); 	at org.broadinstitute.hellbender.utils.OptimizationUtils.max(OptimizationUtils.java:40); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.lambda$calculateContamination$13(ContaminationModel.java:214); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.calculateContamination(ContaminationModel.java:215); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.<init>(ContaminationModel.java:67); 	at org.broadinstitute.hellbende,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6282:1055,Optimiz,OptimizationUtils,1055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6282,1,['Optimiz'],['OptimizationUtils']
Performance,"..... . 12:38:08.377 INFO KnownSitesCache - Number of variants read: 60200001; 12:38:09.341 INFO KnownSitesCache - Number of variants read: 60300001; 12:38:10.131 INFO KnownSitesCache - Number of variants read: 60400001; 12:38:10.940 INFO KnownSitesCache - Number of variants read: 60500001; 12:38:11.719 INFO KnownSitesCache - Number of variants read: 60600001; 12:38:12.568 INFO KnownSitesCache - Number of variants read: 60700001; 12:38:13.432 INFO KnownSitesCache - Number of variants read: 60800001; 12:38:14.137 INFO KnownSitesCache - Number of variants read: 60900001; 12:38:14.833 INFO KnownSitesCache - Number of variants read: 61000001; 12:39:23.200 INFO BaseRecalibrationEngine - The covariates being used here: ; 12:39:23.200 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; ```; Based on the time stamps, the observation is that it took 10 min to read the KnowSites.vcf file (about 10GB), for the code which is a filtering and copy operation:. ```java; private static List<GATKVariant> wrapQueryResults(final Iterator<VariantContext> queryResults ) {; final List<GATKVariant> wrappedResults = new ArrayList<>();; long count = 0;; while ( queryResults.hasNext() ) {; if (count++ % 100000 == 0) {; log.info(""Number of variants read: "" + count);; }; wrappedResults.add(VariantContextVariantAdapter.sparkVariantAdapter(queryResults.next()));; }; return wrappedResults;; }. ```; Seems to me this is awfully slow. The vcf file resides on HDFS (with a 100Gbps switch and backed up by a NVMe storage with over 1GBps bandwidth). Assuming we can achieve a persistent 100MBps bandwidth (which IMO is quite modest), it would take 100 sec to fetch the file. Added the overhead, it should take only 2 to 3 minutes to finish this process. If it turns out that IO performance cannot be improved, I have the impression that knownSites file is relatively stable. Would it make sense to convert the KnownSites into an intermediate format which can be read much faster? Or smaller than the 2GB threshold?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4264:2511,perform,performance,2511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4264,1,['perform'],['performance']
Performance,../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2910,cache,cachemanager,2910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,2,"['Cache', 'cache']","['CacheNode', 'cachemanager']"
Performance,".0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; GenotypeGVCFs \; -R ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.fasta"" \; -O ""$tmp_vcf"" \; -D ""/share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e64f393e-2ac6-43e6-9b20-cbfa905e7c33/call-GenotypeGVCFs/shard-17/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf"" \; -G StandardAnnotation \; --only-output-calls-starting-in-intervals \; --use-new-qual-calculator \; -V gendb://""$genomicsdb"" \; -L ""chr18:1-80373285"". ""/share/ClusterShare/software/contrib/evaben/gatk/prebuilt/4.0.4.0/bin/gatk"" --java-options ""-Xmx8g -Xms8g"" \; VariantFiltration \; --filter-expression ""ExcessHet > 54.69"" \; --filter-name ExcessHet \; -O ""output.vcf.gz"" \; -V ""$tmp_vcf""; ```. And a SGE hard memory limit of 40G (GenotypeGVCFs has -Xmx8g).; On gatk 4.0.4.0 I see peak memory usage of 15.7G, while with gatk 4.0.6.0 I get:. ```; ...; 19:06:23.757 INFO GenotypeGVCFs - Initializing engine; 19:06:24.785 INFO FeatureManager - Using codec VCFCodec to read file file:///share/ScratchGeneral/evaben/cromwell/cromwell-executions/JointGenotyping/e9bf8c5e-3e70-476a-99a2-833f9d38cb2f/call-GenotypeGVCFs/shard-0/inputs/1017648146/Homo_sapiens_assembly38.dbsnp138.vcf; terminate called after throwing an instance of 'std::length_error'; what(): vector::_M_default_append; ```. It seems unlikely to be just a performance regression, maybe something is wrong with my commandline/inputs that only the new version is revealing. This may be in the genomicsdb part of the codebase, as that is the input file I am reading. . [stderr of failure (4.0.6.0) ](https://github.com/broadinstitute/gatk/files/2204252/gengvcferr.txt); [stderr of success (4.0.4.0) ](https://github.com/broadinstitute/gatk/files/2204253/gengvcfgood.txt); [script of failure](https://github.com/broadinstitute/gatk/files/2204254/gengvcfscript.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024:2240,perform,performance,2240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024,1,['perform'],['performance']
Performance,.018 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.213 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 12:28:19.227 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Cosmic.db -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic/hg38/Cosmic.db; > 12:28:19.401 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_tissue.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic_tissue/hg38/cosmic_tissue.tsv; > 12:28:19.487 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_a_bed : 100000; > 12:28:19.495 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.500 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.config; > 12:28:19.505 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.507 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > WARNING 2020-07-21 12:28:19 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:13006,cache,cache,13006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,".0; 10:46:05.153 INFO CNNScoreVariants - Built for Spark Version: 2.4.5; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:46:05.153 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:46:05.153 INFO CNNScoreVariants - Deflater: IntelDeflater; 10:46:05.153 INFO CNNScoreVariants - Inflater: IntelInflater; 10:46:05.153 INFO CNNScoreVariants - GCS max retries/reopens: 20; 10:46:05.153 INFO CNNScoreVariants - Requester pays: disabled; 10:46:05.153 INFO CNNScoreVariants - Initializing engine; 10:46:05.598 INFO FeatureManager - Using codec VCFCodec to read file file:///lustre/scratch/scratch/regmova/tmp/TEST_DATA/TR017_GERMLINE_VARIANTS/TR017.GL.vcf.gz; 10:46:05.638 INFO CNNScoreVariants - Done initializing engine; 10:46:05.639 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/lustre/home/regmova/tools/gatk/build/libs/gatk-package-4.2.0.0-19-ge60cdf8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 10:46:35.436 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/1d_cnn_mix_train_full_bn.8208762367402959162.json and weights:/tmp/1d_cnn_mix_train_full_bn.2787226329292768726.hd5; 10:46:35.438 INFO CNNScoreVariants - Done scoring variants with CNN.; 10:46:35.438 INFO CNNScoreVariants - Shutting down engine; [12 May 2021 10:46:35 BST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.51 minutes.; Runtime.totalMemory()=2132279296; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/regmova/miniconda3/envs/gatk/lib/python3.6/site-packages/vq",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7250:3066,Load,Loading,3066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7250,1,['Load'],['Loading']
Performance,".1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1086,Load,Loading,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,".2; 06:47:33.116 INFOÂ  ProgressMeter - NC_038255.2:27256247Â Â Â Â Â Â Â Â Â Â Â Â  21.2Â Â Â Â Â Â Â Â Â Â Â Â  122736000Â Â Â Â Â Â Â  5795288.5; 06:47:42.432 INFOÂ  CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509); at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:22797,load,loadNextNovelFeature,22797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['load'],['loadNextNovelFeature']
Performance,".679 INFO HaplotypeCaller - Requester pays: disabled 13:39:56.680 INFO HaplotypeCaller - Initializing engine 13:39:56.968 INFO HaplotypeCaller - Done initializing engine 13:39:56.971 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled 13:39:57.000 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output 13:39:57.000 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output 13:39:57.020 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so 13:39:57.026 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported 13:39:57.026 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation! 13:39:57.108 INFO ProgressMeter - Starting traversal 13:39:57.110 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute 13:40:07.119 INFO ProgressMeter - chr19:8969701 0.2 29900 179382.1 13:40:17.116 INFO ProgressMeter - chr19:20264701 0.3 67550 202609.5 13:40:27.115 INFO ProgressMeter - chr19:31874701 0.5 106250 212471.7 13:40:37.116 INFO ProgressMeter - chr19:44792701 0.7 149310 223937.0 13:40:49.251 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes 13:40:49.413 INFO ProgressMeter - chr19:55910600 0.9 186370 213817.0; 13:40:55.466 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter; 0 r",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:13703,multi-thread,multi-threaded,13703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['multi-thread'],['multi-threaded']
Performance,".689 s]; [INFO] External Example ................................... FAILURE [ 0.051 s]; [INFO] GATK Queue ......................................... SKIPPED; [INFO] GATK Queue Extensions Generator .................... SKIPPED; [INFO] GATK Queue Extensions Public ....................... SKIPPED; [INFO] GATK Aggregator Public ............................. SKIPPED; [INFO] GATK Tools Protected ............................... SKIPPED; [INFO] GATK Package Distribution .......................... SKIPPED; [INFO] GATK Queue Extensions Distribution ................. SKIPPED; [INFO] GATK Queue Package Distribution .................... SKIPPED; [INFO] GATK Aggregator Protected .......................... SKIPPED; [INFO] GATK Tools Private ................................. SKIPPED; [INFO] GATK Package Internal .............................. SKIPPED; [INFO] NA12878 KB Utilities ............................... SKIPPED; [INFO] GATK Queue Private ................................. SKIPPED; [INFO] GATK Queue Extensions Internal ..................... SKIPPED; [INFO] GATK Queue Package Internal ........................ SKIPPED; [INFO] GATK Aggregator Private ............................ SKIPPED; [INFO] ------------------------------------------------------------------------; [INFO] BUILD FAILURE; [INFO] ------------------------------------------------------------------------; [INFO] Total time: 01:23 min; [INFO] Finished at: 2018-04-20T20:52:19+02:00; [INFO] Final Memory: 67M/922M; [INFO] ------------------------------------------------------------------------; [ERROR] Failed to execute goal on project external-example: Could not resolve dependencies for project org.mycompany.app:external-example:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.broadinstitute.gatk:gatk-tools-public:jar:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-utils:jar:tests:3.8-SNAPSHOT, org.broadinstitute.gatk:gatk-engine:jar:tests:3.8-SNAPSHOT: Could not find artifact org.broadinstitute.gatk:gatk-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4686:1979,Queue,Queue,1979,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686,1,['Queue'],['Queue']
Performance,".Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:40 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:41 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:42 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:43 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:44 INFO yarn.Client: Application report for application_1603353714322_0004 (state: FAILED); 20/10/22 12:02:44 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: Application application_1603353714322_0004 failed 2 times due to AM Container for appattempt_1603353714322_0004_000002 exited with exitCode: 13; For more detailed output, check application tracking page:http://jacky:8088/cluster/app/application_1603353714322_0004Then, click on links to logs of each attempt.; Diagnostics: Exception from container-launch.; Container id: container_1603353714322_0004_02_000001; Exit code: 13; Stack trace: ExitCodeException exitCode=13: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545); 	at org.apache.hadoop.util.Shell.run(Shell.java:456); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:6625,concurren,concurrent,6625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,3,['concurren'],['concurrent']
Performance,.ExceptionInInitializerError; 	at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:37); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:883); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:605); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: org.genomicsdb.exception.GenomicsDBException: Could not load genomicsdb native library; 	at org.genomicsdb.GenomicsDBUtilsJni.<clinit>(GenomicsDBUtilsJni.java:33); 	... 10 more; Caused by: java.lang.UnsatisfiedLinkError: /tmp/libtiledbgenomicsdb8918780584607909502.so: libcurl.so.4: cannot open shared object file: No such file or directory; 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:147); 	at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:47); 	at org.genomicsdb.GenomicsDBUtilsJni.<clinit>(GenomicsDBUtilsJni.java:30); 	... 10 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24378/error-while-running-genomicsdbimport/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6122:1961,load,load,1961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6122,6,['load'],"['load', 'loadLibrary', 'loadLibraryFromJar']"
Performance,.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:14575,cache,cache,14575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:7750,cache,cache,7750,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,.ReadStateManager.collectPendingReads(ReadStateManager.java:159); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.lazyLoadNextAlignmentContext(LocusIteratorByState.java:315); 	at org.broadinstitute.hellbender.utils.locusiterator.LocusIteratorByState.hasNext(LocusIteratorByState.java:252); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContext(IntervalAlignmentContextIterator.java:104); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.advanceAlignmentContextToCurrentInterval(IntervalAlignmentContextIterator.java:99); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:69); 	at org.broadinstitute.hellbender.utils.locusiterator.IntervalAlignmentContextIterator.next(IntervalAlignmentContextIterator.java:21); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:143); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); 	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:5432,load,loadNextAssemblyRegion,5432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['load'],['loadNextAssemblyRegion']
Performance,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:45293,concurren,concurrent,45293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,2,['concurren'],['concurrent']
Performance,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648:2310,concurren,concurrent,2310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648,2,['concurren'],['concurrent']
Performance,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2173,concurren,concurrent,2173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575,2,['concurren'],['concurrent']
Performance,".WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:24); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```. for multi sample:; ```; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 31, column 50:. <title>gatk/mutect2_multi_sample.wdl at master ? broadinstitute/gatk ? GitHub</title>; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:170); 	at scala.util.Try$.apply(Try.scala:213); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:630); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:131); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:162); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:167); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:24); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6261:2333,load,load,2333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6261,1,['load'],['load']
Performance,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1839:1443,concurren,concurrent,1443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839,5,['concurren'],['concurrent']
Performance,".csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9843"",; ""NIST controlindelPrecision"": ""0.9895"",; ""NIST controlsnpF1Score"": ""0.9908"",; ""NIST controlsnpPrecision"": ""0.992"",; ""NIST controlsnpRecall"": ""0.9896"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFControlSample/Benchmark/eaf4d582-e197-4e13-8122-5e1ec22591ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9843"",; ""NIST evalindelPrecision"": ""0.9895"",; ""NIST evalsnpF1Score"": ""0.9908"",; ""NIST evalsnpPrecision"": ""0.992"",; ""NIST evalsnpRecall"": ""0.9896"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFTestSample/Benchmark/87985440-93fa-4a33-ac09-e4cbead32bfb/call-CombineSummaries/summary.csv""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:14468,cache,cacheCopy,14468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,".driver.maxResultSize=0,spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ,spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2301,load,load,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['load']
Performance,".exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai \; > -O HG00190_cram.bam \; > -L chr17; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai -O HG00190_cram.bam -L chr17; 15:00:28.170 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 3:00:28 PM EDT] PrintReads --output HG00190_cram.bam --intervals chr17 --input gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.crai --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:21367,Load,Loading,21367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['Load'],['Loading']
Performance,.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:17459,concurren,concurrent,17459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['concurren'],['concurrent']
Performance,".gz --tmp-dir=/tmp --sample-ploidy 24 -L chrom2; ```. It failed at the same region it was failing before, with this error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:1697,load,loadNextFeature,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,1,['load'],['loadNextFeature']
Performance,.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9183,Cache,CacheStep,9183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,.java:91); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:144); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551); 	at com.esotericsoftware.kryo.Kryo.readObj,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:2088,concurren,concurrent,2088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,2,['concurren'],['concurrent']
Performance,.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2473,Cache,CacheSupplementedGoogleCloudStorage,2473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:112); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:589); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.initial,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:8636,concurren,concurrent,8636,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['concurren'],['concurrent']
Performance,".reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7699,concurren,concurrent,7699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,".reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to determine candidate codecs for reading a VCF file. The unit test files FeatureDataSourceUnitTest and FeatureManagerUnitTest pass when running the unit tests all at once, and also pass individually. The test files correctly generate under appropriate directories under src/test/resources, as far as I can tell. . Attached is a zip archive of the test results:; [test_resu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:6008,concurren,concurrent,6008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['concurren'],['concurrent']
Performance,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.hapl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7519,concurren,concurrent,7519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); ```. However, when trying to run the unit tests that failed using commands like:; ```; ./gradlew test --tests VctOutputRendererUnitTest; ```; The same tests will pass. Following the stack trace, I found that several of these failures were because the FeatureManager class threw a GATKException. Per the source code in FeatureManager.java, the exception was thrown because of either an InstantiationException, IllegalAccessException, NoSuchMethodException, or an InvocationTargetException caught when trying to determine candidate codecs for reading a VCF file. The unit test files FeatureDataSourceUnitTest and FeatureManagerUnitTest pass when running the unit tests all at once, and also pass indivi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6748:5828,concurren,concurrent,5828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6748,1,['concurren'],['concurrent']
Performance,".samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at htsjdk.samtools.metrics.MetricsFile.loadClass(MetricsFile.java:471); 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:353); 	... 8 more; ```. If it is replaced, the tool still errors but with a different error:; ```; java.lang.IllegalArgumentException: Features added out of order: previous (TabixFeature{referenceIndex=0, start=118314029, end=118314036, featureStartFilePosition=1403632633, featureEndFilePosition=-1}) > next (TabixFeature{referenceIndex=0, start=33414233, end=33414234, featureStartFilePosition=1403632876, featureEndFilePosition=-1}); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.addFeature(TabixIndexCreator.java:89); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:1994,load,loadClass,1994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,1,['load'],['loadClass']
Performance,.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar BaseRecalibratorSpark --spark-master local[8] -R /docker/reference/Data/B37/GATKBundle/2.8_subset_arup_v0.1/human_g1k_v37_decoy_phiXAdaptr.fasta -I bam/rmdup_gatkrealign.bam -O gatk_base_recal_table.txt --known-sites /docker/reference/Data/B37/DbSNP/dbSNP_147_20160601/All_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:7113,concurren,concurrent,7113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['concurren'],['concurrent']
Performance,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:3584,concurren,concurrent,3584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['concurren'],['concurrent']
Performance,.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/appender/AbstractAppender; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:411); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.decodeCacheFiles(PluginRegistry.java:181); at org.apache.logging.log4j.core.config.plugins.util.PluginRegistry.loadFromMainClassLoader(PluginRegistry.java:119); at org.apache.logging.log4j.core.config.plugins.util.PluginManager.collectPlugins(PluginManager.java:132); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:131); at org.apache.logging.log4j.core.pattern.PatternParser.<init>(PatternParser.java:112); at org.apache.logging.log4j.core.layout.PatternLayout.createPatternParser(PatternLayout.java:220); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:138); at org.apache.logging.log4j.core.layout.PatternLayout.<init>(PatternLayout.java:57); at org.apache.logging.log4j.core.layout.PatternLayout$Builder.build(PatternLayout.java:446); at org.apache.logging.log4j.core.config.AbstractConfiguration.setToDefault(AbstractConfiguration.java:518,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:3228,load,loadClass,3228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,1,['load'],['loadClass']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.clou,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6580,concurren,concurrent,6580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685:6092,concurren,concurrent,6092,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.UnknownHostException: www.googleapis.com; 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264); 	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.p,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094:6223,concurren,concurrent,6223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:114); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at shaded.cloud_nio.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.Abstra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:4758,concurren,concurrent,4758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['concurren'],['concurrent']
Performance,.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:140); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:121); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:170); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:130); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:67); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337:6410,concurren,concurrent,6410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337,2,['concurren'],['concurrent']
Performance,.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Shutdown hook called** ; **20/03/05 09:28:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e0e0327-45a3-46e8-872a-f5a63c3c7a98** ; **Using GATK jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar** ; **Running:** ; **java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx200G -jar /mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar PathSeqPipelineSpark --input /clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam --filter-bwa-image /clinix1/Analysis/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:51433,concurren,concurrent,51433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)** ; **20/03/05 09:28:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, PROCESS\_LOCAL, 7972 bytes)** ; **20/03/05 09:28:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)** ; **20/03/05 09:28:58 WARN TaskSetManager: Lost task 34.0 in stage 0.0 (TID 34, localhost, executor driver): com.esotericsoftware.kryo.KryoException: Buffer underflow.** ; **at com.esotericsoftware.kryo.io.Input.require(Input.java:199)** ; **at com.esotericsoftware.kryo.io.Input.readLong(Input.java:686)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet.<init>(LongHopscotchSet.java:83)** ; **at org.broadinstitute.hellbender.tools.spark.utils.LongHopscotchSet$Serializer.read(LongHopscotchSet.java:527)** ; **at org.broadinstitute.hellbe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:29001,concurren,concurrent,29001,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **20/03/05 09:28:58 ERROR TaskSetManager: Task 34 in stage 0.0 failed 1 times; aborting job** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Cancelling stage 0** ; **20/03/05 09:28:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 30.0 in stage 0.0 (TID 30), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 9.0 in stage 0.0 (TID 9), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled** ; **20/03/05 09:28:58 INFO Executor: Executor is",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:32449,concurren,concurrent,32449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **20/03/05 09:28:58 INFO DAGScheduler: Job 0 failed: count at PathSeqPipelineSpark.java:245, took 63.806676 s** ; **20/03/05 09:28:58 INFO SparkUI: Stopped Spark web UI at http://cm132:4040** ; **20/03/05 09:28:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!** ; **20/03/05 09:28:58 INFO NewHadoopRDD: Input split: file:/clinix1/Analysis/mongol/phenomata/04.GC\_CC/01.Alignment/Aligned/17039\_N.bam:1342177280+33554432** ; **20/03/05 09:28:58 INFO MemoryStore: MemoryStore cleared** ; **20/03/05 09:28:58 INFO BlockManager: BlockManager stopped** ; **20/03/05 09:28:58 INFO BlockManagerMaster: BlockManagerMaster stopped** ; **20/03/05 09:28:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!*",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:41063,concurren,concurrent,41063,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45490,concurren,concurrent,45490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['concurren'],['concurrent']
Performance,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5647:3444,concurren,concurrent,3444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647,2,['concurren'],['concurrent']
Performance,".url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30,55] [info] Unspecified type (Unspecified version) workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 submitted; [2020-07-14 05:09:30,66] [info] SingleWorkflowRunnerActor: Workflow submitted 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:09:30,67] [info] 1 new workflows fetched by cromid-ca5c695: 968be82c-eef3-4bdb-a1ab-3d4e2ca70674; [2020-07-14 05:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:2279,throttle,throttle,2279,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['throttle'],['throttle']
Performance,".walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:302); 	at org.broadinstitute.gatk.tools.walkers.variantutils.GenotypeGVCFs.map(GenotypeGVCFs.java:135); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:267); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano$TraverseLociMap.apply(TraverseLociNano.java:255); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.executeSingleThreaded(NanoScheduler.java:274); 	at org.broadinstitute.gatk.utils.nanoScheduler.NanoScheduler.execute(NanoScheduler.java:245); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:144); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:92); 	at org.broadinstitute.gatk.engine.traversals.TraverseLociNano.traverse(TraverseLociNano.java:48); 	at org.broadinstitute.gatk.engine.executive.ShardTraverser.call(ShardTraverser.java:98); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ##### ERROR ------------------------------------------------------------------------------------------; ##### ERROR A GATK RUNTIME ERROR has occurred (version 3.7-0-gcfedb67):; ##### ERROR; ##### ERROR This might be a bug. Please check the documentation guide to see if this is a known problem.; ##### ERROR If not, please post the error message, with stack trace, to the GATK forum.; ##### ERROR Visit our website and forum for extensive documentation and answers to ; ##### ERROR commonly asked questions https://software.broadinstitute.org/gatk; ##### ERROR; ##### ERROR MESSAGE: the number of genotypes is too large for ploidy 20 and allele 16: approx. 3247943160; ##### ERROR ----------------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2946:11571,concurren,concurrent,11571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2946,1,['concurren'],['concurrent']
Performance,///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.851 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar_VCF : 100000; 15:41:49.852 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:49.938 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_20180401.vcf -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.021 INFO FeatureManager - Using codec VCFCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar/hg19/clinvar_20180401.vcf; 15:41:50.092 INFO DataSourceUtils - Setting lookahead cache for data source: ClinVar : 100000; 15:41:50.093 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.093 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.config; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clinvar_hgmd/hg19/clinvar_hgmd.tsv; 15:41:50.158 INFO DataSourceUtils - Resolved data source file path: file:///home/shiyang/softwares/gatk-4.1.8.1/clinvar_hgmd.tsv -> file:///home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s/clin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:14246,cache,cache,14246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['cache'],['cache']
Performance,/3146?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ellbender/tools/exome/CalculateTargetCoverage.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVUYXJnZXRDb3ZlcmFnZS5qYXZh) | `92.265% <Ã¸> (Ã¸)` | `32 <0> (Ã¸)` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `77.143% <0%> (-2.024%)` | `10% <0%> (+4%)` | |; | [...bender/tools/exome/germlinehmm/xhmm/XHMMModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9nZXJtbGluZWhtbS94aG1tL1hITU1Nb2RlbC5qYXZh) | `100% <0%> (Ã¸)` | `14% <0%> (+7%)` | :arrow_up: |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <0%> (Ã¸)` | `6% <0%> (+3%)` | :arrow_up: |; | [...ender/utils/hmm/segmentation/HMMPostProcessor.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vc2VnbWVudGF0aW9uL0hNTVBvc3RQcm9jZXNzb3IuamF2YQ==) | `85.439% <0%> (+1.215%)` | `92% <0%> (+15%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (Ã¸)` | :arrow_down: |; | [.../coverage/pca/HDF5PCACoveragePoNCreationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216:1843,Perform,PerformSegmentation,1843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216,1,['Perform'],['PerformSegmentation']
Performance,/CNVGuts/-947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list; 21:06:12.640 DEBUG FeatureDataSource - Cache statistics for FeatureInput /paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohort8/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermli947966988/Homo_sapiens_assembly38.bed.preprocessed.filtered.scattered.0154.interval_list:; 21:06:12.640 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 21:06:12.645 INFO IntervalArgumentCollection - Processing 4999155 bp from intervals; 21:06:12.656 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 21:06:18.914 WARN GermlineCNVCaller - Sequence dictionary in annotated-intervals file does not match the master sequence dictionary.; 21:06:19.130 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 21:06:19.200 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 21:06:19.200 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:07:11.897 DEBUG ScriptExecutor - Executing:; 21:07:11.897 DEBUG ScriptExecutor - python; 21:07:11.897 DEBUG ScriptExecutor - /paedyl01/disk1/louisshe/tmp/gatk/cohort_denoising_calling.418897092082188314.py; 21:07:11.897 DEBUG ScriptExecutor - --ploidy_calls_path=/paedyl01/disk1/louisshe/work/NGS/wdl/test_workflow_cnv/germline/cromwell-executions/CNVGermlineCohortWorkflow/d53c0a; 21:07:11.897 DEBUG ScriptExecutor - --output_calls_path=/paedyl01/disk1/louisshe/out/NMD/batch1_2023/batch1_all/cnv/cohort_calls/batch1_all-calls; 21:07:11.897 DEBUG ScriptExecutor - --output_tracking_path=/paedyl01/disk1/louisshe/out/NMD/batch1_2023/batch1_all/cnv/cohort_calls/batch1_all-tracking; 21:07:11.897 DEBUG ScriptExecutor - --random_seed=1984; 21:07:11.897 DEBUG S,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:6902,perform,performed,6902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['perform'],['performed']
Performance,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3093,perform,performance,3093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,2,['perform'],['performance']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). 16/11/16 23:25:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 16/11/16 23:25:11 INFO TaskSchedulerImpl: Cancelling stage 1; 16/11/16 23:25:11 INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) failed in 0.276 s; 16/11/16 23:25:11 INFO DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 1.029776 s; 16/11/16 23:25:11 INFO SparkContext: SparkContext already stopped.; [November 16, 2016 11:25:11 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2058354688; org.apache.spark.SparkException: Job aborted due to stage failure:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:15994,concurren,concurrent,15994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:18256,concurren,concurrent,18256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]; java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:13005,concurren,concurrent,13005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkContext: Invoking stop() from shutdown hook; 16/11/16 23:25:11 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:14485,concurren,concurrent,14485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 16/11/16 23:25:11 INFO SparkUI: Stopped Spark web UI at http://172.32.65.22:4040; 16/11/16 23:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 16/11/16 23:25:11 INFO MemoryStore: MemoryStore cleared; 16/11/16 23:25:11 INFO BlockManager: BlockManager stopped; 16/11/16 23:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 16/11/16 23:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 16/11/16 23:25:11 INFO SparkContext: Successfully stopped SparkContext; 16/11/16 23:25:11 INFO ShutdownHookManager: Shutdown hook called; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting directory /gpfs/ngsdata/sparkcache/spark-29e7cb29-06dd-4145-ad9a-aa75971badb8; 16/11/16 23:25:11 INFO ShutdownHookManager: Deleting direc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:24642,concurren,concurrent,24642,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 17/10/11 14:19:28 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 1.0 (TID 2, com2, executor 1, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:28 INFO cluster.YarnClientSchedulerBackend: Disabling executor 1.; 17/10/11 14:19:28 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 1); 17/10/11 14:19:28 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.; 17/10/11 14:19:28 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, com2, 38568); 17/10/11 14:19:28 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor; 17/10/11 14:19:28 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:16830,concurren,concurrent,16830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 17/10/11 14:19:37 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 1.0 (TID 4, com2, executor 2, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Disabling executor 2.; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: Executor lost: 2 (epoch 1); 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, com2, 46254); 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor; 17/10/11 14:19:38 ERROR cluster.YarnScheduler: Lost executor 2 on com2: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:23843,concurren,concurrent,23843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"/arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.l",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1995,load,load,1995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,/codecov.io/gh/broadinstitute/gatk/pull/4902?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [...e/hellbender/engine/FeatureDataSourceUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2VVbml0VGVzdC5qYXZh) | `88.318% <Ã¸> (Ã¸)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `74.603% <57.143%> (+24.603%)` | :arrow_up: |; | [...ender/engine/cache/SideReadInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (Ã¸)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3J,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:2336,cache,cache,2336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an Asci,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:9578,cache,cache,9578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,"/gatk:471: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?; if not len(filesToAdd) is 0:; Using GATK jar /home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar FilterMutectCalls -V MT.vcf.gz -R human_g1k_v37.main.fasta -O MT.filtered.vcf.gz --stats MT.vcf.gz.stats --mitochondria-mode; ```. #### Steps to reproduce. The data is sensitive human data, so I will not share that data. However, I have made up some data that appears to fail as well. Using the command:. ```bash; gatk FilterMutectCalls -V mu.2.vcf -R human_g1k_v37.main.fasta -O MT.filtered.vcf.gz --stats MT.vcf.gz.stats --mitochondria-mode; ```; I get an output to STDERR:; ```; 11:30:15.521 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/warkre/miniconda3/envs/gatk4.1.4.0/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 07, 2019 11:30:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:15.673 INFO FilterMutectCalls - ------------------------------------------------------------; 11:30:15.674 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.0; 11:30:15.674 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:15.674 INFO FilterMutectCalls - Executing as warkre@fuji on Linux v4.9.0-9-amd64 amd64; 11:30:15.674 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 11:30:15.674 INFO FilterMutectCalls - Start Date/Time: November 7, 2019 11:30:15 AM CET; 11:30:15.674 INFO FilterMutectCalls - --------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:6919,Load,Loading,6919,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Load'],['Loading']
Performance,"/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam \; -L chr17; ...; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -I /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -O /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam -L chr17; 14:52:10.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 2:52:10 PM EDT] PrintReads --output /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/test_decram_20171002.bam --intervals chr17 --input /humgen/gsa-hpprojects/dev/shlee/1kg_GRCh38_exome/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:1354,Load,Loading,1354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['Load'],['Loading']
Performance,"/hg19-homopolymers-save.interval_list`. Stacktrace:; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; 	at java.util.BitSet.initWords(BitSet.java:166); 	at java.util.BitSet.<init>(BitSet.java:161); 	at htsjdk.samtools.GenomicIndexUtil.regionToBins(GenomicIndexUtil.java:90); 	at htsjdk.samtools.BinningIndexContent.getChunksOverlapping(BinningIndexContent.java:121); 	at htsjdk.samtools.CachingBAMFileIndex.getSpanOverlapping(CachingBAMFileIndex.java:71); 	at htsjdk.samtools.BAMFileReader.getFileSpan(BAMFileReader.java:898); 	at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:915); 	at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:575); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:528); 	at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:400); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:404); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:330); 	at java.lang.Iterable.spliterator(Iterable.java:101); 	at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1055); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:252); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.Comma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4334:1294,load,loadNextIterator,1294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4334,1,['load'],['loadNextIterator']
Performance,/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 12:28:17.925 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/achilles_lineage_results.import.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/achilles/hg38/achilles_lineage_results.import.txt; > 12:28:17.932 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8388,cache,cache,8388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,/reference/funcotator_dataSources.v1.7.20200521g/clinvar/hg38/clinvar_20180429_hg38.vcf; 06:42:41.615 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg_lof.tsv -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_lof/hg38/acmg_lof.tsv; 06:42:41.617 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/acmg59_test_cleaned.txt -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/acmg_rec/hg38/acmg59_test_cleaned.txt; 06:42:41.617 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; 06:42:41.618 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.618 INFO DataSourceUtils - Setting lookahead cache for data source: LMMKnown : 100000; 06:42:41.622 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.641 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.652 INFO FeatureManager - Using codec VCFCodec to read file file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/lmm_known/hg38/LMM_Path_LP_VUS5-variants-6-12-18.sorted_liftover_b38.corrected.vcf; 06:42:41.663 INFO DataSourceUtils - Resolved data source file path: file:///data/nws/WES/gencode.v34.annotation.REORDERED.gtf -> file:///data/nws/WES/reference/funcotator_dataSources.v1.7.20200521g/gencod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:5288,cache,cache,5288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,1,['cache'],['cache']
Performance,/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Åuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6490,Load,Loading,6490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; + /spark-1.6.2-bin-hadoop2.6//bin/spark-submit --master spark://hpcgenomicn24:6311 --conf spark.executor.memory=2g --conf spark.driver.memory=2g --conf spark.local.dir=/gpfs/ngsdata/sparkcache --class org.broadinstitute.hellbender.Main /gpfs/software/spark/gatk4onspark.jar PrintReadsSpark -I /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O /gpfs/home/tpathare/test/; 23:25:07.475 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/gpfs/software/spark/gatk4onspark.jar!/com/intel/gkl/native/libIntelGKL.so; 23:25:07.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [November 16, 2016 11:25:07 PM AST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /gpfs/home/tpathare/test/ --input /gpfs/home/tpathare/gatk/src/test/resources/NA12878.chr17_69k_70k.dictFix.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [November 16, 2016 11:25:07 PM AST] Executing as root@hpcgenomicn24 on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17; Version: Version:4.alpha.2-98-g8fa5092-SNAPSHOT; 23:25:07.556 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 23:25:07.556 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 5; 23:25:07.556 INFO PrintReadsSpark - Defaults.CREA",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:1408,load,loaded,1408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['load'],['loaded']
Performance,"/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3603,concurren,concurrent,3603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ; Â  Â  java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Åuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Åuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Åuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO Â NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN Â NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO Â NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN Â NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO Â BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO Â BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO Â BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://so,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:9102,Load,Loading,9102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['Load'],['Loading']
Performance,"0 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; 04:37:39.167 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.169 INFO GenomicsDBImport - Starting batch input file preload; 04:37:39.170 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:4699,concurren,concurrent,4699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:8705,Concurren,ConcurrentModificationException,8705,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['Concurren'],['ConcurrentModificationException']
Performance,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:8716,Concurren,ConcurrentModificationException,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['Concurren'],['ConcurrentModificationException']
Performance,"0-0/gatk-package-4.2.5.0-local.jar --version ; ; The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; HTSJDK Version: 2.24.1 ; ; Picard Version: 2.25.4. Exact command:. gatk CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor > /media/analyst/Data/CNNScoreVariants.log. Entire console output:. Running: ; ; Â  Â  java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar CNNScoreVariants -I 73318\_WES\_hg19\_recalibrated.sorted.bam -V 73318\_80\_IDTv1.vcf.gz -R /media/analyst/Data/Reference\_data/hg19.fa -O /media/analyst/Data/73318\_CNNScore\_test.vcf.gz -tensor-type read\_tensor ; ; 11:17:58.509 INFO Â NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/analyst/anaconda3/envs/snakemake\_env/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Apr 25, 2022 11:17:58 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 11:17:58.668 INFO Â CNNScoreVariants - ------------------------------------------------------------ ; ; 11:17:58.668 INFO Â CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 11:17:58.669 INFO Â CNNScoreVariants - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 11:17:58.669 INFO Â CNNScoreVariants - Executing as analyst@WGS on Linux v5.13.0-40-generic amd64 ; ; 11:17:58.669 INFO Â CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v10.0.2+13 ; ; 11:17:58.669 INFO Â CNNScoreVariants - Start Date/Time: April 25, 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811:2430,Load,Loading,2430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811,1,['Load'],['Loading']
Performance,"0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807:1791,perform,performed,1791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807,1,['perform'],['performed']
Performance,0.3 35000 101621.1; 14:56:26.027 INFO ProgressMeter - 1:5856032 0.5 55000 105867.6; ...; 19:37:05.295 INFO ProgressMeter - GL000209.1:48811 281.2 30739000 109323.8; 19:37:15.543 INFO ProgressMeter - GL000224.1:65537 281.3 30758000 109324.9; 19:37:25.847 INFO ProgressMeter - GL000248.1:21736 281.5 30768000 109293.8; 19:37:25.906 INFO FilterMutectCalls - Finished pass 0 through the variants; 19:50:04.590 INFO FilterMutectCalls - Shutting down engine; [9 January 2020 7:50:04 PM] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 294.19 minutes.; Runtime.totalMemory()=14966849536; java.lang.IllegalArgumentException: Values in probability array sum to a negative number NaN; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:731); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeSumToOne(MathUtils.java:731); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:336); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:306); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:158); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:159); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:4476,perform,performEMIteration,4476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,2,['perform'],['performEMIteration']
Performance,0/11 14:19:28 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor; 17/10/11 14:19:28 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 ERROR cluster.YarnScheduler: Lost executor 1 on com2: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:18556,concurren,concurrent,18556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"0/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:42 INFO yarn.Client: Application report for application_1507856833944_0003 (state: RUNNING); 17/10/13 18:11:42 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.145; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:42 INFO cluster.YarnClientSchedulerBackend: Application application_1507856833944_0003 has started running.; 17/10/13 18:11:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44818.; 17/10/13 18:11:42 INFO netty.NettyBlockTransferService: Server created on 10.131.101.159:44818; 17/10/13 18:11:42 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/10/13 18:11:42 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:44818 with 366.3 MB RAM, BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:13009,queue,queue,13009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,2,['queue'],['queue']
Performance,"0/757d9552-3464-11e5-8244-46931fed8383.png). Logs indicated it was running on a single worker, regardless of how many were specified for the job. **The Cause**. The underlying cause is a combination of ReadBAM's design and Dataflow's own perhaps over-eager optimization. ReadBAM is implemented as a series of transforms. It could also have been implemented as a Dataflow BoundedSource, but the latter is much more complicated. The transforms are as follows:; Start with a collection of filenames and a collection of contigs.; Transform 1 -- input: filenames, side input: contigs. Generates a list of regions to read (""BAMShard""); Transform 2 -- input: `PCollection<BAMShard>`, output: `PCollection<Read>`. Each worker reads from the BAM file, using the index to find where to read from. Dataflow sees that transform 2 takes as input transform 1's output, and so these two can be run in sequence on the same machines, skipping a serialization/deserialization step. This optimization is called ""fusing"" and it's generally a very good thing. However in this case, the input PCollection has a single element (the file we want to read), so only one worker is involved. Because of the fusion, that same worker then ends up doing all of the reading work, ruining our day. **The Solutions**. There are multiple ways to solve this problem. ; 1. change transform 1 to have the contig collection as a primary input in the hope that we always have more than one contig. ; This solution's very brittle (our benchmark, for example, reads a single chromosome so the contig list has effectively only one element). I did not pursue it.; 2. Insert a groupby step between the two transforms.; pro: this gets all the workers involved again; con: the groupby itself takes some time, unnecessarily.; 3. Compute the BAMShards at the client and then send those to workers.; pro: this gets all the workers involved again, and they do not have to spend any time on groupby; con: an existing Dataflow bug will cause the program ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/756:1214,optimiz,optimization,1214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/756,1,['optimiz'],['optimization']
Performance,00 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl-x86_64.so.1; 2b5f56d5e000-2b5f56d60000 ---p 00000000 00:00 0 ; 2b5f56d60000-2b5f56d63000 ---p 00000000 00:00 0 ; 2b5f56d63000-2b5f56e61000 rw-p 00000000 00:00 0 [stack:85483]; 2b5f56e61000-2b5f56e62000 r--p 00000000 00:00 0 ; 2b5f56e62000-2b5f56e63000 rw-p 00000000 00:00 0 ; 2b5f56e63000-2b5f56e6b000 rw-s 00000000 08:01 69704933 /tmp/hsperfdata_iiipe01/85482; 2b5f56e6b000-2b5f56f3c000 rw-p 00000000 00:00 0 ; 2b5f56f3c000-2b5f56f3d000 r--s 00000000 08:01 67151449 /etc/localtime; 2b5f56f3d000-2b5f56f58000 r--s 001d6000 07:00 6686 /usr/lib/jvm/java-1.8-openjdk/jre/lib/ext/nashorn.jar; 2b5f56f58000-2b5f5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29684,load,loading,29684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance,"00 pipeline. But when I run the command above, it throws out an error, seems something related with malformat. I check my VCF file and think it should be OK. So I wonder if you can kindly tell me how to fix this bug?; The ERROR is:; `Using GATK jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator --variant /home/shiyang/Project/BGB900_101/TSO_result/TSO_somatic_vcf/112-0005-0031-B1_L1.UP12.tmb.tsv.tso.somatic.vcf --reference /storage01/ref_genome/hg19/bwa/ucsc.hg19.fasta --ref-version hg19 --data-sources-path /home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s --output /home/shiyang/Project/BGB900_101/TSO_result/test.maf --output-file-format MAF; 15:41:48.793 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 19, 2020 3:41:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:41:49.028 INFO Funcotator - ------------------------------------------------------------; 15:41:49.028 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.1; 15:41:49.028 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:41:49.028 INFO Funcotator - Executing as shiyang@r740 on Linux v3.10.0-957.el7.x86_64 amd64; 15:41:49.028 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-b09; 15:41:49.028 INFO Funcotator - Start Date/Time: August 19, 2020 3:41:48 PM CST; 15:41:49.029 INFO Funcotator - ------------------------------------------------------------; 15:41:49.029 INFO Fun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:2591,Load,Loading,2591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['Load'],['Loading']
Performance,004.bam -O test.dup.bam -M marked_dup_metrics.txt; **********. 15:26:21.393 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed Jul 03 15:26:21 CEST 2024] MarkDuplicates INPUT=[WA02_i5-537_i7-98_S11819_L004.bam] OUTPUT=test.dup.bam METRICS_FILE=marked_dup_metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true DUPLEX_UMI=false FLOW_MODE=false FLOW_DUP_STRATEGY=FLOW_QUALITY_SUM_STRATEGY USE_END_IN_UNPAIRED_READS=false USE_UNPAIRED_CLIPPED_END=false UNPAIRED_END_UNCERTAINTY=0 UNPAIRED_START_UNCERTAINTY=0 FLOW_SKIP_FIRST_N_FLOWS=0 FLOW_Q_IS_KNOWN_END=false FLOW_EFFECTIVE_QUALITY_THRESHOLD=15 ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=2 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Wed Jul 03 15:26:21 CEST 2024] Executing as qnick@Barbus on Linux 6.5.0-41-generic amd64; OpenJDK 64-Bit Server VM 17.0.11+9-Ubuntu-122.04.1; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: 4.6.0.0; INFO 2024-07-03 15:26:21 MarkDuplicates Start of doWork freeMemory: 189357416; totalMemory: 234881024; maxMemory: 32178700288; INFO 2024-07-03 15:26:21 MarkDuplicates Reading input file and constructing read end information.; INFO 2024-07-03 15:26:21 MarkDuplicates Will retain up to 116589493 data points before spilling to disk.; [Wed Jul ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8904:7110,optimiz,optimized,7110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8904,1,['optimiz'],['optimized']
Performance,"00; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-8u252-b09-1~16.04-b09; >; > 16:17:05.843 INFO HaplotypeCaller - Start Date/Time: September 4, 2020 4:17:04 PM UTC; >; > 16:17:05.843 INFO HaplotypeCaller - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2412,load,load,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1522,perform,performs,1522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,2,['perform'],['performs']
Performance,"015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command â€./gradlew bundleâ€ï¼Œ it appeared the error in the last ï¼Œdid this matterï¼Ÿ**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020333/jackson-databind-2.6.5.jar(com/fasterxml/jackson/databind/annotation/JsonSerialize$Inclusion.class)]]; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-core/2.5/7ed845de1dfe070d43511fab1784e6c4118398/log4j-core-2.5.jar(org/apache/logging/log4j/core/config/plugins/PluginVisitorStrategy.class)]]; [done in 5759 ms]; 1 error; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/opt/Software/gatk/build/tmp/gatkTabComplete/jadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 7.431 secs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2897,load,loading,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,4,"['cache', 'load']","['caches', 'loading']"
Performance,"03:15:02.578 INFO IntervalArgumentCollection - Processing <redacted> bp from intervals; 03:15:02.588 INFO HaplotypeCaller - Done initializing engine; 03:15:02.590 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 03:15:02.593 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:15:02.598 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 03:15:02.599 INFO IntelSmithWaterman - Using CPU-supported AVX-512 instructions; 03:15:02.599 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 03:15:02.601 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 03:15:02.601 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 03:15:02.623 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:conda/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:15:02.667 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 03:15:02.667 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:15:02.667 INFO IntelPairHmm - Available threads: 16; 03:15:02.667 INFO IntelPairHmm - Requested threads: 32; 03:15:02.667 WARN IntelPairHmm - Using 16 available threads, but 32 were requested; 03:15:02.667 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; ```. Any insight into what's going on and how to diagnose it would be greatly appreciated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889:6117,Load,Loading,6117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889,2,"['Load', 'multi-thread']","['Loading', 'multi-threaded']"
Performance,04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:28:26.332 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:30:24.296 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:4->5; > 21:30:24.299 DEBUG Gen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:6034,cache,cache,6034,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance,0418.tsv /tmp/tintest/sample-3678073345547351139852.tsv /tmp/tintest/sample-3687670170611066239891.tsv /tmp/tintest/sample-3697896582170286914413.tsv /tmp/tintest/sample-3704126767513966037718.tsv /tmp/tintest/sample-3718480935718374256974.tsv /tmp/tintest/sample-3729118447183914284068.tsv /tmp/tintest/sample-3734484951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:61415,Load,Loading,61415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['Load'],['Loading']
Performance,"05f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFControlSample/Benchmark/3b068fb2-7140-4c1e-8860-df8df21821ec/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFTestSample/Benchmark/7f7c4522-e293-4a03-ada8-9541a585250b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:11445,cache,cacheCopy,11445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,"06669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.; 2-45-ga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:1345,load,loaded,1345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['load'],['loaded']
Performance,07/25 01:37:55 Done container setup.; 2020/07/25 01:37:56 Starting localization.; 2020/07/25 01:38:02 Localization script execution started...; 2020/07/25 01:38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlig,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:1941,Load,Loading,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"08 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15383,perform,performed,15383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['perform'],['performed']
Performance,08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:24:08.558 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:24:08.558 INFO Mutect2 - Deflater: IntelDeflater; 13:24:08.558 INFO Mutect2 - Inflater: IntelInflater; 13:24:08.558 INFO Mutect2 - GCS max retries/reopens: 20; 13:24:08.558 INFO Mutect2 - Requester pays: disabled; 13:24:08.558 INFO Mutect2 - Initializing engine; 13:24:09.048 INFO FeatureManager - Using codec VCFCodec to read file file://ref/1000g_pon.hg38.vcf.gz; 13:24:09.207 INFO FeatureManager - Using codec VCFCodec to read file file://ref/af-only-gnomad.hg38.vcf.gz; 13:24:09.374 INFO Mutect2 - Done initializing engine; 13:24:09.435 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 13:24:09.438 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 13:24:09.472 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 13:24:09.472 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 13:24:09.473 INFO IntelPairHmm - Available threads: 24; 13:24:09.473 INFO IntelPairHmm - Requested threads: 4; 13:24:09.473 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 13:24:09.501 INFO ProgressMeter - Starting traversal; 13:24:09.502 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 13:24:19.721 INFO ProgressMeter - chr1:634040 0.2 2460 14443.7; 13:24:29.736 INFO ProgressMeter - chr1:1564703 0.3 7220 21409.5; .; .; .; 15:28:55.286 INFO ProgressMeter - chrM:12891 124.8 11474080 91967.0; 15:29:08.985 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 10.162159898; 15:29:08.986 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6851:3250,Load,Loading,3250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6851,1,['Load'],['Loading']
Performance,097.log; Use `git lfs logs last` to view the log.; portage$ cat /scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects/logs/20180420T221032.955218097.log; git-lfs/2.3.4 (GitHub; linux amd64; go 1.10); git version 2.16.3. $ git-lfs pull --include src/main/resources/large; No default remote. No remotes defined. Current time in UTC: ; 2018-04-20 20:10:32. ENV:; LocalWorkingDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999; LocalGitDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalGitStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git; LocalMediaDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/objects; LocalReferenceDir=; TempDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs/tmp; ConcurrentTransfers=3; TusTransfers=false; BasicTransfersOnly=false; SkipDownloadErrors=false; FetchRecentAlways=false; FetchRecentRefsDays=7; FetchRecentCommitsDays=0; FetchRecentRefsIncludeRemotes=true; PruneOffsetDays=3; PruneVerifyRemoteAlways=false; PruneRemoteName=origin; LfsStorageDir=/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/.git/lfs; AccessDownload=none; AccessUpload=none; DownloadTransfers=basic; UploadTransfers=basic. Client IP addresses:; xx.xx.xx.xx; xx.xx.xx.xx; xx.xx.xx.xx; portage$ ls -latr; total 188; -rw-r--r-- 1 portage portage 428 Apr 20 22:05 codecov.yml; -rwxr-xr-x 1 portage portage 5741 Apr 20 22:05 build_docker.sh; -rw-r--r-- 1 portage portage 32161 Apr 20 22:05 build.gradle; -rw-r--r-- 1 portage portage 37502 Apr 20 22:05 README.md; -rw-r--r-- 1 portage portage 1502 Apr 20 22:05 LICENSE.TXT; -rw-r--r-- 1 portage portage 1555 Apr 20 22:05 Dockerfile; -rw-r--r-- 1 portage portage 1128 Apr 20 22:05 AUTHORS; -rw-r--r-- 1 portage portage 8237 Apr 20 22:05 .travis.yml; -rw-r--r-- 1 portage portage 395 Apr 20 22:05 .gitignore; -rw-r--r-- 1 portage portage 128 Apr 20 22:05 .gitattributes; -rw-r--r-- 1 portage portage ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:15983,Concurren,ConcurrentTransfers,15983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Concurren'],['ConcurrentTransfers']
Performance,0b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `0% <0%> (-89.583%)` | `0% <0%> (-12%)` | |; | [...adinstitute/hellbender/engine/ReadContextData.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZENvbnRleHREYXRhLmphdmE=) | `0% <0%> (-70.37%)` | `0% <0%> (-6%)` | |; | [...n/java/org/broadinstitute/hellbender/utils/KV.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9LVi5qYXZh) | `0% <0%> (-57.143%)` | `0% <0%> (-5%)` | |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...ls/funcotator/metadata/VcfFuncotationMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1ZjZkZ1bmNvdGF0aW9uTWV0YWRhdGEuamF2YQ==) | `71.429% <0%> (-28.571%)` | `8% <0%> (+3%)` | |; | [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `69.231% <0%> (-26.007%)` | `6% <0%> (+2%)` | |; | ... and [600 more](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671:3052,Concurren,ConcurrentAFCalculatorProvider,3052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,"1	NM:i:7	MQ:i:60	AS:i:105	XS:i:20; EOF. bind 'set disable-completion off'. samtools view reads.sam -b > reads.bam; samtools index reads.bam. gatk HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF ; bcftools view output.g.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail ; gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf ; bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail. ```. output:; ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar Running: java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true 13:39:56.569 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 13:39:56.647 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.660 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0 13:39:56.666 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 13:39:56.666 INFO HaplotypeCaller - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64 13:39:56.666 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724 13:39:56.667 INFO HaplotypeCaller - Start Date/Time: October 26, 2023 at 1:39:56 PM CEST 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.669 INFO HaplotypeCaller - HTSJDK Ver",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:11059,Load,Loading,11059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,1 from BlockManagerMaster.; 18/01/09 18:31:12 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000003 on host: tele-2. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:12 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 18/01/09 18:31:12 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 2; 18/01/09 18:31:12 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000004 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.n,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:18426,concurren,concurrent,18426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,"1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40778 40780 35116 1412 0.5373 0.9665 0.6907; None 41994 41994 43760 196 0.4897 0.9954 0.6564; ::::::::::::::; NA12878/O1D1/STANDARD_NGS/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40743 40745 35255 1447 0.5361 0.9657 0.6895; None 41955 41954 43903 235 0.4886 0.9944 0.6553; ::::::::::::::; NA12878/O1D2/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:1711,perform,performance,1711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,1,['perform'],['performance']
Performance,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:22562,concurren,concurrent,22562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,2,['concurren'],['concurrent']
Performance,"1-07 11:33:52 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153), /proxy/application_1542127286896_0153; 2019-01-07 11:33:52 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-07 11:33:52 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:53 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-07 11:33:53 INFO Client:54 - Application report for application_1542127286896_0153 (state: RUNNING); 2019-01-07 11:33:53 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.193; ApplicationMaster RPC port: 0; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:53 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0153 has started running.; 2019-01-07 11:33:53 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45270.; 2019-01-07 11:33:53 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:45270; 2019-01-07 11:33:53 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:45270 with 408.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:15896,queue,queue,15896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['queue'],['queue']
Performance,"1-09 13:35:32 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166), /proxy/application_1542127286896_0166; 2019-01-09 13:35:33 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-09 13:35:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-09 13:35:33 INFO Client:54 - Application report for application_1542127286896_0166 (state: RUNNING); 2019-01-09 13:35:33 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.195; ApplicationMaster RPC port: 0; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0166 has started running.; 2019-01-09 13:35:33 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43627.; 2019-01-09 13:35:33 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:43627; 2019-01-09 13:35:33 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:43627 with 372.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:15195,queue,queue,15195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['queue'],['queue']
Performance,"1. Add new arg `genomicsdb-shared-posixfs-optimizations` to help with shared posix filesystems like NFS and Lustre. This turns on `disable file locking` and for GenomicsDB import it minimizes writes to disks. The performance on some of the gatk datasets for the import of about 10 samples went from 23.72m to 6.34m on NFS which was comparable to importing to a local filesystem. Hopefully this helps with Issue #6487 and #6627. Also, fixes Issue #6519.; 2. This version of GenomicsDB also uses pre-compression filters for offset and compression files for new workspaces and genomicsdb arrays. The total sizes for a GenomicsDB workspace using the same dataset as above and the 10 samples went from 313MB to 170MB with no change in import and query times. Smaller GenomicsDB arrays also help with performance on distributed and cloud file systems.; 3. This version has added support to handle MNVs similar to deletions as described in Issue #6500. See [GenomicsDB PR 88](https://github.com/GenomicsDB/GenomicsDB/pull/88). Thanks @kgururaj.; 4. There is added support in the GenomicsDBImporter to have multiple contigs in the same GenomicsDB partition/array. This will hopefully help import times in cases where users have many thousands of contigs. See [GenomicsDB PR 91](https://github.com/GenomicsDB/GenomicsDB/pull/91). Changes will be needed from the gatk side to avail this support. Thanks @mlathara.; 5. Logging has been improved somewhat with the native C/C++ code using [spdlog](https://github.com/gabime/spdlog) and [fmt](https://github.com/fmtlib/fmt) and the Java layer using apache log4j and log4j.properties provided by the application. Also, info messages like `No valid combination operation found for INFO field AA - the field will NOT be part of INFO fields in the generated VCF records` will only be output once for the operation. Also see open PR #6514 for code to actually suppress these warnings for known fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6654:42,optimiz,optimizations,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6654,3,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,1. Allow for using separate threads for reading / processing / writing (max 3); 2. Use NM SAM tag instead of edit distance; 3. Perform likelihood scoring on trimmed read. Additional changes include:; 1. CachingIndexedFastaSequenceFile is enhanced to be thread safe and allow for adjusting its cache size. The change involved synchronizing the main query method (getSubsequenceAt) and deriving the cache size from a settable variable rather than from a constant.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8982:127,Perform,Perform,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8982,3,"['Perform', 'cache']","['Perform', 'cache']"
Performance,"1. Why can't standard tools operate on the distributed workspaces? You could run `GenotypeGVCFs` on these workspaces in a distributed fashion and then concatenate the results together if you want. I think you were initially considering this route - and still think it would be more performant. 2. Again, you can process/query a whatever set of intervals you want -- in a distributed fashion, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782:282,perform,performant,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782,1,['perform'],['performant']
Performance,"1.774 INFO HaplotypeCaller - HTSJDK Version: 2.20.3; 02:07:51.774 INFO HaplotypeCaller - Picard Version: 2.21.1; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:07:51.774 INFO HaplotypeCaller - Deflater: IntelDeflater; 02:07:51.774 INFO HaplotypeCaller - Inflater: IntelInflater; 02:07:51.774 INFO HaplotypeCaller - GCS max retries/reopens: 20; 02:07:51.775 INFO HaplotypeCaller - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genoty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:2940,Load,Loading,2940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['Load'],['Loading']
Performance,"1.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:21.993 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 14:40:21.993 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 14:40:21.993 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 14:40:21.993 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:40:21.994 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:40:21.994 INFO MarkDuplicatesSpark - Initializing engine; 14:40:21.994 INFO MarkDuplicatesSpark - Done initializing engine; 14:40:22.338 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:3882,load,load,3882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['load'],['load']
Performance,"10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 10:20:12.111 INFO case_denoising_calling - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 10:20:12.273 INFO root - Loading modeling interval list from the provided model...; 10:20:12.475 INFO gcnvkernel.io.io_intervals_and_counts - The given interval list provides the following interval annotations: {'GC_CONTENT'}; 10:20:12.491 INFO root - The model contains 11901 intervals and 23 contig(s); 10:20:12.491 INFO root - Loading 1 read counts file(s)...; 10:20:12.545 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 10:20:12.554 INFO root - Loading denoising model configuration from the provided model...; 10:20:12.555 INFO root - - bias factors enabled: True; 10:20:12.555 INFO root - - explicit GC bias modeling enabled: True; 10:20:12.555 INFO root - - bias factors in active classes disabled: False; 10:20:12.555 INFO root - - maximum number of bias factors: 5; 10:20:12.555 INFO root - - number of GC curve knobs: 20; 10:20:12.555 INFO root - - GC curve prior standard deviation: 1.0; 10:20:12.954 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the denoising model...; 10:20:15.806 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the sampler...; 10:20:15.807 INFO gcnvkernel.tasks.task_case_denoising_calling - Instantiating the copy number caller...; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Global model variables: {'log_mean_bias_t', 'psi_t_log__', 'W_tu', 'ard_u_log__'}; 10:20:18.549 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'read_depth_s_log__', 'psi_s_log__', 'z_sg', 'z_su'}",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8740:6489,Load,Loading,6489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8740,1,['Load'],['Loading']
Performance,"113 / 643 of them were CNV test files that I deleted in #3907, which is a shade bit more than 1%... only a small fraction of these are loaded dynamically or are index/dict files, which tests will catch (and they have already, now that I check---6 / 113). I will concede that it is likely that most of the remaining files are index files, etc., but I do see a few more bams, etc. that could stand deletion. I'm curious as to what the best way to do this sort of thing actually is!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077:135,load,loaded,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077,1,['load'],['loaded']
Performance,"119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6178,concurren,concurrent,6178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Haplotype count 128; 11:36:56.119 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:56.120 DEBUG Mutect2Engine - Kmer sizes values []; 11:39:06.762 DEBUG Mutect2 - Processing assembly region at chrM:6630-6929 isActive: false numReads: 30053; 11:39:07.547 DEBUG Mutect2 - Processing assembly region at chrM:6930-7229 isActive: false numReads: 0; 11:39:07.574 DEBUG Mutect2 - Processing assembly region at chrM:7230-7493 isActive: false numReads: 359; 11:39:07.584 DEBUG Mutect2 - Processing assembly region at chrM:7494-7771 isActive: true numReads: 718; 11:39:07.668 DEBUG ReadThreadingGraph - Recovered 32 of 33 dangling tails; 11:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:13197,cache,cache,13197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /ref/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter2.test.vcf.gz --use-jdk-inflater true --use-jdk-deflater true; 19:41:38.956 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:41:39.332 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation; ```; Hope this helps and we're looking forward the GKL fix. Cheers,; Richard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:5815,Load,Loading,5815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"12** 25380275Â Â  .Â Â Â  TÂ Â Â  GÂ Â Â  .Â Â Â  .Â Â Â Â  AS\_SB\_TABLE=3911,5343|26,21;DP=9485;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=18;POPAF=7.30;TLOD=53.53Â Â Â Â  GT:AD:AF:DP:F1R2:F2R1:SBÂ Â  0/1:9254,47:4.970e-03:9301:5321,21:3867,26:3911,5343,26,21. The input and the output BAMs show this call with the variant. ![](https://gatk.broadinstitute.org/hc/user_images/FVlI3WhNIzYK7NB7PakCmw.png). In the logs, it shows the detection of an active region here:. 08:01:23.642 INFOÂ  Mutect2Engine - Assembling chr12:**2538**0238-**2538**0327 with 19912 reads:Â Â Â  (with overlap region = chr12:**2538**0138-**2538**0427). 08:01:24.119 INFOÂ  EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. 08:01:24.154 INFOÂ  AssemblyResultSet - Trimming active region AssemblyRegion chr12:**2538**0238-**2538**0327 active?=true nReads=19912 with 2 haplotypes. 08:01:24.154 INFOÂ  AssemblyResultSet - Trimmed region to chr12:**2538**0255-**2538**0295 and reduced number of haplotypes from 2 to only 2. 08:01:25.383 INFOÂ  EventMap - >> Events = EventMap{chr12:**2538**0275-**2538**0275 \[T\*, G\],}. I have tried troubleshooting with the steps stated in this \[blog\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant). However, it does not change the output vcf. I used the force-calling mode by giving the above call in an input vcf and the call did appear in the vcf file. **chr12** 25398285Â Â  .Â Â Â  CÂ Â Â  AÂ Â Â  .Â Â Â  .Â Â Â Â  AS\_SB\_TABLE=4312,3630|14,8;DP=8096;ECNT=1;MBQ=36,36;MFRL=0,0;MMQ=42,42;MPOS=22;POPAF=7.30;TLOD=14.69Â Â Â Â  GT:AD:AF:DP:F1R2:F2R1:SBÂ Â  0/1:7942,22:2.576e-03:7964:3609,8:4268,13:4312,3630,14,8. However, I cannot rely on force-calling mutations on a set of calls. I am unsure if I am missing out more calls that are not showing up. Are there any parameters I need to tune so that I do not miss calls like above?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/136765'>Zendesk ticket #136765</a>)<br>gz#136765</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232:3992,tune,tune,3992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232,1,['tune'],['tune']
Performance,"12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:52:37.524 INFO GenomicsDBImport - Deflater: IntelDeflater; 12:52:37.524 INFO GenomicsDBImport - Inflater: IntelInflater; 12:52:37.524 INFO GenomicsDBImport - GCS max retries/reopens: 20; 12:52:37.524 INFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5740:3652,perform,performance,3652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740,1,['perform'],['performance']
Performance,13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - cache miss 1791 > 1790 expanding to 3582; > 21:14:17.894 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.930 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.937 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:18.507 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:3->4; > 21:14:18.510 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:27:38.720 DEBUG GenotypeLikelihoodCalculators - Expanding capa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:5787,cache,cache,5787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance,"14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8778"",; ""CHM controlindelPrecision"": ""0.8968"",; ""CHM controlsnpF1Score"": ""0.9813"",; ""CHM controlsnpPrecision"": ""0.9774"",; ""CHM controlsnpRecall"": ""0.9852"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFControlSample/Benchmark/16cd1efe-5cea-403e-8e85-aec15e71bd1d/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8778"",; ""CHM evalindelPrecision"": ""0.8968"",; ""CHM evalsnpF1Score"": ""0.9813"",; ""CHM evalsnpPrecision"": ""0.9774"",; ""CHM evalsnpRecall"": ""0.9852"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFTestSample/Benchmark/2071078a-158e-4c3e-9b2f-907bd501821b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.7573"",; ""EXOME1 controlindelPrecision"": ""0.6882"",; ""EXOME1 controlsnpF1Score"": ""0.9896"",; ""EXOME1 controlsnpPrecision"": ""0.9852"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-EXOME1Sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:11446,cache,cacheCopy,11446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.S",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:2068,load,load,2068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"1508#issuecomment-259727960). I did find that using SWParameterSet.STANDARD_NGS as the parameters instead of CigarUtils.NEW_SW_PARAMETERS will resolve this particular case, but a more comprehensive analysis would be required before we make the change since this will impact a lot of other events too. ---. @ldgauthier commented on [Thu Nov 10 2016](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-259744146). And just to provide a little more context, this is a problem in the clinical context because there's often an analysis step where potential causal variants are filtered out if they are seen in a normal reference panel (like ExAC) at more than some threshold allele frequency. If the representation doesn't match, then that step gets a lot more complicated than the way it's currently done. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287818162). I head our Intel friends are working on a new SW implementation (or maybe ""just"" optimization?). . @droazen is this relevant to the Intel SW work? And should any/all improvements along these lines be done in GATK4 going forward?. ---. @droazen commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287885846). @vdauwera Yes, Intel is working on a native implementation of `SmithWaterman` at our request -- it should materialize this quarter. Any changes/fixes to the Java implementation are relevant to Intel's work, since that is the implementation whose output they will have to match :) Of course, we'd certainly like such improvements to be implemented in GATK4 -- whether they are backported to GATK3 as well is up to you. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1508#issuecomment-287889598). @ldgauthier Any objections to this getting tackled in GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2498:2677,optimiz,optimization,2677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2498,1,['optimiz'],['optimization']
Performance,"1625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFControlSample/Benchmark/5388d7b6-6bcd-451d-9a4e-925b386ecd0c/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.03499722222222"",; ""NIST evalHCsystemhours"": ""0.17304166666666665"",; ""NIST evalHCwallclockhours"": ""67.81165555555557"",; ""NIST evalHCwallclockmax"": ""3.691061111111111"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFTestSample/Benchmark/faae76f3-8378-4271-9822-5d2587113415/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpCl,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6077,concurren,concurrent,6077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf. 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals. 16:17:06.551 INFO HaplotypeCaller - Done initializing engine. 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output. 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported. 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so. 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)). 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded. 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; ```. Since the calculation takes quite long, I checked the WARN messages of the output above. Especially the last one about the AVX instruction set where it says that a **MUCH** slow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794:4606,multi-thread,multi-threaded,4606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794,1,['multi-thread'],['multi-threaded']
Performance,19-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0775.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0784.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ0793.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/000006KQ1479.rb.g.vcf.gz -V /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-5/inputs/1422537242/00000. ...(all the shards fail in the same way). (this is stderr.background for one shard; all 10 shards log the same error). lee04110@ln0005 \[/scratch.global/lee04110/batch\] % cat /scratch.global/lee04110/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.backgroundÂ . INFO:Â  Â  Using cached SIF image. INFO:Â  Â  Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:. Â  Â  java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\]Â --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFOÂ  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:14167,cache,cached,14167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['cache'],['cached']
Performance,"19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:53:34.608 INFO ValidateVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:53:34.608 INFO ValidateVariants - Deflater: IntelDeflater; 19:53:34.608 INFO ValidateVariants - Inflater: IntelInflater; 19:53:34.608 INFO ValidateVariants - GCS max retries/reopens: 20; 19:53:34.608 INFO ValidateVariants - Requester pays: disabled; 19:53:34.608 INFO ValidateVariants - Initializing engine; 19:53:35.169 INFO FeatureManager - Using codec VCFCodec to read file file://chr1-22.phased.rename.reheader.vcf.gz; 19:53:35.594 INFO ValidateVariants - Done initializing engine; 19:53:35.594 WARN ValidateVariants - IDS validation cannot be done because no DBSNP file was provided; 19:53:35.594 WARN ValidateVariants - Other possible validations will still be performed; 19:53:35.594 INFO ProgressMeter - Starting traversal; 19:53:35.595 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 19:53:35.660 INFO ValidateVariants - Shutting down engine; [October 25, 2020 7:53:35 PM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=2114453504; java.lang.ArrayIndexOutOfBoundsException: -87; 	at org.broadinstitute.hellbender.utils.BaseUtils.convertIUPACtoN(BaseUtils.java:123); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.getSubsequenceAt(CachingIndexedFastaSequenceFile.java:340); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.queryAndPrefetch(ReferenceFileSource.java:78); 	at org.broadinstitute.hellbender.engine.ReferenceDataSource.queryAndPrefetch(ReferenceDataSource.java:64); 	at org.broadinstitute.hellbender.engine.ReferenceContext.getBases(ReferenceContext.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6911:3055,perform,performed,3055,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6911,1,['perform'],['performed']
Performance,2 INFO GermlineCNVCaller - Inflater: IntelInflater; 17:28:28.782 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 17:28:28.782 INFO GermlineCNVCaller - Requester pays: disabled; 17:28:28.782 INFO GermlineCNVCaller - Initializing engine; 17:28:34.716 INFO GermlineCNVCaller - Done initializing engine; 17:28:34.723 INFO GermlineCNVCaller - Intervals specified...; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 17:28:35.689 INFO FeatureManager - Using codec IntervalListCodec to read file file:///media/Data/AnnotationDBs/CNV/Genom/hdf5/../Genom.filtered.interval_list; 17:28:42.892 INFO IntervalArgumentCollection - Processing 2741406000 bp from intervals; 17:28:43.237 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 17:28:51.740 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 17:28:57.410 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 17:28:57.410 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 17:28:57.940 INFO GermlineCNVCaller - Aggregating read-count file 0028-21.hdf5 (1 / 44); 17:29:00.837 INFO GermlineCNVCaller - Aggregating read-count file 0045-21.hdf5 (2 / 44); 17:29:03.690 INFO GermlineCNVCaller - Aggregating read-count file 0098-18.hdf5 (3 / 44); 17:29:06.658 INFO GermlineCNVCaller - Aggregating read-count file 0156-21.hdf5 (4 / 44); 17:29:09.435 INFO GermlineCNVCaller - Aggregating read-count file 0429-20.hdf5 (5 / 44); 17:29:12.235 INFO GermlineCNVCaller - Aggregating read-count file 0779-18.hdf5 (6 / 44); 17:29:14.939 INFO GermlineCNVCaller - Aggregating read-count file 1030-20.hdf5 (7 / 44); 17:29:17.822 INFO GermlineCNVCaller - Aggregating read-count file 1098-13.hdf5 (8 / 44); 17:29:20.668 INFO GermlineCNVCaller - Aggregating,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7234:5465,perform,performed,5465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7234,1,['perform'],['performed']
Performance,"2.0 => GCCcore/12.2.0. WARNING: GATK v4.1.4.1 support for Java 11 is in beta state. Use at your own risk. The following have been reloaded with a version change:; 1) GATK/4.5.0-java-17 => GATK/4.1.4.1-GCCcore-8.3.0-Java-11; 2) GCCcore/12.2.0 => GCCcore/8.3.0; 3) GMP/6.2.1-GCCcore-11.2.0 => GMP/6.1.2-GCCcore-8.3.0; 4) Java/17.0.4 => Java/11.0.16; 5) Python/3.9.6-GCCcore-11.2.0 => Python/3.7.4-GCCcore-8.3.0; 6) SQLite/3.36-GCCcore-11.2.0 => SQLite/3.29.0-GCCcore-8.3.0; 7) Tcl/8.6.11-GCCcore-11.2.0 => Tcl/8.6.9-GCCcore-8.3.0; 8) XZ/5.2.5-GCCcore-11.2.0 => XZ/5.2.4-GCCcore-8.3.0; 9) binutils/2.37-GCCcore-11.2.0 => binutils/2.32-GCCcore-8.3.0; 10) bzip2/1.0.8-GCCcore-11.2.0 => bzip2/1.0.8-GCCcore-8.3.0; 11) libffi/3.4.2-GCCcore-11.2.0 => libffi/3.2.1-GCCcore-8.3.0; 12) libreadline/8.1-GCCcore-11.2.0 => libreadline/8.0-GCCcore-8.3.0; 13) ncurses/6.2-GCCcore-11.2.0 => ncurses/6.1-GCCcore-8.3.0; 14) zlib/1.2.11-GCCcore-11.2.0 => zlib/1.2.11-GCCcore-8.3.0. 13:26:41.785 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nbt_main/share/module_new/modules/software/GATK/4.1.4.1-GCCcore-8.3.0-Java-11/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 09, 2024 1:26:42 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:26:42.114 INFO CombineGVCFs - ------------------------------------------------------------; 13:26:42.115 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.4.1; 13:26:42.115 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:26:42.115 INFO CombineGVCFs - Executing as ----@gb-fat-01.nbthpc.local on Linux v4.18.0-305.el8.x86_64 amd64; 13:26:42.115 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.16+8; 13:26:42.115 INFO CombineGVCFs - Start Date/Time: September 9, 2024 at 1:26:41 PM ICT; 13:26:42.115 INFO CombineGVCFs - -----------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8974:1289,Load,Loading,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8974,1,['Load'],['Loading']
Performance,2.544 DEBUG GenomeLocParser - chrUn_KI270752v1 (27745 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270753v1 (62944 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270754v1 (40191 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270755v1 (36723 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270756v1 (79590 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270757v1 (71251 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000214v1 (137718 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270742v1 (186739 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLoc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:19485,Cache,Cache,19485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Cache'],['Cache']
Performance,"20$17960187$17981445/genomicsdb_meta_dir; hdfsBuilderConnect(forceNewInstance=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:1234,Cache,Cache,1234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['Cache'],['Cache']
Performance,"20-g00a40ea-SNAPSHOT-spark.jar; Running:; /mnt/raid5/frankliu/code/SPARK/spark-2.0.2//bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --conf spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1121,load,load,1121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls to: jacky; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing view acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: Changing modify acls groups to: ; 20/10/22 12:02:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jacky); groups with view permissions: Set(); users with modify permissions: Set(jacky); groups with modify permissions: Set(); 20/10/22 12:02:33 INFO yarn.Client: Submitting application application_1603353714322_0004 to ResourceManager; 20/10/22 12:02:33 INFO impl.YarnClientImpl: Submitted application application_1603353714322_0004; 20/10/22 12:02:34 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:34 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1603360953394; 	 final status: UNDEFINED; 	 tracking URL: http://jacky:8088/proxy/application_1603353714322_0004/; 	 user: jacky; 20/10/22 12:02:35 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:36 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:37 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:38 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:39 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:40 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:41 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/10/22 12:02:42 INFO yarn.Client: Application report for application_1603353714322_0004 (state: ACCEPTED); 20/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:4261,queue,queue,4261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['queue'],['queue']
Performance,"20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf; > 15:16:55.199 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf15:16:55.375 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 15:16:57.746 INFO Funcotator - Initializing Funcotator Engine...; > 15:16:57.777 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 15:16:57.894 INFO ProgressMeter - Starting traversal; > 15:16:57.894 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 15:16:57.979 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; > 15:16:57.981 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; > 15:16:57.991 INFO Funcotator - Shutting down engine; > [July 17, 2020 3:16:57 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.31 minutes.; > Runtime.totalMemory()=883949568; > java.lang.IllegalArgumentException: Unexpected value: lncRNA; > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1052); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:158); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:753); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.create(GencodeGtfFeature.java:320); > at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:17096,cache,cache,17096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['cache'],['cache']
Performance,"27 WARN Funcotator - _ _ _ __ __ _ _ _ _ ; 12:11:32.827 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || | ; 12:11:32.828 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || | ; 12:11:32.828 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_| ; 12:11:32.828 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_) ; 12:11:32.828 WARN Funcotator - |___/ ; 12:11:32.828 WARN Funcotator - --------------------------------------------------------------------------------; 12:11:32.828 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this ; 12:11:32.828 WARN Funcotator - run was misconfigured. ; 12:11:32.828 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 12:11:32.828 WARN Funcotator - ================================================================================; 12:11:32.829 INFO VcfFuncotationFactory - ClinVar_VCF 20180401 cache hits/total: 0/0; 12:11:32.829 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; 12:11:32.830 INFO Funcotator - Shutting down engine; [March 24, 2021 12:11:32 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.22 minutes.; Runtime.totalMemory()=1793064960; Tool returned:; true; (gatk) root@75181703d894:/gatk# . ----------------------------------------------------------------------------------------------------------------------------------. the variants.funcotated.maf:. #version 2.4; ##; ## fileformat=VCFv4.2; ## FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ## FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ## FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ## source=Funcotator; ## GATKCommandLine=<ID=Funcotator,CommandLine=""Funcotator --output ./my_data/variants.funcotated.maf --ref-version hg19 --data-sources-path ./my",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:17851,cache,cache,17851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache']
Performance,"29876123--Did-not-inflate-expected-amount-Error). \--. Hi! I'm doing WGS analysis of a pedigree of three individuals using GATK 4.2.0.0. Everything went on well for the first individual. However, in the step of generating gvcf file from bam file, I encountered the error \[htsjdk.samtools.SAMFormatException: Did not inflate expected amount\] in the other two of the individuals. Please help me! Thank you in advance!. a) GATK version used:. GATK 4.2.0.0. b) Exact command used:. java -jar /home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar \\ ; ; HaplotypeCaller \\ ; ; \-R /media/ngs/NGS0/Database/RefSeq/Homo\_sapiens\_NCBI\_GRCh38Decoy/Homo\_sapiens/NCBI/GRCh38Decoy/Sequence/WholeGenomeFasta/NewIndex/genome.fa \\ ; ; \-I /media/ngs/BAM5T/WGS\_analysis/Data/9\_BQSRBam/Ped-San-3\_merged\_realigned\_bqsr.bam \\ ; ; \-ERC GVCF \\ ; ; \-O /media/ngs/BAM5T/WGS\_analysis/Data/10\_gvcf/Ped-San-3\_merged\_realigned\_bqsr.g.vcf. c) Entire error log:. 14:14:32.075 INFO NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/ngs/biosoft/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Nov 01, 2021 2:14:32 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 14:14:32.573 INFO HaplotypeCaller - ------------------------------------------------------------ ; ; 14:14:32.573 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.0.0 ; ; 14:14:32.573 INFO HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 14:14:32.573 INFO HaplotypeCaller - Executing as ngs@ngs-linux on Linux v5.8.0-59-generic amd64 ; ; 14:14:32.573 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_292-8u292-b10-0ubuntu1~20.04-b10 ; ; 14:14:32.573 INFO HaplotypeCaller - Start Date/Time: 2021å¹´11æœˆ1æ—¥ ä¸‹åˆ02æ—¶14åˆ†31ç§’ ; ; 14",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582:1439,Load,Loading,1439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582,1,['Load'],['Loading']
Performance,"29_hg38.vcf -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf; > 15:16:55.199 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/clinvar/hg38/clinvar_20180429_hg38.vcf15:16:55.375 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator2/funcotator_dataSources.v1.7.20200521s/gencode_xhgnc/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 15:16:57.746 INFO Funcotator - Initializing Funcotator Engine...; > 15:16:57.777 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 15:16:57.894 INFO ProgressMeter - Starting traversal; > 15:16:57.894 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 15:16:57.979 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/0; > 15:16:57.981 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/0; > 15:16:57.991 INFO Funcotator - Shutting down engine; > [July 17, 2020 3:16:57 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.31 minutes.; > Runtime.totalMemory()=883949568; > java.lang.IllegalArgumentException: Unexpected value: lncRNA; > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$GeneTranscriptType.getEnum(GencodeGtfFeature.java:1052); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature.<init>(GencodeGtfFeature.java:158); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.<init>(GencodeGtfGeneFeature.java:19); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfGeneFeature.create(GencodeGtfGeneFeature.java:23); > at org.broadinstitute.hellbender.utils.codecs.gtf.GencodeGtfFeature$FeatureType$1.create(GencodeGtfFeature.java:753); > at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708:17013,cache,cache,17013,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708,1,['cache'],['cache']
Performance,"2:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 02:07:51.774 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:07:51.774 INFO HaplotypeCaller - Deflater: IntelDeflater; 02:07:51.774 INFO HaplotypeCaller - Inflater: IntelInflater; 02:07:51.774 INFO HaplotypeCaller - GCS max retries/reopens: 20; 02:07:51.775 INFO HaplotypeCaller - Requester pays: disabled; 02:07:51.775 INFO HaplotypeCaller - Initializing engine; 02:07:52.246 INFO HaplotypeCaller - Done initializing engine; 02:07:52.303 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 02:07:52.312 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 02:07:52.314 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/administrator/IGIB/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 02:07:52.355 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 02:07:52.355 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 02:07:52.356 INFO IntelPairHmm - Available threads: 104; 02:07:52.356 INFO IntelPairHmm - Requested threads: 4; 02:07:52.356 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 02:07:52.408 INFO ProgressMeter - Starting traversal; 02:07:52.408 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 02:07:53.316 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 02:07:53.598 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.49244E-4; 02:07:53.598 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0078887480",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6292:3126,Load,Loading,3126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6292,1,['Load'],['Loading']
Performance,"2; > 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; > 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6543,concurren,concurrent,6543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region chrM:6254-6729; 11:36:56.119 DEBUG Mutect2Engine - Ref haplotype coords chrM:6254-6729; 11:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:12535,cache,cache,12535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"2_BM.microbe_aligned.paired.bam:33554432+33554432; 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5); java.util.NoSuchElementException: next on empty iterator; 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); 	at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); 	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)`. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360,2,['concurren'],['concurrent']
Performance,3 from BlockManagerMaster.; 18/01/09 18:31:15 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000005 on host: tele-2. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:15 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000006 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:21470,concurren,concurrent,21470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,3); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.too,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:2535,concurren,concurrent,2535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,1,['concurren'],['concurrent']
Performance,3); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.ca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:14399,cache,cache,14399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,3); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.Defa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:7574,cache,cache,7574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,"30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv --output_file=small_NA12878.seg --log2_input=TRUE --min_width=2 --alpha=0.01 --nperm=10000 --pmethod=hybrid --kmax=25 --nmin=200 --eta=0.05 --trim=0.025 --undosplits=none --undoprune=0.05 --undoSD=3; Stdout: $sample_name; [1] ""NA12878"". $targets_file; [1] ""/cromwell_root/broad-dsde-methods/cromwell-execution-24/TumorOnly/f30dd8c6-eec3-45ba-b7f2-f845d308d59d/call-TumorNormalizeSomaticReadCounts/small_NA12878.tn.tsv"". $output_file; [1] ""small_NA12878.seg"". $log2_input; [1] ""TRUE"". $min_width; [1] 2. $alpha; [1] 0.01. $nperm; [1] 10000. $pmethod; [1] ""hybrid"". $kmax; [1] 25. $nmin; [1] 200. $eta; [1] 0.05. $trim; [1] 0.025. $undosplits; [1] ""none"". $undoprune; [1] ""0.05"". $undoSD; [1] 3. $help; [1] FALSE. Stderr: Error in sort(abs(diff(genomdat)))[1:n.keep] : ; only 0's may be mixed with negative subscripts; Calls: source ... segment -> inherits -> smooth.CNA -> trimmed.variance; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:163); 	at org.broadinstitute.hellbender.utils.segmenter.RCBSSegmenter.writeSegmentFile(RCBSSegmenter.java:114); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.applySegmentation(PerformSegmentation.java:185); 	at org.broadinstitute.hellbender.tools.exome.PerformSegmentation.doWork(PerformSegmentation.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2944:3008,Perform,PerformSegmentation,3008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2944,4,['Perform'],['PerformSegmentation']
Performance,34); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:140); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:264); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(Cloud,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631:4695,concurren,concurrent,4695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631,1,['concurren'],['concurrent']
Performance,"359270660945146060; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55); 	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144); 	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.Files.walkFileTree(Files.java:2662); 	at java.nio.file.Files.walkFileTree(Files.java:2742); 	at htsjdk.samtools.util.IOUtil.recursiveDelete(IOUtil.java:1344); 	... 3 more; 15:51:41.426 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gss1/home/ldl20190322/a_haoxiaoshuai/z_software/gatk/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 09, 2020 3:51:43 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:51:43.109 INFO ApplyVQSR - ------------------------------------------------------------; 15:51:43.109 INFO ApplyVQSR - The Genome Analysis Toolkit (GATK) v4.1.1.0; 15:51:43.109 INFO ApplyVQSR - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:51:43.109 INFO ApplyVQSR - Executing as ldl20190322@compute20 on Linux v2.6.32-642.el6.x86_64 amd64; 15:51:43.109 INFO ApplyVQSR - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_261-b12; 15:51:43.109 INFO ApplyVQSR - Start Date/Time: November 9, 2020 3:51:41 PM CST; 15:51:43.109 INFO ApplyVQSR - -----------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6948:4552,Load,Loading,4552,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6948,1,['Load'],['Loading']
Performance,38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:12798,Load,Loading,12798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2095,Load,Loading,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,3YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <Ã¸> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <Ã¸> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <Ã¸> (Ã¸)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <Ã¸> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <Ã¸> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=gith,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:2711,scalab,scalable,2711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"3a2bb0d"" in the JAR file name is why I presume that it's based off commit 3a2bb0d. . From the gatk-protected repo code (and also ""gatk"" repo) I added some debug/print statements and saved to a differently named JAR ""eddie.jar"" to help me distinguish my hacking from the original JAR. . The JAVA file where I added the most helpful statements was in CommandLineProgram.java which is actually in ""gatk"" repo (not ""gatk-protected"" repo). If I look at a LOG, I can see ""EAS"" my initials and see c40e75b which appears to be a more recent commit compared to 3a2bb0d. ```; EAS in main!!!!; EAS to call instanceMain second....; EAS to call instanceMain first....; 17:28:40.295 INFO SparkGenomeReadCounts - EAS ABOUT TO CALL instanceMainPostParseArgs in instanceMain in clp.java ; 17:28:40.396 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/cromwell_root/fc-7ac504fc-7fe4-4bc1-89d3-7f16317b8ff4/eddie.jar!/com/intel/gkl/native/libIntelGKL.so; 17:28:40.498 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [December 1, 2016 5:28:40 PM UTC] org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts --binsize 5000 --outputFile this.entity_id.coverage.tsv --reference Homo_sapiens_assembly19.fasta --input firecloud-tcga-open-access/tutorial/bams/C835.HCC1143_BL.4.bam --keepXYMT false --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [December 1, 2016 5:28:40 PM UTC] Executing as root@71bfa07f6996 on Linux 3.16.0-0.bpo.4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:c40e75b-SNAPSHOT; 17:28:40.501 INFO SparkGenomeReadCounts - Defaults.BUFFER_SIZE : 131072; ```. ---. @eddiebroad commented on [Wed Dec 07 2016](https",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2922:4983,load,loaded,4983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2922,1,['load'],['loaded']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 14; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 0; cpu cores	: 14; apicid		: 32; initial apicid	: 32; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:59432,cache,cache,59432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 10; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 22; initial apicid	: 22; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat ps",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:53557,cache,cache,53557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:60606,cache,cache,60606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:61780,cache,cache,61780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:62954,cache,cache,62954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:64128,cache,cache,64128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:65302,cache,cache,65302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:66476,cache,cache,66476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:67650,cache,cache,67650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 23; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 52; initial apicid	: 52; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:68824,cache,cache,68824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['cache'],['cache']
Performance,"4 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:09:55,53] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,54] [info] CallCacheWriteActor stopped; [2020-07-14 05:09:55,54] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2020-07-14 05:09:55,54] [info] IoProxy stopped; [2020-07-14 05:09:55,54] [info] DockerHashActor stopped; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=1 idleQueues.size=1 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] KvWriteActor Shutting down: 0 queued messages to process; [2020-07-14 05:09:55,55] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2020-07-14 05:09:55,55] [info] ServiceRegistryActor stopped; [2020-07-14 05:09:55,58] [info] Database closed; [2020-07-14 05:09:55,58] [info] Stream materializer shut down; [2020-07-14 05:09:55,58] [info] WDL HTTP import resolver closed; Workflow 968be82c-eef3-4bdb-a1ab-3d4e2ca70674 transitioned to state Failed",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:10077,queue,queued,10077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,2,['queue'],['queued']
Performance,"4 INFO HaplotypeCaller - Initializing engine; 03:58:34.384 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:58:34.461 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:58:34.491 INFO HaplotypeCaller - Done initializing engine; 03:58:34.509 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:58:34.532 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:58:34.536 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:58:34.580 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:58:34.582 INFO IntelPairHmm - Available threads: 8; 03:58:34.582 INFO IntelPairHmm - Requested threads: 4; 03:58:34.582 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:58:34.623 INFO ProgressMeter - Starting traversal; 03:58:34.623 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:58:35.812 INFO HaplotypeCaller - 58 read(s) filtered by: ((((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter) AND WellformedReadFilter); 58 read(s) filtered by: (((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter) AND NotDuplicateReadFilter) AND PassesVendorQualityCheckReadFilter) AND NonZeroReferenceLengthAlignmentReadFilter) AND GoodCigarReadFilter); 58 read(s) filtered by: ((((((MappingQualityReadFilter AND MappingQualityAvailableReadFilter) AND MappedReadFilter) AND NotSecondaryAlignmentReadFilter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:11765,multi-thread,multi-threaded,11765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['multi-thread'],['multi-threaded']
Performance,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like itâ€™s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so itâ€™s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2454,optimiz,optimizing,2454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,4,"['cache', 'optimiz']","['cached', 'optimizing']"
Performance,"4.0.0/test/./02/callset.json; 10:19:39.951 INFO GenomicsDBImport - Complete VCF Header will be written to /home/test/Software/gatk-4.4.0.0/test/./02/vcfheader.vcf; 10:19:39.951 INFO GenomicsDBImport - Importing to workspace - /home/test/Software/gatk-4.4.0.0/test/./02; 10:19:40.060 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 10:19:40.075 INFO GenomicsDBImport - Shutting down engine; org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; java.lang.NumberFormatException: For input string: ""G""; 	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67); 	at java.base/java.lang.Integer.parseInt(Integer.java:668); 	at java.base/java.lang.Integer.parseInt(Integer.java:786); 	at htsjdk.tribble.readers.TabixReader.getIntv(TabixReader.java:337); 	at htsjdk.tribble.readers.TabixReader.access$500(TabixReader.java:48); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:438); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); 	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.<init>(TabixFeatureReader.java:159); 	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$1.query(GenomicsDBImport.java:971); 	at org.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); 	at org.genomicsdb.importer.GenomicsDBImporter.lambda$null$4(GenomicsDBImporter.java:732); 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635); 	at java.base/java.lang.Thread.run(Thread.java:833)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8517:4326,concurren,concurrent,4326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8517,3,['concurren'],['concurrent']
Performance,4.1.7.0 VariantAnnotator Performance issue,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6663:25,Perform,Performance,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6663,1,['Perform'],['Performance']
Performance,41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG Mutect2 - Processing assembly region at chrM:1-300 isActive: false numReads: 0; 11:35:42.497 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.520 DEBUG IntToDoubleFunctionCache - cache miss 1 > -1 expanding to 11; 11:35:42.619 DEBUG Mutect2 - Processing assembly region at chrM:301-600 isActive: false numReads: 0; 11:35:42.757 DEBUG IntToDoubleFunctionCache - cache miss 18 > 11 expanding to 28; 11:35:42.758 DEBUG IntToDoubleFunctionCache - cache miss 2649 > 28 expanding to 2659; 11:35:42.766 DEBUG IntToDoubleFunctionCache - cache miss 2666 > 11 expanding to 2676; 11:35:42.789 DEBUG IntToDoubleFunctionCache - cache miss 2667 > 2659 expanding to 5320; 11:35:42.790 DEBUG IntToDoubleFunctionCache - cache miss 2679 > 2676 expanding to 5354; 11:35:43.244 DEBUG Mutect2 - Processing assembly region at chrM:601-900 isActive: false numReads: 0; 11:35:43.823 DEBUG Mutect2 - Processing assembly region at chrM:901-1153 isActive: false numReads: 2725; 11:35:44.025 DEBUG Mutect2 - Processing assembly region at chrM:1154-1397 isActive: true numReads: 5446; 11:35:45.183 DEBUG ReadThreadingGraph - Recovered 0 of 0 dangling tails; 11:35:45.190 DEBUG ReadThreadingGraph - Recovered 0 of 1 dangling heads; 11:35:45.409 DEBUG IntToDoubleFunctionCache - cache miss 0 > -1 expanding to 10; 11:35:45.413 DEBUG Mutect2Engine - Active Region chrM:1154-1397; 11:35:45.413 DEBUG Mutect2Engine - Extended Act Region chrM:1054-1497; 11:35:45.413 DEBUG Mutect2Engi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:7415,cache,cache,7415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"4180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.N",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1761,load,loadLibrary,1761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['loadLibrary']
Performance,45 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:43.161 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:43.161 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:43.161 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:43.161 DEBUG GenomeLocParser - chr7 (159345973 bp); 23:44:43.161 DEBUG GenomeLocParser - chr8 (145138636 bp); 23:44:43.161 DEBUG GenomeLocParser - chr9 (138394717 bp); 23:44:43.161 DEBUG GenomeLocParser - chr10 (133797422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr11 (135086622 bp); 23:44:43.161 DEBUG GenomeLocParser - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:20118,perform,performed,20118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['perform'],['performed']
Performance,45816 +473 ; Branches 16090 16107 +17 ; ===============================================; + Hits 126517 126891 +374 ; - Misses 12966 13048 +82 ; - Partials 5860 5877 +17; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5601?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `92.857% <100%> (Ã¸)` | `17 <0> (Ã¸)` | :arrow_down: |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...nder/utils/downsampling/PositionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUG9zaXRpb25hbERvd25zYW1wbGVyLmphdmE=) | `88.462% <0%> (-11.538%)` | `22% <0%> (+1%)` | |; | [...er/engine/spark/datasources/VariantsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvVmFyaWFudHNTcGFya1NpbmsuamF2YQ==) | `78.125% <0%> (-11.53%)` | `8% <0%> (-1%)` | |; | [...broadinstitute/hellbender/engine/FeatureInput.java](https://codecov.io/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343:1611,Concurren,ConcurrentAFCalculatorProvider,1611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,"463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add flag in prepare to print out sql instead of executing (#7501); - Workflow to re-scatter and then merge ""problematic"" intervals from ExtractCallset [VS-209] (#7495); - changed README to reflect comments from Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of Spl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:18785,load,loaded,18785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['load'],"['load', 'loaded']"
Performance,"47:00.976 INFO HaplotypeCaller - HTSJDK Version: 2.19.0; 15:47:00.976 INFO HaplotypeCaller - Picard Version: 2.19.0; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:00.979 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:00.980 INFO HaplotypeCaller - Deflater: JdkDeflater; 15:47:00.981 INFO HaplotypeCaller - Inflater: JdkInflater; 15:47:00.981 INFO HaplotypeCaller - GCS max retries/reopens: 20; 15:47:00.981 INFO HaplotypeCaller - Requester pays: disabled; 15:47:00.981 INFO HaplotypeCaller - Initializing engine; 15:47:15.632 INFO HaplotypeCaller - Done initializing engine; 15:47:20.372 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 15:47:20.380 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:20.391 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/glier_ubuntu/gatk-4.1.1.0/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:20.423 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 15:47:20.423 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:20.423 INFO IntelPairHmm - Available threads: 40; 15:47:20.423 INFO IntelPairHmm - Requested threads: 4; 15:47:20.423 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:22.213 INFO ProgressMeter - Starting traversal; 15:47:22.213 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:22.231 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 15:47:22.231 INFO PairHMM - Total compute time in P",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6604:5724,Load,Loading,5724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6604,1,['Load'],['Loading']
Performance,"4g; gatk Funcotator \; 	--variant ../relapse.filtered.snps.indels.vcf \; 	--reference $fa \; 	--ref-version hg38 \; 	--data-sources-path $func \; 	--output relapse.funcotated.maf \; 	--output-file-format MAF; ```; after running the script above, it stucked and show nothing anymore, is there something I ignored ?. ```; Using GATK jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar Funcotator --variant ../relapse.filtered.snps.indels.vcf --reference /share/share/data/NGS/ref_index/GATK_bundle/hg38/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g --output relapse.funcotated.maf --output-file-format MAF; 10:24:47.787 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:50.558 INFO Funcotator - ------------------------------------------------------------; 10:24:50.559 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 10:24:50.559 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:50.560 INFO Funcotator - Executing as javis@node4 on Linux v3.10.0-514.el7.x86_64 amd64; 10:24:50.560 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:50.560 INFO Funcotator - Start Date/Time: April 24, 2019 10:24:47 AM CST; 10:24:50.560 INFO Funcotator - ------------------------------------------------------------; 10:24:50.560 INFO Funcotator - ------------------------------------------------------------; 10:24:50.561 INFO Funcotator - HTSJDK Version: 2.18.2; 10:24:50.561 INFO Funcotator - Picard Version: 2.18.25; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5903:1311,Load,Loading,1311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5903,1,['Load'],['Loading']
Performance,5 from BlockManagerMaster.; 18/01/09 18:31:18 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000007 on host: tele-2. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:18 INFO storage.BlockManagerMaster: Removal of executor 6 requested; 18/01/09 18:31:18 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.; 18/01/09 18:31:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 6; 18/01/09 18:31:18 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000008 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:24514,concurren,concurrent,24514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,5); at htsjdk.samtools.MemoryMappedFileBuffer.readBytes(MemoryMappedFileBuffer.java:34); at htsjdk.samtools.AbstractBAMFileIndex.readBytes(AbstractBAMFileIndex.java:439); at htsjdk.samtools.AbstractBAMFileIndex.verifyIndexMagicNumber(AbstractBAMFileIndex.java:376); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:70); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:1367,load,loadNextIterator,1367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709,1,['load'],['loadNextIterator']
Performance,"5.6; 10:25:54.821 INFO ProgressMeter - chr2:237512416 1.5 1069999 691712.8; 10:26:04.863 INFO ProgressMeter - chr3:54999378 1.7 1197525 698570.8; 10:26:09.642 INFO CalibrateDragstrModel - Shutting down engine; [January 2, 2023 at 10:26:09 AM GMT] org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel done. Elapsed time: 1.81 minutes.; Runtime.totalMemory()=47647293440; java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.get(ForkJoinTask.java:1006); at org.broadinstitute.hellbender.utils.Utils.runInParallel(Utils.java:1479); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.collectCaseStatsParallel(CalibrateDragstrModel.java:551); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.traverse(CalibrateDragstrModel.java:202); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:6079,concurren,concurrent,6079,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['concurren'],['concurrent']
Performance,5/NA24631-chr15_68578892_84670250-block-regions.bed --interval_set_rule INTERSECTION --sparkMaster local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH --annotation ClippingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:2691,concurren,concurrent,2691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['concurren'],['concurrent']
Performance,50% speedup for ApplyBQSR by removing indel quals and other optimizations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1099:60,optimiz,optimizations,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1099,1,['optimiz'],['optimizations']
Performance,"538 samples. Chrom 11-22 don't have problems, but 1-11 don't work.; What should I do?; thanks. ```; java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs -R /data/reference/update_gatk_v0/Homo_sapiens_assembly38.fasta -V gendb://Genomicsdb.2 -O /storage/project/collaborators/UH_Burdentest/1.running/genotypeGvcf/UH_Burdentest2222.vcf --tmp-dir /storage/GenomesDbimport/Agilent_WES/tmp -L chr2 -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -D /data/reference/update_gatk_v0//Homo_sapiens_assembly38.dbsnp138.vcf; 20:09:21.335 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 20:09:21.383 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/solivehong/miniconda3/envs/bio_base/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 20:09:21.521 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.521 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 20:09:21.521 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 20:09:21.521 INFO GenotypeGVCFs - Executing as solivehong@solivehong on Linux v5.10.0-25-amd64 amd64; 20:09:21.521 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.13+7-b1751.21; 20:09:21.522 INFO GenotypeGVCFs - Start Date/Time: September 23, 2023 at 8:09:21 PM CST; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - ------------------------------------------------------------; 20:09:21.522 INFO GenotypeGVCFs - HTSJDK Version:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8527:1074,Load,Loading,1074,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8527,1,['Load'],['Loading']
Performance,55v1 (36723 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270756v1 (79590 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270757v1 (71251 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000214v1 (137718 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_KI270742v1 (186739 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000216v2 (176608 bp); 23:44:42.545 DEBUG GenomeLocParser - chrUn_GL000218v1 (161147 bp); 23:44:42.545 DEBUG GenomeLocParser - chrEBV (171823 bp); 23:44:42.632 INFO FeatureManager - Using codec IntervalListCodec to read file file:///gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list; 23:44:42.739 DEBUG FeatureDataSource - Cache statistics for FeatureInput /gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:/gpfs/hpc/home/lijc/xiangxud/project/test/NGS_WES_test/4_tools_vcf/gatk4/info/scatter/scatter_30.interval_list:; 23:44:42.740 DEBUG FeatureCache - Cache hit rate was 0.00% (0 out of 0 total queries); 23:44:42.743 INFO IntervalArgumentCollection - Processing 1022379 bp from intervals; 23:44:42.756 INFO GermlineCNVCaller - Reading and validating annotated intervals...; 23:44:43.119 INFO GermlineCNVCaller - GC-content annotations for intervals found; explicit GC-bias correction will be performed...; 23:44:43.160 INFO GermlineCNVCaller - Running the tool in COHORT mode...; 23:44:43.160 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:44:43.160 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:43.160 DEBUG GenomeLocParser - chr1 (248956422 bp); 23:44:43.161 DEBUG GenomeLocParser - chr2 (242193529 bp); 23:44:43.161 DEBUG GenomeLocParser - chr3 (198295559 bp); 23:44:43.161 DEBUG GenomeLocParser - chr4 (190214555 bp); 23:44:43.161 DEBUG GenomeLocParser - chr5 (181538259 bp); 23:44:43.161 DEBUG GenomeLocParser - chr6 (170805979 bp); 23:44:43.161 DEBUG GenomeLocParser ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:19777,Cache,Cache,19777,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['Cache'],['Cache']
Performance,"581.5; 06:47:23.111 INFOÂ  ProgressMeter - NC_038255.2:27050560Â Â Â Â Â Â Â Â Â Â Â Â  21.0Â Â Â Â Â Â Â Â Â Â Â Â  121742000Â Â Â Â Â Â Â  5793973.2; 06:47:33.116 INFOÂ  ProgressMeter - NC_038255.2:27256247Â Â Â Â Â Â Â Â Â Â Â Â  21.2Â Â Â Â Â Â Â Â Â Â Â Â  122736000Â Â Â Â Â Â Â  5795288.5; 06:47:42.432 INFOÂ  CombineGVCFs - Shutting down engine; [March 13, 2024 at 6:47:42 AM GMT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 21.46 minutes.; Runtime.totalMemory()=920649728; htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Transport endpoint is not connected; at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:205); at htsjdk.tribble.TabixFeatureReader$FeatureIterator.next(TabixFeatureReader.java:149); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:408); at org.broadinstitute.hellbender.engine.MultiVariantDataSource$1.next(MultiVariantDataSource.java:393); at htsjdk.samtools.util.PeekableIterator.advance(PeekableIterator.java:71); at htsjdk.samtools.util.PeekableIterator.next(PeekableIterator.java:57); at htsjdk.samtools.util.MergingIterator.next(MergingIterator.java:101); at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133); at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1845); at java.base/java.util.stream.AbstractPipeline.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:22683,load,loadNextFeature,22683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['load'],['loadNextFeature']
Performance,"58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6318,concurren,concurrent,6318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,"598-b0c5267647ee/call-ImportGVCFs/shard-9/execution/stderr.backgroundÂ . INFO:Â  Â  Using cached SIF image. INFO:Â  Â  Using cached SIF image. Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar. Running:. Â  Â  java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xms8000m -Xmx25000m -DGATK\_STACKTRACE\_ON\_USER\_EXCEPTION=true -jar /gatk/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L /cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/inputs/-1806236336/0009-scattered.interval\_list \[...list of input gvcs\]Â --reader-threads 1 --merge-input-intervals true --consolidate false. Picked up \_JAVA\_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/JointGenotyping/9743b28a-3819-49a7-8598-b0c5267647ee/call-ImportGVCFs/shard-9/tmp.bd1b0bc7. 20:38:55.819 INFOÂ  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so. 20:38:56.233 INFOÂ  GenomicsDBImport - ------------------------------------------------------------. 20:38:56.233 INFOÂ  GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.6.1. 20:38:56.234 INFOÂ  GenomicsDBImport - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/). 20:38:56.237 INFOÂ  GenomicsDBImport - Executing as [lee04110@cn2006.mesabi.msi.umn.edu](mailto:lee04110@cn2006.mesabi.msi.umn.edu) on Linux v3.10.0-1160.76.1.el7.x86\_64 amd64. 20:38:56.237 INFOÂ  GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_242-8u242-b08-0ubuntu3~18.04-b08. 20:38:56.238 INFOÂ  GenomicsDBImport - Start Date/Time: October 18, 2022 8:38:55 PM GMT. 20:38:56.238 INFOÂ  GenomicsDBImport - ------------------------------------------------------------. 20:38:56.238 INFOÂ  GenomicsDBImport - ---",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:15082,Load,Loading,15082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['Load'],['Loading']
Performance,"6 . AC=2;AF=1.00;AN=2;DP=13;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.08;SOR=1.179 GT:AD:DP:GQ:PL 1/1:0,13:13:39:392,39,0; 13 32929232 . A G 168.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=1.335;DP=16;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=15.33;ReadPosRankSum=-1.442;SOR=0.446 GT:AD:DP:GQ:PL 0/1:5,6:11:99:176,0,121; 13 32929387 . T C 209.02 . AC=2;AF=1.00;AN=2;DP=7;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.86;SOR=1.609 GT:AD:DP:GQ:PL 1/1:0,7:7:21:223,21,0; ```. Execution log:; ```; Using GATK jar /gatk/gatk-package-4.2.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.2.2.0-local.jar HaplotypeCaller --input sample.bam --annotation OrientationBiasReadCounts --intervals b37.chr13.bed --reference hs37d5.fa --output sample.vcf.gz; 03:56:44.380 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2023 3:56:44 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 03:56:44.703 INFO HaplotypeCaller - ------------------------------------------------------------; 03:56:44.704 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.2.0; 03:56:44.704 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 03:56:44.705 INFO HaplotypeCaller - Executing as root@d2b0ea7e4079 on Linux v5.10.76-linuxkit amd64; 03:56:44.705 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 03:56:44.705 INFO HaplotypeCaller - Start Date/Time: January 6, 2023 3:56:44 AM GMT; 03:56:44.705 INFO HaplotypeCaller - ------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:2567,Load,Loading,2567,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['Load'],['Loading']
Performance,"6 INFO Mutect2 - Initializing engine; 14:13:46.660 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz; 14:13:48.823 INFO FeatureManager - Using codec VCFCodec to read file gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz; 14:13:54.570 INFO FeatureManager - Using codec IntervalListCodec to read file gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:3402,Load,Loading,3402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['Load'],['Loading']
Performance,6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2557,concurren,concurrent,2557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"6/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list; 14:13:55.076 INFO IntervalArgumentCollection - Processing 308828640 bp from intervals; 14:13:55.233 INFO Mutect2 - Done initializing engine; 14:13:56.023 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:13:56.039 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:13:56.116 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:13:56.122 INFO IntelPairHmm - Available threads: 1; 14:13:56.123 INFO IntelPairHmm - Requested threads: 4; 14:13:56.123 WARN IntelPairHmm - Using 1 available threads, but 4 were requested; 14:13:56.127 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:13:56.302 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 14:13:56.492 INFO ProgressMeter - Starting traversal; 14:13:56.493 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:14:08.796 INFO ProgressMeter - chr1:16085 0.2 60 292.6; 14:14:09.377 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.008674977; 14:14:09.378 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.28976746200000003; 14:14:09.378 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.41 sec; 14:14:09.384 INFO Mutect2 - Shutting down engine; [May 13, 2022 2:14:09 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.49 minutes.; Runtime.totalMemory()=850644992; java.lang.ArrayIndexOutOfBoundsException: -1; at java.util.ArrayList.ele",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:3851,multi-thread,multi-threaded,3851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,1,['multi-thread'],['multi-threaded']
Performance,603-4043; 11:36:16.348 DEBUG Mutect2Engine - Haplotype count 254; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes count 0; 11:36:16.348 DEBUG Mutect2Engine - Kmer sizes values []; 11:36:40.673 DEBUG Mutect2 - Processing assembly region at chrM:3944-4243 isActive: false numReads: 2581; 11:36:40.736 DEBUG Mutect2 - Processing assembly region at chrM:4244-4543 isActive: false numReads: 0; 11:36:40.749 DEBUG Mutect2 - Processing assembly region at chrM:4544-4843 isActive: false numReads: 0; 11:36:40.760 DEBUG Mutect2 - Processing assembly region at chrM:4844-5143 isActive: false numReads: 0; 11:36:40.765 DEBUG Mutect2 - Processing assembly region at chrM:5144-5443 isActive: false numReads: 0; 11:36:40.771 INFO ProgressMeter - chrM:5144 1.0 20 20.4; 11:36:40.774 DEBUG Mutect2 - Processing assembly region at chrM:5444-5743 isActive: false numReads: 0; 11:36:41.211 DEBUG IntToDoubleFunctionCache - cache miss 11898 > 5320 expanding to 11908; 11:36:41.213 DEBUG IntToDoubleFunctionCache - cache miss 17632 > 11908 expanding to 23818; 11:36:41.254 DEBUG IntToDoubleFunctionCache - cache miss 29537 > 23818 expanding to 47638; 11:36:42.578 DEBUG Mutect2 - Processing assembly region at chrM:5744-6043 isActive: false numReads: 0; 11:36:47.533 DEBUG Mutect2 - Processing assembly region at chrM:6044-6343 isActive: false numReads: 30078; 11:36:47.979 DEBUG Mutect2 - Processing assembly region at chrM:6344-6353 isActive: false numReads: 30081; 11:36:48.322 DEBUG Mutect2 - Processing assembly region at chrM:6354-6629 isActive: true numReads: 60135; 11:36:55.630 DEBUG ReadThreadingGraph - Recovered 8 of 11 dangling tails; 11:36:55.645 DEBUG ReadThreadingGraph - Recovered 7 of 16 dangling heads; 11:36:55.737 DEBUG IntToDoubleFunctionCache - cache miss 26606 > 4800 expanding to 26616; 11:36:55.741 DEBUG IntToDoubleFunctionCache - cache miss 26873 > 26616 expanding to 53234; 11:36:56.119 DEBUG Mutect2Engine - Active Region chrM:6354-6629; 11:36:56.119 DEBUG Mutect2Engine - Extended Act Region c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:12444,cache,cache,12444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,"617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:6317,concurren,concurrent,6317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFControlSample/Benchmark/8cf95ec9-48a7-4e20-a8fe-816dc3e652ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFTestSample/Benchmark/6b79227b-3ca8-4f5b-96b6-60d57760cc5b/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:22032,cache,cacheCopy,22032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFControlSample/Benchmark/9f6d4e85-981d-4607-8ff6-97495034807f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""96.65376666666666"",; ""NIST evalHCsystemhours"": ""0.17881944444444442"",; ""NIST evalHCwallclockhours"": ""68.38394444444445"",; ""NIST evalHCwallclockmax"": ""3.8226138888888888"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFTestSample/Benchmark/e62b142c-c39c-4c1f-9a08-c41a96647879/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/ga",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1970,Load,Loading,1970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"6:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:20:57.252 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:20:57.252 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:20:57.253 INFO GenomicsDBImport - Inflater: IntelInflater; 16:20:57.253 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:20:57.253 INFO GenomicsDBImport - Requester pays: disabled; 16:20:57.253 INFO GenomicsDBImport - Initializing engine; 16:20:57.921 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/PoN_gvcf/xgen_plus_spikein.b38.bed; 16:20:58.514 INFO IntervalArgumentCollection - Processing 38997831 bp from intervals; 16:20:58.591 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 16:20:58.594 INFO GenomicsDBImport - Done initializing engine; 16:20:58.829 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/PoN_gvcf/pon_db/vidmap.json; 16:20:58.829 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/PoN_gvcf/pon_db/callset.json; 16:20:58.829 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/PoN_gvcf/pon_db/vcfheader.vcf; 16:20:58.829 INFO GenomicsDBImport - Importing to array - /mnt/PoN_gvcf/pon_db/genomicsdb_array; 16:20:58.830 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial VCF reader initialization.; 16:20:58.830 INFO ProgressMeter - Starting traversal; 16:20:58.830 INFO ProgressMeter - Current Loc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6158:3736,perform,performance,3736,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6158,1,['perform'],['performance']
Performance,7-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/gatk-haplotype/chr15/NA24631-chr15_68578892_84670250-block-regions.bed --interval_set_rule INTERSECTION --sparkMaster local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH --annotation ClippingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:2606,concurren,concurrent,2606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['concurren'],['concurrent']
Performance,"7.131 INFO PostprocessGermlineCNVCalls - Shutting down engine ; ; \[August 30, 2021 11:04:37 AM HKT\] org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls done. Elapsed time: 0.27 minutes. ; ; Runtime.totalMemory()=2463105024 ; ; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; ; python exited with 1 ; ; Command Line: python /tmp/segment\_gcnv\_calls.8152704641395924200.py --ploidy\_calls\_path /staging/wes/healthy\_bams\_for\_CNV/using\_v7\_probe/v7\_case\_ploidy/v7\_cases\_ploidy\_1\_sample\_20210615-calls --model\_shards /staging/wes/healthy\_bams\_for\_C ; ; Stdout: 11:04:36.532 INFO segment\_gcnv\_calls - THEANO\_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast\_run,compute\_test\_value=ignore,openmp=true,blas.ldflags=-lmkl\_rt,openmp\_elemwise\_minsize=10 ; ; 11:04:36.532 INFO segment\_gcnv\_calls - Loading ploidy calls... ; ; 11:04:36.533 INFO gcnvkernel.io.io\_metadata - Loading germline contig ploidy and global read depth metadata... ; ; 11:04:36.543 INFO segment\_gcnv\_calls - Instantiating the Viterbi segmentation engine... Stderr: Traceback (most recent call last): ; ; File ""/tmp/segment\_gcnv\_calls.8152704641395924200.py"", line 92, in <module> ; ; args.intervals\_vcf, args.clustered\_vcf) ; ; TypeError: \_\_init\_\_() takes 6 positional arguments but 8 were given. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) ; ; at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:112) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:193) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:5460,Load,Loading,5460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['Load'],['Loading']
Performance,7.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.01,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2278,Load,Loading,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,7/10/11 14:19:38 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor; 17/10/11 14:19:38 ERROR cluster.YarnScheduler: Lost executor 2 on com2: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:25566,concurren,concurrent,25566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,"716.2 MB]; Executing build with daemon context: DefaultDaemonContext[uid=7e8a7a6d-190b-445f-9873-f0329477e561,javaHome=/usr/lib/jvm/java-8-oracle,daemonRegistryDir=/home/axverdier/.gradle/daemon,pid=32687,idleTimeout=10800000,daemonOpts=-XX:MaxPermSize=256m,-XX:+HeapDumpOnOutOfMemoryError,-Xmx1024m,-Dfile.encoding=US-ASCII,-Duser.country=US,-Duser.language=en,-Duser.variant]; Starting Build; Settings evaluated using settings file '/home/axverdier/Tools/GATK4/git/gatk/settings.gradle'.; Projects loaded. Root project using build file '/home/axverdier/Tools/GATK4/git/gatk/build.gradle'.; Included projects: [root project 'gatk']; Evaluating root project 'gatk' using build file '/home/axverdier/Tools/GATK4/git/gatk/build.gradle'.; build for version:4.0.0.0-32-gf700774-SNAPSHOT; All projects evaluated.; No tasks specified. Using project default tasks 'bundle'; Selected primary task 'bundle' from project :; Tasks to be executed: [task ':createPythonPackageArchive', task ':compileJava', task ':processResources', task ':classes', task ':gatkTabComplete', task ':shadowJar', task ':sparkJar', task ':bundle']; In-memory cache of /home/axverdier/Tools/GATK4/git/gatk/.gradle/3.1/taskArtifacts/fileHashes.bin: Size{2449}, CacheStats{hitCount=9796, missCount=2449, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; In-memory cache of /home/axverdier/Tools/GATK4/git/gatk/.gradle/3.1/taskArtifacts/fileSnapshots.bin: Size{3}, CacheStats{hitCount=0, missCount=3, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; In-memory cache of /home/axverdier/Tools/GATK4/git/gatk/.gradle/3.1/taskArtifacts/taskArtifacts.bin: Size{2}, CacheStats{hitCount=8, missCount=2, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=0}; :createPythonPackageArchive (Thread[Daemon worker Thread 2,5,main]) started.; :createPythonPackageArchive; Executing task ':createPythonPackageArchive' (up-to-date check took 0.003 secs) due to:; Output propert",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4248:3016,cache,cache,3016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4248,1,['cache'],['cache']
Performance,"74aa3480b67b321dc66426b1c600a/src/main/java/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFsEngine.java#L377. and the `--genotype-assignment-method` thus has no effect:; for example (same data as in [#5727#issuecomment-1781017195](https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195)) :; ```; $ gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf --genotype-assignment-method SET_TO_NO_CALL; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf --genotype-assignment-method SET_TO_NO_CALL; Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true; 14:10:28.241 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:10:28.300 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.305 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 14:10:28.305 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:10:28.305 INFO GenotypeGVCFs - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64; 14:10:28.305 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724; 14:10:28.306 INFO GenotypeGVCFs - Start Date/Time: October 29, 2023 at 2:10:28 PM CET; 14:10:28.306 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.306 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:10:28.307 INFO GenotypeGVCFs - HTSJDK Version: 3.0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8569:1166,Load,Loading,1166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8569,1,['Load'],['Loading']
Performance,"756762-69756762 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.575 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756763-69756763 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:14.580 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:69756764-69756764 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:16.681 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr12:70289137-70289137 due to alternate allele: \* ; 07:33:17.957 INFO VcfFuncotationFactory - dbSNP 9606\_b150 cache hits/total: 521/453691 ; 07:33:18.138 INFO Funcotator - Shutting down engine ; [May 28, 2020 7:33:18 AM EDT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 34.35 minutes. ; Runtime.totalMemory()=3822059520 ; java.lang.StringIndexOutOfBoundsException: String index out of range: 545 ; at java.lang.String.substring(String.java:1963) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:256) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:93) ; at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371) ; at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2003) ; at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProtei",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651:1935,cache,cache,1935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651,1,['cache'],['cache']
Performance,"7:04:15.666 INFO FeatureManager - Using codec VCFCodec to read file file:///db_students1/gatk_out/db_raw_call_bbe_6largest.vcf; 17:04:15.912 INFO HaplotypeCaller - Done initializing engine; 17:04:15.914 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 17:04:15.948 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 17:04:15.948 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 17:04:15.960 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_utils.so; 17:04:15.962 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 17:04:16.002 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 17:04:16.003 INFO IntelPairHmm - Available threads: 40; 17:04:16.003 INFO IntelPairHmm - Requested threads: 4; 17:04:16.003 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 17:04:16.052 INFO ProgressMeter - Starting traversal; 17:04:16.052 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 17:04:16.589 WARN InbreedingCoeff - InbreedingCoeff will not be calculated; at least 10 samples must have called genotypes; 17:04:17.126 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 17:04:17.126 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6260:4136,Load,Loading,4136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6260,1,['Load'],['Loading']
Performance,7]a7bb49eaece47a172e2d/TMP/jeter.vcf.gz ; 18:15:23.374 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.385 WARN IntelInflater - Zero Bytes Written : 0 ; 18:15:23.403 INFO IntervalArgumentCollection - Processing 1028 bp from intervals ; 18:15:23.411 INFO HaplotypeCaller - Done initializing engine ; 18:15:23.430 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so ; 18:15:23.475 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/LAB-DATA/BiRD/users/lindenbaum-p/packages/gatk/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so ; 18:15:23.651 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM ; 18:15:23.651 INFO IntelPairHmm - Available threads: 4 ; 18:15:23.651 INFO IntelPairHmm - Requested threads: 4 ; 18:15:23.651 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation ; 18:15:23.671 INFO ProgressMeter - Starting traversal ; 18:15:23.671 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute ; 18:15:26.788 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr1:30191420 and possibly subsequent; at least 10 samples must have called genotypes ; 18:15:27.190 WARN DepthPerSampleHC - Annotation will not be calculated at position chr1:30477350 and possibly subsequent; genotype for sample B00I9EL is not called; 18:15:35.547 INFO ProgressMeter - chr1:32128426 0.2 40 202.1 ; 18:15:48.416 INFO ProgressMeter - chr1:36398656 0.4 80 194.0 ; 18:15:51.025 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.012874514 ; 18:15:51.026 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 5.818477527000001; 18:15:51.026 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 1.35 sec ; 18:15:51.027 INFO HaplotypeCaller - Shutting d,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8106:5073,multi-thread,multi-threaded,5073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8106,1,['multi-thread'],['multi-threaded']
Performance,"8 from BlockManagerMaster.; 18/01/09 18:31:21 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1515493209401_0001_01_000010 on host: tele-6. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_1515493209401_0001_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:262); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); 	at java.lang.Thread.run(Thread.java:745). Container exited with a non-zero exit code 1. 18/01/09 18:31:21 INFO storage.BlockManagerMaster: Removal of executor 9 requested; 18/01/09 18:31:21 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 9; 18/01/09 18:31:21 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 9 from BlockManagerMaster.; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms); 18/01/09 18:31:26 INFO server.AbstractConnector: Stopped Spark@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:31:26 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:4040; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:29080,concurren,concurrent,29080,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['concurren'],['concurrent']
Performance,8); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellb,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4680,concurren,concurrent,4680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"8.2; 23:10:12.683 INFO CountReadsSpark - Picard Version: 2.18.25; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:10:12.684 INFO CountReadsSpark - Deflater: IntelDeflater; 23:10:12.684 INFO CountReadsSpark - Inflater: IntelInflater; 23:10:12.684 INFO CountReadsSpark - GCS max retries/reopens: 20; 23:10:12.684 INFO CountReadsSpark - Requester pays: disabled; 23:10:12.684 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:10:12.685 INFO CountReadsSpark - Initializing engine; 23:10:12.685 INFO CountReadsSpark - Done initializing engine; 19/02/05 23:10:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/02/05 23:10:15 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 19/02/05 23:10:18 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 806177853; 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(6,WrappedArray()); 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(13,WrappedArray()); 23:11:51.429 INFO CountReadsSpark - Shutting down engine; [February 5, 2019 11:11:51 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 1.67 minutes.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:5473,load,load,5473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,['load'],"['load', 'loaded']"
Performance,8.627 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:54:28.627 WARN GermlineCNVCaller - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:54:28.627 INFO GermlineCNVCaller - Initializing engine; 21:54:31.994 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:54:33.457 INFO GermlineCNVCaller - Intervals specified...; 21:54:34.113 INFO IntervalArgumentCollection - Processing 10999816 bp from intervals; 21:54:34.145 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 21:54:34.217 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 21:54:34.217 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 21:54:34.241 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00096.lc.soohee1k.hdf5 (1 / 24); 21:54:36.539 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00268.lc.soohee1k.hdf5 (2 / 24); 21:54:37.967 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00419.lc.soohee1k.hdf5 (3 / 24); 21:54:40.147 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG00759.lc.soohee1k.hdf5 (4 / 24); 21:54:41.782 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/low_coverage_1k/HG01051.lc.soohee1k.hdf5 (5 / 24); 21:54:43.197 INFO GermlineCNVCaller - Aggregating read-count file /home/shlee/gcnv/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4826:2783,perform,performed,2783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826,1,['perform'],['performed']
Performance,"826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai \; > -O HG00190_cram.bam \; > ; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai -O HG00190_cram.bam; 14:59:57.284 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 5, 2017 2:59:57 PM EDT] PrintReads --output HG00190_cram.bam --input gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --clo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:13860,Load,Loading,13860,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['Load'],['Loading']
Performance,"89); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were trying to grep a binary file (#7837); - Cleanup scripts/variantstore [VS-414] (#7834); - Merge VAT TSV files into single bgzipped file [VS-304] (#7848); - Handle fully and partially loaded samples [VS-262] [VS-258] (#7843); - Ingest Error Handling Fixes [VS-261] (#7841); - First cut at a python notebook to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:24005,Perform,Performance,24005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,"['Perform', 'load']","['Performance', 'loaded']"
Performance,"8:31:04 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:05 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:05 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 18/01/09 18:31:05 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> tele-1, PROXY_URI_BASES -> http://tele-1:8088/proxy/application_1515493209401_0001), /proxy/application_1515493209401_0001; 18/01/09 18:31:05 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 18/01/09 18:31:06 INFO yarn.Client: Application report for application_1515493209401_0001 (state: RUNNING); 18/01/09 18:31:06 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 192.168.1.4; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.sun; 	 start time: 1515493860237; 	 final status: UNDEFINED; 	 tracking URL: http://tele-1:8088/proxy/application_1515493209401_0001/; 	 user: sun; 18/01/09 18:31:06 INFO cluster.YarnClientSchedulerBackend: Application application_1515493209401_0001 has started running.; 18/01/09 18:31:06 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44190.; 18/01/09 18:31:06 INFO netty.NettyBlockTransferService: Server created on 192.168.1.4:44190; 18/01/09 18:31:06 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/01/09 18:31:06 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.1.4:44190 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.1.4, 44190, None); 18/01/09 18:31:06 INFO storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:14360,queue,queue,14360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['queue'],['queue']
Performance,"8v1:61596-61748), so it looks like an issue with the estimation of the number of repeats. This is not the most important location, but the error could affect more important calls for other people. The log is the following: ; ```gatk Mutect2 --java-options ""-Xmx15G"" -R /data/references/Homo_sapiens/GATK/hg38/Homo_sapiens_assembly38.fasta -I test1.bam -I test2.bam -O tests.vcf -L test_err.bed ; Using GATK jar /home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15G -jar /home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar Mutect2 -R /data/references/Homo_sapiens/GATK/hg38/Homo_sapiens_assembly38.fasta -I test1.bam -I test2.bam -O tests.vcf -L test_err.bed; 10:34:24.578 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/alcalan/.conda/mutect2-cd161e2f51ff2240ce6390abc942bbdd/share/gatk4-4.1.5.0-1/gatk-package-4.1.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 23, 2020 10:34:24 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:34:24.819 INFO Mutect2 - ------------------------------------------------------------; 10:34:24.820 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.5.0; 10:34:24.820 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:34:24.820 INFO Mutect2 - Executing as alcalan@hn.pioneerx on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 10:34:24.820 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 10:34:24.820 INFO Mutect2 - Start Date/Time: March 23, 2020 10:34:24 AM CET; 10:34:24.820 INFO Mutect2 - --------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6516:1525,Load,Loading,1525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6516,1,['Load'],['Loading']
Performance,"9 INFO HaplotypeCaller - Initializing engine; 03:56:45.204 INFO FeatureManager - Using codec BEDCodec to read file file:///data/b37.chr13.bed; 03:56:45.276 INFO IntervalArgumentCollection - Processing 595907 bp from intervals; 03:56:45.305 INFO HaplotypeCaller - Done initializing engine; 03:56:45.324 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 03:56:45.349 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 03:56:45.351 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 03:56:45.373 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 03:56:45.375 INFO IntelPairHmm - Available threads: 8; 03:56:45.375 INFO IntelPairHmm - Requested threads: 4; 03:56:45.375 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 03:56:45.415 INFO ProgressMeter - Starting traversal; 03:56:45.416 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 03:56:46.180 WARN VariantAnnotatorEngine - Jumbo genotype annotations requested but fragment likelihoods or haplotype likelihoods were not given.; 03:56:46.210 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position 13:32911888 and possibly subsequent; at least 10 samples must have called genotypes; 03:56:46.621 INFO HaplotypeCaller - 1 read(s) filtered by: MappingQualityReadFilter ; 0 read(s) filtered by: MappingQualityAvailableReadFilter ; 0 read(s) filtered by: MappedReadFilter ; 0 read(s) filtered by: NotSecondaryAlignmentReadFilter ; 57 read(s) filtered by: NotDuplicateReadFilter ; 0 read(s) filtered by: PassesVendorQualityCheckReadFilter ; 0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter ; 0 read(s) filtered by: GoodCigarReadFilter ; 0 read(s) filtered b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8149:5453,multi-thread,multi-threaded,5453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8149,1,['multi-thread'],['multi-threaded']
Performance,"9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - -----------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:2970,Load,Loading,2970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['Load'],['Loading']
Performance,"9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - -----------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:1622,Load,Loading,1622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['Load'],['Loading']
Performance,98); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2418,Cache,CacheSupplementedGoogleCloudStorage,2418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,"99027448764 2.1547132944470522E-5; CSCC_0007-M1 0.00135679065051943 2.0692791516445317E-5; CSCC_0008-M1 0.004394182081805844 3.748225078434626E-5; CSCC_0009-M1 0.0019614948575730397 2.4555494660711082E-5; CSCC_0010-M1 0.004122282756273677 3.6210336627748355E-5; CSCC_0010-P1 0.001888852796306713 3.040210329291157E-5; CSCC_0011-M1 0.004616869166852859 3.9987828482322895E-5; CSCC_0012-M1 0.0013866025034395032 2.1835070542119526E-5; CSCC_0012-P1 0.9856060967650699 0.0023006992152694522; CSCC_0013-M1 0.014792148767770472 9.498068474793835E-5; CSCC_0014-M1 0.0028227703351458118 4.122117743000552E-5; CSCC_0015-M1 0.01467099675552882 9.531218938517413E-5; CSCC_0016-P1 0.0014411085088999514 2.716291906758587E-5; CSCC_0017-P1 0.0015213899870480127 2.712650453920576E-5; CSCC_0018-P1 0.001694677662867099 2.913615483470931E-5; CSCC_0019-P1 0.0016654868517623346 2.8602851235266697E-5; CSCC_0020-P1 0.0015496166402163914 2.7824601469663656E-5; ```. The procedure done is. > Base quality score recalibration was performed with GATK 4.1.2.0 (Van der Auwera et al. 2013). GATK SplitIntervals was used to define 32 evenly-sized genomic intervals over which GATK BaseRecalibrator was run for each sample. The 32 recalibration tables per sample were merged into one table per sample with GATK GatherReports.; > ; > Base-recalibrated BAM files were produced with GATK ApplyBQSR in parallel over each of the 3,366 contigs in the hg 38 + alt reference genome, plus the unmapped reads. These were merged into a final BAM per sample with GATK GatherBamFiles.; > ; > SplitIntervals was used to define 3,200 evenly sized genomic intervals across hg38 + alt contigs, excluding telomeres, centromeres, unplaced contigs, unlocalized contigs, decoy and the Epstein-Barr viral sequence (Supplementary data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:1563,perform,performed,1563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,"9931-Funcotator-Information-and-Tutorial#1.1.2.2.1), if I expand both `gnomAD_exome.tar.gz` and `gnomAD_genome.tar.gz`, funcotator dies at startup with a `400 Bad Request` error. This also happens if I expand either one of the `gnomad_*.tar.gz` files individually. . #### Expected behavior; Funcotator annotates my VCF and includes gnomAD annotations in the output VCF. . #### Actual behavior. Crash with 400 Bad Request:. ```; Using GATK jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar Funcotator --variant cohort.vcf.gz --reference GRCh38.d1.vd1/GRCh38.d1.vd1.fa --ref-version hg38 --data-sources-path funcotator_dataSources.v1.7.20200521g --output cohort.funcotator.vcf.gz --output-file-format VCF; 14:24:33.589 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/share/gatk4-4.1.9.0-0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:24:33.842 INFO Funcotator - ------------------------------------------------------------; 14:24:33.842 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:24:33.842 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:24:33.843 INFO Funcotator - Executing as alanh@r820-2-0.local on Linux v3.10.0-1062.el7.x86_64 amd64; 14:24:33.843 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-b11; 14:24:33.843 INFO Funcotator - Start Date/Time: October 29, 2020 2:24:33 PM UTC; 14:24:33.843 INFO Funcotator - ------------------------------------------------------------; 14:24:33.843 INFO Funcotator - ------------------------------------------------------------; 14:24:33.844 INFO Funcotator - HTSJDK Version: 2.23.0; 14:24:33.844 INFO Funcotator - Picard Version: 2.23.3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6926:1926,Load,Loading,1926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6926,1,['Load'],['Loading']
Performance,9scy9leG9tZS9Bbm5vdGF0ZVRhcmdldHMuamF2YQ==) | `78.049% <Ã¸> (Ã¸)` | `7 <0> (Ã¸)` | :arrow_down: |; | [...llbender/tools/exome/plotting/PlotACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9wbG90dGluZy9QbG90QUNOVlJlc3VsdHMuamF2YQ==) | `84.615% <Ã¸> (Ã¸)` | `22 <0> (Ã¸)` | :arrow_down: |; | [...adinstitute/hellbender/tools/exome/PadTargets.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QYWRUYXJnZXRzLmphdmE=) | `100% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...hellbender/tools/genome/SparkGenomeReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWUvU3BhcmtHZW5vbWVSZWFkQ291bnRzLmphdmE=) | `91.089% <Ã¸> (Ã¸)` | `18 <0> (Ã¸)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <Ã¸> (Ã¸)` | `2 <0> (Ã¸)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <Ã¸> (Ã¸)` | `28 <0> (Ã¸)` | :arrow_down: |; | [...ute/hellbender/tools/exome/ConvertACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Db252ZXJ0QUNOVlJlc3VsdHMuamF2YQ==) | `87.805% <Ã¸> (Ã¸)` | `4 <0> (Ã¸)` | :arrow_down: |; | [...idation/AnnotateVcfWithExpectedAlleleFraction.java](https://codecov.io/gh/broadinstitute/gatk/pull/313,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624:2345,Perform,PerformAlleleFractionSegmentation,2345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2329,concurren,concurrent,2329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,": false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:32.582 INFO GenomicsDBImport - Importing batch 1 with 1115",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance,":///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 09:39:56.024 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 09:39:56.032 INFO Mutect2 - Done initializing engine; 09:39:56.044 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:39:56.077 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 09:39:56.139 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 09:39:56.139 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 09:39:56.139 INFO IntelPairHmm - Available threads: 36; 09:39:56.139 INFO IntelPairHmm - Requested threads: 4; 09:39:56.139 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 09:39:56.146 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 09:39:56.146 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 09:39:56.146 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec; 09:39:56.148 INFO Mutect2 - Shutting down engine; [July 3, 2020 9:39:56 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=2233991168; htsjdk.samtools.util.RuntimeIOException: File not found: mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:451); 	at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:415); 	at org.broadinstitute.hellbender.utils.variant.GATKVariantContextUtils.createVCFWriter(GATKVariantContextUtils.java:121); 	at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:3880,multi-thread,multi-threaded,3880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['multi-thread'],['multi-threaded']
Performance,":///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect2 - Shutting down engine; [July 2, 2020 11:47:52 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2511863808; java.lang.IllegalArgumentException: Read bases and read quality arrays aren't the same size: Bases: 38 vs Base Q's: 38 vs Insert Q's: 146 vs Delete Q's: 146.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:734); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:4348,multi-thread,multi-threaded,4348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['multi-thread'],['multi-threaded']
Performance,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2026:4485,Load,LoadSnappy,4485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance,":01.494 INFO HaplotypeCaller - Deflater: IntelDeflater. 18:58:01.494 INFO HaplotypeCaller - Inflater: IntelInflater. 18:58:01.494 INFO HaplotypeCaller - GCS max retries/reopens: 20. 18:58:01.494 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes. 18:58:01.494 INFO HaplotypeCaller - Initializing engine. 18:58:02.043 INFO HaplotypeCaller - Done initializing engine. 18:58:02.053 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output. 18:58:02.053 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output. 18:58:02.886 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_utils.so. 18:58:02.888 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/bigdata/operations/pkgadmin/opt/linux/centos/7.x/x86_64/pkgs/gatk/4.0.1.2/gatk-package-4.0.1.2-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so. 18:58:02.932 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM. 18:58:02.933 INFO IntelPairHmm - Available threads: 8. 18:58:02.933 INFO IntelPairHmm - Requested threads: 4. 18:58:02.933 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. 18:58:02.986 INFO ProgressMeter - Starting traversal. 18:58:02.987 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute. 18:58:12.992 INFO ProgressMeter - chr1:6969901 0.2 23290 139712.1. 18:58:22.989 INFO ProgressMeter - chr1:13470130 0.3 44960 134866.5. 18:58:32.991 INFO ProgressMeter - chr1:21393130 0.5 71370 142721.0. INFO 18:58:37,986 HelpFormatter - ---------------------------------------------------------------------------------- . INFO 18:58:37,989 HelpForm",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4788:3624,Load,Loading,3624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788,1,['Load'],['Loading']
Performance,":03:49.911 INFO ProgressMeter - D01:106344 16018.7 6806170 424.9; 12:04:23.685 INFO ProgressMeter - D01:108187 16019.3 6806180 424.9; 12:04:33.872 INFO ProgressMeter - D01:131768 16019.5 6806320 424.9; 12:04:44.080 INFO ProgressMeter - D01:142033 16019.6 6806390 424.9; 12:05:30.576 INFO ProgressMeter - D01:151103 16020.4 6806460 424.9; 12:05:40.721 INFO ProgressMeter - D01:171985 16020.6 6806600 424.9; 12:05:52.649 INFO ProgressMeter - D01:179627 16020.8 6806650 424.9; 12:06:05.001 INFO ProgressMeter - D01:191227 16021.0 6806730 424.9; 12:06:32.206 INFO ProgressMeter - D01:211153 16021.4 6806880 424.9; 12:06:46.312 INFO ProgressMeter - D01:220323 16021.7 6806940 424.9; 12:06:56.646 INFO ProgressMeter - D01:232399 16021.8 6807020 424.9; 12:07:10.267 INFO ProgressMeter - D01:235919 16022.1 6807050 424.9; 12:07:22.064 INFO ProgressMeter - D01:256207 16022.3 6807200 424.9. **_The later LOG_**. nohup: ignoring input and appending output to â€˜nohup.outâ€™; 09:13:38.398 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:13:57.227 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.228 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.10.1; 09:13:57.228 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:13:57.229 INFO HaplotypeCaller - Executing as chenwei@localhost.localdomain on Linux v3.10.0-1160.31.1.el7.x86_64 amd64; 09:13:57.229 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_292-b10; 09:13:57.230 INFO HaplotypeCaller - Start Date/Time: September 3, 2021 9:13:38 AM CST; 09:13:57.230 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.230 INFO HaplotypeCaller - ------------------------------------------------------------; 09:13:57.232 INFO HaplotypeCaller - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:6927,Load,Loading,6927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['Load'],['Loading']
Performance,":04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 223 > 222 expanding to 446; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 447 > 446 expanding to 894; > 21:14:17.873 DEBUG MathUtils$Log10Cache - cache miss 895 > 894 expanding to 1790; > 21:14:17.874 DEBUG MathUtils$Log10Cache - ca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:5197,cache,cache,5197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['cache'],['cache']
Performance,":05:02.975 INFO HaplotypeCaller - Requester pays: disabled; 09:05:02.976 INFO HaplotypeCaller - Initializing engine; 09:05:05.609 INFO HaplotypeCaller - Done initializing engine; 09:05:05.616 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 09:05:06.026 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 09:05:06.026 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 09:05:06.055 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/share/home/chenwei/biosoft/gatk-4.0.10.1/gatk-package-4.0.10.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:05:06.083 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:05:06.083 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:05:06.411 INFO ProgressMeter - Starting traversal; 09:05:06.412 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:05:16.445 INFO ProgressMeter - A01:27883 0.2 110 657.9; 09:05:27.780 INFO ProgressMeter - A01:45980 0.4 240 673.9; 09:05:38.219 INFO ProgressMeter - A01:64498 0.5 360 679.1; .......; 11:56:31.165 INFO ProgressMeter - A13:109860605 16011.4 6803200 424.9; 11:56:41.437 INFO ProgressMeter - A13:109878042 16011.6 6803330 424.9; 11:56:51.882 INFO ProgressMeter - A13:109913861 16011.8 6803540 424.9; 11:57:02.097 INFO ProgressMeter - A13:109924924 16011.9 6803620 424.9; 11:57:14.212 INFO ProgressMeter - A13:109930687 16012.1 6803650 424.9; 11:57:26.561 INFO ProgressMeter - A13:109956963 16012.3 6803",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454:3170,multi-thread,multi-threaded,3170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454,1,['multi-thread'],['multi-threaded']
Performance,":118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6247,concurren,concurrent,6247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,":160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:21931,concurren,concurrent,21931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance,":22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2851,Load,Loading,2851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Generated by:. java -Xmx16g -jar /dsde/working/slee/CNV-1.5_HCC1143/resources/gatk-package-4.beta.5-97-g066c0b4-SNAPSHOT-local.jar SparkGenomeReadCounts \; --input /dsde/working/slee/CNV-1.5_HCC1143/run/cromwell-executions/CNVSomaticPairWorkflow/859bdb60-ba9c-4cf8-98c7-0fb9847a7ee0/call-CollectReadCountsNormal/inputs/seq/picard_aggregation/G15511/HCC1143_BL/current/HCC1143_BL.bam \; --reference /dsde/working/slee/CNV-1.5_HCC1143/run/cromwell-executions/CNVSomaticPairWorkflow/859bdb60-ba9c-4cf8-98c7-0fb9847a7ee0/call-CollectReadCountsNormal/inputs/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta \; --binLength 250 \; --keepXYMT false \; --disableToolDefaultReadFilters false \; --disableSequenceDictionaryValidation true \; --disableReadFilter NotDuplicateReadFilter \; --output HCC1143_BL.readCounts.tsv \; --writeHdf5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:12006,concurren,concurrent,12006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['concurren'],['concurrent']
Performance,:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 21/04/13 07:32:24 ERROR SparkHadoopWriter: Task attempt_20210413073224_0026_r_000000_0 aborted.; 21/04/13 07:32:24 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 105); org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:5173,concurren,concurrent,5173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['concurren'],['concurrent']
Performance,:40.201 INFO Mutect2 - Inflater: JdkInflater; 11:35:40.202 INFO Mutect2 - GCS max retries/reopens: 20; 11:35:40.202 INFO Mutect2 - Requester pays: disabled; 11:35:40.202 INFO Mutect2 - Initializing engine; 11:35:41.694 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.695 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.699 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.699 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.702 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 11:35:41.702 DEBUG GenomeLocParser - chrM (16299 bp); 11:35:41.703 INFO Mutect2 - Done initializing engine; 11:35:41.748 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:35:41.775 DEBUG NativeLibraryLoader - Extracting libgkl_utils.so to /tmp/libgkl_utils9151568277466250840.so; 11:35:41.777 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/user/bin/GATK/4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:35:41.802 DEBUG NativeLibraryLoader - Extracting libgkl_pairhmm_omp.so to /tmp/libgkl_pairhmm_omp8179002917276126697.so; 11:35:41.847 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:35:41.848 INFO IntelPairHmm - Available threads: 64; 11:35:41.848 INFO IntelPairHmm - Requested threads: 4; 11:35:41.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:35:41.882 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; 11:35:41.997 INFO ProgressMeter - Starting traversal; 11:35:41.997 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:35:42.019 DEBUG ReadsPathDataSource - Preparing readers for traversal; 11:35:42.470 DEBUG ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:5946,Load,Loading,5946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['Load'],['Loading']
Performance,:409); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.samtools.SAMException: Fasta index file could not be opened: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta.fai; at htsjdk.samtools.reference.FastaSequenceIndex.<init>(FastaSequenceIndex.java:74); at htsjdk.samtools.reference.IndexedFastaSequenceFile.<init>(IndexedFastaSequenceFile.java:98); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:139); at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.<init>(CachingIndexedFastaSequenceFile.java:148); ... 24 more; Caused by: java.nio.file.FileSystemException: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642:3609,concurren,concurrent,3609,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642,1,['concurren'],['concurrent']
Performance,:41:57.940 DEBUG Mutect2Engine - Haplotype count 128; 11:41:57.941 DEBUG Mutect2Engine - Kmer sizes count 0; 11:41:57.942 DEBUG Mutect2Engine - Kmer sizes values []; 11:53:42.116 DEBUG Mutect2 - Processing assembly region at chrM:9130-9143 isActive: true numReads: 148251; 11:53:58.336 DEBUG ReadThreadingGraph - Recovered 4 of 9 dangling tails; 11:53:58.398 DEBUG ReadThreadingGraph - Recovered 0 of 20 dangling heads; 11:54:11.645 DEBUG ReadThreadingGraph - Recovered 20 of 23 dangling tails; 11:54:11.670 DEBUG ReadThreadingGraph - Recovered 0 of 60 dangling heads; 11:54:11.843 DEBUG Mutect2Engine - Active Region chrM:9130-9143; 11:54:11.852 DEBUG Mutect2Engine - Extended Act Region chrM:9030-9243; 11:54:11.861 DEBUG Mutect2Engine - Ref haplotype coords chrM:9030-9243; 11:54:11.870 DEBUG Mutect2Engine - Haplotype count 232; 11:54:11.879 DEBUG Mutect2Engine - Kmer sizes count 0; 11:54:11.889 DEBUG Mutect2Engine - Kmer sizes values []; 11:54:21.878 DEBUG IntToDoubleFunctionCache - cache miss 96632 > 95278 expanding to 190558; 11:54:22.252 DEBUG Mutect2 - Processing assembly region at chrM:9144-9301 isActive: false numReads: 273760; 11:54:28.421 DEBUG Mutect2 - Processing assembly region at chrM:9302-9584 isActive: true numReads: 250870; 11:55:47.246 DEBUG ReadThreadingGraph - Recovered 13 of 14 dangling tails; 11:55:47.346 DEBUG ReadThreadingGraph - Recovered 6 of 47 dangling heads; 11:55:47.787 DEBUG Mutect2Engine - Active Region chrM:9302-9584; 11:55:47.792 DEBUG Mutect2Engine - Extended Act Region chrM:9202-9684; 11:55:47.796 DEBUG Mutect2Engine - Ref haplotype coords chrM:9202-9684; 11:55:47.800 DEBUG Mutect2Engine - Haplotype count 128; 11:55:47.803 DEBUG Mutect2Engine - Kmer sizes count 0; 11:55:47.807 DEBUG Mutect2Engine - Kmer sizes values []; 12:05:48.002 DEBUG Mutect2 - Processing assembly region at chrM:9585-9884 isActive: false numReads: 125080; 12:05:51.435 DEBUG Mutect2 - Processing assembly region at chrM:9885-10184 isActive: false numReads: 0; 12:05:51.44,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7281:16950,cache,cache,16950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7281,1,['cache'],['cache']
Performance,":42.093 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:18:42.093 INFO HaplotypeCaller - Deflater: IntelDeflater; 12:18:42.093 INFO HaplotypeCaller - Inflater: IntelInflater; 12:18:42.093 INFO HaplotypeCaller - GCS max retries/reopens: 20; 12:18:42.093 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 12:18:42.093 INFO HaplotypeCaller - Initializing engine; 12:18:42.597 INFO FeatureManager - Using codec VCFCodec to read file file:///beegfs/work/zxmai83/Reference/dbs/b37/dbsnp_138.b37.vcf; 12:18:42.723 INFO HaplotypeCaller - Done initializing engine; 12:18:42.732 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 12:18:42.732 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 12:18:43.546 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 12:18:43.549 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/usr/bin/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 12:18:43.599 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00002b5f92e39fab, pid=85482, tid=0x00002b5f56e60ae8; #; # JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12); # Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops); # Derivative: IcedTea 3.6.0; # Distribution: Custom build (Tue Nov 21 11:22:36 GMT 2017); # Problematic frame:; # C [libgomp.so.1+0x7fab] omp_get_max_threads+0xb; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java aga",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:3440,Load,Loading,3440,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['Load'],['Loading']
Performance,":49.226 INFO GenomicsDBImport - Done importing batch 2/5; 23 Feb 2022 18:26:19,107 DEBUG: 	18:26:19.105 INFO GenomicsDBImport - Done importing batch 3/5; 23 Feb 2022 19:20:18,500 DEBUG: 	19:20:18.478 INFO GenomicsDBImport - Done importing batch 4/5; 24 Feb 2022 16:51:19,017 DEBUG: 	[TileDB::utils] Error: (gzip_handle_error) Cannot decompress with GZIP: inflateInit error: Z_MEM_ERROR; 24 Feb 2022 16:51:19,048 DEBUG: 	[TileDB::Codec] Error: Could not decompress with GZIP.; 24 Feb 2022 16:51:19,056 DEBUG: 	[TileDB::ReadState] Error: Cannot decompress tile for /home/exacloud/gscratch/prime-seq/workDir/0950f56b-7565-103a-a738-f8f3fc8675d2/Job2.work/WGS_1852_consolidated.gdb/2$1$196197964/__e7217c9e-767d-4295-b75a-9162c22c6996139785909643008_1613563029631/END.tdb.; 24 Feb 2022 16:51:51,388 DEBUG: 	16:51:51.388 erro NativeGenomicsDB - pid=225263 tid=225739 VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,405 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:51:51,412 DEBUG: 	terminate called after throwing an instance of 'VariantStorageManagerException'; 24 Feb 2022 16:51:51,419 DEBUG: 	 what(): VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,427 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:52:27,478 WARN : 	process exited with non-zero value: 134; ```. Does that give anything to suggest troubleshooting steps?. The full command is:; ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	Xmx497g -Xms497g -Xss2m \; 	-jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar \; 	GenomicsDBImport \; 	-V 25780.g.vcf.gz \; 	-V <total of 92 gVCFs> \; 	--genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; 	--batch-size 10 \; 	--reader-threads 12 \; 	--consolidate \; 	--genomicsdb-shared-posixfs-optimizations \; 	--bypass-feature-reader \; 	-R 128_Mmul_10.fasta; ```. this is GATK v4.2.5.0. Thanks i advance for any ideas.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022:2931,optimiz,optimizations,2931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022,1,['optimiz'],['optimizations']
Performance,":50:16.818 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:50:16.818 INFO HaplotypeCaller - Inflater: IntelInflater; 14:50:16.818 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:50:16.818 INFO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:24",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6625,Load,Loading,6625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:53); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:447); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.<init>(TribbleIndexedFeatureReader.java:390); at htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638:4455,concurren,concurrent,4455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638,1,['concurren'],['concurrent']
Performance,"; 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 ERROR cluster.YarnScheduler: Lost executor 1 on com2: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, com2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:19767,concurren,concurrent,19767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['concurren'],['concurrent']
Performance,; 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:228); 	... 26 more; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getBytesFromStream(StreamingProcessController.java:261); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.lambda$waitForAck$0(StreamingProcessController.java:208); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). but if I change memory as below: it works. ; qlogin -l s_vmem=20G -l mem_req=20G,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:3601,concurren,concurrent,3601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,5,['concurren'],['concurrent']
Performance,; 	at org.broadinstitute.hellbender.tools.GatherVcfsCloud.doWork(GatherVcfsCloud.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 10 more; Caused by: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:526); 	at com.goog,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:2823,concurren,concurrent,2823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['concurren'],['concurrent']
Performance,"; --apiKey $APIKEY \; -I $bamIn \; -- \; --sparkRunner GCS \; --driver-memory 8G \; --cluster $CLUSTERNAME \; --executor-cores 3 \; --executor-memory 25G \; --conf spark.yarn.executor.memoryOverhead=2500""; ```. Fails with:. ```; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/Logging; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); at java.net.URLClassLoader.access$100(URLClassLoader.java:73); at java.net.URLClassLoader$1.run(URLClassLoader.java:368); at java.net.URLClassLoader$1.run(URLClassLoader.java:362); at java.security.AccessController.doPrivileged(Native Method); at java.net.URLClassLoader.findClass(URLClassLoader.java:361); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:52); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at org.bdgenomics.adam.serialization.ADAMKryoRegistrator.registerClasses(ADAMKryoRegistrator.scala:85); at org.broadinstitute.hellbender.engine.spark.GATKRegistrator.registerClasses(GATKRegistrator.java:74); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$6.apply(KryoSerializer.scala:125); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:125); at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:274); at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:259); at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:175); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:1349,load,loadClass,1349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,1,['load'],['loadClass']
Performance,"; 04:37:30.680 INFO ProgressMeter - Starting traversal; 04:37:30.680 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 04:37:33.253 INFO GenomicsDBImport - Starting batch input file preload; 04:37:35.079 INFO GenomicsDBImport - Finished batch preload; 04:37:35.079 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:37.079 INFO GenomicsDBImport - Starting batch input file preload; 04:37:38.712 INFO GenomicsDBImport - Finished batch preload; 04:37:38.712 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 04:37:39.162 INFO GenomicsDBImport - Shutting down engine; [October 8, 2018 4:37:39 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=4116185088; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:1:29867-31003 queried with: 1:68590-70510; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:769); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:165); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:604); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFutu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300:4351,concurren,concurrent,4351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300,1,['concurren'],['concurrent']
Performance,"; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:14 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 428.57 minutes.; Runtime.totalMemory()=8988393472; Tool returned:; true; Using GATK jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx24g -jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar GenotypeGVCFs -R /home/WangBS/Reference/Qrobur/Qrob_PM1N.fa -V gendb:///home/WangBS/Analyses/vcf/test/chr02 -all-sites -O /home/WangBS/Analyses/vcf/test/chr02.vcf; 06:50:19.236 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:51:21.116 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.116 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 06:51:21.116 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:51:21.117 INFO GenotypeGVCFs - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 06:51:21.117 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 06:51:21.117 INFO GenotypeGVCFs - Start Date/Time: January 27, 2019 6:50:19 AM CST; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.118 INFO GenotypeGVCFs - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5865:4993,Load,Loading,4993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865,1,['Load'],['Loading']
Performance,; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4364,concurren,concurrent,4364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,; Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReade,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2151,concurren,concurrent,2151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,; at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:15874,cache,cache,15874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['cache'],['cache']
Performance,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1949:2806,concurren,concurrent,2806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949,2,['concurren'],['concurrent']
Performance,; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:1769,perform,performEMIteration,1769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,2,['perform'],['performEMIteration']
Performance,"; gatk MarkDuplicates I=WA02_i5-537_i7-98_S11819_L004.bam O=test.dup.bam M=marked_dup_metrics.txt; Using GATK jar /opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar MarkDuplicates I=WA02_i5-537_i7-98_S11819_L004.bam O=test.dup.bam M=marked_dup_metrics.txt; INFO 2024-07-03 15:26:21 MarkDuplicates. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; **********; https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** MarkDuplicates -I WA02_i5-537_i7-98_S11819_L004.bam -O test.dup.bam -M marked_dup_metrics.txt; **********. 15:26:21.393 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.6.0.0/gatk-package-4.6.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Wed Jul 03 15:26:21 CEST 2024] MarkDuplicates INPUT=[WA02_i5-537_i7-98_S11819_L004.bam] OUTPUT=test.dup.bam METRICS_FILE=marked_dup_metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true DUPLEX_UMI=false FLOW_MODE=false FLOW_DUP_STRATEGY=FLOW_QUALITY_SUM_STRATEGY USE_END_IN_UNPAIRED_READS=false USE_UNPAIRED_CLIPPED_END=false UNPAIRED_END_UNCERTAINTY=0 UNPAIRED_START_UNCERTAINTY=0 FLOW_SKIP_FIRST_N_FLOWS=0 FLOW_Q_IS_KNOWN_END=false FLOW_EFFECTIVE_QUALITY_THRESHOLD=15 ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimiz",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8904:6119,Load,Loading,6119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8904,1,['Load'],['Loading']
Performance,; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Åuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Dat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6708,load,load,6708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,"; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar CombineGVCFs -R /db_students1/cc/genetic_map/snp_calling/bbv18h27rm.fa --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1to128.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_1280to18.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_180to25.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_250to35.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_350to5.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_50to69.g.vcf.gz --variant /db_students1/cc/gatk_out/tmp_vcf/raw_new52_off_xL4_70_690to999.g.vcf.gz -O /db_students1/cc/gatk_out/raw_new52_off_xL4_70.g.vcf.gz; 17:52:22.080 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/cc/gatk/gatk_dir/gatk/build/libs/gatk-package-4.1.3.0-25-g8d88f6e-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 11, 2020 5:52:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:52:23.841 INFO CombineGVCFs - ------------------------------------------------------------; 17:52:23.841 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.3.0-25-g8d88f6e-SNAPSHOT; 17:52:23.841 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:23.842 INFO CombineGVCFs - Executing as cc@hr18b on Linux v3.10.0-957.el7.x86_64 amd64; 17:52:23.842 INFO CombineGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 17:52:23.842 INFO CombineGVCFs - Start Date/Time: January 11, 2020 5:52:22 PM CST; 17:52:23.842 INFO CombineGVCFs - ----------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6368:1395,Load,Loading,1395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6368,1,['Load'],['Loading']
Performance,;)I> (0x000000067b20f3e0) thrown at [/home/buildozer/aports/community/openjdk8/s; Event: 3.490 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$comparingInt$7b0bb60$1(Ljava/util/function/ToIntFunction;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b219168) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.; Event: 3.491 Thread 0x00005648765c2000 Exception <a 'java/lang/NoSuchMethodError': java.lang.Object.lambda$thenComparing$36697e65$1(Ljava/util/Comparator;Ljava/lang/Object;Ljava/lang/Object;)I> (0x000000067b220588) thrown at [/home/buildozer/aports/community/openjdk8/src/icedtea-3.6.0/openjdk/. Events (10 events):; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmmOMP done; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm; Event: 4.325 loading class com/intel/gkl/pairhmm/IntelPairHmm done; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils; Event: 4.326 loading class com/intel/gkl/IntelGKLUtils done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder done; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder; Event: 4.379 loading class org/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder done. Dynamic libraries:; 3c0000000-41b600000 rw-p 00000000 00:00 0 ; 41b600000-66ab00000 ---p 00000000 00:00 0 ; 66ab00000-6aef00000 rw-p 00000000 00:00 0 ; 6aef00000-7c0000000 ---p 00000000 00:00 0 ; 7c0000000-7c0520000 rw-p 00000000 00:00 0 ; 7c0520000-800000000 ---p 00000000 00:00 0 ; 2b5f56cd5000-2b5f56d5e000 r-xp 00000000 07:00 565 /lib/ld-musl-x86_64.so.1; 2b5f56d5e000-2b5f56d60000 ---p 00000000 00:00 0 ; 2b5f56d60000-2b5f56d63000 ---p 00000000 00:00 0 ; 2b5f56d63000-2b5f56e61000 rw-p 00000000 00:00 0 [stack:85483]; 2b5f56e61000-2b5f,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:29287,load,loading,29287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['load'],['loading']
Performance,"=0, nn=gs://hellbender-test-logs, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:; java.io.IOException: Must supply a value for configuration setting: fs.gs.project.id; 	at com.google.cloud.hadoop.util.ConfigurationUtil.getMandatoryConfig(ConfigurationUtil.java:39); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createOptionsBuilderFromConfig(GoogleHadoopFileSystemBase.java:2185); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1832); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1013); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:976); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:171); 	at org.apache.hadoop.fs.FileSystem$1.run(FileSystem.java:168); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:168); 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:176); 	at org.genomicsdb.reader.Gen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6522:1311,Cache,Cache,1311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6522,1,['Cache'],['Cache']
Performance,===========; Files 1139 1146 +7 ; Lines 60902 62029 +1127 ; Branches 9437 9684 +247 ; ===============================================; + Hits 48705 49752 +1047 ; - Misses 8401 8435 +34 ; - Partials 3796 3842 +46; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ellbender/tools/exome/FilterByOrientationBias.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9GaWx0ZXJCeU9yaWVudGF0aW9uQmlhcy5qYXZh) | `83.019% <Ã¸> (Ã¸)` | `14 <0> (Ã¸)` | :arrow_down: |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `100% <0%> (Ã¸)` | `10% <0%> (+3%)` | :arrow_up: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <0%> (Ã¸)` | `4% <0%> (+2%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92.593% <0%> (Ã¸)` | `32% <0%> (+16%)` | :arrow_up: |; | [...s/spark/pathseq/PSBuildReferenceTaxonomyUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTQnVpbGRSZWZlcmVuY2VUYXhvbm9teVV0aWxzLmphdmE=) | `88.961% <0%> (Ã¸)` | `39% <0%> (?)` | |; | [.../hellbender/tools/spark/utils/LongBloomFilter.java](https://codecov.io/gh/broadinstitute/gat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974:1560,Perform,PerformAlleleFractionSegmentation,1560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,====================; Files 1138 1142 +4 ; Lines 62637 62823 +186 ; Branches 9521 9548 +27 ; ===============================================; + Hits 49731 49871 +140 ; - Misses 9110 9142 +32 ; - Partials 3796 3810 +14; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ber/coverage/readcount/ReadCountFileHeaderKey.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2NvdmVyYWdlL3JlYWRjb3VudC9SZWFkQ291bnRGaWxlSGVhZGVyS2V5LmphdmE=) | `0% <0%> (Ã¸)` | `0 <0> (?)` | |; | [...der/utils/test/IntegerReadCountFileComparator.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVnZXJSZWFkQ291bnRGaWxlQ29tcGFyYXRvci5qYXZh) | `70.732% <70.732%> (Ã¸)` | `6 <6> (?)` | |; | [...pynumber/utils/CachedBinarySearchIntervalList.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0NhY2hlZEJpbmFyeVNlYXJjaEludGVydmFsTGlzdC5qYXZh) | `74.603% <74.603%> (Ã¸)` | `18 <18> (?)` | |; | [...bender/tools/copynumber/CollectFragmentCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RGcmFnbWVudENvdW50cy5qYXZh) | `88.406% <88.406%> (Ã¸)` | `11 <11> (?)` | |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `90.517% <0%> (-0.431%)` | `63% <0%> (-1%)` | |; | [...ute/hellbender/utils/read/ArtificialReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?sr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649:1557,Cache,CachedBinarySearchIntervalList,1557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649,1,['Cache'],['CachedBinarySearchIntervalList']
Performance,========================; + Coverage 86.362% 86.626% +0.264% ; + Complexity 39551 38919 -632 ; ===============================================; Files 2362 2336 -26 ; Lines 186121 182603 -3518 ; Branches 20305 20062 -243 ; ===============================================; - Hits 160738 158181 -2557 ; + Misses 18236 17379 -857 ; + Partials 7147 7043 -104 ; ```. | [Files Changed](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `Ã¸` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `Ã¸` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:1950,scalab,scalable,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (Ã¸)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (Ã¸)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (Ã¸)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (Ã¸)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3501,cache,cache,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,"> 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); > at; > org.broadinstitute.hellbender.utils.read.SAMFi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6761,concurren,concurrent,6761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,"> > Hi, Meng,; > > I also meet this issue, have you solve it?; > > Thank you; > ; > Hello,; > Actually, I didn't solve it completely.; > When I change to another server, it can run 2.14GB's data well.; > I think it's because the data is too large, and the server can't perform it normally.; > If it's not necessary, you can choose other tools.; > ; > Best wish; > Meng. Hi @MengZhang2019 Meng, Thank you; I also didn't solve this problem.; Do you have any tools recommend? I have tried other methods, but the running time was too long, they do not suit large sample data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636:269,perform,perform,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636,1,['perform'],['perform']
Performance,"> > In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?; > ; > You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity. Okay. Rip it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509:221,optimiz,optimization,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509,1,['optimiz'],['optimization']
Performance,"> > Not to mention, in theory one could have some job trying to read the original workspace, which might get hosed if some other job is trying to edit that workspace in place.; > ; > This is not possible as new GenomicsDB workspace fragments are created for the incremental updates. During the actual finalization of the fragments, the array in the workspace is locked using Posix file locks for concurrency. As @nalinigans says, individual arrays/intervals in a GenomicsDB workspace will be consistent during incremental import. One caveat though, since each array is independently updated, different arrays/intervals will finish adding samples at different times while incremental import is in progress. So, querying a workspace that is being incrementally imported into isn't recommended.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801:396,concurren,concurrency,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801,1,['concurren'],['concurrency']
Performance,> @Bowen1992 Could you please try running with the latest GATK release (`4.2.6.1`) and reporting whether the issue persists?. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022å¹´5æœˆ22æ—¥ ä¸‹åˆ05æ—¶49åˆ†50ç§’; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097:574,optimiz,optimizations,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"> At the very least we should add a unit test that generates the evidenceIndexBySampleIndex cache, then calls marginalize() (both types) and asserts that we have emptied the cache. I would do the same for appendEvidence() and addMissingAlleles(). It's simpler than this because allele operations such as `marginalize()` and `addMissingAlleles` don't modify the evidence list. While they require care with the likelihoods arrays they don't require anything at all from the evidence-to-index caches. As I mentioned above, I left the cache updating in `appendEvidence` as it was because it was so simple. I will try to write the test for removing evidence tomorrow. Tempting to try tonight, but I'm trying to accept the reality that working until 2 am is a bad idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869:92,cache,cache,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869,8,['cache'],"['cache', 'caches']"
Performance,"> Do you have numbers for the performance here? How big is this code in the profiler before or after? I'm curious. The unit tests are about 5% faster. In practice, this won't affect HaplotyeCaller because the overwhelming majority of the CPU cost of those tests comes from the ploidy = 20, allele count = 6 cases. For anything else the genotyping likelihoods calculation is not only not a bottleneck, it's completely negligible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"> Hi, I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception: 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes 01:13:16.078 INFO HaplotypeCaller - Initializing engine 01:13:17.087 INFO HaplotypeCaller - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955,1,['Load'],['Loading']
Performance,"> Hi, Meng,; > I also meet this issue, have you solve it?; > Thank you. Hello,; Actually, I didn't solve it completely.; When I change to another server, it can run 2.14GB's data well.; I think it's because the data is too large, and the server can't perform it normally. ; If it's not necessary, you can choose other tools. Best wish; Meng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600:251,perform,perform,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600,1,['perform'],['perform']
Performance,> I also encounter this error when most samples have been imported. I ran importing in batches '--batch-size 50 --consolidate '. The error occured at the last batch. Can I reuse some of the imported data files or have to rerun the whole importing again?. ...; 13:13:26.069 INFO GenomicsDBImport - Done importing batch 21/22; 13:13:26.069 INFO GenomicsDBImport - Starting batch input file preload; 13:13:27.440 INFO GenomicsDBImport - Finished batch preload; 13:13:27.440 INFO GenomicsDBImport - Importing batch 22 with 22 samples; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while consolidating TileDB array chrY$1$57227415; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886:640,load,load,640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886,2,['load'],['load']
Performance,"> I think we've seen similar issues before. Libgomp needs to be installed and findable. I think it typically is installed when you install gcc. See #6012 for more discussion. Thank you, I checked my system and found gcc module not loaded. I reloaded my gcc by typing; `module load gcc/5.4.0`; and it works now. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969:231,load,loaded,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969,2,['load'],"['load', 'loaded']"
Performance,"> If one thinks about a genomicsDB workspace more like a database than single file, are there defrag/shrink-like tasks that need to be performed on the workspace for efficiency?. Did you use the `--consolidate` option with GenomicsDBImport? This option consolidates fragments to help with query performance later. Usually not needed for very small batch sizes.; Also, available from 4.1.7.0 is a `--genomicsdb-shared-posixfs-optimizations` option. Can you try GenotypeGVCFs with this knob turned on if your workspace is on NFS/Lustre and let us know?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356:135,perform,performed,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356,3,"['optimiz', 'perform']","['optimizations', 'performance', 'performed']"
Performance,"> It would appear this does what it says on the tin, though I'm curious what the scenario was where the absence of the timestamp caused a call caching issue. I reuse a dataset for development (rsa_gvs_quickstart_dev). One past run was did not use compressed references, so that is always used when call caching is turned on, even though the dataset has reingested compressed references since then. This is the exact scenario that `GetBQTableLastModifiedDatetime` was created for â€” database-based tasks that we want to be able to call cache accurately.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421:534,cache,cache,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421,1,['cache'],['cache']
Performance,"> Just curious, why no last modified checks? Was it to keep the code simpler?. Mostly because I couldn't readily think of a scenario where I would actually want this to call cache, but I could easily imagine call caching leading to undesired clobbering of previously generated results. We can certainly revisit this decision if it turns out we're using the script in ways where we really would want call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990:174,cache,cache,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990,2,['cache'],['cache']
Performance,"> One final thing: i'm happy to try to debug this, and was going to write a test case based on the existing GenomicsDB integration tests. However, when I try to run any integration test involving genomicsdb, I get an exception like the following. I am on windows, so perhaps this is the issue?; > ; > 09:03:37.460 FATAL GenomicsDBLibLoader -; > java.io.FileNotFoundException: File /tiledbgenomicsdb.dll was not found inside JAR.; > at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:118) ~[genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:55) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtilsJni.(GenomicsDBUtilsJni.java:30) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:46) [genomicsdb-1.3.2.jar:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:1005) [classes/:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:661) [classes/:?]; > at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056) [classes/:?]. Yes, Windows is not supported by GenomicsDB. This is mentioned obliquely in the requirements for gatk too -; ```; Operating system. The GATK runs natively on most if not all flavors of UNIX, which includes MacOSX, Linux and BSD. It is possible to get it ; running on some recent versions of Windows, but we don't provide any support nor instructions for that. If you need to run on; a Windows machine, consider using Docker.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359:470,load,loadLibraryFromJar,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359,2,['load'],"['loadLibrary', 'loadLibraryFromJar']"
Performance,"> Running with default arguments locally the runtime (for a WGS full chr15) drops from ~8.9 minutes to ~4.7 minutes after this patch. If I had to peg something else to optimize it would be replacing CSVWriter which seems to be somewhat slow but I can be contented that this tool is reasonably fast when nothing pathological is being triggered. Hello, you mentioned that running DepthOfCoverage for a WGS full chr15 only takes ~4.7 minutes. Would you mind letting me know what is the coverage for the BAM you use? It took days for me to run DepthOfCoverage on a 80X WGS. And, will there be a Spark implementation for DepthOfCoverage in the near future? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687:168,optimiz,optimize,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687,1,['optimiz'],['optimize']
Performance,"> Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from HypergeometricDistribution. The problem is mostly inside `HypergeometricDistribution`. A better implementation of this class should cache the last value, such that computing `hypergeo(i)` and then `hypergeo(i+1)` consecutively does not unnecessarily trigger full computation, which is quite expensive. Anyway, the current implementation is fine given that exact test is unlikely to be a bottleneck in its current use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573:267,cache,cache,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573,2,"['bottleneck', 'cache']","['bottleneck', 'cache']"
Performance,"> What happen to the bundling performance improvement changes by the way?. The large 2D file array can be handled by the latest Cromwell versions, so we do not need to bundle. It is much more elegant and readable this way and should actually improve performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385,2,['perform'],['performance']
Performance,> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/hgnc/hg19/hgnc_download_Nov302017.tsv; 12:11:28.270 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.270 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; 12:11:28.277 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.426 INFO DataSourceUtils - Resolved data source file path: file:///gatk/hg19_All_20180423.vcf.gz -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.771 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/dbsnp/hg19/hg19_All_20180423.vcf.gz; 12:11:28.877 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; 12:11:28.882 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.883 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.config; 12:11:28.905 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; 12:11:28.906 INFO DataSourceUtils - Resolved data source file path: file:///gatk/oreganno.tsv -> file:///gatk/./my_data/funcotator_dataSources.v1.7.20200521s/oreganno/hg19/oreganno.tsv; WARNING 2021-03-24 12:11:28 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 12:11:28.910 INFO DataSourceUtils - Re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7158:11521,cache,cache,11521,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7158,1,['cache'],['cache']
Performance,"> ls -l genome.*; -rw-rw---- 1 kh3 kh3 784809415 Sep 16 10:16 genome.2bit; -rw-rw---- 1 kh3 kh3 3168829906 Feb 4 2014 genome.fa; -rw-r----- 1 kh3 kh3 106669 Sep 16 11:32 genome.fa.amb; -rw-r----- 1 kh3 kh3 3276 Sep 16 11:32 genome.fa.ann; -rw-r----- 1 kh3 kh3 3137454592 Sep 16 11:31 genome.fa.bwt; -rw-rw---- 1 kh3 kh3 2984 Feb 4 2014 genome.fa.fai; -rw-rw---- 1 kh3 kh3 2984 Sep 16 13:18 genome.fai; -rw-r----- 1 kh3 kh3 784363628 Sep 16 11:32 genome.fa.pac; -rw-r----- 1 kh3 kh3 1568727304 Sep 16 11:44 genome.fa.sa. Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaAndMarkDuplicatesPipelineSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/ge; nome.2bit --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 15:47:28.760 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 15:47:28.809 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 3:47:28 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark --threads 16 --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark; .aligned.bam --reference /home/kh3/Resources/genome_b37/genome.2bit --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSiz; e 100000 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedO; utput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 3:47:28 PM EDT] Executing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171:1154,load,load,1154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171,1,['load'],['load']
Performance,>10% performance improvement seems worthwhile,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7869:5,perform,performance,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7869,1,['perform'],['performance']
Performance,">In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?. You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493:212,optimiz,optimization,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493,1,['optimiz'],['optimization']
Performance,"@Aqoolare Hello. There are a a few things going on here. The `unrecognized runtime attribute keys` warning is coming from cromwell. It's telling you that the cromwell **local** backend doesn't understand those keys, which is true. That means it's just ignoring them. I think the actual problem is different though. You're running the spark tool in spark local mode, which in this case isn't configured to use the correct amount of cores or memory. I think the intent of this wdl script was that it would be run in a container on a cluster and the container would restrict the cores and memory options. In any case, it's not configured correctly for what you need. I would skip running cromwell and just invoke gatk directly since this wdl only executes a single job. Since this is going to run spark in local mode you need to specify the number of cores using the `--spark-master` argument, and set the memory using the `--java-options ""-Xmx""` arguments. For example:. ```; gatk ReadsPipelineSpark ; --java-options ""-Xmx16G"" ; --spark-master 'local[8]'; --I yourbam.; ... etc; ```. The above command is specifying to use 8 (that's what the local[**8**] means) cores for spark and give it 16G of memory. Your job was accidentally using 200 cores so it doesn't surprise me that it would run into memory issues. Using spark with more than 16ish cores in a single process is going to bog down a lot. I think 8 is a good starting place to try. If you want to go wider you should really look into running a proper cluster (or using dataproc), but there's pretty heavy diminishing returns. Try 8 or 16 and tune the memory from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771:1599,tune,tune,1599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771,1,['tune'],['tune']
Performance,"@Aqoolare You might, but there are scaling issues when running within a single java process which is what you're doing. There are issues with lock contention and garbage collection which cause more cores to not be utilized very well. (Lots of cores waiting while garbage collection stops the world, that sort of thing.). . You could definitely test it. We found that 8-16 cores was optimal for our use cases for running in spark local mode, but spark performance is extremely finicky and it's very possible your system might do better than what I'm used to. If you wantt to go very parallel it works better to run an actual spark cluster. You should be able to utilize cores more efficiently that way, but it's more complicated to set up and operatte and there are still bottlenecks that keep it from being infinitely scaleable. (IO bandwidth and network traffic being important ones). . There are lots of articles on the internet about how to set up a local yarn cluster that can help walk you through it if you want to try. . I'm going to close this ticket since it seems like the problem is solved. Feel free to reopen or open a new one if you have other issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992:451,perform,performance,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"@Bowen1992 **I got the same error, do you have a solution now?**. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx76800m -jar /home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R data/ref/CL200105941_L02.fa -V gendb://results/genotype/genodb/group2 -O results/genotype/vcfs/group2.vcf.gz --tmp-dir ./tmp; 18:24:10.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:24:10.451 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.452 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 18:24:10.452 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:24:10.452 INFO GenotypeGVCFs - Executing as zwc1988@fat01 on Linux v3.10.0-957.el7.x86_64 amd64; 18:24:10.453 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 18:24:10.453 INFO GenotypeGVCFs - Start Date/Time: June 19, 2022 6:24:10 PM CST; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 18:24:10.454 INFO GenotypeGVCFs - Picard Version: 2.27.1; 18:24:10.454 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:24:10.455 INFO Genotype",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894:523,Load,Loading,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894,1,['Load'],['Loading']
Performance,"@Cashalow Can you include the full command line? 50 human genomes (ploidy 2) would need less than 7GB memory in my experience, even for highly multi-allelic indel sites. I would expect ploidy 1 calls (as most users run microbial genomes) would require even less, but we have some diploid-specific optimizations. Are you using the `new-qual` argument? That QUAL calculation algorithm is less computationally intensive and I believe it is applicable to all ploidies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977:297,optimiz,optimizations,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977,1,['optimiz'],['optimizations']
Performance,"@DarioS I wonder if you're looking at the JVM garbage collector threads -- by default, Java uses a multi-threaded garbage collector. You can control the number of threads it uses via the `-XX:ParallelGCThreads=N` argument, where N is the number of garbage collector threads. To pass this option into GATK, use the `--java-options` argument. Eg., `./gatk --java-options '-XX:ParallelGCThreads=4'`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075:99,multi-thread,multi-threaded,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075,1,['multi-thread'],['multi-threaded']
Performance,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"@EdwardDixon I did not know that! In that case master does already require AVX. If it only impacts this tool and we provide sufficient warning and instructions, I think the single intel-optimized conda environment will be so much easier to test and maintain. Users who don't have AVX can simply install an older tensorflow in their environment, but GATK doesn't need to worry about it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837:186,optimiz,optimized,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837,2,['optimiz'],['optimized']
Performance,"@EdwardDixon Thanks for this! I think the AVX check in CNNScoreVariants is good. As @droazen points out, we still want the split environments, though now with the check in place I think we can make intel optimized the default. Other thoughts @droazen, @cmnbroad ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123:204,optimiz,optimized,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123,1,['optimiz'],['optimized']
Performance,"@EdwardDixon Well, you'd be surprised at some of the hardware we have to deal with. Even some machines here at the Broad don't have AVX. In general, our policy with hardware-dependent optimizations in GATK has been to insist on having a transparent fallback mechanism when the required hardware isn't present -- I'd really prefer not to start making exceptions to that rule. Could the Intel-optimized Tensorflow be patched to fall back to vanilla tensorflow when AVX is not present? Is that an option? Or could it at least be patched to not actually crash in that case?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151:184,optimiz,optimizations,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151,2,['optimiz'],"['optimizations', 'optimized']"
