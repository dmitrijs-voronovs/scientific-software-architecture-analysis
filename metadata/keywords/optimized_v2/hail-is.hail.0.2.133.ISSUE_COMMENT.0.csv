quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); E 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:454); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:490); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:342); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:487); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:486); E 	at jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source); E 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); E 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); E 	at java.base/java.lang.Thread.run(Thread.java:834); E ; E ; E ; E Hail version: 0.2.109-c163bcb21073; E Error summary: AssertionError: assertion failed. hail/backend/py4j_backend.py:35: FatalError. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:6547,Error,Error,6547,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['Error'],['Error']
Availability," 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.propertie",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3222,echo,echo,3222,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability," --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so); java.lang.UnsatisfiedLinkError: /data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail691432050703",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:2791,ERROR,ERROR,2791,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,1,['ERROR'],['ERROR']
Availability," 0.2.128-ce3ca9c77507; Error summary: SocketTimeoutException: connect timed out; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 223, in _rpc; raise fatal_error_from_java_error_triplet(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/test.py"", line 6, in <module>; ht.write('gs://ehigham-hail-tmp/test_hail_in_notebook.ht'); hail.utils.java.FatalError: SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:11649,Error,Error,11649,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['Error'],['Error']
Availability," 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2891,Down,Downloading,2891,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability," 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1630,failure,failure,1630,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['failure'],['failure']
Availability," 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13872,AVAIL,AVAILABLE,13872,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability, 3 * 10^16. ~~I can't find the referenced case analysis in Google's latest code. [It is present in this fork](https://github.com/leogamas/java-storage/blob/2af8dfd95cdebc9e4d8252b0bbe3f092844d9f2c/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java#L68-L198) from a few years ago.~~. Here's the [referenced case analysis in 2.17.1](https://github.com/googleapis/java-storage/blame/v2.17.1/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java). There seems to have been a rewrite [two months ago](https://github.com/googleapis/java-storage/blame/main/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java) (here's [the main commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:1311,recover,recover,1311,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Availability," = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\; 1301 proto.END_COMMAND_PART; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1307 for temp_arg in temp_args:; 1308 temp_arg._detach(). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:21, in handle_java_exception.<locals>.deco(*args, **kwargs); 19 import pyspark; 20 try:; ---> 21 return f(*args, **kwargs); 22 except py4j.protocol.Py4JJavaError as e:; 23 s = e.java_exception.toString(). File ~/mambaforge/lib/python3.9/site-packages/py4j/protocol.py:330, in get_return_value(answer, gateway_client, target_id, name); 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value); 329 else:; --> 330 raise Py4JError(; 331 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; 332 format(target_id, ""."", name, value)); 333 else:; 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; 336 format(target_id, ""."", name)). Py4JError: An error occurred while calling o83._1. Trace:; java.lang.NegativeArraySizeException: -1966455376; 	at py4j.Base64.encodeToChar(Base64.java:681); 	at py4j.Base64.encodeToString(Base64.java:734); 	at py4j.Protocol.encodeBytes(Protocol.java:154); 	at py4j.ReturnObject.getPrimitiveReturnObject(ReturnObject.java:150); 	at py4j.Gateway.getReturnObject(Gateway.java:188); 	at py4j.Gateway.invoke(Gateway.java:283); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.base/java.lang.Thread.run(Thread.java:829); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:3191,error,error,3191,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,3,['error'],['error']
Availability," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:2069,error,error,2069,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,2,['error'],['error']
Availability," The error I get when I run the code you provide is; ```; ""Key type mismatch: cannot index table with given expressions:; Table key: <<<empty key>>>; Index Expressions: locus<GRCh38>, array<str>, set<str>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, struct{z_score: float32}, struct{region_type_ids: array<int32>}, locus<GRCh37>, str, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, str, int64, struct{PHRED: float32}, struct{alleleId: int32, conflictingPathogenicities: array<struct{pathogenicity_id: int32, count: int32}>, goldStars: int32, pathogenicity_id: int32, assertion_ids: array<int32>}, struct{REVEL_score: float32, VEST4_score: float32, MutPred_score: float32, SIFT_pred_id: int32, Polyphen2_HVAR_pred_id: int32, MutationTaster_pred_id: int32, fathmm_MKL_coding_pred_id: int32}, struct{Eigen_phred: float32}, struct{AF_POPMAX: float3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465:687,error,error,687,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465,1,['error'],['error']
Availability," Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:2425,Error,ErrorHandling,2425,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,2,['Error'],"['Error', 'ErrorHandling']"
Availability," \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing meta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1704,Down,Downloading,1704,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:1515,down,download,1515,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,['down'],['download']
Availability," for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. crea",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1894,error,error,1894,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['error'],['error']
Availability," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10674,Error,Error,10674,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,4,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability," mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 13:51:03 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 13:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; ```. And the other initialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 15:16:23 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:7161,error,error,7161,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['error'],['error']
Availability," multi, and `describe()`, `count()`, and `_force_count_rows()` all behave as expected. ```; import hail as hl. giab_gs_path = 'gs://xxxxxxxx'; giab_ds = hl.import_vcf(giab_gs_path, reference_genome='GRCh38'); giab_sm = hl.SplitMulti(giab_ds); giab_ds = giab_sm.result(); giab_ds.describe(); gaib_ds.count(); gaib_ds._force_count_rows(); # all good. sent_gs_path = 'gs://xxxxxxxxxxx'; sent_ds = hl.import_vcf(sent_gs_path, reference_genome='GRCh38'); sent_sm = hl.SplitMulti(sent_ds); sent_ds = sent_sm.result(); sent_ds.describe(); sent_ds.count(); sent_ds._force_count_rows(); # all good. # then this gives the stack trace below. giab_22_ds = hl.filter_intervals(giab_ds, [hl.parse_locus_interval('chr22', reference_genome='GRCh38')]); ```. Stack trace:. ```; FatalError: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 20 times, most recent failure: Lost task 0.19 in stage 19.0 (TID 312, gilson-validation-test-2-w-4.c.perfect-atrium-179917.internal, executor 2): java.lang.ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1641); 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1637); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun$apply$20.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:1145,failure,failure,1145,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['failure'],['failure']
Availability," of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:1116,recover,recover,1116,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Availability," recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Fr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1448,echo,echo,1448,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,2,['echo'],['echo']
Availability," secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need to maintain their own state look a lot like HTML wrapped in a function:. ```js; export default function() { <div>Hello World </div> }; ```. or in JS ES6 form:; ```js; export default () => <div>Hello World</div>; ``` . Performance is excellent. SSR should run about as fast as jinja2 (will be getting faster in 2019). Client side interactions are obviously far more performant. Bundle sizes are on a downward trajectory; react + react-dom is about as big as jQuery today, and reducing that is a focus on facebook in 2019. There are alternatives to react-dom that are under 10kb, but in practice 20kb is nothing to worry about, especially when initial load doesn't require it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:3793,down,downward,3793,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,['down'],['downward']
Availability," simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipped) by replacing the dark mode icon svg with a reference to the material-design-icons font.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:1517,down,down,1517,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,2,['down'],['down']
Availability," that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests are failing I would love to hear it. Thanks again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:2319,echo,echo,2319,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721,3,['echo'],['echo']
Availability," write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:1903,Error,Error,1903,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,2,['Error'],['Error']
Availability," write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(Let(__iruid_369,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:2183,Error,Error,2183,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,2,['Error'],['Error']
Availability,"![Screen Shot 2022-06-10 at 10 08 59 AM](https://user-images.githubusercontent.com/106194/173083939-aea57012-ddcc-4240-9ad0-55163eb6df04.png). Average utilization is marginally improved (maybe 2.5% -> 5%), but total number of wasted cores goes way down because the total number of cores is 16 rather than 16 * 8 = 128. This also suggests the autoscaler/scheduler could be made substantially smarter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11902#issuecomment-1152405036:248,down,down,248,https://hail.is,https://github.com/hail-is/hail/pull/11902#issuecomment-1152405036,1,['down'],['down']
Availability,"""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 157, in <module>; main(args); File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 98, in main; pca_mt = hl.ld_prune(pca_mt, r2=0.1, n_cores=args.num_cores); File ""<decorator-gen-788>"", line 2, in ld_prune; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/typecheck/check.py"", line 490, in _typecheck; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/methods/statgen.py"", line 2918, in ld_prune; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 530 in stage 9.0 failed 20 times, most recent failure: Lost task 530.19 in stage 9.0 (TID 19101, gt1-w-78.c.broad-mpg-gnomad.internal, executor 199): java.lang.ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:643); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9$$anonfun$apply$10.apply(RVD.scala:222); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9$$anonfun$apply$10.apply(RVD.scala:221); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collectio",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:1084,failure,failure,1084,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['failure'],['failure']
Availability,"""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 18:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 18:18:30 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:1608,avail,available,1608,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['avail'],['available']
Availability,"""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 19:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 19:16:02 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:1446,avail,available,1446,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['avail'],['available']
Availability,"""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 08:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 08:41:32 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: ; ```; -----------------------------; Step2 : read the file with sc.textFile; ```; In [1]: rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); ```; -----------------------------; Step3, import hail and read the file with hail:; ```; In [2]: from hail import *; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-3181c5d8fca5> in <module>(); ----> 1 from hail import *. /opt/Software/hail/python/hail/__init__.py in <module>(); ----> 1 import hail.expr; 2 from hail.representation import *; 3 from hail.context import HailContext; 4 from hail.dataset import VariantDataset; 5 from hail.expr import *. /opt/Software/hail/python/hail/expr.py in <module>(); 1 import abc; 2 from hail.java import scala_object, Env, jset; ----> 3 from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; 4 ; 5 . /opt/Software/hail/python/hail/represent",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:2032,avail,available,2032,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,1,['avail'],['available']
Availability,"""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 09:10:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 09:10:21 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 09:10:21 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 09:10:21 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:1557,avail,available,1557,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,1,['avail'],['available']
Availability,"""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/15 08:58:31 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/15 08:58:31 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/15 08:58:31 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile(""/hail/test/BRCA1.raw_indel.vcf"").count(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 1008, in count; return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 999, in sum; return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 873, in fold; vals = self.mapPartitions(func).collect(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 776, in collect; port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/sql/utils",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367:1364,avail,available,1364,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367,1,['avail'],['available']
Availability,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6283,error,error,6283,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,4,"['error', 'failure']","['error', 'failure']"
Availability,"""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2143,avail,available,2143,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['avail'],['available']
Availability,"""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""4eaaa3a5-69ce-4eb0-ba8c-266bd66c821e"",; ""zones"": null; },; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_E4d_v4"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:5577,avail,availabilitySet,5577,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['avail'],['availabilitySet']
Availability,"""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""2612958c-ef4e-4678-8482-29726290ae20"",; ""zones"": null; },; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_D16d_v4"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:9920,avail,availabilitySet,9920,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['avail'],['availabilitySet']
Availability,"""the implementations should rely directly on java.util.Random"" Umm, why? From my outsiders perspective I would have assumed that high quality software worked on by the Broad Institute would use a half decent Random Number Generator (RNG). . If I had time to spend on this I would be pushing to change it to something else, perhaps the Apache Commons RNG: commons.apache.org/proper/commons-rng/userguide/rng.html I'm not a Java programmer though, and don't really aspire to be. . The C++ standard rand() function is also well known to be quite bad, though C++11 distributions use an ok implementation (I think default is Mersenne Twister, but there are also other options http://en.cppreference.com/w/cpp/numeric/random). In some C code I was replacing rand() and found this nice library: http://www.pcg-random.org/ no Java implementation though http://www.pcg-random.org/download.html#java-implementation T.T, but the PCG algorithm is actually really simple to implement. The PCG site is worth exploring in general to understand the important differences between RNGs. The GNU Scientific Library also provides C/C++ coders with some RNG implementations https://www.gnu.org/software/gsl/manual/html_node/Random-Number-Generation.html . I realize that it is quite early in development (in terms of versioning, 0.2) so maybe this seems like an insignificant thing, but I also hear that it is actively being used, so.... it also may not matter much, I just would like to bring some attention to it in case it hasn't been considered because I know that the RNG is something that is frequently overlooked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2314#issuecomment-384085976:871,down,download,871,https://hail.is,https://github.com/hail-is/hail/issues/2314#issuecomment-384085976,2,['down'],['download']
Availability,"## The DSL. For IBS2, we need this mess:. ``` c; __m256i nxor = _mm256_xor_si256(_mm256_xor_si256(x, y), allones);. // if both bits are one, then the genotype is missing; __m256i xna_tmp = _mm256_xor_si256(_mm256_xor_si256(allNA, x), allones);; __m256i xna = _mm256_and_si256(_mm256_srli_epi64(xna_tmp, 1), xna_tmp);; __m256i yna_tmp = _mm256_xor_si256(_mm256_xor_si256(allNA, y), allones);; __m256i yna = _mm256_and_si256(_mm256_srli_epi64(yna_tmp, 1), yna_tmp);; // if either sample is missing a genotype, we ignore that genotype pair; __m256i na = _mm256_and_si256(_mm256_or_si256(xna, yna), rightAllele);; // 1. shift the left alleles over the right ones; // 2. and the alleles; // 3. mask to the right ones; __m256i ibs2 = _mm256_andnot_si256(na, _mm256_and_si256(_mm256_and_si256(_mm256_srli_epi64(nxor, 1), nxor), rightAllele));; // 4. popcnt; uint64_t ibs2sum = _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 0));; ibs2sum += _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 1));; ibs2sum += _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 2));; ibs2sum += _mm_popcnt_u64(_mm256_extract_epi64(ibs2, 3));; ```. It'd be nice to specify it as:. ``` racket; (define (bit-eq x y) (negate (xor x y))). (let ((nxor (bit-eq x y)); (xna (bit-eq allNA x)); (yna (bit-eq allNA x)); (na (and (or xna yna) rightAllele))); (popcnt (andnot na (and (and (>>64 nxor 1) nxor) rightAllele)))); ```. And then have a little compiler that can generate C code for 128/256/512-wide registers and 2/4/16/256 alleles.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1082#issuecomment-260067923:689,mask,mask,689,https://hail.is,https://github.com/hail-is/hail/issues/1082#issuecomment-260067923,1,['mask'],['mask']
Availability,"#### Summary. It has happened twice. The failing partition is different in each run. 1. 49340 https://batch.hail.is/batches/8069235/jobs/51280; 2. 25997 https://batch.hail.is/batches/8083195/jobs/27937. The pipeline runs two table collects to get sample information, then converts the matrix table to a table of ndarrays of the value `hl.int(hl.is_defined(mt.GT))`. The entries are getting subsetted, so there is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:673,error,error,673,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,3,['error'],['error']
Availability,"#13839 is I think the right way to implement this. It passes `test_block_matrix_entries`, which is the only test I could find that exercises this path. Others that should, like `test_to_table`, can't yet be fully lowered. Maybe try rebasing on my branch and see if you still see the indexing errors?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13807#issuecomment-1766713503:292,error,errors,292,https://hail.is,https://github.com/hail-is/hail/pull/13807#issuecomment-1766713503,1,['error'],['errors']
Availability,#5872 fixes the problem and tests more robustly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5871#issuecomment-482348287:39,robust,robustly,39,https://hail.is,https://github.com/hail-is/hail/pull/5871#issuecomment-482348287,1,['robust'],['robustly']
Availability,"#9435 may at long last, finally be a solution to this. We currently keep a static map per jvm that maps general FASTA paths to a local fasta path that we copy the FASTA into. My change serializes the downloading of these files (using a lock), and we never remove keys from the map, so we should finally have 1 FASTA file per executor. There are still circumstances that can blow up on us like restarting executors (like when yarn shuts them down) and then starts them up again. The broadcast changes fixed some of the issues, and I hope that my change can finally fix this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371#issuecomment-690852316:200,down,downloading,200,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-690852316,2,['down'],"['down', 'downloading']"
Availability,#9569 Fixes the `HailUserException` error you encountered. `LocalBackend` now correctly handles `HailUserException`s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9549#issuecomment-704921949:36,error,error,36,https://hail.is,https://github.com/hail-is/hail/pull/9549#issuecomment-704921949,1,['error'],['error']
Availability,"&& bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30140,Down,Downloading,30140,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9931,echo,echo,9931,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"'s memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory setti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:2183,avail,available,2183,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['avail'],['available']
Availability,"(346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:4183,Down,Downloading,4183,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,(can fix by defining `def __iter__` on MT/Table to throw an error),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3038#issuecomment-369615427:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/issues/3038#issuecomment-369615427,1,['error'],['error']
Availability,"(note this is coming from a downstream commit where I've written a lowerer for MatrixRead, that's what the `TableZipUnchecked` is for)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-449744063:28,down,downstream,28,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449744063,1,['down'],['downstream']
Availability,); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91004,ERROR,ERROR,91004,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:83270,ERROR,ERROR,83270,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"*Update*. If it helps, our configuration includes three VMs. This includes a master and two workers with autoscaling enabled. We have tried using n1-himem-8 and n1-himem-64 machines. Both configurations failed with similar errors. The one above is form the n1-himem-64 configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420:223,error,errors,223,https://hail.is,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420,1,['error'],['errors']
Availability,+ arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2206,echo,echo,2206,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,", **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2196,Error,Error,2196,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Error'],['Error']
Availability,", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7161,Error,Error,7161,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Error'],['Error']
Availability,", output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2504,failure,failure,2504,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['failure'],['failure']
Availability,", we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBufferSpec""}}}}; ```; Error for run 1.; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:65",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:1460,failure,failure,1460,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['failure'],['failure']
Availability,"----------------------------------------------------------; Gradle 6.8.3; ------------------------------------------------------------. Build time: 2021-02-22 16:13:28 UTC; Revision: 9e26b4a9ebb910eaa1b8da8ff8575e514bc61c78. Kotlin: 1.4.20; Groovy: 2.5.12; Ant: Apache Ant(TM) version 1.10.9 compiled on September 27 2020; JVM: 1.8.0_362 (Private Build 25.362-b09); OS: Linux 5.4.0-1042-gcp amd64. real	0m3.621s; user	0m4.448s; sys	0m0.623s; + retry make jars wheel HAIL_DEBUG_MODE=1; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.0"" which is different from old value """"; printf ""3.3.0"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=' >> src/main/resources/build-info.properties; echo 'revision=e1d86e1908f0911d45b03ef08a694d07e1c4627b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-03-09T23:23:56Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.0' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.110' >> src/main/resources/build-info.properties; HAIL_DEBUG_MODE is set to ""1"" which is different from old value """"; printf ""1"" > env/HAIL_DEBUG_MODE; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; javac -d build/classes/scala/debug -Xlint:all -Werror -XDenableSunApiLintControl -XDignore.symbol.file src/debug/scala/is/hail/annotations/Memory.java; ./gradlew shadowJar -Dscala.version=2.12.13 -Dspark.version=3.3.0 -Delasticsearch.major-version=7; Starting a Gradle Daemon (subsequent builds will be faster); > Task :compileJava NO-SOURCE; > Task :compileScala; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala:122: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/TableWriter.scala:57: value of is not",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450:1022,echo,echo,1022,https://hail.is,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450,1,['echo'],['echo']
Availability,"-------. Build time: 2021-02-22 16:13:28 UTC; Revision: 9e26b4a9ebb910eaa1b8da8ff8575e514bc61c78. Kotlin: 1.4.20; Groovy: 2.5.12; Ant: Apache Ant(TM) version 1.10.9 compiled on September 27 2020; JVM: 1.8.0_362 (Private Build 25.362-b09); OS: Linux 5.4.0-1042-gcp amd64. real	0m3.621s; user	0m4.448s; sys	0m0.623s; + retry make jars wheel HAIL_DEBUG_MODE=1; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.0"" which is different from old value """"; printf ""3.3.0"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=' >> src/main/resources/build-info.properties; echo 'revision=e1d86e1908f0911d45b03ef08a694d07e1c4627b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-03-09T23:23:56Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.0' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.110' >> src/main/resources/build-info.properties; HAIL_DEBUG_MODE is set to ""1"" which is different from old value """"; printf ""1"" > env/HAIL_DEBUG_MODE; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; javac -d build/classes/scala/debug -Xlint:all -Werror -XDenableSunApiLintControl -XDignore.symbol.file src/debug/scala/is/hail/annotations/Memory.java; ./gradlew shadowJar -Dscala.version=2.12.13 -Dspark.version=3.3.0 -Delasticsearch.major-version=7; Starting a Gradle Daemon (subsequent builds will be faster); > Task :compileJava NO-SOURCE; > Task :compileScala; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala:122: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/TableWriter.scala:57: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/TableWriter.scala:674: value of is ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450:1171,echo,echo,1171,https://hail.is,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450,1,['echo'],['echo']
Availability,"---; Gradle 6.8.3; ------------------------------------------------------------. Build time: 2021-02-22 16:13:28 UTC; Revision: 9e26b4a9ebb910eaa1b8da8ff8575e514bc61c78. Kotlin: 1.4.20; Groovy: 2.5.12; Ant: Apache Ant(TM) version 1.10.9 compiled on September 27 2020; JVM: 1.8.0_362 (Private Build 25.362-b09); OS: Linux 5.4.0-1042-gcp amd64. real	0m3.621s; user	0m4.448s; sys	0m0.623s; + retry make jars wheel HAIL_DEBUG_MODE=1; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.0"" which is different from old value """"; printf ""3.3.0"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=' >> src/main/resources/build-info.properties; echo 'revision=e1d86e1908f0911d45b03ef08a694d07e1c4627b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-03-09T23:23:56Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.0' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.110' >> src/main/resources/build-info.properties; HAIL_DEBUG_MODE is set to ""1"" which is different from old value """"; printf ""1"" > env/HAIL_DEBUG_MODE; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; javac -d build/classes/scala/debug -Xlint:all -Werror -XDenableSunApiLintControl -XDignore.symbol.file src/debug/scala/is/hail/annotations/Memory.java; ./gradlew shadowJar -Dscala.version=2.12.13 -Dspark.version=3.3.0 -Delasticsearch.major-version=7; Starting a Gradle Daemon (subsequent builds will be faster); > Task :compileJava NO-SOURCE; > Task :compileScala; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala:122: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/TableWriter.scala:57: value of is not a member of object java.nio.file.Path; [Error] /io/repo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450:1100,echo,echo,1100,https://hail.is,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450,1,['echo'],['echo']
Availability,"--; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen_count.py in <module>; 10 mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); 11 mt.describe(); ---> 12 print(""Count:"",mt.count()); 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1393,Error,Error,1393,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['Error'],['Error']
Availability,"-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13117,AVAIL,AVAILABLE,13117,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6325,AVAIL,AVAILABLE,6325,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"-01-22 13:12:01 TaskSetManager: INFO: Starting task 38.1 in stage 0.0 (TID 58, scc-q10.scc.bu.edu, executor 19, partition 38, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:01 TaskSetManager: INFO: Starting task 8.1 in stage 0.0 (TID 59, scc-q10.scc.bu.edu, executor 19, partition 8, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q10.scc.bu.edu:42152 with 21.2 GB RAM, BlockManagerId(19, scc-q10.scc.bu.edu, 42152, None); 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 13.; 2019-01-22 13:12:02 DAGScheduler: INFO: Executor lost: 13 (epoch 13); 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(13, scc-q16.scc.bu.edu, 35524, None); 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removed 13 successfully in removeExecutor; 2019-01-22 13:12:02 DAGScheduler: INFO: Shuffle files lost for executor: 13 (epoch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:158806,ERROR,ERROR,158806,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:1756,error,error,1756,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['error'],['error']
Availability,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,A",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12864,AVAIL,AVAILABLE,12864,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6072,AVAIL,AVAILABLE,6072,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13744,AVAIL,AVAILABLE,13744,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHand",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5684,AVAIL,AVAILABLE,5684,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.620 GoogleStorageFS$: INFO: closed: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.621 : INFO: TaskReport: stage=0, partition=7028, attempt=0, peakBytes=62266032, peakBytesReadable=59.38 MiB, chunks requested=72126, cache hits=72121; 2023-09-27 16:44:22.622 : INFO: RegionPool: FREE: 59.4M allocated (25.2M blocks / 34.2M chunks), regions.size = 11, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.623 WorkerTimer$: INFO: executeFunction took 71843.446957 ms.; 2023-09-27 16:44:22.623 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:6333,ERROR,ERROR,6333,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['ERROR'],['ERROR']
Availability,"-beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:1008,error,error,1008,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,2,['error'],['error']
Availability,-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-dock,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4456,echo,echo,4456,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:1778,error,error,1778,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['error'],['error']
Availability,"........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:7806,ERROR,ERROR,7806,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['ERROR'],['ERROR']
Availability,".0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.has",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1876,failure,failure,1876,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['failure'],['failure']
Availability,".3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4795,AVAIL,AVAILABLE,4795,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,".6/python/pyspark/rdd.py"", line 1008, in count; return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 999, in sum; return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 873, in fold; vals = self.mapPartitions(func).collect(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 776, in collect; port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd()); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/sql/utils.py"", line 63, in deco; return f(*a, **kw); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285); 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228); 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313); 	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367:2560,error,error,2560,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367,1,['error'],['error']
Availability,".; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated code generated for an SQL query, so very much in; > the same ballpark as parts of Hail),; > which contributes to my concern about this, especially on linux where g++; > is the default.; >; > So in the long run I expect we'll ship a compiler, or specify a compiler.; > But that becomes a problem in itself; > if we want the sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:1380,error,error,1380,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,1,['error'],['error']
Availability,.hail.io.LEB128InputBuffer.skipInt(InputBuffers.scala:260) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at __C796collect_distributed_array_table_native_writer.__m872SKIP_o_int32(Unknown Source) ~[?:?]; 	at __C796collect_distributed_array_table_native_writer.__m869INPLACE_DECODE_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32END_TO_o_struct_of_o_callANDo_int32ANDo_stringANDo_array_of_o_int32END(Unknown Source) ~[?:?]; 	at __C796collect_distributed_array_table_native_writer.__m868INPLACE_DECODE_r_array_of_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32END_TO_r_array_of_o_struct_of_o_callANDo_int32ANDo_stringANDo_array_of_o_int32END(Unknown Source) ~[?:?]; 	at __C796collect_distributed_array_table_native_writer.__m867DECODE_r_struct_of_r_array_of_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32ENDEND_TO_SBaseStructPointer(Unknown Source) ~[?:?]; ```. Error for run 2; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:655) ~[gs:__hail-query-ger0g_jars_ee77707f4fab22b1c253321b082a70aff3f44d1c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readBytes(InputBuffers.scala:444) ~[gs:__hail-query-ger0g_jars_ee77707f4fab22b1c253321b082a70aff3f44d1c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.LEB128InputBuffer.readBytes(InputBuffers.scala:253) ~[gs:__hail-query-ger0g_jars_ee77707f4fab22b1c253321b082a70aff3f44d1c.jar.jar:0.0.1-SNAPSHOT]; 	at __C816collect_distributed_array_table_native_writer.__m893INPLACE_DECODE_o_binary_TO_o_string(Unknown Source) ~[?:?]; 	at __C816collect_distributed_array_table_native_writer.__m889INPLACE_DECODE_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32END_TO_o_struct_of_o_callANDo_int32A,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:4017,Error,Error,4017,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['Error'],['Error']
Availability,".map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1711,failure,failures,1711,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,1,['failure'],['failures']
Availability,".nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2799,Failure,Failure,2799,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,2,"['Failure', 'failure']","['Failure', 'failure']"
Availability,".sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:4032,echo,echo,4032,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,".spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs://hail-common/builds/devel/jars/hail-devel-38dbf156b630-Spark-2.2.0.jar', '--py-files=gs://hail-common/builds/devel/python/hail-devel-38dbf156b630.zip,/var/folders/rn/t2xcx1ps4h96txll46qkkfsj2q8bnl/T/pyscripts_fYVAte.zip', '--properties=spark.executor.extraClassPath=./hail-devel-38dbf156b630-Spark-2.2.0.jar,spark.driver.extraClassPath=./hail-devel-38dbf156b630-Spark-2.2.0.jar,spark.files=./hail-devel-38dbf156b630-S",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7675,ERROR,ERROR,7675,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['ERROR'],['ERROR']
Availability,"/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1687,failure,failure,1687,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['failure'],['failure']
Availability,/us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1742,echo,echo,1742,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pycparser-2.21 pygments-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post; 1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 s3transfer-0.6.2 scipy-1.11.2 six-1.16.0 sortedcontainers-2.4.0 tabulate-0.9.0 tenacity-8.2.3 tornado-6.3.3 typer-0.9.0 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16 uvloop-0.17.0 wrapt-1.15.0 xyzservices; -2023.7.0 yarl-1.9.2. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; python3 -m pip uninstall -y hail; WARNING: Skipping hail as it is not installed.; python3 -m pip install build/deploy/dist/hail-0.2.124-py3-none-any.whl --no-deps; Defaulting to user installation because normal site-packages is not writeable; Processing ./build/deploy/dist/hail-0.2.124-py3-none-any.whl; Installing collected packages: hail; Successfully installed hail-0.2.124. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; hailctl config set query/backend spark; </p>; </details>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:44428,avail,available,44428,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['avail'],['available']
Availability,"0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30772,avail,available,30772,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['avail'],['available']
Availability,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:1491,down,down,1491,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,8,"['down', 'error']","['down', 'error']"
Availability,"00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:5796,Down,Downloading,5796,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1310,ERROR,ERROR,1310,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['ERROR'],['ERROR']
Availability,"1-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,A",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4420,AVAIL,AVAILABLE,4420,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Cr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14004,AVAIL,AVAILABLE,14004,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"1-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/th",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5432,AVAIL,AVAILABLE,5432,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:2528,error,errors,2528,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,6,['error'],"['error', 'errors']"
Availability,"1. Functionally, it's already possible -- should be able to do `vds.annotate_variants_db('va.dann')` and get the `va.dann.score` annotation in your VDS. Taking a step further, you can even do `vds.annotate_variants_db('va')` and get all of the annotations. It's just a matter of designing the query builder to encourage people using the function in whatever way we think is optimal, I suppose. What are the downsides to carrying around a lot of annotations in a VDS? I worry that if we supplied a select-all `va` option, everybody would just use that -- but if there aren't any major drawbacks, maybe that's the way to go and we don't even really need a query builder. Or maybe just allowing top-level selections like `va.dann`, `va.chromHMM`, etc. would be a good intermediate solution. 2. Yes! I'd definitely be interested in working on a Scala implementation, if one of you would be willing to work with a newbie :). Though I think if we can get this Python version working & usable first, that may be best. 3. I'll look into this. I don't have a solution yet. That's why in the method documentation, I used ; `.. code-block:: python` statements to add example code snippets. With the `>>> ...` syntax, the build was trying to run those examples and throwing weird errors like you anticipated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1914#issuecomment-308780378:407,down,downsides,407,https://hail.is,https://github.com/hail-is/hail/pull/1914#issuecomment-308780378,4,"['down', 'error']","['downsides', 'errors']"
Availability,"1. I really don't understand what this code is trying to do. Can you give me a short explanation in English?. 2. I think you just need single-use Let forwarding to optimize this. It looks like:. Push down the TableCount. (TableCount (Paralellelize ...)) should turn into (ArrayLen (GetField rows ...)). Push the ArrayLen/GetField into the Let. (Is this what your ""MaximizeLets"" is doing? I think that would traditionally be called let lifting.). Now you have (Let __cols_and_globals (ArrayLen (GetField __cols (Ref __cols_and_globals))). Then you forward the single-use Let, and the rest of the code simplifies into (ArrayLen (TableCollect (TableRead ""cols""))), and that should have the static number of rows and be able to be simplified. 3. So how should single-use Let forwarding work? You need two things: to determine there is only one use, and that the single use isn't in a more expensive context, e.g. you don't want to forward a single-use Let into a loop: (Let expensive X (ArrayMap a x <use expensive once>)). Since our control flow is structured, there is a static notion of ""loop nesting depth"", e.g. in the above code, the Let has nesting depth 0, and the use in the ArrayMap body has nesting depth 1. You can only forward into the same nesting depth. 4. Final question is, do you want the let forwarding pass to also delete unused Lets? If you delete a let, that might delete references that make other lets single (or zero) use. In this case, it might be nice to build a ""use-def chain"" data structure, that keeps, for each let, its list of uses (and vice versa). Then, when you delete a Let, you can dynamically delete the references for the right hand side, possibly creating additional optimization opportunities.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737:200,down,down,200,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-452863737,2,['down'],['down']
Availability,"1. I'm not sure why we don't throw an error. My bash isn't good enough to run both commands and then detect if either failed if the exit code is indeed not equal to 0. My only thought is that maybe the exit code isn't 0 if there are no VMs or disks to delete. 2. Yes, I'll see if I can PR the fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737459046:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737459046,1,['error'],['error']
Availability,"1. It's feasible to build and ship the compiler + libraries for a limited number of known platforms; (at Physics Speed I did this for Ubuntu-16.04 and one particular version of CentOS). It gets nuts if; you have many different OS'es each of which needs its own compiler build (and it then becomes; another build-system/packaging issue to get all those compilers built correctly for each OS).; Possibly a good thing to do in the long run. Probably not something I could do in the limited time; available. 2. If you build your own compiler + library, then you risk becoming incompatible with other ; libraries on the target system which were built against that system's ""standard"" compiler; and library and header files. e.g. BLAS. [Though this only applies to libraries compiled from C++,; not libraries in C, which might conatin the damage]. So it seemed like the least disruptive path in the short term was to excise the few uses of; std::string and std::stringstream, so that we can build a libhail.so which should work across; a wide variety of Linux systems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733:493,avail,available,493,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733,1,['avail'],['available']
Availability,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5532,error,errors,5532,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,6,['error'],['errors']
Availability,11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1620,echo,echo,1620,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2774,echo,echo,2774,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213882,failure,failure,213882,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['failure'],['failure']
Availability,"13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:5084,Down,Downloading,5084,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12733,AVAIL,AVAILABLE,12733,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5941,AVAIL,AVAILABLE,5941,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterato",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2562,failure,failure,2562,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['failure'],['failure']
Availability,"19-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5558,AVAIL,AVAILABLE,5558,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"19-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5311,AVAIL,AVAILABLE,5311,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"1:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13252,AVAIL,AVAILABLE,13252,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"1:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6460,AVAIL,AVAILABLE,6460,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,1=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:7096,echo,echo,7096,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@8587",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5815,AVAIL,AVAILABLE,5815,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:12988,AVAIL,AVAILABLE,12988,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"2 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6196,AVAIL,AVAILABLE,6196,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"2 6d 65 64 69 61 74 65 73 2f 2d 50 74 33 |termediates/-Pt3|; 00000050 67 4e 74 51 57 35 57 6f 42 64 43 54 44 50 51 69 |gNtQW5WoBdCTDPQi|; 00000060 77 48 64 61 39 63 32 36 35 66 32 2d 66 62 64 38 |wHda9c265f2-fbd8|; 00000070 2d 34 66 31 62 2d 62 63 64 65 2d 66 62 66 32 39 |-4f1b-bcde-fbf29|; 00000080 31 38 30 63 33 34 37 00 00 00 00 |180c347....|; 0000008b; ```. code:; ```ipython3; In [1]: import hail as hl; ...: import gnomad.utils.sparse_mt; ...: ; ...: ; ...: tmp_dir = 'gs://danking/tmp/'; ...: vds_file = 'gs://neale-bge/bge-wave-1.vds'; ...: out = 'gs://danking/foo.vcf.bgz'; ...: ; ...: vds = hl.vds.read_vds(vds_file); ...: mt = hl.vds.to_dense_mt(vds); ...: t = gnomad.utils.sparse_mt.default_compute_info(mt); ...: t = t.annotate(info=t.info.drop('AS_SB_TABLE')); ...: t = t.annotate(info = t.info.drop(; ...: 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; ...: )); ...: t = t.drop('AS_lowqual'); ...: ; ...: hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```; worker failure:; ```; 2023-09-27 16:43:10.389 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-27 16:43:10.389 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar; 2023-09-27 16:43:10.389 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 2: /batch/83e7aee9e9244f6884b8a84ea81b4c7a; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 3: /batch/83e7aee9e9244f6884b8a84ea81b4c7a/log; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 4: gs://hail-test-ezlis/dking/jars/ch4g3zvqceyo/09526a168d57dac1a26f8caa4ab49593931ed2ef.jar; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 5: worker; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 7: 7028; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 8: 9060; 2023-09-27 16:43:10.390 JVMEntryway: INFO: Yielding",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:1936,failure,failure,1936,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['failure'],['failure']
Availability,"2019-01-22 13:11:58 DAGScheduler: INFO: Executor lost: 18 (epoch 10); 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(18, scc-q02.scc.bu.edu, 37258, None); 2019-01-22 13:11:58 BlockManagerMaster: INFO: Removed 18 successfully in removeExecutor; 2019-01-22 13:11:58 DAGScheduler: INFO: Shuffle files lost for executor: 18 (epoch 10); 2019-01-22 13:11:58 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 11.; 2019-01-22 13:11:58 DAGScheduler: INFO: Executor lost: 11 (epoch 11); 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:58 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(11, scc-q17.scc.bu.edu, 37841, None); 2019-01-22 13:11:58 BlockManagerMaster: INFO: Removed 11 successfully in removeExecutor; 2019-01-22 13:11:58 DAGScheduler: INFO: Shuffle files lost for executor: 11 (epoch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:136897,ERROR,ERROR,136897,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"2019-01-22 13:12:04 DAGScheduler: INFO: Executor lost: 21 (epoch 14); 2019-01-22 13:12:04 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:04 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:04 BlockManagerMaster: INFO: Removed 21 successfully in removeExecutor; 2019-01-22 13:12:04 DAGScheduler: INFO: Shuffle files lost for executor: 21 (epoch 14); 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 12.; 2019-01-22 13:12:05 DAGScheduler: INFO: Executor lost: 12 (epoch 15); 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(12, scc-q03.scc.bu.edu, 36955, None); 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removed 12 successfully in removeExecutor; 2019-01-22 13:12:05 DAGScheduler: INFO: Shuffle files lost for executor: 12 (epoch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:180854,ERROR,ERROR,180854,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"2079a85bce (41 minutes):. I could try to make the tests even more fine-grained and split up even more long-running tests. Seems like some of the bottlenecks I'm hitting now are:; 1. Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency.; 2. The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down). I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. <img width=""2032"" alt=""Screen Shot 2023-05-22 at 12 30 47"" src=""https://github.com/hail-is/hail/assets/106194/aaa3fbb7-176d-4487-b65e-586c235e2089"">; <img width=""541"" alt=""Screen Shot 2023-05-22 at 12 31 23"" src=""https://github.com/hail-is/hail/assets/106194/016f1089-d08d-4555-ae86-c01353f39c78"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015:401,down,down,401,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015,1,['down'],['down']
Availability,"22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4548,AVAIL,AVAILABLE,4548,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13626,AVAIL,AVAILABLE,13626,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"23 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:4864,Down,Downloading,4864,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"23-09-13 16:37:36.613 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/be9d88a80695b04a2a9eb5826361e0897d94c042.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:1640,error,error,1640,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['error'],['error']
Availability,"24, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3530, in any; collection = arg_check(args[1], 'any', 'collection', collection_type); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 586, in arg_check; raise TypeError(""{fname}: parameter '{argname}': ""; TypeError: any: parameter 'collection': expected expression of type set<any> or array<any>, found list: [['10', 123, 'G', 'C'], ['10', 456, 'T', 'A']]; ```; So, hail doesn't support heterogeneous arrays. Converting to a homogeneous array:. ```python; variants = [(""10"", 123, [""G"", ""C""]), (""10"", 456, [""T"", ""A""])]. expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2])),; variants; ). hl.eval(expr). ```; Leads to the following error (which looks like the bug!):; ```; Traceback (most recent call last):; File ""test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3531, in any; return collection.any(f); File ""<decorator-gen-510>"", line 2, in any; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in any; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""<decorator-gen-518>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in fold; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""<decorator-gen-650>"", line 2, in fold; File ""/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:5135,error,error,5135,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['error'],['error']
Availability,"24.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; +",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29985,Down,Downloading,29985,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"24d070c815e02f6f5b), and gnomad_qc (0c52cf47e48fa5b503d874e96482ea4286474c71). I cloned the repo in question; ```bash; pip3 uninstall hail gnomad gnomad_qc. pip3 install -U \; hail \; git+https://github.com/broadinstitute/gnomad_methods.git \; git+https://github.com/broadinstitute/gnomad_qc.git. git clone git@github.com:broadinstitute/gnomad-readviz.git; ```. I applied this patch:; ```diff; diff --git a/step1__select_samples.py b/step1__select_samples.py; index c159207..9ba1812 100644; --- a/step1__select_samples.py; +++ b/step1__select_samples.py; @@ -38,14 +38,7 @@ def hemi_expr(mt):; ; def main(args):; ; - hl.init(log=""/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); - meta_ht = hl.import_table(args.sample_metadata_tsv, force_bgz=True); - meta_ht = meta_ht.key_by(""s""); - meta_ht = meta_ht.filter(hl.is_defined(meta_ht.cram_path) & hl.is_defined(meta_ht.crai_path), keep=True); - meta_ht = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants with at least one non-ref GT""); mt = mt.filter_rows(hl.agg.any(mt.GT.is_non_ref",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:1069,checkpoint,checkpoint,1069,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,1,['checkpoint'],['checkpoint']
Availability,"280; 2. 25997 https://batch.hail.is/batches/8083195/jobs/27937. The pipeline runs two table collects to get sample information, then converts the matrix table to a table of ndarrays of the value `hl.int(hl.is_defined(mt.GT))`. The entries are getting subsetted, so there is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBuff",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:1120,error,error,1120,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['error'],['error']
Availability,28e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:12196,echo,echo,12196,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,10,"['Error', 'echo']","['Error', 'echo']"
Availability,"2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:2467,Error,ErrorHandling,2467,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,1,['Error'],['ErrorHandling']
Availability,2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:227); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:347); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:341); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more. test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt FAILURE; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:9368,FAILURE,FAILURE,9368,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['FAILURE'],['FAILURE']
Availability,"31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(Res",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:4509,failure,failure,4509,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['failure'],['failure']
Availability,35); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6595,error,error,6595,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['error'],['error']
Availability,"4.7 to download on laptop, 1.5 to download on k8s. About 1.2 seconds to untar in either setting. That yields the ~7s on my laptop and 3s in k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560450115:7,down,download,7,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560450115,2,['down'],['download']
Availability,"4.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/vep_data//human_ancestor.fa.gz,conservation_file:/vep_data//loftee.sql', '--dir_plugins', '/vep/ensembl-vep/Plugins/', '--dir_cache', '/vep_data/', '-o', 'STDOUT'] failed with non-zero exit status -9; VEP error output:; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:2226,error,error,2226,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,1,['error'],['error']
Availability,"4; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1819,failure,failure,1819,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['failure'],['failure']
Availability,"4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2233,avail,available,2233,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['avail'],['available']
Availability,"5]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnection.java:214); [13:46:55]	[:makeHailDocs] 	at java.lang.Thread.run(Thread.java:745); [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Hail version: devel-b94d386; [13:46:55]	[:makeHailDocs] Error summary: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] make: *** [doctest] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:2741,Error,Error,2741,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,3,"['Error', 'error']","['Error', 'error']"
Availability,"6' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3387,failure,failure,3387,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['failure'],['failure']
Availability,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11412,Error,Error,11412,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,3,"['Error', 'failure']","['Error', 'failure']"
Availability,78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_G,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:14664,echo,echo,14664,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3329,failure,failure,3329,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['failure'],['failure']
Availability,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14865,toler,tolerationSeconds,14865,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['toler'],['tolerationSeconds']
Availability,"9); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs://hail-common/builds/devel/jars/hail-devel-38dbf156b630-Spark-2.2.0.jar', '--py-files=gs://hail-common/builds/devel/python/hail-devel-38dbf156b630.zip,/var/folders/rn/t2xcx1ps4h96txll46qkkfsj2q8bnl/T/pyscripts_fYVAte.zip', '--properties=spark.executor.extraClassPath=./hail-devel-38dbf156b630-Spark-2.2.0.jar,spark.driver.extraClassPat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7576,ERROR,ERROR,7576,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['ERROR'],['ERROR']
Availability,"9-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputforma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6834,AVAIL,AVAILABLE,6834,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"910eaa1b8da8ff8575e514bc61c78. Kotlin: 1.4.20; Groovy: 2.5.12; Ant: Apache Ant(TM) version 1.10.9 compiled on September 27 2020; JVM: 1.8.0_362 (Private Build 25.362-b09); OS: Linux 5.4.0-1042-gcp amd64. real	0m3.621s; user	0m4.448s; sys	0m0.623s; + retry make jars wheel HAIL_DEBUG_MODE=1; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.0"" which is different from old value """"; printf ""3.3.0"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=' >> src/main/resources/build-info.properties; echo 'revision=e1d86e1908f0911d45b03ef08a694d07e1c4627b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-03-09T23:23:56Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.0' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.110' >> src/main/resources/build-info.properties; HAIL_DEBUG_MODE is set to ""1"" which is different from old value """"; printf ""1"" > env/HAIL_DEBUG_MODE; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; javac -d build/classes/scala/debug -Xlint:all -Werror -XDenableSunApiLintControl -XDignore.symbol.file src/debug/scala/is/hail/annotations/Memory.java; ./gradlew shadowJar -Dscala.version=2.12.13 -Dspark.version=3.3.0 -Delasticsearch.major-version=7; Starting a Gradle Daemon (subsequent builds will be faster); > Task :compileJava NO-SOURCE; > Task :compileScala; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala:122: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/TableWriter.scala:57: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/TableWriter.scala:674: value of is not a member of object java.nio.file.Path; three errors found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450:1828,Error,Error,1828,https://hail.is,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450,4,"['Error', 'error']","['Error', 'errors']"
Availability,"946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6497,Down,Downloading,6497,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,": ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; ib-in-f128.1e100.net; ic-in-f128.1e100.net; jo-in-f128.1e100.net; jp-in-f128.1e100.net; jq-in-f128.1e100.net; jr-in-f128.1e100.net; jt-in-f128.1e100.net; jv-in-f128.1e100.net; jw-in-f128.1e100.net; jx-in-f128.1e100.net; jy-in-f128.1e100.net; jz-in-f128.1e100.net; ie-in-f128.1e100.net; if-in-f128.1e100.net; ig-in-f128.1e100.net; ik-in-f128.1e100.net; Google DNS thinks your IP is: ; ; CPU Count: ; 16; CPU Load Average: ; [32.39, 33.2, 19.0]; Total Memory: ; 57.5 GiB; Free Memory: ; 38.41 GiB; TCP segment counts not available because ""netstat"" was not found during test runs; Disk Counter Deltas:; disk reads writes rbytes wbytes rtime wtime ; loop0 0 0 0 0 0 0 ; loop1 0 0 0 0 0 0 ; loop3 0 0 0 0 0 0 ; loop4 0 0 0 0 0 0 ; loop5 0 0 0 0 0 0 ; nvme0n1 4385 4694 581857280 1743810560 6453 527129 ; sda1 0 544 0 3731456 0 429 ; sda14 0 0 0 0 0 0 ; sda15 0 0 0 0 0 0 ; TCP /proc values:; tcp_timestamps = 1; tcp_sack = 1; tcp_window_scaling = 1; Boto HTTPS Enabled: ; True; Requests routed through proxy: ; False; Latency of the DNS lookup for Google Storage server (ms): ; 1.5; Latencies connecting to Google Storage server IPs (ms):; 74.125.126.128 = 1.1. ------------------------------------------------------------------------------; In-Process HTTP Statistics ; ------------------------------------------------------------------------------; Total HTTP requests made: 149; HTTP 5xx errors: 0; HTTP connections broken: 0; Availability: 100%. Output file written to '/tmp/output.json'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:3689,avail,available,3689,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,3,"['Avail', 'avail', 'error']","['Availability', 'available', 'errors']"
Availability,": Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:3126,Down,Downloading,3126,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1990,error,error,1990,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,3,"['FAILURE', 'error']","['FAILURE', 'error']"
Availability,":00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.0rc1 Requires-Python >=3.8; 1.22.0rc2 Requires-Python >=3.8; 1.22.0rc3 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.8.0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:7642,Down,Downloading,7642,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,":00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6957,Down,Downloading,6957,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,":46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnectio",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1506,error,error,1506,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,1,['error'],['error']
Availability,"; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuff",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7734,failure,failure,7734,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['failure'],['failure']
Availability,"; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7677,failure,failure,7677,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['failure'],['failure']
Availability,"; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5180,AVAIL,AVAILABLE,5180,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:5332,Down,Downloading,5332,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:7344,echo,echo,7344,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"; ```sh; $ pyspark; Python 3.9.18 (main, Oct 25 2023, 05:26:35) ; [GCC 7.3.1 20180712 (Red Hat 7.3.1-17)] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populati",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2053,avail,available,2053,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['avail'],['available']
Availability,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2326,failure,failureThreshold,2326,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,2,['failure'],['failureThreshold']
Availability,"; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2 TeamCityAgent3). case ""$1"" in; start); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh start; done; ;;; stop); for agent in ${AGENTS[@]}; do; start-stop-daemon --start -c teamcity --exec /home/teamcity/$agent/bin/agent.sh stop; done; ;;; *); echo ""usage start/stop""; exit 1; ;;. esac. exit 0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/675#issuecomment-245383790:2307,echo,echo,2307,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790,1,['echo'],['echo']
Availability,"<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6704,Down,Downloading,6704,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(O",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8063,failure,failure,8063,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['failure'],['failure']
Availability,"> 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. OK, I can add the two other failures I found, but it's infuriating to get most of the way through 100k jobs and then have one fail for some new 500. It feels like there's a long tail of rare docker errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7783#issuecomment-568581120:151,error,errors,151,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568581120,3,"['error', 'failure']","['errors', 'failures']"
Availability,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:558,down,down,558,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311,1,['down'],['down']
Availability,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403:366,down,down,366,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403,2,['down'],['down']
Availability,"> @lgruen are y'all running with this change now? I was vaguely concerned that with ~32 JVMs alive that would negatively impact the machine. Do you find that the JVM's RAM gets swapped out and the machines are generally stable?. Yes, we haven't seen any issues with this so far, but also have only been running with this change for a few days.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12248#issuecomment-1267773430:93,alive,alive,93,https://hail.is,https://github.com/hail-is/hail/pull/12248#issuecomment-1267773430,1,['alive'],['alive']
Availability,> @tpoterba do we think this is the actual cause of the 2.4.2 incompatibility?. Didn't you see things fail on initialization? That's not related to serialization. > how do we verify this fixes the issues our users are seeing?. The people seeing those errors are using jars not compiled for the version of Spark (and json4s) they have installed. I don't think we need to test for this case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526:251,error,errors,251,https://hail.is,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526,1,['error'],['errors']
Availability,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,3,['error'],"['error', 'errors']"
Availability,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464:511,error,errors,511,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464,2,['error'],['errors']
Availability,"> Almost by definition I'd suspect that adding a system administrator is a high security impact (that's not a judgement on you, just a statement about the security boundary getting wider).; > ; > This is obviously fine in this case because we want you to be a system administrator, but we should let appsec know regardless. They'll also probably want to send you some standard trainings (and maybe background check forms?). I'll ping them. Ah right that's an extremely fair point. Should I put ""high"" back in this PR description, or just sit tight until I hear from appsec?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400720230:429,ping,ping,429,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400720230,1,['ping'],['ping']
Availability,"> Also all the tests need to be fixed. Yep, working on it. Serialization failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-491423599:73,failure,failures,73,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-491423599,1,['failure'],['failures']
Availability,"> Also, it looks like we didn't define the operator syntax. Sounds like an easy PR to farm out to someone else!. @danking I intentionally didn't add this yet - there is a case I am worried about:. ```; mt.filter_rows(mt.pass & mt.variant_qc.AF[1] > 0.01); ```. Right now this is a type error. With bit operators, this is the same as:. ```; mt.filter_rows((hl.bit_and(hl.int(mt.pass), mt.variant_qc.AF[1]) > 0.01); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5149#issuecomment-454835409:286,error,error,286,https://hail.is,https://github.com/hail-is/hail/pull/5149#issuecomment-454835409,1,['error'],['error']
Availability,"> Are there any CSS conventions within Hail? I assume I need to migrate the ad-hoc ""style"" tags into CSS?. See the PR #14562 where Daniel introduced the new UI. He decided to use [tailwind](https://tailwindcss.com), which apparently (I don't know much about frontend stuff) prefers to keep styling in the html. > I've moved the status indicator to the front of the line. Is that ok?. I like it. > I'm not really sure I like the change to Pending. Curious for others' thoughts. In the template do we have the number of running jobs available? If so maybe we split ""Pending"" into ""Pending"" and ""Running"". Otherwise I think this is fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14640#issuecomment-2274259369:531,avail,available,531,https://hail.is,https://github.com/hail-is/hail/pull/14640#issuecomment-2274259369,1,['avail'],['available']
Availability,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664:457,error,error,457,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664,2,['error'],['error']
Availability,"> But I'm confused because I thought that was the same as my initial implementation. Sorry, don't read too much into my first few comments, I was trying to understand the constraints. I think my last comment is very close to your original proposal, but also includes Running -> Error and Running -> Ready as possible transitions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499651541:278,Error,Error,278,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499651541,1,['Error'],['Error']
Availability,"> Can we dummy proof this a bit more to have a nice error message if this is not the case?. Happy to, but I'm not exactly sure what you mean by ""if this is not the case"". What would you like to add?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13180#issuecomment-1593184582:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/pull/13180#issuecomment-1593184582,1,['error'],['error']
Availability,"> Can you lock down this behaviour with a test?. As I said in the description, I tried hard to write a test, but I couldn't manage to find a way to exercise this with a targeted test. And this bug is currently blocking a user, so I want to get it released asap. I can try again, but I suspect it would end up being a day or two of extra work, and would likely still end up brittle and unsatisfactory. I'd rather spend my effort getting back into the line of work that would eventually make it much easier to target specific compiler code paths.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14673#issuecomment-2338662541:15,down,down,15,https://hail.is,https://github.com/hail-is/hail/pull/14673#issuecomment-2338662541,1,['down'],['down']
Availability,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263:286,error,error,286,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263,2,['error'],['error']
Availability,"> Dan's OOO this week. It's a Python lint failure:. Thanks very much, @tpoterba! Should hopefully be better now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10647#issuecomment-876805713:42,failure,failure,42,https://hail.is,https://github.com/hail-is/hail/pull/10647#issuecomment-876805713,1,['failure'],['failure']
Availability,"> Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down. I think I have what I need. I have a ~/.hail/token and ~/.hail/tokens.json. The token file looks like a jwt, decodes to ""email"": ""ako"" followed by some low-ascii characters. . Before I ran this I logged in using hailctl auth login.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223:130,failure,failure,130,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223,2,"['down', 'failure']","['down', 'failure']"
Availability,"> Did you ssh to CI, start a python session, start a batch client, and delete it in that manner?. Yes. And the CI status (https://ci.hail.is/watched_branches/0/pr/6561) continued to error out with 500s (caused by batch lookup 404s in the logs) even after a heal loop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6582#issuecomment-509627134:182,error,error,182,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509627134,1,['error'],['error']
Availability,"> Do we insert finally blocks and clean up our open streams when an exception bubbles through generated code?. Nope. We can add this pretty easily now, I added infrastructure to HailTaskContexts for task-owned resources. Maybe I misunderstand Google write channels -- how would a stale writer to a different object cause this failure? I'm interpreting ""session"" here as a specific object destination, is that wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1531672420:326,failure,failure,326,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531672420,1,['failure'],['failure']
Availability,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:72,error,error,72,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719,1,['error'],['error']
Availability,"> Drive by comment here as I review PRs, but this is The Python Way, for better or worse. People call it ""easer to ask forgiveness than permission"". I don't personally have strong feelings one way or the other but we do this in many places in our codebase. Be that as it may, throwing if something does not exist is clearer and more robust that catching an error because some operation failed for what is arguably unexceptional, and just hoping that the `KeyError` that that operation threw is because the item did not exist, rather than another `KeyError` risen in any subsequent work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12864#issuecomment-1516489544:333,robust,robust,333,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516489544,4,"['error', 'robust']","['error', 'robust']"
Availability,"> First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. Nope. But I didn't run many tests with setup/cleanup containers actually doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7445#issuecomment-549184176:110,error,errors,110,https://hail.is,https://github.com/hail-is/hail/pull/7445#issuecomment-549184176,1,['error'],['errors']
Availability,"> Having the Auth service ping the Batch API with it to verify the token is valid. I believe the way our CRSF is implemented, we don't actually ever ""validate"" the tokens, we only check that the token in the formdata matches the token in the cookie. > Or perhaps we could just make this UI a single page application instead of a bunch of pages on different subdomains that resemble one. . This would be wonderful! Sort of similar-but-better to my thought of hosting the ""top menu bar"" as a separate iframe that always comes from auth. For the same reason (in particular, the apparently lack of regular usage of the logout button), that kind of change is probably larger than the scope of getting this bug fixed... but would cool to look into some day!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2269645369:26,ping,ping,26,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2269645369,1,['ping'],['ping']
Availability,"> Hi @daniel-goldstein. I've taken a look at a few of the records in this table in prod. Do you know what it means to have negative values for `n_running_cancellable_jobs` and/or `running_cancellable_cores_mcpu`? Here's an example:; > ; > ; > ; > ```sql; > ; > select * from job_group_inst_coll_cancellable_resources where batch_id = X;; > ; > +----------+-------+--------------------------+------------------------------+----------------------------+--------------------------------+-----------+-----------------------------+-----------+--------------+; > ; > | batch_id | token | n_ready_cancellable_jobs | ready_cancellable_cores_mcpu | n_running_cancellable_jobs | running_cancellable_cores_mcpu | inst_coll | n_creating_cancellable_jobs | update_id | job_group_id |; > ; > +----------+-------+--------------------------+------------------------------+----------------------------+--------------------------------+-----------+-----------------------------+-----------+--------------+; > ; > | X | 0 | 0 | 0 | 0 | 0 | standard | 0 | 1 | 0 |; > ; > | X | 18 | 0 | 0 | -1 | -2000 | standard | 0 | 1 | 0 |; > ; > | X | 59 | 0 | 0 | 0 | 0 | standard | 0 | 1 | 0 |; > ; > | X | 60 | 0 | 0 | -1 | -2000 | standard | 0 | 1 | 0 |; > ; > | X | 74 | 0 | 0 | 0 | 0 | standard | 0 | 1 | 0 |; > ; > | X | 172 | 0 | 0 | 0 | 0 | standard | 0 | 1 | 0 |; > ; > | X | 185 | 0 | 0 | -1 | -1000 | standard | 0 | 1 | 0 |; > ; > +----------+-------+--------------------------+------------------------------+----------------------------+--------------------------------+-----------+-----------------------------+-----------+--------------+; > ; > ```; > ; > . Good question, looks like a bug to me as these records should eventually settle to sum to 0. Are these from recent batches? Hopefully this is some bug that has already been fixed but these rows were never cleaned up. I'd also check the code around `schedule_job` to see if any additions to this table aren't fault tolerant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14623#issuecomment-2254138069:1948,fault,fault,1948,https://hail.is,https://github.com/hail-is/hail/issues/14623#issuecomment-2254138069,2,"['fault', 'toler']","['fault', 'tolerant']"
Availability,"> Hi Ivan! Thanks so much for picking this up :) I haven't much experience on the batch system but I'll try my best to give accurate feedback. I have a few questions/observations up front:; > ; > 1. Your changes to stored procedures under `batch/sql` make me a little nervous.; > ; > Most of these are migrations applied in the order defined in the build step mentioned in [NOTE 1] except `estimated-current.sql` [NOTE 2].; > ; > I don't think changing these will have the desired effect and may make it impossible for someone to reproduce the database. The only changes to _existing_ sql you'll need to make are in the sql strings in python code.; > ; > 2. This needs to be written as a migration and maybe could be simplified?; > ; > I think this needs to be done as a database migration. We'll have no need for a stored procedure once complete. You can assume current columns and constraints exist, dispense with the error checking and simplify. Can you convert this to a sql script and add it to the end of the list of migrations in `build.yaml`? You'll probably want `online: false` too. I fear you'll have to take inspiration from `rename-job-groups-tables.sql` by applying one `ALTER TABLE` command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. Alternatively, create, execute then drop the procedure within `rename-job-groups-cancelled`.; > ; > [NOTE 1] migration applied in `build.yaml`; > ; > The relevant build step in `build.yaml` can be found by searching for the entry starting with the yaml below. This controls which migrations are applied and in what order.; > ; > ```yaml; > kind: createDatabase2; > name: batch_database; > databaseName: batch; > ```; > ; > [NOTE 2] estimated-current.yaml; > ; > I don't agree with why we have this. It would be nice to generate this automatically. Anyway, please keep your changes to this file as it's meant for documentation purposes only. None ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045:920,error,error,920,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045,2,['error'],['error']
Availability,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:591,error,errors,591,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,3,"['error', 'failure']","['errors', 'failure']"
Availability,"> How about underlining it?. Nice suggestion. I hadn't done this because placing the underline on the li increased the height of the element, again requiring negative margin. But, on display: inline elements, border doesn't affect height, so the solution is just to place it there. CSS. I've also improved the javascript function to be more robust; before it would have considered /docs/0.2/index.html as home. It now matches the `pathname` of the anchor tag and location. This is just the right way to do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8455#issuecomment-609911680:341,robust,robust,341,https://hail.is,https://github.com/hail-is/hail/pull/8455#issuecomment-609911680,1,['robust'],['robust']
Availability,"> I agree this is the most compelling critique. I think the Pythonistas would make two points: a) KeyError is the one specific error you get in this case and b) it should be written like this:; > ; > ```python; > try:; > persisted_bm = self._persisted_locations[bm]; > except KeyError as err:; > raise ValueError(f'{bm} is not persisted') from err; > persisted_bm.__exit__(None, None, None); > ```. Updated my comment - I think my argument still holds with `KeyError`. Furthermore, you're still leaving around state in that dict, opening yourself up to calling `__exit__` twice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12864#issuecomment-1516535470:127,error,error,127,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516535470,1,['error'],['error']
Availability,"> I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:; > ; > > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: [user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com](mailto:user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com) does not have storage.buckets.get access to untitled-folder.; > ; > That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?. Right, I mentioned this above. It appears to be trying to create, or trying to read, untitled_folder as a bucket. If you trick it into using /your_bucket_name as the cwd, folder creation works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480381353:645,error,error,645,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480381353,1,['error'],['error']
Availability,"> I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474:16,down,down,16,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474,1,['down'],['down']
Availability,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:782,error,error,782,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367,9,"['down', 'error', 'failure']","['down', 'error', 'errors', 'failure']"
Availability,"> I confirmed that, with this change, #13915 is resolved. In the interest of fixing that for 0.2.125, I'm gonna approve and get it into this release.; > ; > However, I left a comment inline. It seems that the meaning of pathsUsed was always a bit buggy and I think we should kill that tech debt now before it trips us again.; > ; > I'm also still concerned that `test_glob` didn't catch this bug; we should nail down why. We don't catch this in `test_glob` because the matrixread is nested within a `TableKeyByAndAggregate`, which semhash can't handle yet:; ```; 2023-10-26 12:54:40.576 : WARN: Failed to compute SemanticHash: SemanticHash unknown: is.hail.expr.ir.TableKeyByAndAggregate; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13919#issuecomment-1781498654:412,down,down,412,https://hail.is,https://github.com/hail-is/hail/pull/13919#issuecomment-1781498654,1,['down'],['down']
Availability,"> I do not know why the retries setting in pip.conf did not catch https://ci.hail.is/batches/167314/jobs/27, but more retries never hurt anyone. Is the whole pip process crashing because of the exception? Perhaps a different kind of crash that pip can't recover from and retry. > Another CI-related PR. This one changes the base image of everything else: hail-ubuntu. It's an ubuntu image with two scripts that make pip and apt more resilient. Take a look at docker/hail-ubuntu. Should we do this for `apt` too?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767033129:254,recover,recover,254,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767033129,2,"['recover', 'resilien']","['recover', 'resilient']"
Availability,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:290,error,error,290,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077,3,['error'],['error']
Availability,> I feel like rebinding an argument should be a syntax error in scala. It certainly should warrant some big red intellij squiggles.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6984#issuecomment-527605644:55,error,error,55,https://hail.is,https://github.com/hail-is/hail/pull/6984#issuecomment-527605644,1,['error'],['error']
Availability,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:544,down,down,544,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541,1,['down'],['down']
Availability,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494:364,down,down,364,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494,2,['down'],['down']
Availability,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1023,down,down,1023,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940,8,"['down', 'error']","['down', 'error']"
Availability,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:232,toler,tolerations,232,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039,1,['toler'],['tolerations']
Availability,"> I think you'll still have the doctest failure. It just returns list of lists now ,right?. I'm not sure I follow. What returns a list of lists?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8265#issuecomment-601452175:40,failure,failure,40,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601452175,1,['failure'],['failure']
Availability,"> I tried making a plot with only shape, not color. It should behave the same as color (in the discrete case), only using shapes instead of colors, but right now the legend uses ""trace 0"" etc. instead of the data values. I was able to get the category names showing properly in the legend with just shapes by adding a `shape_legend -> name` mapping to `GeomPoint.trace_args`, but this caused the shape names to override the color names when using both; I added logic to `Geom._add_aesthetics_to_trace_args` to concatenate the names with one legend entry per pair of color and shape, which causes the shapes to stop working and a separate color to be assigned to each legend entry. ```python; import hail as hl; from hail.ggplot import ggplot, aes, geom_point; ht = hl.utils.range_table(10); ht = ht.annotate(squared=ht.idx ** 2); ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); fig = (; ggplot(ht); + aes(x=ht.idx, y=ht.squared, color=ht.even, shape=ht.threeven); + geom_point(); ); fig.show(); ```. ![newplot(1)](https://user-images.githubusercontent.com/84595986/191850065-fa9cf15a-44b5-48cc-ad95-47af67d76ec1.png). I'm having trouble tracking down why this is happening, so I'll ask for some help with that tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12207#issuecomment-1255544008:1234,down,down,1234,https://hail.is,https://github.com/hail-is/hail/pull/12207#issuecomment-1255544008,1,['down'],['down']
Availability,"> I'd rather read a short one-line description of what is being done followed by the example. I agree with this. See, for example, filter_alleles in my PR: https://ci.hail.is/repository/download/HailSourceCode_HailCi/3269:id/www/pyhail/index.html. I'm allowing from some short clarifying remarks after the example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1216#issuecomment-270983676:186,down,download,186,https://hail.is,https://github.com/hail-is/hail/pull/1216#issuecomment-270983676,1,['down'],['download']
Availability,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:846,failure,failures,846,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358,2,"['avail', 'failure']","['available', 'failures']"
Availability,"> I'm not sure how this is unsafe. Could you explain?. Partitioner just says what elements are in what partitions. OrderedRDD also guarantees those elements are in a specific order. It is not safe to take an RDD partitioned with an OrderedPartitioner and just ""make"" it an OrderedRDD. Case where it fails: in read, union, which creates a partition-aware RDD that unions corresponding partition from a set of identically partitioned RDDs. > i think the better solution is to check that the ordered key inside the partitioner is the same as the implicit ordered key supplied. this also could have caught the errors. The error was making it into an OrderedRDD without sorting. It is perfectly valid to have an OrderedPartitioner-partitioned RDD that is sorted. (Above read case.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1864#issuecomment-303237349:606,error,errors,606,https://hail.is,https://github.com/hail-is/hail/pull/1864#issuecomment-303237349,2,['error'],"['error', 'errors']"
Availability,"> I'm seeing issues creating notebooks (500). I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). > reaching CI and Batch (502). Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. > Notebook2 link should be changed to redirect to notebook 1. There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091:849,avail,available,849,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091,1,['avail'],['available']
Availability,"> I'm worried this PR will break all dev deploys. The migration will bring down the service (batch, etc.) inside the dev namespace, but the dev deploy itself is running in the production batch, so there should be no issue. The deploys will still happen as normal after the migrations, e.g. deploy_batch will run after batch_database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718:75,down,down,75,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718,1,['down'],['down']
Availability,"> I've been working on an R interface to Hail through the sparklyr package. this also sounds awesome. To the issue -- this usually means something really bad happened on an executor, like a segfault. Spark usually is reluctant to provide the real error message, but in this case I think it's our fault. . Did you already do our job of bisecting to that commit for us? I'm not sure where exactly the generated C++ is being used (just the decoder, right, team?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258:247,error,error,247,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-428377258,2,"['error', 'fault']","['error', 'fault']"
Availability,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:333,checkpoint,checkpointed,333,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956,2,['checkpoint'],"['checkpoint', 'checkpointed']"
Availability,"> If there's no requester pays, is it just impossible to have ""public"" data in Azure storage safely? Like anything in there could be downloaded infinity times to drive up a bill?. That's correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1004244889:133,down,downloaded,133,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1004244889,1,['down'],['downloaded']
Availability,"> If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to QoBAppender). We might want to do this if we get rid of GSA keys. We can't have any more jars that presume the existence of some key file. It would also be a good time to fully delete the `memory` service, even though old jars should be able to tolerate that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692:436,toler,tolerate,436,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692,1,['toler'],['tolerate']
Availability,"> In particular, the matrixtable available in doc examples as `ds` lives at `hail/hail/python/hail/docs/data/example.mt`. Thanks, this is exactly what I was looking for. I am having trouble testing my new example locally though. When I run `make -C hail doctest-query`, the tests fail with a checksum error. I tried running `make -C hail clean` and retrying, but I still get the same error. ```; E hail.utils.java.FatalError: ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; E; E Java stack trace:; E org.apache.hadoop.fs.ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; E 	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347); E 	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303); E 	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252); E 	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197); E 	at java.base/java.io.DataInputStream.read(DataInputStream.java:149); E 	at is.hail.io.fs.HadoopFS$$anon$2.read(HadoopFS.scala:58); E 	at java.base/java.io.DataInputStream.read(DataInputStream.java:149); E 	at org.apache.commons.compress.utils.CountingInputStream.read(CountingInputStream.java:56); E 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252); E 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.init(GzipCompressorInputStream.java:185); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:168); E 	at is.hail.io.fs.GZipCompressionCodec$.makeInputStream(FS.scala:125); E 	at is.hail.io.fs.FS.open(FS.scala:563); E 	at is.hail.io.fs.FS.open$(FS.scala:560); E 	",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:33,avail,available,33,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,5,"['avail', 'error']","['available', 'error']"
Availability,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:454,failure,failures,454,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285,1,['failure'],['failures']
Availability,"> Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency. Agreed. I think our best path for speed is keeping these images totally cacheable so basically dependencies (nothing that will have to change on every commit, e.g. the wheel). Installing the wheels in the image is just adding more latency and work of localization. > The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down).; I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. Totally agree.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182:484,down,down,484,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182,1,['down'],['down']
Availability,"> It's a little concerning that this is necessary. Are there typecheck cases that don't make assertions about some of their children?. This check would have to go in a LOT of places otherwise. Reading down from the top: CastRename, NA, IsNA, Coalesce, AggLet, TailLoop, MakeArray, ...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9974#issuecomment-772079087:201,down,down,201,https://hail.is,https://github.com/hail-is/hail/pull/9974#issuecomment-772079087,1,['down'],['down']
Availability,"> Just so I understand correctly (and sorry if this is obvious), the current job logs interface is still the same. But if you want a container's logs, then you'll get bytes which the user will have to decode themselves. How does that affect the file download button in the UI and the hailctl batch logs functionality you have? Will you see text or a random byte string?. Good question. For the download button, the file you download is still a normal text file and I've confirmed that I can download and view a log file as I would expect. For the `hailctl batch logs` functionality, I added logic to the CLI in this PR where I download the bytes of the log and if I can decode it as UTF-8 I do, so it prints exactly as before, and if I can't I just print the bytes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1426373373:250,down,download,250,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1426373373,5,['down'],['download']
Availability,"> Looks like we need to set the severity correctly in the worker logs. I'm also seeing a lot of this; WARNING: Published ports are discarded when using host network mode; Also looks like we incorrectly log a ContainerTimeoutError as an error log even though that's a user error: https://cloudlogging.app.goo.gl/TUGWNxnFiBiEdsDo9. For what it's worth, this was showing up as an info log because this is a docker log message not from our code, so it's not going through our logging filters. The reason this showed up in the Google Logging query was because the query included this line; ```; severity=ERROR OR WARNING; ```; which means ""logs whose severity is ERROR or whose log entry contains ""WARNING"""", it is *not* equivalent to `severity=ERROR OR severity=WARNING` which does not show that log entry. Either way, #14252 gets rid of that log message entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14240#issuecomment-1930672702:236,error,error,236,https://hail.is,https://github.com/hail-is/hail/issues/14240#issuecomment-1930672702,5,"['ERROR', 'error']","['ERROR', 'error']"
Availability,> Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1785779141:192,error,error,192,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1785779141,1,['error'],['error']
Availability,> Maybe that's the error when no orphaned disks are found with any labels. Ya I interpret this as the query just returned no results.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13719#issuecomment-1737810077:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/13719#issuecomment-1737810077,1,['error'],['error']
Availability,> Merging in #14233 causes the failure in `test_union_rows1`. Some strangeness with these new dependencies - running without this commit and everything works fine. Doesn't seem to be an issue anymore...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231#issuecomment-1934834223:31,failure,failure,31,https://hail.is,https://github.com/hail-is/hail/pull/14231#issuecomment-1934834223,1,['failure'],['failure']
Availability,"> My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. Where are you running clone+merge here? We time the clone and pull in the build:. > + git clone https://github.com/hail-is/hail.git .; > real	0m12.030s. > + git fetch -q akotlar/hail; > real	0m2.878s. We don't time the merge, so that should be fast. I spot-checked the base_image step from half a dozen builds and the spread was ~11-18s. How was the 3s measured?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560436226:58,down,download,58,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560436226,1,['down'],['download']
Availability,"> Not sure what to do about wait for and ghost. rather silly of ghost to refuse connections on http when the base url is set to https. > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the X-Forwarded-Proto header isn't set. I'm not sure what the right fix is in this case. I still don't quite understand why the wait command doesn't hit gateway. Isn't it just issuing an http request to `f'{base_url}/{endpoint)`? Why doesn't that hit gateway?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962:146,failure,failure,146,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962,1,['failure'],['failure']
Availability,"> Noteworthy as well. Although I can't reproduce it consistently, sometimes conda may not like `-f environment.yml`, but will work perfectly well with `conda env create`. While I prefer `-f environment.yml` because it is more obvious, dropping that argument may be less fragile.; > ; > [conda/conda#3847](https://github.com/conda/conda/issues/3847); > ; > Edit: This was actually a typo in the documentation. Fixed in [3135dc1](https://github.com/hail-is/hail/commit/3135dc124c37ec6987d96f5de0b7dbb634487b7d). I find `conda`'s terminal UI supremely frustrating. That `conda env create -f does-not-exist` doesn't error with ""file not found"" blows my mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5022#issuecomment-449439317:612,error,error,612,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449439317,1,['error'],['error']
Availability,"> OK, but I'm not sure it's the right change to make. Now some jobs will fail silently. I don't think this lets anything fail silently -- the failure from run() is still tracked. Agree that the right solution is to additionally track information about dead JVMs to try to provide a better error message, but that juice isn't worth the squeeze at the moment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527612619:142,failure,failure,142,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527612619,2,"['error', 'failure']","['error', 'failure']"
Availability,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:996,error,error,996,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467,1,['error'],['error']
Availability,"> Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures. Option 4: rebase on master, where ArrayAgg is not an emittable node (only RunAgg / RunAggScan) :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556:505,down,down,505,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556,2,"['down', 'failure']","['down', 'failures']"
Availability,"> Pending -> Ready -> (Error, Running -> (Failed, Success)) . Does Running mean the pod has been created, or it is verified to be running? In the former case, is it Pending -> Ready -> Running -> (Error, Failed, Success)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499306521:23,Error,Error,23,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499306521,2,['Error'],['Error']
Availability,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930,3,['error'],['error']
Availability,"> Still seeing this error in the deploy_batch job:; > ; > ```python; > utils.py	retry_long_running:923	in delete_prev_cancelled_job_group_cancellable_resources_records	; > Traceback (most recent call last):; > File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; > return await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; > await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1485, in delete_prev_cancelled_job_group_cancellable_resources_records; > async for target in targets:\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 334, in execute_and_fetchall; > async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; > await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; > await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; > await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; > await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; > await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; > first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; > packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; > err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; > raise errorclass(errno, errval); > pym",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053,1,['error'],['error']
Availability,"> Supporting `file://` seems to suggest that we would support URIs in general (e.g. `--files gs://foo/bar:/baz`); AFAICT we don't support the latter, right? What's the motivation to support `file://`?. I didn't know what we currently support with regards to file paths, so I just made it that we're resilient to someone adding file://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1836784494:299,resilien,resilient,299,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1836784494,1,['resilien'],['resilient']
Availability,"> That should be a simple fix, though perhaps at this point not worth it as this is not a fruitful optimization for this query. Agreed, although depending on the time line for a good optimization for this query I may circle back on this, as there is currently a bug in this deduplication so if the actual optimizaion won;t be available for a while it might be worth fixing in the meantime",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525:326,avail,available,326,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1839244525,2,['avail'],['available']
Availability,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-470262025:361,failure,failure,361,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470262025,1,['failure'],['failure']
Availability,"> The 9 is about stopping the exponential backoff. At i=9, (0.1 * 2^9 is roughly half a second) we stop backing off and keep polling with delays uniformly chosen between 0 and half second.; > ; > We might actually want a hard limit on the number of backoffs, as written this test could trigger an infinite loop. I'll add a max number of iterations. What does a failure indicate? The only concern I have is that we have a stochastic process, but I don’t know whether 14 indicates a 99.99% success target, or something else. Also, if this test fails for a future batch PR, should we pass the contribution under some circumstances. > Ok, what do you think of this?; > ; > ```; > i = 0; > while len(output) != 4:; > time.sleep(0.100 * (3/2) ** i); > i = i + 1; > if i > 14:; > break; > assert len(output) != 4; > ```; > We exponentially back off with base 3/2. We break as soon as the condition is satisfied. If we wait more than a minute (`0.1 * (3/2)^14` is roughly 30s, so we've waited about a minute in total), we bail (and the assert will fail). Seems completely reasonable to prevent infinite loops in CI. I don’t really understand under which circumstances this should fail, and whether 30s is enough to ensure that we get rare false positive test failures. I trust your judgment on this, so if you say 30s is good enough, I think we should start there and adjust if test failures on batch PRs pile up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694:361,failure,failure,361,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470263694,3,['failure'],"['failure', 'failures']"
Availability,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that I’m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers — right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:34,down,down,34,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975,2,['down'],['down']
Availability,> The error is here:; > `Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default).`. Thanks very much for finding that and sorry for not seeing this issue earlier! This is fixed now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144803938:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144803938,1,['error'],['error']
Availability,"> The failure isn't the dataproc delete, that's just what you get when you try to delete a cluster that wasn't created. You had an import error:; > ; > ```; > File ""/hail/repo/hail/python/hail/fs/google_fs.py"", line 3, in <module>; > import gcsfs; > ModuleNotFoundError: No module named 'gcsfs'; > ```. I see. gcsfs is already dynamically imported. If the apiserver tests are being run by CI, it needs to have the gcsfs module, unless we want to make fs lazy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-485522267:6,failure,failure,6,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485522267,2,"['error', 'failure']","['error', 'failure']"
Availability,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:501,error,error,501,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,2,['error'],['error']
Availability,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549:602,checkpoint,checkpoint,602,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549,2,['checkpoint'],['checkpoint']
Availability,"> The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores. Yep. Let's call that admin operator error and let's not do that. The other reason was we had hardcoded the billing computation in the code, but that's fixed now. But it is hardcoded in the documentation, so we still shouldn't really be changing any of these settings (I see this mainly for the second instance at this point). Separately, we should decouple the billing from the details of the implementation so we get a bit more flexibility on the backend in the main instance, as we've discussed. I'm OK with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109:196,error,error,196,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109,1,['error'],['error']
Availability,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596:1670,down,down,1670,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596,2,['down'],['down']
Availability,> Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. You should also modify the CloudSQL instance to only accept TLS connections. That's an option.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8433#issuecomment-607558384:38,error,errors,38,https://hail.is,https://github.com/hail-is/hail/pull/8433#issuecomment-607558384,1,['error'],['errors']
Availability,> Then we just need to decide if we want to throw an error on file:// for a non-existent Transfer.TARGET_DIR. I say no. I agree. I think the user is being explicit what they intend to have happen.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822#issuecomment-764956189:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764956189,1,['error'],['error']
Availability,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:334,avail,available,334,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945,2,"['avail', 'error']","['available', 'error']"
Availability,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:115,error,error,115,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191,1,['error'],['error']
Availability,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:25,mainten,maintenance,25,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772,4,"['down', 'error', 'failure', 'mainten']","['down', 'error', 'failure', 'maintenance']"
Availability,"> What were the errors? It should be OK to hold the job object around and e.g. use it to ask for logs even if the job is deleted. We were setting the Job id attribute to None on deletion. ci then queried various properties of the job, passing None as the job id in the URL. That caused batch to 500 converting the id to an integer (this is itself a bug). Leaving the id in the Job might also fix it, but it seems wrong in the REST setting to delete an object, but still be able to query it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-476227436:16,error,errors,16,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-476227436,1,['error'],['errors']
Availability,"> When will it be available to use?. 0.2.129. Should be released soon,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14375#issuecomment-2025929109:18,avail,available,18,https://hail.is,https://github.com/hail-is/hail/pull/14375#issuecomment-2025929109,1,['avail'],['available']
Availability,"> Whoops, sorry this got dropped!. No problem. This wasn't important, just a potential ""nice to have"". > how changes like this could unintentionally affect things. This shouldn't change anything unless a user intentionally opts into using it to encode JSON. It was intended as a convenience: it's easy enough for a user to implement this themselves, but it might take a few rounds of trial and error to catch all the classes that need to be handled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11307#issuecomment-1050104675:394,error,error,394,https://hail.is,https://github.com/hail-is/hail/pull/11307#issuecomment-1050104675,1,['error'],['error']
Availability,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636:699,reliab,reliable,699,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636,4,"['error', 'reliab']","['error', 'reliable']"
Availability,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:934,down,down,934,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897,4,"['checkpoint', 'down']","['checkpoint', 'down']"
Availability,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:315,error,error,315,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104,2,['error'],"['error', 'errors']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14533?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14533** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14533?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14509** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14509?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14514** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14514?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14554](https://github.com/hail-is/hail/pull/14554) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14554?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14517** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14517?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14533#issuecomment-2098889324:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14533#issuecomment-2098889324,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14663?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14663** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14663?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14662** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14662?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @patrick-schultz and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Graphite</b></a>; <!-- Current dependencies on/for this PR: -->",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14663#issuecomment-2302524299:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14663#issuecomment-2302524299,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14686#issuecomment-2354274670:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14686#issuecomment-2354274670,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14690?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14690#issuecomment-2356990835:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14690#issuecomment-2356990835,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14691?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14691#issuecomment-2357221524:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14691#issuecomment-2357221524,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" targ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14692#issuecomment-2358958754:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14692#issuecomment-2358958754,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14693#issuecomment-2362149504:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14693#issuecomment-2362149504,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14696#issuecomment-2362438406:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14696#issuecomment-2362438406,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14698#issuecomment-2364681873:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14698#issuecomment-2364681873,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 2 dependent PRs ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>, [#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>) 👈; * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417750658:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417750658,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14732** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14748](https://github.com/hail-is/hail/pull/14748) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14732#issuecomment-2419668108:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14732#issuecomment-2419668108,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14751** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14751?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14747** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14686](https://github.com/hail-is/hail/pull/14686) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @grohli and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Grap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14747#issuecomment-2438734907:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14747#issuecomment-2438734907,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14748** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14748?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14731** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14731?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14732](https://github.com/hail-is/hail/pull/14732) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14732?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14698** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14698?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14696** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14696?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14693** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14693?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14692** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14692?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14748#issuecomment-2444958196:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14748#issuecomment-2444958196,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> [!WARNING]; > <b>This pull request is not mergeable via GitHub because a downstack PR is open. Once all requirements are satisfied, merge this PR as a stack <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14751?utm_source=stack-comment-downstack-mergeability-warning"" >on Graphite</a>.</b>; > <a href=""https://graphite.dev/docs/merge-pull-requests"">Learn more</a>. * **#14751** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14751?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a> 👈; * **#14747** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14747?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * **#14684** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14684?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>: 1 other dependent PR ([#14686](https://github.com/hail-is/hail/pull/14686) <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14686?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>); * **#14683** <a href=""https://app.graphite.dev/github/pr/hail-is/hail/14683?utm_source=stack-comment-icon"" target=""_blank""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""10px"" height=""10px""/></a>; * `main`. This stack of pull requests is managed by Graphite. <a href=""https://stacking.dev/?utm_source=stack-comment"">Learn more about stacking.</a>; <h2></h2>. Join @grohli and the rest of your teammates on <a href=""https://graphite.dev?utm-source=stack-comment""><img src=""https://static.graphite.dev/graphite-32x32-black.png"" alt=""Graphite"" width=""11px"" height=""11px""/> <b>Grap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14751#issuecomment-2457987492:75,down,downstack,75,https://hail.is,https://github.com/hail-is/hail/pull/14751#issuecomment-2457987492,2,['down'],"['downstack', 'downstack-mergeability-warning']"
Availability,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:495,error,error,495,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170,2,['error'],['error']
Availability,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:1617,error,error,1617,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554,2,['error'],['error']
Availability,"> can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Even this may be expensive when we're doing something like ToArray(StreamRange). Let's start by benchmarking and seeing where we're at with the current benchmarks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661888188:11,down,down,11,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661888188,1,['down'],['down']
Availability,> how it's impossible to use timings currently inside cleanup blocks because it could accidentally re-raise a deleted error. Can you explain this to me? I saw Dan had a comment as well in the JVMJob that stated this. Our code seems to be always this:. ```; try:; with self.step('running'):; self.run_until_deleted or completed(); finally:; with self.step('uploading log'):; self.upload_log(); ```. The upload log step should propagate the deletion error from above.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054581169:118,error,error,118,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054581169,2,['error'],['error']
Availability,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233005833:242,error,error,242,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833,2,['error'],['error']
Availability,> infinitely retry transient errors with exponential backoff and no retry of non-transient errors. In other words: Do or do not. There is no try.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574380438:29,error,errors,29,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574380438,2,['error'],['errors']
Availability,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:784,down,down,784,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868,2,['down'],['down']
Availability,"> looks like a compile error in TestUtils.scala.; > ; > The rest looks good, will approve when tests pass. I fixed `TestUtils.scala`; the issue was a missing parameter in a call to the (changed) `MatrixVCFReader`. Should I rebase and squash all of my commits into a single commit, or are you OK with merging my branch with the discrete feature commits (i.e., non-""Merge remote-tracking branch"" commits)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5077#issuecomment-453215900:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-453215900,1,['error'],['error']
Availability,> need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. I want as much as possible for alerts to mean something unexpected is going wrong. If we can I think this should just be done offline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812859285:48,toler,tolerate,48,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812859285,1,['toler'],['tolerate']
Availability,> one approach is to build a Bitnami package. I'll ping them to get a sense on pricing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410367637:51,ping,ping,51,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410367637,1,['ping'],['ping']
Availability,"> printing an error. Error sounds like you're going to exit and not do anything. But you mean print out a message and don't label but still create the cluster? That seems fine, although munging seems more informational, that is, it's an error to us/Sam, not the user, esp. since they can't do anything about it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/6276#issuecomment-499694220,3,"['Error', 'error']","['Error', 'error']"
Availability,"> retry every deadlock in two deadlock prone SQL operations. I prefer @jigold's change which is almost ready: https://github.com/hail-is/hail/pull/7782. > retry every docker 500 error, it's 500, not our fault, just retry, right?. 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. Looking over the other changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7783#issuecomment-568580624:178,error,error,178,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568580624,3,"['error', 'fault']","['error', 'errors', 'fault']"
Availability,"> slight nitpick---can we call this ""error"" or ""raise_error"" or something less dire sounding than ""die""?. or some version of `except` for parallelism with python?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865#issuecomment-634380327:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-634380327,1,['error'],['error']
Availability,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:85,failure,failures,85,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227,4,"['error', 'failure']","['errors', 'failures']"
Availability,"> the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. where do these errors appear?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586598443:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598443,2,['error'],['errors']
Availability,"> the failure from run() is still tracked. Is that true? When we get out-of-memory errors, doesn't `run` just return?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527648997:6,failure,failure,6,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527648997,2,"['error', 'failure']","['errors', 'failure']"
Availability,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233041581:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581,16,['error'],"['error', 'error-catching', 'errors']"
Availability,"> we don't wait for all JVMs to be intitialized before accepting JVM jobs and the queue is FIFO so we reuse the same JVMs that are warm already?. No. We have no way to accept only JVM jobs or only Batch jobs, so we either accept all jobs or no jobs. In main, we accept jobs before the JVMs have initialized. We wait for all JVMs to initialize before giving JVMs to any JVM Job. So, concretely, in main and in this PR we *accept* jobs before JVMs are ready; however, in this PR we don't wait for all JVMs to initialize before *running* jobs. There are two improvements in this PR:; 1. If a JVM with the requested number of cores is available, allow the requesting JVMJob to start before the remaining JVMs are initialized.; 2. Rather than starting all the JVMs in parallel, start JVMs serially *and also* start them in the order that they are requested. If we have three waiting JVM Jobs two requesting 1 core and one requesting 4 cores, prefer to start JVMs with 1 and 4 cores to JVMs with 2 or 8 cores. (2) might sound slower (why start serially when we can stat in parallel?) but it appears that 30 JVMs competing for CPU time dramatically slows down average start up time. In both main and this PR it takes about ~25s for all JVMs to be ready; however, in this PR, some jobs can start much sooner than 25s b/c their JVMs are started first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13870#issuecomment-1775517156:631,avail,available,631,https://hail.is,https://github.com/hail-is/hail/pull/13870#issuecomment-1775517156,2,"['avail', 'down']","['available', 'down']"
Availability,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:518,error,error,518,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050,2,['error'],"['error', 'errors']"
Availability,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:164,error,error,164,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168,1,['error'],['error']
Availability,"@JLama75 We don't support Spark 3.5.0 yet, please downgrade to Spark 3.3.x. You can track progress on 3.5.0 support here: https://github.com/hail-is/hail/issues/13971",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14235#issuecomment-1921923818:50,down,downgrade,50,https://hail.is,https://github.com/hail-is/hail/issues/14235#issuecomment-1921923818,1,['down'],['downgrade']
Availability,"@Sun-shan According to the error message you posted, Spark itself cannot find `/hail/test/BRCA1.raw_indel.vcf`:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; ```. Looking at that error message, it looks like Spark is interpreting your path as a local file system path, _not_ a hadoop path. Moreover, earlier in your posted output this line:; ```; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ```; suggests that you're not actually connecting to a Spark cluster with a properly configured Hadoop installation. ---. Your Spark cluster appears improperly configured. I'm not sure if `pyspark` is even connecting to your cluster. You might try looking at [this StackOverflow post](https://stackoverflow.com/questions/34642292/cant-connect-pyspark-to-master) about connecting `pyspark` to a Spark cluster. I strongly recommend running `pyspark` again and executing:; ```; spark.sparkContext.master; ```; This should print the URL of your Spark master node. If this prints a String starting with `local`, then you're definitely not connecting to a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635,3,['error'],['error']
Availability,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I haven’t had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesn’t divulge much, and my own attempts to give Sanic problems haven’t yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:201,error,error,201,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481,1,['error'],['error']
Availability,"@bw2 Hey looks like there's some weird version issues. Can you set up the gradle to use a Spark 2.1 compatible version of elastic search if spark version is set to a 2.1.x version and use a Spark 2.0 compatible version of elastic search?. I don't fully understand the error, but it definitely looks like there's a spark version mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-319098074:268,error,error,268,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-319098074,1,['error'],['error']
Availability,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:164,error,error,164,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434,5,"['Error', 'error']","['Error', 'error', 'error-raising-operation']"
Availability,"@catoverdrive I did a bad, pip.conf needs to come from `docker/hail-ubuntu/pip.conf`, you also need the download-gcs-connector.py file I added to that branch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9935#issuecomment-773428416:104,down,download-gcs-connector,104,https://hail.is,https://github.com/hail-is/hail/pull/9935#issuecomment-773428416,1,['down'],['download-gcs-connector']
Availability,"@catoverdrive can you rebase and ping me when done? This isn't stacked anymore, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6765#issuecomment-530559600:33,ping,ping,33,https://hail.is,https://github.com/hail-is/hail/pull/6765#issuecomment-530559600,1,['ping'],['ping']
Availability,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966:305,redundant,redundant,305,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966,2,['redundant'],['redundant']
Availability,"@chrisvittal I can't figure out how to comment on unedited code, but here:. https://github.com/hail-is/hail/pull/9604/files#diff-689808a42e9fe9329e347edede9bc12419e29a1634afc5d8c899a41c9d550659R211. you probably want to switch to passing in an EmitCode as well. Not sure if that's the source of the code verification errors, but I think the order I was originally passing in the tuples (m, v) is swapped from how emitCodeParams represent them (v, m), so that might help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9604#issuecomment-713805849:317,error,errors,317,https://hail.is,https://github.com/hail-is/hail/pull/9604#issuecomment-713805849,1,['error'],['errors']
Availability,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:521,error,error,521,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365,1,['error'],['error']
Availability,"@chrisvittal This is all garbage, the failure makes little sense. I've started back from scratch with master, and have re-implemented all of the functionality, from this branch, needed to have the Dataproc test run (which relies on LoadVCF). . It works fine. Something else is amiss. I'm going to finish reimplementing everything in that clean slate, and when everything is running close this PR and reissue. The diff between them will show the problem area, which is in some kind of global state affecting sparkContext or RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-498334543,1,['failure'],['failure']
Availability,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219,1,['error'],['error']
Availability,"@chrisvittal, these tests actually pass the local backend. I changed the tests to reflect this, but wanted to ping you to make sure that seems reasonable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11345#issuecomment-1059604289:110,ping,ping,110,https://hail.is,https://github.com/hail-is/hail/pull/11345#issuecomment-1059604289,1,['ping'],['ping']
Availability,"@cseed - this PR has a bad interaction with the changes made to [optimize inside ArrayAgg emit](https://github.com/hail-is/hail/pull/5765/files). . I've added a [test that catches the problem](https://github.com/hail-is/hail/pull/5710/files#diff-3273df362c814023cfa64428acf395cfR1122). The root of the issue is that we **cannot run NormalizeNames again** inside of that optimization pass -- it generates references that collide/overwrite existing bindings available when ArrayAgg is emitted. We should be creating globally-unique names inside ForwardLets, not normalized names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172:456,avail,available,456,https://hail.is,https://github.com/hail-is/hail/pull/5710#issuecomment-483001172,1,['avail'],['available']
Availability,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:600,downtime,downtime,600,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061,1,['downtime'],['downtime']
Availability,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252825829:445,ERROR,ERROR,445,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829,5,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"@cseed @jbloom22 I don't really like how the examples sections are looking. I'd rather read a short one-line description of what is being done followed by the example. Right now, it's example code with detailed explanation. See https://ci.hail.is/repository/download/HailSourceCode_HailCi/3257:id/www/pyhail/index.html#pyhail.VariantDataset.annotate_variants_bed for an example. Cotton -- I'll defer to you on the variantqc table. Did we come to a consensus what the return type should be for export commands?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1216#issuecomment-270982982:258,down,download,258,https://hail.is,https://github.com/hail-is/hail/pull/1216#issuecomment-270982982,1,['down'],['download']
Availability,"@cseed Added the redirect logic. It feels much slower (although there may be some small optimizations available). What do you think about using popup as the default, and then catch on error and send to redirect method? https://github.com/auth0/auth0.js/issues/868. This could work well as long as blocking happened rarely. So far, I haven't been able to trigger the block in any browser (Safari, Chrome, Firefox, all latest v), with content blockers enabled (which definitely block popups on other sites).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812:102,avail,available,102,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456667812,2,"['avail', 'error']","['available', 'error']"
Availability,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:280,avail,available,280,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996,2,['avail'],['available']
Availability,"@cseed I had to make the CloudSQL instance have a public IP in order for testing locally (not in the cluster) to work. Should I get rid of that option? Or can we have a separate test database? As for permissions for the databases, I couldn't find a way to say a specific user could not create a database. I think we can lock down a database with SQL commands after the database has been created. This will be good to discuss on Monday. See #5615",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473591074:325,down,down,325,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473591074,1,['down'],['down']
Availability,@cseed I shrunk the PR down to the minimal code change and it's still failing the `LDPruneSuite.testNoPrune` test. Do I misunderstand how to use the Encoder/Decorder stuff?. The failure isn't an assertion error. I've somehow changed the number of records that result from an LDPrune.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3391#issuecomment-382377269:23,down,down,23,https://hail.is,https://github.com/hail-is/hail/pull/3391#issuecomment-382377269,3,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,@cseed I think we need to bump the Batch test time limit to 360. The tests passed in 328 seconds but wait-for.py marked it as a failure as it was over 300 seconds.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6015#issuecomment-489803786:128,failure,failure,128,https://hail.is,https://github.com/hail-is/hail/pull/6015#issuecomment-489803786,1,['failure'],['failure']
Availability,"@cseed I'd like to turn on the key checking again for the time being because we were relying on it for some split_multi stuff that's going to take some amount of new infrastructure to fix properly, and in the meantime it's causing some pretty bad/nonsense errors downstream because things are can be out of order (see #6223). Is that going cause any problems?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424#issuecomment-499240905:256,error,errors,256,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-499240905,2,"['down', 'error']","['downstream', 'errors']"
Availability,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769:203,error,error,203,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769,2,['error'],['error']
Availability,@cseed This is ready for review. Please check the logic for _create_pod. I added a Try/Except that sets the pod to failure if pod_creation fails.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6075#issuecomment-491011207:115,failure,failure,115,https://hail.is,https://github.com/hail-is/hail/pull/6075#issuecomment-491011207,1,['failure'],['failure']
Availability,@cseed is this how it should work? seems odd to put `null` as position. I also notice that `filterSamplesMask` does some custom stuff. Will this change slow things down unnecessarily if `drop_samples` is not used immediately after a read?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2285#issuecomment-335862234:164,down,down,164,https://hail.is,https://github.com/hail-is/hail/pull/2285#issuecomment-335862234,1,['down'],['down']
Availability,@cseed ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1946#issuecomment-313167921:7,ping,ping,7,https://hail.is,https://github.com/hail-is/hail/pull/1946#issuecomment-313167921,3,['ping'],['ping']
Availability,"@cseed ping, stacked PRs have proven unsuccessful in the past, so I'm keeping the rest of my work gated, but I'd like to start moving things into master. Can you take a look at the latest changes and confirm if you're cool punting on points 1-5 until later?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186#issuecomment-375368023:7,ping,ping,7,https://hail.is,https://github.com/hail-is/hail/pull/3186#issuecomment-375368023,1,['ping'],['ping']
Availability,@cseed surely now it will pass; set --ignore fs/google_fs.py in the doctest run. Previous error was caused by the testing of this file (since CI doesn't yet have gcsfs). https://storage.googleapis.com/hail-ci-0-1/ci/4d17fb5a7df0bb9d766eb80f4a2926b3ed7bbb70/564ab40014a5aa349a517a8353d09eac4a5273f7/index.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-485139453:90,error,error,90,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485139453,1,['error'],['error']
Availability,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:1625,error,error,1625,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588,2,['error'],['error']
Availability,"@cseed, this carries anchor style down to `code` elements contained within them. I don't know if this is ideal, but it's at least consistent with anchors and informs the user that they can click the command names.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/643#issuecomment-241108325:34,down,down,34,https://hail.is,https://github.com/hail-is/hail/pull/643#issuecomment-241108325,1,['down'],['down']
Availability,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:679,down,download,679,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857,1,['down'],['download']
Availability,@cseed: Is this issue for the files that are known to exist but get 404 errors (due to capitalization with the importbgen etc. files) or all 404 errors? I don't think we can remove the 404 errors from the console for any command that doesn't have a corresponding markdown file in the commands directory. See this link: http://stackoverflow.com/questions/7035466/check-if-file-exists-but-prevent-404-error-in-console-from-showing-up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/606#issuecomment-240906565:72,error,errors,72,https://hail.is,https://github.com/hail-is/hail/issues/606#issuecomment-240906565,4,['error'],"['error-in-console-from-showing-up', 'errors']"
Availability,"@cseed: It would be great if we could merge this into master soon -- there's a lot of changes here!. Highlight of major changes:; 1. Dosage is implemented in Genotype.scala; - A user can get either dosages `.dosage` or PLs `.pl`; - To go from PLs to Dosages: rescale each PL (10^(-PL/10)), take the sum of the rescaled numbers, then divide by the sum. This is assuming equal weights prior (can incorporate alternate prior later); - To go from Dosages to PLs: same transformation as before; 2. INFO score is implemented in variantqc; - No tests for info score yet as still uncertain which method to use; - My computation agrees with SNPTEST but not QCTOOL; 3. `importgen` and `exportgen` are now implemented; 4. SplitMulti will split dosages correctly except for the setting of false ref. If the original dosage with N genotypes had more than one maximum value [ex: (0.2, 0.2, 0.1, 0.1, 0.1, 0.3)], then the original genotype is -1. But after combining dosages, then there is one unique maximum value. The fakeref flag is not set in this case, but the genotype is > 0.; 5. A randomly generated genotype can have two values very close together (0.4035, 0.4036, 0.2...) that when read back in via gen file or bgen file will have rounding error (0.4035, 0.4035, 0.2...) so there is no maximal genotype anymore (gt = -1). I don't think this is a huge concern as it can only happen if the max dosage is <= 0.5, and these will get filtered out by most users anyways. **To-Do:; 1. Finalize INFO score calculation and write tests; 2. Fix null variant in PLINK code (want to do this in separate branch); 3. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 4. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 5. Update the readFam function in `importplink` to utilize functionality Jon wrote already",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/243#issuecomment-218212906:1235,error,error,1235,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-218212906,1,['error'],['error']
Availability,@daniel-goldstein I found an error in async_cancel that was exposed by the new version of nest_asyncio. I'm 99% sure the change is correct. Feel free to ask Dan to double check it. The issue was that `fetch_coro` is a Task and not a Future. Cancelling a task just adds the cancellation event to the event loop. You have to actually wait for it to finish before the state of the task will be cancelled. Dan wrote a bunch of tests that asserted `cancel` results in `cancelled == True`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705#issuecomment-888513583:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/10705#issuecomment-888513583,1,['error'],['error']
Availability,"@daniel-goldstein The logs show this:. ```; insertId: ""88db8ey9nom69366""; jsonPayload: {; asctime: ""2022-02-16 23:07:40,794""; exc_info: ""Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 65, in notify_batch_job_complete; await request(session); File ""/usr/local/lib/python3.7/dist-packages/batch/driver/job.py"", line 56, in request; await session.post(callback, json=batch_record_to_dict(record)); File ""/usr/local/lib/python3.7/dist-packages/batch/batch.py"", line 18, in batch_record_to_dict; elif record['n_failed'] > 0:; KeyError: 'n_failed'""; filename: ""job.py""; funcNameAndLine: ""notify_batch_job_complete:69""; hail_log: 1; levelname: ""ERROR""; message: ""callback for batch 1731 failed, will not retry.""; ```. `notify_batch_job_complete` just needs to join against the volatile table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1049107348:699,ERROR,ERROR,699,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1049107348,1,['ERROR'],['ERROR']
Availability,@daniel-goldstein can you ping #general when this merges about:; > - (hail#12801) Hitting CTRL-C while interactively using Query on Batch cancels the underlying batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12853#issuecomment-1500328024:26,ping,ping,26,https://hail.is,https://github.com/hail-is/hail/pull/12853#issuecomment-1500328024,1,['ping'],['ping']
Availability,@daniel-goldstein looks like there's still a test failure,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5686#issuecomment-477168302:50,failure,failure,50,https://hail.is,https://github.com/hail-is/hail/pull/5686#issuecomment-477168302,1,['failure'],['failure']
Availability,"@daniel-goldstein sorry I was a dummy, what I had didn't actually do what I thought it did. It's a bit complex to get access to test information in a fixture, but there's some docs on how to do it. I did that. Now the fixture checks and only tears down a batch when the test failed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13326#issuecomment-1664664775:248,down,down,248,https://hail.is,https://github.com/hail-is/hail/pull/13326#issuecomment-1664664775,1,['down'],['down']
Availability,@daniel-goldstein still got some parser errors in the python tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5662#issuecomment-476883070:40,error,errors,40,https://hail.is,https://github.com/hail-is/hail/pull/5662#issuecomment-476883070,1,['error'],['errors']
Availability,"@danking , I'm a bit stuck on how to proceed with the credential refreshing. Here's the layout of the problem:. 1. In normal Azure, we accept user-provided SAS tokens. Since they are user-provided, we have no way of obtaining new ones and the onus is on the user to obtain a SAS token for however long they expect to need to use it.; 2. This current design in Terra is to not make the user have to do that, because that seems annoying, and for terra-controlled ABS containers we have an endpoint we can hit to get a SAS token. Ok, but now we need to update our Azure FS infrastructure to refresh a credential if it expires. But, we use the azure client lib and don't control all http requests. For example, for `AzureStorageFS.open`, we call `downloader.readall()` if we want to load the whole file into memory. I went spelunking through their source and looks like `readall` mostly wraps a sequence of range reads, but regardless if we were to use that method we would have to catch credential expiration errors, reset credentials on the blob client and retry hoping that we didn't break any invariants -- I don't want to do that as I wouldn't trust a stream that encountered a non-transient error like that. It could be that getting rid of `downloader.readall` is the only thing we have to worry about, but it makes me uneasy not having control of the http requests we're making to ABS. Do you see a solution other than raking through our `aioazure.fs` and making sure that we only use ""quick"" methods and possibly retrying 401s? It just seems to me like we're going against the grain and even though it feels user-hostile the intention of SAS tokens are to have users own credential expiration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715:743,down,downloader,743,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715,4,"['down', 'error']","['downloader', 'error', 'errors']"
Availability,"@danking , no worry. I am aware that my current EMR environment is not as clean as we wish. Not easy to find the right alignment of version of all the components. Thanks for your pointers. ; `spark-shell --version` was the right spot. It appears that I am runing on `scala 2.12.13` and that would explain the error as you mentioned above . > That SettingsOps is an implicit nested class of the MutableSettings object. It is definitely present in [2.13](https://github.com/scala/scala/blob/2.13.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L70-L88) and [2.12](https://github.com/scala/scala/blob/2.12.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L83-L94). It appears to be missing in [2.11](https://github.com/scala/scala/blob/2.11.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L64-L68). It appears to have arrived in [2.12.14](https://github.com/scala/scala/commit/3bd24299fc34e5c3a480206c9798c055ca3a3439). Let me work on that and find a compatible scala version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1767473029:309,error,error,309,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1767473029,1,['error'],['error']
Availability,"@danking . I can keep digging, but it looks like the ParamSpec feature isn't available in the Python version we're running. I'll see if there's a workaround, but I'd really not like to hold up this change anymore than needed. ```; Module ""typing"" has no attribute ""ParamSpec""; maybe ""_ParamSpec""?; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13237#issuecomment-1632525112:77,avail,available,77,https://hail.is,https://github.com/hail-is/hail/pull/13237#issuecomment-1632525112,1,['avail'],['available']
Availability,@danking Any idea why the new `hail-run` image would error on just the `test_python_docs` step with the error about spark not finding the hail jar?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13038#issuecomment-1551890279:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/13038#issuecomment-1551890279,2,['error'],['error']
Availability,@danking Can you let me know what I'm supposed to do here with these security errors? Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12794#issuecomment-1505755543:78,error,errors,78,https://hail.is,https://github.com/hail-is/hail/pull/12794#issuecomment-1505755543,1,['error'],['errors']
Availability,@danking Do you want me to adopt this PR? It looks like there's a bunch of annoying errors to fix with the doctests as well as maybe a missing field in the spec?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14224#issuecomment-1919651215:84,error,errors,84,https://hail.is,https://github.com/hail-is/hail/pull/14224#issuecomment-1919651215,1,['error'],['errors']
Availability,@danking Erroring in CI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10114#issuecomment-786693004:9,Error,Erroring,9,https://hail.is,https://github.com/hail-is/hail/pull/10114#issuecomment-786693004,1,['Error'],['Erroring']
Availability,@danking Have you actually seen anymore of these errors after your fix of the hadoop tests erasing each others' files? I considering closing this because I've not seen further evidence that hadoop isn't properly addressing transient errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12411#issuecomment-1349563987:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/12411#issuecomment-1349563987,2,['error'],['errors']
Availability,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:135,heartbeat,heartbeat,135,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840,1,['heartbeat'],['heartbeat']
Availability,@danking I made a fix that should make it so we don't need to manually delete instances and batch should recover.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8220#issuecomment-593645975:105,recover,recover,105,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593645975,1,['recover'],['recover']
Availability,"@danking I then get; ```Error summary: HailException: expression has wrong type: expected `Int', got Boolean```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2178#issuecomment-327032041:24,Error,Error,24,https://hail.is,https://github.com/hail-is/hail/issues/2178#issuecomment-327032041,1,['Error'],['Error']
Availability,"@danking I try from the last commin of Hail. That seems to solve the issue of Spark version but not the java error... ```; // Setup EMR + python 3.9 + java 11 without installin hail; // Check pyspark; $ pyspark --version; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21. // Clone last commit of Hail & install; $ export PATH=$PATH:/home/hadoop/.local/bin; $ cd /tmp; $ git clone --depth 1 https://github.com/broadinstitute/hail.git; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; // Check pyspark; $ pyspark --version; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21. // Create symlink to hail-all-spark.jar; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python${PYTHON_VERSION}/site-packages/hail/backend /opt/hail/backend; // Launch spark-shell; $ spark-shell; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings scala.reflect.internal.settings.MutableSettings$.SettingsOps(scala.reflect.internal.settings.MutableSettings)'; ```. Could it be a problem of PATH ? issue with where Hail is installed ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1775200581:109,error,error,109,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1775200581,1,['error'],['error']
Availability,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952,1,['error'],['error']
Availability,"@danking IIUC the TeamCity build is now working with spark-2.1.0 but not spark-2.0.2; (even though running `./gradlew shadowJar archiveZip` on my laptop with spark-2.0.2 works fine.). From looking at the Maven repo; https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11; and the elasticsearch-spark connector docs; https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html; there's no indication that some versions only support v2.1, though it does say; ```; elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234:604,avail,available,604,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234,2,['avail'],['available']
Availability,"@danking Looks like I'm still failing to configure a couple of settings related to references on the `ServiceBackend` but you can feel free to start looking. You'll notice that I made quite a substantial refactor in `ServiceBackend.scala` in an attempt to harmonize the scala backends a bit more. The rationale behind the refactor is I was having a hard time working with the various thunks passed around there. I saw them as a bit of poor-man's-object way to capture some state from the input file while keeping the `ServiceBackend` stateless. IMO there's no harm in keeping the `ServiceBackend` just as stateful as the other backends since it is single use. So I lifted a lot of that state into backend-creation time and created a harder delineation between which part of the input is for configuring the backend and which part is for the action being performed. This made it easier to reuse a couple of methods like `tableType` and such. I'm happy to take suggestions on ways to trim down this PR, but I thought you'd want to take a look at the whole thing given the time-sensitivity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995:987,down,down,987,https://hail.is,https://github.com/hail-is/hail/pull/13797#issuecomment-1766521995,1,['down'],['down']
Availability,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:774,error,errors,774,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071,1,['error'],['errors']
Availability,@danking Should I be able to see the `ci-test` failure details when using my Broad affiliation?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11698#issuecomment-1087141587:47,failure,failure,47,https://hail.is,https://github.com/hail-is/hail/pull/11698#issuecomment-1087141587,1,['failure'],['failure']
Availability,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:86,avail,available,86,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154,3,"['Down', 'avail']","['Downloading', 'available']"
Availability,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:154,error,error,154,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888,1,['error'],['error']
Availability,"@danking This is far from done, evidenced by the fact that for some reason I'm getting tons of QoB test failures (but some are passing! :/) but I would appreciate a first pass on this if you want to do a high-level review. I do have a rough RFC that is not up to date, so let me know if you'd prefer to start the discussion from such a doc instead of the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1581539600:104,failure,failures,104,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1581539600,1,['failure'],['failures']
Availability,@danking What do you think about having a version ID inside the JAR file (MANIFEST???). We already download the JAR file on the worker. Not sure how much extra time it would be to look for the version inside the JAR (maybe cache this?) and then pass the right argument configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909:99,down,download,99,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909,1,['down'],['download']
Availability,"@danking What do you think about this change? I think it's fine, but if it's not fine, then we need to lock down this feature as anyone can just set it to True in the batch_client.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767596412:108,down,down,108,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767596412,1,['down'],['down']
Availability,"@danking and I are giving feedback on this branch. Patrick, the test failure is due to you testing on files in scratch that are only local to your system. You should remove the test annotation @Test on scratch before pushing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904:69,failure,failure,69,https://hail.is,https://github.com/hail-is/hail/pull/1697#issuecomment-297083904,2,['failure'],['failure']
Availability,"@danking i'm also not really certain why this works haha, i pretty much just did a bunch of trial and error. it would probably be worth taking a deeper look at how we manage our jvm dependencies (especially spark) at some point, but i figured it made sense to just revert a couple lines to fix the bug in the short term",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13759#issuecomment-1745640178:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1745640178,1,['error'],['error']
Availability,@danking off by 1 error in test:; `org.scalatest.exceptions.TestFailedException: Some(-9223372036854775807) did not contain -9223372036854775808`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1115#issuecomment-262351018:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/1115#issuecomment-262351018,1,['error'],['error']
Availability,"@danking once this builds again, can you see whether the docs failure is related to your change? it could have also been disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1598#issuecomment-290948269:62,failure,failure,62,https://hail.is,https://github.com/hail-is/hail/pull/1598#issuecomment-290948269,1,['failure'],['failure']
Availability,@danking we should merge the delta change anyway as that's a bug that may result in test failures in the future (I just made PR of fix to 0.1). I think the log reg change is fine to go in as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-326109741:89,failure,failures,89,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-326109741,1,['failure'],['failures']
Availability,"@danking. Here what I have done in my environment ( AWS EMR ); * Create EMR without installing hail; * Update PATH ( this is needed or I get an error with `hailctl not found` at the installation step); ```sh; export PATH=$PATH:/home/hadoop/.local/bin; ```; * Clone latest commit of Hail; ```sh; cd /tmp; git clone --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; ```; * Edit `build.gradle` and add `exclude group: 'org.scala-lang', module: 'scala-reflect'`; * Build Hail; ```sh; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```; * Symlink hail-all-spark.jar into /opt ( At the EMR creation step (before hail installation) I edit the `spark-defaults` properties in order to link `hail-all-spark.jar`... This config was needed & works successfuly for an old version of Hail (0.2.60)... can be revisit if not appropriate for recent version; ```sh; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * start pyspark; ```sh; $ pyspark; Python 3.9.18 (main, Oct 25 2023, 05:26:35) ; [GCC 7.3.1 20180712 (Red Hat 7.3.1-17)] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:144,error,error,144,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['error'],['error']
Availability,"@danking; ```; [root@mg hail]# echo $HAIL_HOME; /opt/Software/hail; [root@mg hail]# echo $PYTHONPATH; :/opt/Software/hail/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip; [root@mg hail]# cd /opt/Software/hail/python; [root@mg python]# ls; hail; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python; [root@mg python]# ls; docs lib MANIFEST.in pylintrc pyspark README.md run-tests run-tests.py setup.cfg setup.py test_support; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/; [root@mg lib]# ls; py4j-0.10.4-src.zip PY4J_LICENSE.txt pyspark.zip; [root@mg lib]# echo $SPARK_CLASSPATH; /opt/Software/hail/build/libs/hail-all-spark.jar; [root@mg lib]# cd /opt/Software/hail/build/libs/; [root@mg libs]# ls; hail-all-spark.jar; ```; the configuration file:; ```; export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177:31,echo,echo,31,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177,4,['echo'],['echo']
Availability,@ihelbig Did you install through brew or pip? Normally we recommend downloading the official Spark distribution. @danking Did you install through brew? Did you see anything like this?. /Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py:77 is trying to start Spark by invoking $SPARK_HOME/bin/spark-submit. What is $SPARK_HOME?. You might modify java_gateway.py before like 77 to print out `command` to see what command in detail it is trying to invoke.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319701721:68,down,downloading,68,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319701721,1,['down'],['downloading']
Availability,@iitalics can you ping me once this is rebased?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6822#issuecomment-531877691:18,ping,ping,18,https://hail.is,https://github.com/hail-is/hail/pull/6822#issuecomment-531877691,1,['ping'],['ping']
Availability,"@iris-garden Let me know if you want to meet to get a low-down on what's happening here, I realize I'm asking for a review on something only dan and I ever really laid eyes on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14450#issuecomment-2040359083:58,down,down,58,https://hail.is,https://github.com/hail-is/hail/pull/14450#issuecomment-2040359083,1,['down'],['down']
Availability,@iris-garden do you have a sense of what work remains to be done here or whether this branch is worth reviving? Just had a support issue because of a `Hail Internal Error: … 404 …` that turned out to be the worker jobs ran out of memory,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12468#issuecomment-2161303529:165,Error,Error,165,https://hail.is,https://github.com/hail-is/hail/pull/12468#issuecomment-2161303529,1,['Error'],['Error']
Availability,"@jbloom I passed a 500x500 matrix and a 5000x5000 matrix of zeros from Scala. The 500x500 matrix took ~.3 seconds, and the 5000x5000 matrix took ~30 seconds to pass through. Almost all of this is spent retrieving the byte array from Java -> Python, so I don't know if we'll be able to get this noticeably faster. The 0x100000 threshold for breaking up the blocks of bytes (semi-arbitrarily) because larger blocks caused me to run out of heap space, but wiggling that value up and down seems not to really affect the amount of time it takes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2751#issuecomment-359146225:480,down,down,480,https://hail.is,https://github.com/hail-is/hail/pull/2751#issuecomment-359146225,1,['down'],['down']
Availability,@jbloom this is my fault. I changed the expression to code without ensuring the VDS actually had the relevant fields. The doc test correctly points this out. I'll fix Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1598#issuecomment-290987968:19,fault,fault,19,https://hail.is,https://github.com/hail-is/hail/pull/1598#issuecomment-290987968,1,['fault'],['fault']
Availability,"@jbloom22 : Back to you. To remove the red error box on the plot output, I ended up adding CSS to hide the stderr div elements in the HTML.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374#issuecomment-280135164:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-280135164,1,['error'],['error']
Availability,@jbloom22 can you give the latest commit on this PR a second set of eyes? I had to fiddle with some tolerances due to https://storage.googleapis.com/hail-ci-0-1/ci/7a0732726e6873e2c0d85fed5183324ac9441d52/194ea22cd9f744a5463340130e799c8a65ca885e/index.html . I expected the test I added (`test_pcrelate_issue_5263`) to be exactly the same but it differed out at the 10th or so position.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5250#issuecomment-461123275:100,toler,tolerances,100,https://hail.is,https://github.com/hail-is/hail/pull/5250#issuecomment-461123275,1,['toler'],['tolerances']
Availability,@jbloom22 ping a ding ding,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3348#issuecomment-380924677:10,ping,ping,10,https://hail.is,https://github.com/hail-is/hail/pull/3348#issuecomment-380924677,1,['ping'],['ping']
Availability,"@jigold Do I understand correctly that g2-standard-4 can only be created as a job private instance because there is no matching pool? If that's right, it looks like, unlike pool jobs, a JPIM job [will be correctly marked as error](https://github.com/hail-is/hail/blob/main/batch/batch/driver/instance_collection/job_private.py#L457-L467) if there are no available regions. Assuming all that is correct, do I also understand correctly that the only reason to block incoming jobs at the front-end is for a better user experience, not to protect the system from bad data? If yes, then I agree that need not be part of this PR because it merely improves user experience rather than being critical for correct functioning of the system. Are any of my assumptions or inferences wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430#issuecomment-1725778672:224,error,error,224,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1725778672,4,"['avail', 'error']","['available', 'error']"
Availability,"@jigold Doesn't this suggest that the error's message is `'job_id'`? ; ```; > assert data['check_resource_aggregation_error'] is None, data; E AssertionError: {'check_incremental_error': None, 'check_resource_aggregation_error': ""'job_id'""}; E assert ""'job_id'"" is None; ```; The select statement for `attempt_by_job_group_resources` doesn't include a `job_id` column.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1940328506:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1940328506,1,['error'],['error']
Availability,@jigold I added a commit with many changes. Sorry. There were several broken links. I fixed all of them and enabled `nitpicky` which forces Sphinx to throw an error on all broken references. I also removed an unnecessary `rm -rf` from the Makefile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9198#issuecomment-673110373:159,error,error,159,https://hail.is,https://github.com/hail-is/hail/pull/9198#issuecomment-673110373,1,['error'],['error']
Availability,"@jigold I stood up batch/ci in my own project from `hail-is/hail:main` and then deployed this branch, taking notes of any changes I needed to make and all seemed to work out OK. I think that's about as much as I can properly test this without trying things out in haildev/hail-vdc. The steps were as follows:. 1. Generate the configmaps used by gateway/internal-gateway. These will have the routing configuration for production services (I've edited the bootstrap instructions to match); `make -C gateway envoy-xds-config && make -C internal-gateway envoy-xds-config`; 2. … wait a few seconds for CI to quietly update these configmaps with information about testing namespaces … (can manually verify changes with `download-configmap gateway-xds-config`); 3. Deploy the new versions of gateway/internal-gateway; `make -C gateway deploy NAMESPACE=default && make -C internal-gateway NAMESPACE=default`. This worked for me in my project with no downtime, but either way I would probably do the same thing as with the previous PR where I test it in azure before making changes to hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346:714,down,download-configmap,714,https://hail.is,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346,2,"['down', 'downtime']","['download-configmap', 'downtime']"
Availability,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145:257,down,download,257,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145,2,['down'],['download']
Availability,@jigold So the pylint errors are now resolved?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471734576:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471734576,1,['error'],['errors']
Availability,@jigold This should fix your build failure :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1232#issuecomment-271977973:35,failure,failure,35,https://hail.is,https://github.com/hail-is/hail/pull/1232#issuecomment-271977973,1,['failure'],['failure']
Availability,@jigold could your recent changes have prevented doctest from flagging this? Erm. Well. This must be a really old error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3700#issuecomment-394033932:114,error,error,114,https://hail.is,https://github.com/hail-is/hail/pull/3700#issuecomment-394033932,1,['error'],['error']
Availability,"@jigold https://azure.github.io/Storage/docs/application-and-user-data/code-samples/concurrent-uploads-with-versioning. Looks like this is the expected behavior from Azure Blob Storage when concurrently uploading blobs. It seems like *all* blocks are purged when any single upload succeeds. Unfortunately, it doesn't seem possible to distinguish between a truly invalid block list and this transiently invalid block list. I dislike this interface, but it is what we have. It seems to me that the right fix is to deduplicate the file list before uploading. Multiple files uploading to the same target should be an error if they're different source files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1882088862:613,error,error,613,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1882088862,1,['error'],['error']
Availability,@jigold ready; I also fixed the recently added `echo` image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9594#issuecomment-724108391:48,echo,echo,48,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-724108391,1,['echo'],['echo']
Availability,@jigold rightly pointed out I didn't add a regression test for this. @jigold next time request changes! @jbloom22 next time request tests! @cseed write tests for failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1272#issuecomment-273972707:162,failure,failures,162,https://hail.is,https://github.com/hail-is/hail/pull/1272#issuecomment-273972707,1,['failure'],['failures']
Availability,@jigold sorry I pushed a change literally as you approved it! It was just removing a redundant cast of `this.asInstanceOf[ReferenceGenome]` that was called on a ReferenceGenome object.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6947#issuecomment-525030201:85,redundant,redundant,85,https://hail.is,https://github.com/hail-is/hail/pull/6947#issuecomment-525030201,1,['redundant'],['redundant']
Availability,"@jigold sorry about that CI frigged up, but things look good now, there's an error:; ```; =================================== FAILURES ===================================; ___________________ [doctest] hail.methods.impex.import_bgen ___________________; [gw0] linux -- Python 3.6.6 /home/hail/.conda/envs/hail/bin/python; 067 ; 068 Import a BGEN file as a matrix table with genotype dosage entry field:; 069 ; 070 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; 071 ... entry_fields=['dosage'],; 072 ... sample_file=""data/example.8bits.sample""); 073 ; 074 Load a single variant from a BGEN file:; 075 ; 076 >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; UNEXPECTED EXCEPTION: TypeError(""import_bgen: parameter 'variants': expected (None or Sequence[hail.utils.struct.Struct] or hail.expr.expressions.typed_expressions.StructExpression or hail.table.Table), found list: [<StructExpression of type struct{locus: locus<GRCh37>, alleles: array<str>}>]"",); Traceback (most recent call last):. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 487, in check_all; args_.append(checker.check(arg, name, arg_name)). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 59, in check; raise TypecheckFailure(). hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):. File ""/home/hail/.conda/envs/hail/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.methods.impex.import_bgen[2]>"", line 4, in <module>. File ""<decorator-gen-904>"", line 2, in import_bgen. File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 559, in wrapper; args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method). File ""/hail/repo/hail/build/tmp/doctest/python/hail/typecheck/check.py"", line 513, in check_all; )) from e. TypeError: import_bgen: parameter 'variants': exp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590:77,error,error,77,https://hail.is,https://github.com/hail-is/hail/pull/4291#issuecomment-421486590,2,"['FAILURE', 'error']","['FAILURES', 'error']"
Availability,"@jigold test are fixed, properly clean up; I think the test should be improved to check that deletion properly cleans up expected resources (rather than simply doesn't throw an error, which will happen if deletion fails for any reason other than 404), but I think that could wait for a subsequent PR, because as written, the only way they will fail to do so is if the wrong name or namespace are supplied (else they will throw an error and the test will fail).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367:177,error,error,177,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478175367,4,['error'],['error']
Availability,"@jigold this jogged a memory, is there, perhaps, a really bad error message for `g.ad.sum()`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/846#issuecomment-249638782:62,error,error,62,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249638782,1,['error'],['error']
Availability,"@jigold to address your points from before:; 1. Pairing db update and/or self._pod_name with the k8s call, this might be a good idea but I think is orthogonal to this change. I want to focus on cleaning up k8s use and getting pods deleted, then think a little harder about how we should restructure the code more generally to be more understandable; 2. I always return at least the error so that the calling code has to decide what an error means; 3. I now always delete the pod after the db update, which I think is right b/c we don't want the refresh loop to recreate the pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954:382,error,error,382,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954,2,['error'],['error']
Availability,"@jjfarrell I've been working with some other Broadies that also noted unusual results in Hail's PC-Relate. In this case, we found that about a quarter of variants being used in PC-Relate had terrible HWE p-values. We're tracking this down a bit as a possible explanation for the poor performance of PC-Relate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-388150598:234,down,down,234,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-388150598,1,['down'],['down']
Availability,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992:439,down,down,439,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992,4,['down'],['down']
Availability,"@jjfarrell Thanks for the information! This will be very helpful as I try to tease out what the issue is here. Also, I'm sorry my initial response was curt! I was a bit tired at the time and probably shouldn't have been responding to GitHub issues 😅. Hail's version of pc-relate does not identify an initial set of related and unrelated individuals. The R `pcrelate` implementation (the official / reference implementation by the authors of the paper) does this to identify a set of individuals on which to run the principal components analysis. It is not entirely clear to me why this is necessary, and we don't currently have a mechanism for doing so (since pc_relate _is_ our mechanism for determining related and unrelated individuals when there is population structure in the data set). If you have prior knowledge about related samples, you might try filtering to an known unrelated set and computing the scores from that set. I'm curious if that makes any difference in the results. Your invocations look very reasonable. I'll get in touch with the gnomAD team here at the broad to learn more about their experiences with pc_relate and see if I can better understand what's happening with the replicate samples. It's definitely possible there is an implementation error; however, I also want to rule out that the pc_relate model itself isn't breaking down here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-386639531:1271,error,error,1271,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-386639531,4,"['down', 'error']","['down', 'error']"
Availability,"@jmarshall another way to do this might be to ignore/do nothing during the initial call of `job.command('')` when the argument is empty. Assuming we never want to do anything for empty/None commands we might as well eliminate them at source. That way you should only have to handle the situation in one place, and not have to handle the invalid data structure in multiple downstream places. I do like the idea of fixing this in the hail library, but you could also consider altering your upstream code so that it never makes an invalid ""`job.command('')`"" call in the first place. Eg by doing something like ; ```py; job.command('touch before'); for msg in messages:; job.command(f'echo {shlex.quote(msg)}); job.command('touch after'); ```. (assuming it doesn't matter to you whether the messages are handled in the same section of the resulting command or not). What do you think?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2374458692:372,down,downstream,372,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2374458692,2,"['down', 'echo']","['downstream', 'echo']"
Availability,"@jmarshall, thanks doing this - would you mind adding a simple unit test to lock down the behaviour?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2397543555:81,down,down,81,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2397543555,2,['down'],['down']
Availability,"@johnc1231 I uncommented some of the tests that I'd commented out before because they don't hit the prune error. I had to implement some stuff on PNDArray (mostly copy, and adding a case for setRequired) in order to make it work; let me know if those were missing for a reason and I'll take it out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8073#issuecomment-585372375:106,error,error,106,https://hail.is,https://github.com/hail-is/hail/pull/8073#issuecomment-585372375,1,['error'],['error']
Availability,@johnc1231 oh john this is definitely not your fault :P I lead you astray,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2274#issuecomment-333975584:47,fault,fault,47,https://hail.is,https://github.com/hail-is/hail/pull/2274#issuecomment-333975584,1,['fault'],['fault']
Availability,@konradjk @jtkoskel Ah the issue is two-fold: the compiler doesn't automatically cast types to booleans AND (perhaps more importantly) it doesn't signal a type error when the condition of an if expression is not a boolean. I'll look into this more on Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2178#issuecomment-326819470:160,error,error,160,https://hail.is,https://github.com/hail-is/hail/issues/2178#issuecomment-326819470,1,['error'],['error']
Availability,@konradjk I think this error would have been specific enough for you to figure out what the problem was pretty quickly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4088#issuecomment-410759919:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/4088#issuecomment-410759919,1,['error'],['error']
Availability,"@konradjk pssh, what is this ""server"" you speak of. The kubernetes pod indeed needs a newer version of cloud tools. We always grab the latest when running the hail PR jobs. Unfortunately the deploy for cloud tools on python3 was broken. That's being fixed by https://github.com/Nealelab/cloudtools/pull/101. `pip`, unhelpfully, tells you the latest version is X.Y even if X.Y isn't available for your version of python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422134593:382,avail,available,382,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422134593,1,['avail'],['available']
Availability,"@lfrancioli look at the built docs (under TeamCity, artifacts, index):; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/9716:id/www/hail/types.html#set-t. There is an issue of variable naming: your a is implicit (not named), and your b is our a. So for example:; ```; add(a: T): Set[T] – Returns the result of adding the element b to Set a.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672:102,down,download,102,https://hail.is,https://github.com/hail-is/hail/pull/1491#issuecomment-284773672,1,['down'],['download']
Availability,"@lfrancioli see the docs, `parallel` takes a string. The PR adds a typechecker to give a better error message to future you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033#issuecomment-408956343:96,error,error,96,https://hail.is,https://github.com/hail-is/hail/issues/4033#issuecomment-408956343,1,['error'],['error']
Availability,@lgruen are y'all running with this change now? I was vaguely concerned that with ~32 JVMs alive that would negatively impact the machine. Do you find that the JVM's RAM gets swapped out and the machines are generally stable?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12248#issuecomment-1266128264:91,alive,alive,91,https://hail.is,https://github.com/hail-is/hail/pull/12248#issuecomment-1266128264,1,['alive'],['alive']
Availability,"@liameabbott I think you should go ahead and merge #3859. Once this is in, you can then use `locus_windows` to simplify, reduce memory req, and be more robust to catching out-of-order loci.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022:152,robust,robust,152,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022,2,['robust'],['robust']
Availability,"@maccum I made block_size an optional parameter, can you take a quick look before i merge? Using the larger block size fixed hadoop failure in UKBB case, but a smaller block size may still be preferable for fewer samples to increase write parallelism, so best to make it settable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697#issuecomment-393947795:132,failure,failure,132,https://hail.is,https://github.com/hail-is/hail/pull/3697#issuecomment-393947795,1,['failure'],['failure']
Availability,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222:391,echo,echo,391,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222,12,['echo'],['echo']
Availability,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:91,echo,echo,91,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119,4,"['echo', 'failure']","['echo', 'failure']"
Availability,"@natestockham there's been some data science-y investigations on our end, final results not yet ready, but it looks like that test is way over constrained. Our confidence in the LMM method has not yet changed as a result of this failure. I'll be pushing a commit to remove the test failure by Monday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-282453922:229,failure,failure,229,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282453922,2,['failure'],['failure']
Availability,"@patrick-schultz @tpoterba I removed the dependence on #6534 and wrote a test to exercise the IR nodes and all the aggregators we had written. This can now be reviewed/go in independently of the other one, which was failing on a match error in Emit. (I'll request changes to block the other one from going in until we've discussed a plan for using it, since that's now the one that will send everything through the new path.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664:235,error,error,235,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664,1,['error'],['error']
Availability,"@patrick-schultz Are you okay with where the checkpoints are? If yes, then I'll do one last set of benchmarks and then this is ready to go!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12629#issuecomment-1409418048:45,checkpoint,checkpoints,45,https://hail.is,https://github.com/hail-is/hail/pull/12629#issuecomment-1409418048,1,['checkpoint'],['checkpoints']
Availability,"@patrick-schultz Do I understand the error message correctly to mean I need to implement an Interpret step for this? Is that like an hour of work, a day, a week?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13336#issuecomment-1664627297:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/13336#issuecomment-1664627297,1,['error'],['error']
Availability,@patrick-schultz I resolved the conflict by keeping main which seems to have encompassed your change but with a more descriptive error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11003#issuecomment-1476992958:129,error,error,129,https://hail.is,https://github.com/hail-is/hail/pull/11003#issuecomment-1476992958,1,['error'],['error']
Availability,"@patrick-schultz I rewrote the `ReadIterator` stuff as a wrapper for a `Reader` object that actually manages the state. I've left the Region management out of this for now; it's currently pulling the region from the downstream ContextRDD, but the next step is definitely to move the region management into c++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4729#issuecomment-437161860:216,down,downstream,216,https://hail.is,https://github.com/hail-is/hail/pull/4729#issuecomment-437161860,1,['down'],['downstream']
Availability,@patrick-schultz I'm closing this for now because there's something erroring in the python tests that I don't understand. I'll re-open once I've fixed that and am more certain that I actually understand what's happening.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4165#issuecomment-413875178:68,error,erroring,68,https://hail.is,https://github.com/hail-is/hail/pull/4165#issuecomment-413875178,1,['error'],['erroring']
Availability,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:108,error,errors,108,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692,3,"['down', 'error', 'failure']","['down', 'errors', 'failures']"
Availability,@patrick-schultz Lots of test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10848#issuecomment-914526677:30,failure,failures,30,https://hail.is,https://github.com/hail-is/hail/pull/10848#issuecomment-914526677,1,['failure'],['failures']
Availability,"@patrick-schultz OK, I understand now. Can you ping Ed or find someone else to review? Let's get this in post-haste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13939#issuecomment-1785257005:47,ping,ping,47,https://hail.is,https://github.com/hail-is/hail/pull/13939#issuecomment-1785257005,1,['ping'],['ping']
Availability,@patrick-schultz ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4836#issuecomment-445967344:17,ping,ping,17,https://hail.is,https://github.com/hail-is/hail/pull/4836#issuecomment-445967344,2,['ping'],['ping']
Availability,"@patrick-schultz rebase failure, I'll remove.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186#issuecomment-375789384:24,failure,failure,24,https://hail.is,https://github.com/hail-is/hail/pull/3186#issuecomment-375789384,1,['failure'],['failure']
Availability,@patrick-schultz this is downstream of a `.entries()`. The latest errors indicate its a shuffle read error. Seems like shuffling our tiny 1kg downsample blows out spark's memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4242#issuecomment-417780228:25,down,downstream,25,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417780228,4,"['down', 'error']","['downsample', 'downstream', 'error', 'errors']"
Availability,"@ryerobinson, ah! This is because of xargs. You're building a Dockerfile, is that right? From where did you get this Dockerfile?. I think you need `xargs -d '\n'` or `xargs -0` to prevent xargs from splitting on the space between the semicolon and the sys_platform. e.g. in the python:3.8 container, this works:; ```; root@0c1415c108de:/# echo ""uvloop==0.16.0; sys_platform!='win32'"" | xargs -0 python3 -m pip install -U; Collecting uvloop==0.16.0; Downloading uvloop-0.16.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.7 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 3.6 MB/s eta 0:00:00; Installing collected packages: uvloop; Successfully installed uvloop-0.16.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.; You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.; ```; but this does not:; ```; root@0c1415c108de:/# echo ""uvloop==0.16.0; sys_platform!='win32'"" | xargs python3 -m pip install -U; Requirement already satisfied: uvloop==0.16.0 in /usr/local/lib/python3.8/site-packages (0.16.0); ERROR: Could not find a version that satisfies the requirement sys_platform!=win32 (from versions: none); ERROR: No matching distribution found for sys_platform!=win32; WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.; You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255256786:339,echo,echo,339,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255256786,7,"['Down', 'ERROR', 'avail', 'echo']","['Downloading', 'ERROR', 'available', 'echo']"
Availability,"@tlangs Nothing on your end, we've had some CI instability lately that's causing some unrelated failures. I'll make sure to retest this once that's fixed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13129#issuecomment-1581217232:96,failure,failures,96,https://hail.is,https://github.com/hail-is/hail/pull/13129#issuecomment-1581217232,1,['failure'],['failures']
Availability,"@tmwong2003 OK, some progress on the CI front. Thanks for your patience. A team member needs to kick off the CI job. The assigned reviewer will be responsible for that. Right now, the tests involve some sensitive tokens which need more work to be protected, so the CI logs aren't public yet. Again, the assigned reviewer should be able to share the relevant part of the logs for failures, etc. @tpoterba I kicked off the build. Can you take another look, it looks like the comments were addressed. Finally, it looks like the PR history is a bit tangled. Is it possible to clean it up so we can a reasonable commit message? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788:379,failure,failures,379,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-501887788,1,['failure'],['failures']
Availability,"@tpoterba . Hi Tim ,Thank you for answering , I run the ""gradle check --info"" again , there is a lot of infomation, so I save the standard output and standard error in ""check_info_1.txt"",as attached. Thank you for your help. [check_info_1.txt](https://github.com/broadinstitute/hail/files/345679/check_info_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230213155:159,error,error,159,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230213155,1,['error'],['error']
Availability,"@tpoterba . Thanks for pointing out the extra step. ; So I have compiled hail to run on Centos 6 and it is running python scripts fine locally (master=local[*]). However, the following error occurs when running it with yarn. Any suggestions on this? . ```; [Stage 0:> (0 + 1) / 292]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen_count.py in <module>; 10 mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); 11 mt.describe(); ---> 12 print(""Count:"",mt.count()); 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from containe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:185,error,error,185,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['error'],['error']
Availability,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230429438:962,echo,echo,962,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438,1,['echo'],['echo']
Availability,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230191234:878,error,errors,878,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234,1,['error'],['errors']
Availability,"@tpoterba @cseed. I ran into some issues building images that I addressed as a part of this PR. The main issue was that we needed the `beta` `gcloud` commands, so I added a line to the Dockerfile to load those. I also made a couple changes to the Docker build commands to ensure we at least reuse all images that are available in our GCR. I added `hail-pr-builder` as a cache source in an attempt to take advantage of successful local builds. This, unfortunately, does not allow us to reuse successful layers from a failing build. I noticed that if I build and fail once using `--cache-from`, then I cannot re-use the succeeding layers of the failed build, regardless of whether I supplied `--cache-from`. I also added `:latest` to the `docker images` command because I have a couple tagged images from earlier iterations of this script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734:317,avail,available,317,https://hail.is,https://github.com/hail-is/hail/pull/4362#issuecomment-424066734,1,['avail'],['available']
Availability,"@tpoterba @danking We at Databricks are still interested in this. Although Hail's frontend is in Python, it's still useful to publish to maven central. First, it makes the dependency information available. I've seen people write pipelines that are partly in Hail and partly in PySpark and can include Java libraries for things like data sources. There's a lot of tooling for resolving dependency conflicts between different libraries, but they're not very accessible unless all your dependencies are published to maven repos and have dependency poms available. It's also easier to update pipelines to the latest Hail version if the artifacts published to a standard location.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319:195,avail,available,195,https://hail.is,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319,2,['avail'],['available']
Availability,"@tpoterba @jbloom22 I've reviewed this PR but haven't approved it because I was waiting for the other one to go in. If someone pings me when it does, I'm happy to approve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4971#issuecomment-448317746:127,ping,pings,127,https://hail.is,https://github.com/hail-is/hail/pull/4971#issuecomment-448317746,1,['ping'],['pings']
Availability,@tpoterba @jigold ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4720#issuecomment-437412681:18,ping,ping,18,https://hail.is,https://github.com/hail-is/hail/pull/4720#issuecomment-437412681,1,['ping'],['ping']
Availability,"@tpoterba Compilation error, I think maybe you can only mark things that are `val` or `var` transient",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9407#issuecomment-686630822:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/9407#issuecomment-686630822,1,['error'],['error']
Availability,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:67,failure,failures,67,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374,1,['failure'],['failures']
Availability,"@tpoterba Do you have a specific example where this fails? I think the columns are already unkeyed before export with this line:. ```; dataset = dataset._select_all(col_exprs=fam_exprs,; col_key=[],; row_exprs=bim_exprs,; entry_exprs=entry_exprs); ```. I tried making the Python test more robust where I permute the columns first so not in alphabetical order before exporting, but couldn't replicate the error. The same is true for `export_gen`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4508#issuecomment-428976439:289,robust,robust,289,https://hail.is,https://github.com/hail-is/hail/issues/4508#issuecomment-428976439,2,"['error', 'robust']","['error', 'robust']"
Availability,@tpoterba How do I get the java stack trace to debug the error in the docs build (failed in the Tutorial testing)?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1430#issuecomment-282137009:57,error,error,57,https://hail.is,https://github.com/hail-is/hail/pull/1430#issuecomment-282137009,1,['error'],['error']
Availability,"@tpoterba I added [another PR](https://github.com/hail-is/hail/pull/1613) which adds a `./configure` script which walks the user through setting a spark version (in the future we can add other parameters too). Perhaps that's the best way to manage this going forward?. If the gradle.properties file doesn't exist, our gradle script errors and asks the user to run `./configure`. The `./configure` script queries the user for sparkVersion and generates a valid `gradle.properties` file. Afterwards, the user can execute gradle normally without any `-D` parameters. Users may still override the `sparkVersion` variable on the command line by specifying `-PsparkVersion=2.1.1`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582:332,error,errors,332,https://hail.is,https://github.com/hail-is/hail/pull/1608#issuecomment-290198582,1,['error'],['errors']
Availability,"@tpoterba I also added a `hail-ci-build.sh` so we'll build the images in the PRs, ensuring we don't get deploy problems from image build failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4965#issuecomment-447034807:137,failure,failures,137,https://hail.is,https://github.com/hail-is/hail/pull/4965#issuecomment-447034807,1,['failure'],['failures']
Availability,@tpoterba I also got rid of max_shift and (hopefully) made things more robust. Can you take a quick look to see if you're happy with what I did?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2359#issuecomment-339549140:71,robust,robust,71,https://hail.is,https://github.com/hail-is/hail/pull/2359#issuecomment-339549140,1,['robust'],['robust']
Availability,"@tpoterba I had everything working locally, but hadn't updated master. Unfortunately, there's a test failure in the new test you added for `index_globals` and I can't figure out how to fix it. I tried creating the environment separately and the `pli` field was there. Could you please take a look?. ```; _____________ [doctest] hail.matrixtable.MatrixTable.index_globals _____________; [gw0] darwin -- Python 3.6.1 //anaconda/envs/py36/bin/python; UNEXPECTED EXCEPTION: AttributeError(""StructExpression instance has no field, method, or property 'pli'\n Hint: use 'describe()' to show the names of all data fields."",); Traceback (most recent call last):. File ""//anaconda/envs/py36/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.matrixtable.MatrixTable.index_globals[0]>"", line 1, in <module>. File ""/Users/jigold/hail/build/tmp/doctest/python/hail/expr/expressions/typed_expressions.py"", line 1161, in __getattr__; raise AttributeError(get_nice_attr_error(self, item)). AttributeError: StructExpression instance has no field, method, or property 'pli'; Hint: use 'describe()' to show the names of all data fields. /Users/jigold/hail/build/tmp/doctest/python/hail/matrixtable.py:2197: UnexpectedException; ============== 1 failed, 420 passed, 13 skipped in 79.03 seconds ===============; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3620#issuecomment-390841993:101,failure,failure,101,https://hail.is,https://github.com/hail-is/hail/pull/3620#issuecomment-390841993,1,['failure'],['failure']
Availability,@tpoterba I think your error message is much better.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1406#issuecomment-280845674:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280845674,1,['error'],['error']
Availability,"@tpoterba That example loses the singletons (nodes with no edges that are not passed to the maximal_independent_set method). With the old method, we worked around this by collecting all the nodes in python (both nodes with edges and singleton nodes) and then filtering to remove the nodes returned by maximal_independent_set. But collecting all the nodes in python and then passing them to filter_rows is slow. So I'm moving the logic of collecting all the nodes and filtering down to Scala.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2975#issuecomment-368108496:477,down,down,477,https://hail.is,https://github.com/hail-is/hail/pull/2975#issuecomment-368108496,1,['down'],['down']
Availability,"@tpoterba That test doesn't pass, even with this fix:. ```; 2021-04-26 15:57:13 Hail: INFO: Running Hail version 0.2.65-77eb6e1a1cf4; 2021-04-26 15:57:14 Hail: ERROR: error from strategy JvmCompile. java.lang.RuntimeException: unrealizable value unused asymmetrically: eos=false, ped=true. 	at is.hail.expr.ir.streams.StreamProducer$.defineUnusedLabels(EmitStream.scala:29); 	at is.hail.expr.ir.Emit.$anonfun$emitI$68(Emit.scala:1080); 	at is.hail.expr.ir.IEmitCodeGen.map(Emit.scala:304); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:1065); 	at is.hail.expr.ir.Emit.emitInNewBuilder$1(Emit.scala:674); 	at is.hail.expr.ir.Emit.$anonfun$emitI$26(Emit.scala:816); 	at is.hail.expr.ir.EmitCode$.fromI(Emit.scala:391); 	at is.hail.expr.ir.Emit.$anonfun$emitI$25(Emit.scala:816); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); 	at scala.collection.AbstractTraversable.map(Traversable.scala:108); 	at is.hail.expr.ir.Emit.emitI(Emit.scala:815); 	at is.hail.expr.ir.Emit$.$anonfun$apply$4(Emit.scala:99); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:24); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1044); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1192); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:97); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:160,ERROR,ERROR,160,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"@tpoterba fixed the config issue and changed n_partitions to ensure workers are scheduled for the FASTA reading. I tested this on a single batch worker so the jobs overlapped and flexed the shared mount code, but we don't really have a guarantee in our test setup because batch has no way to force collocation of jobs (and even so we can't exactly force that the runtimes will overlap). I suppose if there's an issue here it will bubble up as a nondeterministic failure. Not great but perhaps good enough for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688:462,failure,failure,462,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688,1,['failure'],['failure']
Availability,@tpoterba is it obvious why this went wrong? Are `case` expressions allowed to have missing default statements (and we throw an error if we don't match anything?),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3696#issuecomment-393678445:128,error,error,128,https://hail.is,https://github.com/hail-is/hail/issues/3696#issuecomment-393678445,1,['error'],['error']
Availability,@tpoterba ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-257354062:10,ping,ping,10,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-257354062,1,['ping'],['ping']
Availability,"@tpoterba ping, are you cool with the page I mentioned?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379044933:10,ping,ping,10,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379044933,1,['ping'],['ping']
Availability,"@tpoterba this actually fails tests, am I being overly optimistic about what extract intervals can achieve? The following are the failures and they're all about the number of partitions after a filter.; ```; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_lt[locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_le[locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_ge[locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_gt[locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_lt[locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_le[locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_ge[locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_gt[locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_lt[Locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_le[Locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_ge[Locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_mt_gt[Locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_lt[Locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_le[Locus] - assert 20 == 15; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_ge[Locus] - assert 20 == 6; FAILED test/hail/extract_intervals/test_full_key.py::test_ht_gt[Locus] - assert 20 == 6; ======================================================================================================================================= 16 failed, 34 passed in 19.17s =======================================================================================================================================; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12484#issuecomment-1319322867:130,failure,failures,130,https://hail.is,https://github.com/hail-is/hail/pull/12484#issuecomment-1319322867,1,['failure'],['failures']
Availability,@violetbrina there are just a couple of lint errors left on this branch. If you locally run `make check-ci` you should see the couple of suggestions to fix. I'm also happy to push the fixes to `populationgenomics:azure-upstream` if that's fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12065#issuecomment-1289400014:45,error,errors,45,https://hail.is,https://github.com/hail-is/hail/pull/12065#issuecomment-1289400014,1,['error'],['errors']
Availability,"@vladsaveliev no specific reason, we were just trying to roll up a bunch of updates at once instead of tons of tiny PRs and then got bogged down with some changes that needed to be made. Feel free to PR any version updates and I'll see them through!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520#issuecomment-1123673359:140,down,down,140,https://hail.is,https://github.com/hail-is/hail/pull/11520#issuecomment-1123673359,1,['down'],['down']
Availability,"@zenghz What version of Hail are you using? It must be ancient. We no longer support Spark 1 and haven't for quite some time. We saw this in sporadically in some deployments, but never understood it or developed a reliable workaround. My advice would be to upgrade to Spark 2 and the latest version of Hail if possible. You might search on other forums for ideas/work arounds, for example: https://stackoverflow.com/questions/29960686/parquet-error-when-saving-from-spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578:214,reliab,reliable,214,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578,2,"['error', 'reliab']","['error-when-saving-from-spark', 'reliable']"
Availability,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:35,robust,robust,35,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146,3,"['ERROR', 'down', 'robust']","['ERROR', 'down', 'robust']"
Availability,"A few new test failures coming from better generators, I assume. IBD one looks like a weird corner case where we differ from plink when there are only three variants. I'll look at the rest soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1902#issuecomment-306294818:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/1902#issuecomment-306294818,1,['failure'],['failures']
Availability,"A miracle. It finally passed. That was a real slog. I pushed a bunch of non-trivial changes, so it is probably good if you give a skeptical, fresh look. Summary of new changes:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions. Let me know if you have any questions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8371#issuecomment-609070509:831,down,downcast,831,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-609070509,1,['down'],['downcast']
Availability,"A test failed because `hailctl config unset` now returns an error if the config variable does not exist. Let me know if you think we should maintain the current behavior -- otherwise, I slightly modified the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1677602623:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1677602623,1,['error'],['error']
Availability,"AFAIK, everything has been addressed. Are you referring to [this](https://github.com/hail-is/hail/pull/7875#discussion_r367430472)? Now we user `insert` and catch the duplicate key error instead of the previous `select ... for update` followed by an `insert`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575750888:181,error,error,181,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575750888,1,['error'],['error']
Availability,"AFAIK, the ClientResponseError only contains information from the original request or the headers. The RequestInfo contains information from our request to Google, like the URL. You're right, incorporating this into is_transient_error is a bit complex. We need to somehow communicate the body, if any, to is_transient_error. I started to go down this route with httpx.py. I wanted to wrap ClientSession with a new HailClientSession whose `get`, `post` etc. methods would check for non-successful statuses themselves, read the body, and raise HailHTTPException which included the status code *and the body*. Then in is_transient_error we can look for HailHTTPException and use the body to determine if we should retry. For now, we can just fix the compute client.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-840652602:341,down,down,341,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-840652602,1,['down'],['down']
Availability,AIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:11442,echo,echo,11442,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"ARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:2240,Error,Error,2240,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,1,['Error'],['Error']
Availability,"ARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2735,avail,available,2735,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['avail'],['available']
Availability,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214246,ERROR,ERROR,214246,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"According to @johnc1231 in https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Troubles.20getting.20started.20(Python.203.2E8):. > It's intentionally limited to 3.7 because of precisely that error; >; > John Compitello: This is because of our dependence on spark 2. Spark 3 won't have that restriction, we are in the process of upgrading, hopefully should be fixed next week. I'll close this and wait for the release with Spark 3.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197#issuecomment-800706947:215,error,error,215,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800706947,1,['error'],['error']
Availability,"According to the makefile documentation https://www.gnu.org/software/make/manual/html_node/Splitting-Recipe-Lines.html:; ```; Notice how the backslash/newline pair was removed inside the string quoted with double quotes (""…""), ; but not from the string quoted with single quotes ('…'). This is the way the default shell (/bin/sh) ; handles backslash/newline pairs. If you specify a different shell in your makefiles it may treat them differently.; ```. Seems you (or `brew`) may have configured `make` to use something other than `/bin/sh`.; Quick way to verify:. ```Makefile; .PHONY: print-shell; print-shell:; 	@echo $(SHELL); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1890075303:614,echo,echo,614,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1890075303,1,['echo'],['echo']
Availability,"Actually I get a different error when doing a small reproducible example, but looks related:. ```; ht = hl.utils.range_table(10).annotate_globals(test='yay'); ht2 = hl.utils.range_table(10).annotate_globals(test='yay'); ht.join(ht2).show(); ```; gives:; ```; Hail version: devel-c2508f35dc41; Error summary: HailException: optimization changed type!; before: Table{global:Struct{test:String,test_1:String},key:[idx],row:Struct{idx:Int32}}; after: Table{global:Struct{test:String},key:[idx],row:Struct{idx:Int32}}; Before IR:; ----------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))))); After IR:; ---------; (TableHead 11; (TableJoin inner 1; (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global); (test; (Str ""yay"")))); (TableMapGlobals; (TableRange 10 8); (InsertFields; (Ref Struct{} global))))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420251986,2,"['Error', 'error']","['Error', 'error']"
Availability,"Actually I just went ahead and looked at this PR to make sure highcpus were indeed highcpus. They are!; <img width=""1476"" alt=""Screen Shot 2021-12-09 at 11 56 56 AM"" src=""https://user-images.githubusercontent.com/106194/145441305-fec38573-9c66-4a95-9fb7-0e6dc3a7c2e9.png"">. I also grabbed all the VM details and all the things that should be different (vm name, Nic name, etc.) are different. The userData is myseriously null, but its null for every VM in Azure currently (other PRs, namespaces, and default). ```; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",; ""extendedLocation"": null,; ""extensionsTimeBudget"": null,; ""hardwareProfile"": {; ""vmSize"": ""Standard_F16s_v2"",; ""vmSizeProperties"": null; },; ""host"": null,; ""hostGroup"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/virtualMachines/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""identity"": {; ""principalId"": null,; ""tenantId"": null,; ""type"": ""UserAssigned"",; ""userAssignedIdentities"": {; ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.ManagedIdentity/userAssignedIdentities/batch-worker"": {; ""clientId"": ""890af904-42f1-4136-810a-c52f4e132c6b"",; ""principalId"": ""b952a3bb-1091-4f11-803b-9d5199219a27""; }; }; },; ""instanceView"": null,; ""licenseType"": null,; ""location"": ""eastus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Network/networkInterfaces/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-nic"",; ""primary"": null,; ""resourceGroup"": ""dgoldste""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:581,avail,availabilitySet,581,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['avail'],['availabilitySet']
Availability,Actually looks like just a bad error message. One of the Tables had 0 rows.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4216#issuecomment-416051659:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/issues/4216#issuecomment-416051659,1,['error'],['error']
Availability,"Actually the endpoint does seem to be an issue. https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ works great. edit: Endpoint is / (`+ python3 wait-for.py 60 pr-7381-default-sx9ail9zkm77 Service -p 80 blog --endpoint /`), when I think it should be /blog? We could try setting the endpoint to /blog/ or /default/blog/. The failure also has a line about not being able to connect to hostname blog.pr-7381-default-sx9ail9zkm77. I don't know enough about CI to determine whether this is a problem, but my guess is that is normal. edit2: The wait command's port is 80, not 443. Do we need to force X-Forward-Proto to https to fix it? Although if this is going through gateway, I think the protocol should be https after the redirect from 80/http. edit3: Actually, wait-for.py allows a port to be set, so it seems appropriate to set `port: 443` in the wait command. edit4: Nevermind, 443 will not set protocol to https. It shouldn't matter, I don't think, as long as gateway is redirecting to https, but you could try setting X-Forwarded-Proto to https. I suspect the issue is in the url or domain.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835:332,failure,failure,332,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835,1,['failure'],['failure']
Availability,"Actually, comparing singular vectors is not a robust test, even accounting for sign. Suppose `A` has two equal (or nearly equal) singular values. Then there is a 2-dimensional subspace of vectors, all of which are equally good singular vectors for that singular value. If the singular values are sufficiently separated, then comparing singular vectors should be safe, but I don't think it's necessary; the other checks should force that. I think we only need to check (all approximate comparisons),; * we got the right singular values, by comparing with numpy (unless we constructed a test matrix with known singular values); * the singular vectors are orthonormal (i.e. `Ut U = Id` and `Vt V = Id`); * the factorization `A = U Sigma Vt`. Then it follows that for each right singular vector `V_i`, `A v_i = sigma_i u_i` holds approximately, so `v_i` is a good singular vector, i.e. it really does capture `sigma_i` variance, and we checked that `sigma_i` is close to the true singular value.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936:46,robust,robust,46,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936,1,['robust'],['robust']
Availability,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:227,down,downsampled,227,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052,16,['down'],"['downed', 'downsample', 'downsampled']"
Availability,"Actually, maybe this code just suppresses the error. I was hoping there would be a missing error to retry in `retry_transient_errors`, but all the downloads succeeded.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534#issuecomment-853101143:46,error,error,46,https://hail.is,https://github.com/hail-is/hail/pull/10534#issuecomment-853101143,3,"['down', 'error']","['downloads', 'error']"
Availability,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:270,error,errors,270,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671,2,['error'],"['error', 'errors']"
Availability,"Added some stuff, since `stop` didn't actually work on `LocalBackend` since we weren't shutting down the java gateway",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11669#issuecomment-1079149099:96,down,down,96,https://hail.is,https://github.com/hail-is/hail/pull/11669#issuecomment-1079149099,1,['down'],['down']
Availability,"Adding ""WIP"" tag since I had a test failure from `make test-dataproc`. Seems like I have a mix of reference genomes somewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8268#issuecomment-596564071:36,failure,failure,36,https://hail.is,https://github.com/hail-is/hail/pull/8268#issuecomment-596564071,1,['failure'],['failure']
Availability,"Adding a simple reproducible example. ```python; ht = hl.Table.from_pandas(pd.DataFrame({""variant"":['chr1:123:C:T']})); ht = ht.key_by(**hl.parse_variant(ht.variant)); pd_table = ht.to_pandas(); pd_table.to_pickle(os.path.join(bucket, 'test.pkl')); ```. The two examples below do not cause the same error. ; ```python; ht = hl.Table.from_pandas(pd.DataFrame({""foo"":['bar']})); ht = hl.Table.from_pandas(pd.DataFrame({""foo"":[1, 2, 3]})); ```. Hope this helps.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004#issuecomment-1808416604:299,error,error,299,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1808416604,2,['error'],['error']
Availability,Addressed comments (apart from question on tolerance),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2248#issuecomment-332370854:43,toler,tolerance,43,https://hail.is,https://github.com/hail-is/hail/pull/2248#issuecomment-332370854,1,['toler'],['tolerance']
Availability,Addressed comments. ; - Refactored to a separate module and added module-level tests. ; - Cleaned up TypeChecker interface to call recursively down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790:143,down,down,143,https://hail.is,https://github.com/hail-is/hail/pull/1727#issuecomment-299293790,1,['down'],['down']
Availability,Addressed comments. Rebooting IntelliJ didn't fix it. I added some extra braces and now it looks correct. Also included @danking's suggestion.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/885#issuecomment-251244777:20,Reboot,Rebooting,20,https://hail.is,https://github.com/hail-is/hail/pull/885#issuecomment-251244777,1,['Reboot'],['Rebooting']
Availability,"Addressed your initial problem and merge conflicts, but looks like there's more that's going wrong. Seems like it's mostly data comparison failures with reasonableish looking data, probably a striding issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9960#issuecomment-771116898:139,failure,failures,139,https://hail.is,https://github.com/hail-is/hail/pull/9960#issuecomment-771116898,1,['failure'],['failures']
Availability,"After spending a couple of hours reading about g++ ABI versions, I'm feeling much less; positive about the plan of building multiple libraries. It seems there are 11 different ABI versions; (most of them are minor bugfixes which were never default behavior for any version of g++,; but still ...). I'm mulling an alternative plan of saying ""well, you've got to have g++, c++, or clang++ somewhere; on your $PATH, or else you've got to define CXX, and also make, but I've got the C++ sources in the ; jarfile and I'll build you a fresh libboot.so and libhail.so if I haven't done it already"". That would involve a little bit more jarfile/Resource magic - but nothing any harder than I've already; done with the header files; avoid a big testing headache; and I hope get us past the whole; ""locking-down"" argument. And then at a later date I'll think about how to have the option of packaging; a recent clang so that we can get C++17 (and perhaps more consistent compile speed than g++); across a wide range on Linuxes. Accordingly I'll close this for now and re-open it when I have a working solution for the library issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410111491:797,down,down,797,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410111491,1,['down'],['down']
Availability,"After the first commit, lots of unrelated type errors popped up from mypy. I think this is because I have the new mypy installed and it's actually catching more errors that were there all along. It might help to see the errors that it gave me (also visible in old CI builds of the PR:. ```; ci/github.py:508: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:551: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:554: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:574: error: Unsupported operand types for > (""int"" and ""None""); ci/github.py:574: note: Left operand is of type ""Optional[int]""; ci/github.py:575: error: Unsupported operand types for + (""None"" and ""int""); ci/github.py:575: note: Left operand is of type ""Optional[int]""; ci/github.py:817: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:828: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:840: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:842: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:849: error: Item ""MergeFailureBatch"" of ""Union[Batch, Any, MergeFailureBatch]"" has no attribute ""id""; ci/github.py:849: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; Found 11 errors in 1 file (checked 19 source files); ```. It might be helpful to look at the first commit and last commit in isolation. Or if you'd like I can make a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775:47,error,errors,47,https://hail.is,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775,15,['error'],"['error', 'errors']"
Availability,"Agh, the unsafeRow and UnsafeIndexedSeq optimizations must be wrong, getting out of bounds memory errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9892#issuecomment-770085173:98,error,errors,98,https://hail.is,https://github.com/hail-is/hail/pull/9892#issuecomment-770085173,1,['error'],['errors']
Availability,"Agh, this isn't good enough. It feels like the right thing might be to lift up the lets, optimize, then push down. This has the added benefit of making it trivial to collapse multiple identical let bindings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-449743945:109,down,down,109,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-449743945,1,['down'],['down']
Availability,Ah I didn't notice that the two loops were in different scripts (startup vs. run). If the run-script exits cleanly will the worker shut down?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10640#issuecomment-874953507:136,down,down,136,https://hail.is,https://github.com/hail-is/hail/pull/10640#issuecomment-874953507,1,['down'],['down']
Availability,"Ah I didn't see you had posted this a couple of days ago. I'm also getting this error on the pipeline below (which works on ~15K genomes, but not ~125K exomes):; ```; mt = mt.select_cols(group_membership=tuple(x[1] for x in sample_group_filters), project_id=mt.meta.project_id); mt = mt.select_rows(*mt.row_key); mt = mt.select_entries(n_alt=mt.GT.n_alt_alleles(), adj=mt.adj). frequency_expression = []; for i in range(len(sample_group_filters)):; subgroup_dict = sample_group_filters[i][0]; subgroup_dict['group'] = 'adj'. freq_expression = hl.struct(; ac=hl.agg.sum(hl.agg.filter(mt.group_membership[i] & mt.adj, mt.n_alt)),; an=2 * hl.agg.count_where(mt.group_membership[i] & mt.adj & hl.is_defined(mt.n_alt)),; hom=hl.agg.count_where(mt.group_membership[i] & mt.adj & (mt.n_alt == 2)),; meta=subgroup_dict; ); frequency_expression.append(freq_expression); freq_expression = hl.struct(; ac=hl.agg.sum(mt.n_alt),; an=2 * hl.agg.count_where(hl.is_defined(mt.n_alt)),; hom=hl.agg.count_where(mt.n_alt == 2),; meta={'group': 'raw'}; ); frequency_expression.insert(1, freq_expression). mt = mt.annotate_rows(freq=frequency_expression); mt.rows().write(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-386942393:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-386942393,1,['error'],['error']
Availability,"Ah ok. I was seeing some `kill -9` that are now gone when excluding the `current_task`. > It is problematic because somewhere else we're not properly stopping an infinite loop. Ideally we'll get to a place where we try to kill a pod and if it doesn't terminate in, say, 5 seconds, we fail the CI tests. Ya it might not deliver great clarity on where the error is coming from but at least we can actively kill and log an error if it's unable to do the ""right"" thing within say 5 seconds. That and I'm unsure how else to solve this problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9944#issuecomment-769239445:354,error,error,354,https://hail.is,https://github.com/hail-is/hail/pull/9944#issuecomment-769239445,2,['error'],['error']
Availability,"Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path `f'{path}/foo`, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like `'{path}zzzzz/foo` or `'{path}szzzzz`. We need to ignore these as they don't provide any information on whether `{path}` is a file or directory. This is where `isChildOf` is needed because we need to make sure the blob is actually a child of the path such as `'{path}/file` and not `{path}zzzzz/file`. As for `getValues` versus `iterateAll`, I just used the one that was in the Java documentation for using the `list` method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274:511,error,errors,511,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274,1,['error'],['errors']
Availability,"Ah the timings were slightly off as I had not downloaded all the data and was using some from `hail_search/fixtures`.; After pulling down the `SNV_INDELS` data, my updated timings are:. | query | results | elapsed |; | ----- | ------- | ------- |; | 0 | 4 | 7s |; | 1 | 83 | 50s |",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1821595905:46,down,downloaded,46,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1821595905,2,['down'],"['down', 'downloaded']"
Availability,"Ah yeah good point I forgot about that. You have to construct a string to avoid truncation a la:; ```; (base) dking@wm28c-761 /tmp % cat foo.py; def test():; assert False, 'b' * 1000; =========================================== test session starts ============================================; (base) dking@wm28c-761 /tmp % pytest foo.py; platform darwin -- Python 3.10.9, pytest-7.4.3, pluggy-1.3.0; rootdir: /private/tmp; configfile: pytest.ini; plugins: xdist-2.5.0, timeout-2.2.0, instafail-0.5.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, anyio-4.2.0, forked-1.6.0, accept-0.1.9, image-diff-0.0.11; asyncio: mode=strict; collected 1 item . foo.py F [100%]. ================================================= FAILURES =================================================; ___________________________________________________ test ___________________________________________________. def test():; > assert False, 'b' * 1000; E AssertionError: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb; E assert False. foo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019:751,FAILURE,FAILURES,751,https://hail.is,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019,1,['FAILURE'],['FAILURES']
Availability,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:47,recover,recoverable,47,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683,4,"['Error', 'error', 'recover']","['Error', 'error', 'recoverable']"
Availability,"Ah, I misunderstood. So you can't ask pip-compile to accept as input a requirements.txt and a pinned-requirements.txt and output a new pinned-requirements.txt? That's what I was imagining. With the main effect being if we add something to requirements.txt without changing pinned-requirements.txt, we'd get an error. Having pip-compile constantly bump requirements in every PR seems not ideal. What I'm trying to achieve is that our pinned-requirements always represent *a* version-compatible, transitive closure of requirements.txt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131901848:310,error,error,310,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131901848,1,['error'],['error']
Availability,"Ah, I see. If those paths point to the same location then it shouldn't make any difference. This error almost certainly means that `pyspark` cannot find your hail jar. I suspect that Spark 2.2.x has dropped support for the `SPARK_CLASSPATH` environment variable. Can you try starting `pyspark` with these options:; ```; pyspark \; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf=spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \; --conf=spark.executor.extraClassPath=./hail-all-spark.jar. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337639898:97,error,error,97,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337639898,1,['error'],['error']
Availability,"Ah, I thought that besides MatrixIR, TableIR, and BlockMatrixIR, aggregations were included (since I saw these as being operations over rows/columns. What is the definition of a relational ir? Brief search didn't yield much, just the concept of relational operators, which appear to be too general to apply here (==, >= are relational operator). Besides various Agg* irs, we also have CollectDistributedArray, and a bunch of others. I will add the missing ones (that are present in InferType), until I no longer get match errors, and we can review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7959#issuecomment-578544609:522,error,errors,522,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-578544609,1,['error'],['errors']
Availability,"Ah, I was accidentally modifying an installed version of Hail. I recovered the files and brought them in. I deleted that other debug_info. It doesn't include the batch information. Now every batch test in batch/ and in hail/ should be consistently using `Batch.debug_info`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10953#issuecomment-939112813:65,recover,recovered,65,https://hail.is,https://github.com/hail-is/hail/pull/10953#issuecomment-939112813,1,['recover'],['recovered']
Availability,"Ah, I was hoping the pca tests would start passing on the local backend with this change, but there are of course pruner issues. I'll have to try and craft a test that runs into this error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-820620715:183,error,error,183,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-820620715,1,['error'],['error']
Availability,"Ah, duh, thanks! And ya I do want to make it easier to get to error logs from the PR namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1049187048:62,error,error,62,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1049187048,1,['error'],['error']
Availability,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:39,ERROR,ERROR,39,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Ah, my Make is interpreting the slashes it self, which is what I would have expected from Make. . ```; # make echo; echo 'hello \; foo'; hello foo; # echo 'hello \; quote> foo'; hello \; foo; # cat Makefile; echo:; 	echo 'hello \; foo'. # ; ```. I guess newer GNU Make doesn't interpret the slashes before sending it to sh?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961:110,echo,echo,110,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961,5,['echo'],['echo']
Availability,"Ah, that explains the pylint error. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9797#issuecomment-738988432:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/pull/9797#issuecomment-738988432,1,['error'],['error']
Availability,"Ah, the reason this never went in is a lint failure. . ```; /usr/local/lib/python3.7/dist-packages/hailtop/hailctl/dataproc/submit.py:27:59: E713 test for membership should be 'not in'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10863#issuecomment-941145164:44,failure,failure,44,https://hail.is,https://github.com/hail-is/hail/pull/10863#issuecomment-941145164,1,['failure'],['failure']
Availability,"Ah. This has nothing to do with notebook2. We have a wildcard DNS entry for *.hail.is so we don't have to modify DNS every time we add/remove a service. However, Let's Encrypt didn't support wildcard certificates when I wrote that code. So anything.hail.is will get a cert error. To fix this we either need to get a wildcard cert or fix the subdomains we use in DNS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416:273,error,error,273,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416,1,['error'],['error']
Availability,All python tests are failing with import errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8656#issuecomment-621225819:41,error,errors,41,https://hail.is,https://github.com/hail-is/hail/pull/8656#issuecomment-621225819,1,['error'],['errors']
Availability,"All test/checks for this pr pass. There is a CI related error, a cluster being issued a delete operation when existing delete operations:. """"""; + gcloud dataproc clusters delete ci-test-n42my5i1 --async; The cluster 'ci-test-n42my5i1' and all attached disks will be deleted. Do you want to continue (Y/n)? ; ERROR: (gcloud.dataproc.clusters.delete) FAILED_PRECONDITION: Cannot delete cluster 'ci-test-n42my5i1' while it has other pending delete operations.; """"""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266:56,error,error,56,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-485153266,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Almost by definition I'd suspect that adding a system administrator is a high security impact (that's not a judgement on you, just a statement about the security boundary getting wider). This is obviously fine in this case because we want you to be a system administrator, but we should let appsec know regardless. They'll also probably want to send you some standard trainings (and maybe background check forms?). I'll ping them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400614863:420,ping,ping,420,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400614863,1,['ping'],['ping']
Availability,"Alright, I'll look at that. I was copying pc_relate above, which does the balding nichols and checkpoint process as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583033913:94,checkpoint,checkpoint,94,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583033913,1,['checkpoint'],['checkpoint']
Availability,Also add test case for unterminated string with nice error message (use `interceptFatal`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/493#issuecomment-235044203:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/issues/493#issuecomment-235044203,1,['error'],['error']
Availability,Also added redundant interval pruning for `filtervariants intervals`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1041#issuecomment-257949279:11,redundant,redundant,11,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949279,1,['redundant'],['redundant']
Availability,"Also ask Tim about limiting RAM available to Hail & Java, we should probably keep at least a gig dedicated to Python. Maybe that will cause memory errors to appear closer to where they belong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12731#issuecomment-1507077467:32,avail,available,32,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1507077467,2,"['avail', 'error']","['available', 'errors']"
Availability,"Also sorry for the high latency on a response, apparently Verizon has an outage in my neighborhood until tomorrow morning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701854993:73,outage,outage,73,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701854993,1,['outage'],['outage']
Availability,Also those service backend failures are :100: because we need #11624 to merge. Feel free to @fails them and I'll deal with the conflict.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10792#issuecomment-1075927168:27,failure,failures,27,https://hail.is,https://github.com/hail-is/hail/pull/10792#issuecomment-1075927168,1,['failure'],['failures']
Availability,"Also when there is a binding outside the tiebreaker used in the tiebreaker it fails in python. The error is very inscrutable, but it simply does not work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1431843216:99,error,error,99,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1431843216,2,['error'],['error']
Availability,"Also, I created `gs://hail-common/vep/vep/GRCh37`, `gs://hail-common/vep/vep/GRCh38`; directories with VEP configs and loftee data files, so you can now run ; ```; gcloud dataproc clusters create $CLUSTER; ...; --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-init.sh. or . --initialization-actions gs://hail-common/hail-init.sh,gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh; ```; along with ; ```; gs://hail-common/vep/vep/GRCh37/vep85-GRCh37-gcloud.properties. or . gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-gcloud.properties; ```. though the init.sh script ties the cluster to a particular genome build. . Also, it would be nice if hail could throw an error if trying to annotate a GRCh37 callset with GRCh38 VEP, etc. Would it make sense to put this check in the VEP command?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946:716,error,error,716,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-299721946,1,['error'],['error']
Availability,"Also, I'm forging ahead for the rest of the day at least. I can't seem to figure out where the issue is for the validation errors that I'm seeing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7803#issuecomment-571283839:123,error,errors,123,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571283839,1,['error'],['errors']
Availability,"Also, I'm gonna add parameters to `hl.init` and set the flags in there. That punts the interface decision down the road by slightly restricting users (you can't change user project mid-pipeline).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1230490168:106,down,down,106,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1230490168,1,['down'],['down']
Availability,"Also, having the migration be online and in multiple stages was making my head hurt a lot. Maybe on Monday we should sit down and white board this and make sure there's no edge cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109901:121,down,down,121,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109901,1,['down'],['down']
Availability,"Also, now that we are not going with the ""run a highmem node premptible pool"" approach, should we still have Prometheus tolerate preemptibles? It would probably be better if it was on one of our always running nodes, especially since it has a nontrivial amount of startup time in my experience",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6774#issuecomment-518774162:120,toler,tolerate,120,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-518774162,1,['toler'],['tolerate']
Availability,"Also, strangely https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ now goes to 403 Forbidden, and a maintenance error is generated in Ghost. Shortly after the PR was built, that link worked. edit: The maintenance error potentially suggests we should wait longer to initiate our probes, although I can't tell until I know where the /blog GET at 31 minutes came from. That request happens 5 minutes before Ghost is actually up, so maybe a previous PR? Not certain. Separately, it takes ghost 6 seconds to actually boot, so if our readiness probe fires off 5s after the container is running, that may not be enough.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548098857:109,mainten,maintenance,109,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098857,4,"['error', 'mainten']","['error', 'maintenance']"
Availability,"Also, the quota documentation says the error should be this:. If you exceeded a quota with an HTTP/REST request, Google Cloud returns an HTTP 429 TOO MANY REQUESTS status code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-832973603:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-832973603,1,['error'],['error']
Availability,"Also, wrt the `hail` alias, that only sets the environment variable for that single execution of `python`. You will need to run:; ```bash; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python; ```; before running `./gradlew test`, otherwise it's very likely that you will see a variety of errors related to Spark. I am surprised that you saw an error about Breeze natives. An inappropriate `$PYTHON_PATH` should trigger a failure much earlier than the section of code that uses of Breeze natives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423:334,error,errors,334,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281862423,3,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1205,down,down,1205,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,2,['down'],['down']
Availability,"Amanda added some stuff so that `hl.is_snp` and friends will run without error for these alleles, but we never support to scala for sampleqc. I'll do that now. I think I'll just relax this error to treat ""invalid"" (which includes symbolic at the moment) alleles the same as ""Complex"" alleles, which aren't counted toward any sampleqc field.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3413#issuecomment-386181958:73,error,error,73,https://hail.is,https://github.com/hail-is/hail/issues/3413#issuecomment-386181958,2,['error'],['error']
Availability,An error is being treated as a timeout but then the job is still considered running?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11397#issuecomment-1076794055:3,error,error,3,https://hail.is,https://github.com/hail-is/hail/pull/11397#issuecomment-1076794055,1,['error'],['error']
Availability,"An example of an error that wasn't retried properly:; https://ci.hail.is/batches/7377528/jobs/81; ```; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/copy.py"", line 129, in <module>; asyncio.run(main()); File ""/usr/lib/python3.8/asyncio/runners.py"", line 44, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/copy.py"", line 119, in main; await copy_from_dict(; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/copy.py"", line 85, in copy_from_dict; await copy(; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/copy.py"", line 58, in copy; copy_report = await Copier.copy(; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/fs/copier.py"", line 455, in copy; await copier._copy(sema, copy_report, transfer, return_exceptions); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/fs/copier.py"", line 548, in _copy; raise e; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/fs/copier.py"", line 540, in _copy; await bounded_gather2(sema, *[; File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 545, in bounded_gather2; return await bounded_gather2_raise_exceptions(sema, *pfs, cancel_on_error=cancel_on_error); File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 525, in bounded_gather2_raise_exceptions; return await asyncio.gather(*tasks); File ""/usr/local/lib/python3.8/dist-packages/hailtop/utils/utils.py"", line 515, in run_with_sema; return await pf(); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiotools/fs/copier.py"", line 525, in _copy_one_transfer; raise e; File ""/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13029#issuecomment-1542329456:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/13029#issuecomment-1542329456,1,['error'],['error']
Availability,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:102,FAILURE,FAILURE,102,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637,1,['FAILURE'],['FAILURE']
Availability,And mendel errors (which still needs to be rewritten in Python),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3066#issuecomment-370152354:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/3066#issuecomment-370152354,1,['error'],['errors']
Availability,"And to directly respond to this comment:. > If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. This sounds great! This would resolve question 1 and eliminate the risk. We should charge the highest possible price: 0.23 USD/GiB. Answering question 2 can proceed slowly and carefully knowing that we don't have a cost risk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692171657:88,down,downloaded,88,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692171657,1,['down'],['downloaded']
Availability,"And what is this mean :; ```; EnvironmentError: no Hail context initialized, create one first; ```; how to initialize the hail context?; ```; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-289>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); File ""/opt/Software/hail/python/hail/java.py"", line 42, in hc; raise EnvironmentError('no Hail context initialized, create one first'); EnvironmentError: no Hail context initialized, create one first; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337917356:338,Error,Error,338,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337917356,1,['Error'],['Error']
Availability,"Another set of eyes on this would be great. My current thoughts on this:. I only looked at the failure in PCA. I was never able to reproduce. My next step to try to reproduce was to run PCA on Lindo's full dataset on dataproc (can't use batch because the error is in spark PCA). I did look carefully through the stack trace, trying to understand what could possibly be happening. The number 177860 from the error isn't either matrix dimension, which is 210234 by 8893. Everything in `org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106)` is independent of the number of rows, so only the number 8893 of cols should be relevent. I wrote a simple test to execute spark PCA with 8893 rows in scala, so I could step through with a debugger:; ```scala; var mt = rangeMatrix(10000, 8893); mt = MatrixMapEntries(mt, InsertFields(Ref(""g"", mt.typ.entryType), Seq(""a"" -> F64(1)))); val t = MatrixToTableApply(mt, PCA(""a"", 10, false)); val n = TableToValueApply(t, ForceCountTable()); assertEvalsTo(n, 8893L); ```; The array `v` in `symmetricEigs` has length 177860 = 8893*20, and I didn't find anything else with that size. The only line I could find that could generate an exception that looks like this is line 555 of `dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd`; ```scala; public void dsaupd(org.netlib.util.intW ido, String bmat, int n, String which, int nev, org.netlib.util.doubleW tol, double[] resid, int offsetresid, int ncv, double[] v, int offsetv, int ldv, int[] iparam, int offsetiparam, int[] ipntr, int offsetipntr, double[] workd, int offsetworkd, double[] workl, int offsetworkl, int lworkl, org.netlib.util.intW info) {; if (debug) System.err.println(""dsaupd"");; checkArgument(""DSAUPD"", 2, lsame(""I"", bmat) || lsame(""G"", bmat));; checkArgument(""DSAUPD"", 3, n >= 0);; checkArgument(""DSAUPD"", 4, lsame(""LA"", which) || lsame(""SA"", which) || lsame(""LM"", which) || lsame(""SM"", which) || lsame(""BE"", which));; checkArgument(""DSAUPD"", 5, 0 < ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313:95,failure,failure,95,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313,6,"['error', 'failure']","['error', 'failure']"
Availability,"Another strange dataproc failure:. ```; + cluster submit ci-test-e8jon1wrnx2o python/cluster-tests/cluster-read-vcfs-check.py; Job [38fe2b2b5b92430d9961e3226e0c0731] submitted.; Waiting for job output...; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [38fe2b2b5b92430d9961e3226e0c0731] failed with error:; Task not found; ```. I'm not even sure what a task is in this context. Will bump to retest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265:25,failure,failure,25,https://hail.is,https://github.com/hail-is/hail/pull/6010#issuecomment-489360265,3,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"Another thing I just thought of, I think we should add raise some form of error if a job that is `always_run` has inputs from a job that is not `always_copy_output`. I can't imagine something like that being intentional, but if there is a valid use case, maybe we have a warning instead of an error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11884#issuecomment-1162862002:74,error,error,74,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1162862002,2,['error'],['error']
Availability,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858:687,error,error,687,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858,2,['error'],['error']
Availability,"Arcturus -- I'm assigning this to you, but please don't take off the WIP tag as merging this will cause Batch to shutdown (database migration). If we're not prepared for it, then it could cause an extended outage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759:206,outage,outage,206,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759,1,['outage'],['outage']
Availability,Are you asking how to detect that one command of many in a bash *pipeline* failed? We need pipe fail enabled for that:. ```bash; # bad; (base) dking@wm28c-761 ~ % ls fdsafds | xargs echo hello; ls: fdsafds: No such file or directory; (base) dking@wm28c-761 ~ % echo $?; 0; ```; ```bash; # good; (base) dking@wm28c-761 ~ % set -o pipefail; (base) dking@wm28c-761 ~ % ls fdsafds | xargs echo hello; ls: fdsafds: No such file or directory; (base) dking@wm28c-761 ~ % echo $?; 1; ```. Permissions issues indeed fail the `gcloud` command:; ```; (base) dking@wm28c-761 ~ % gcloud compute instances list --project notmyproject; API [compute.googleapis.com] not enabled on project [notmyproject]. Would you ; like to enable and retry (this will take a few minutes)? (y/N)? y. Enabling service [compute.googleapis.com] on project [notmyproject]...; ERROR: (gcloud.compute.instances.list) PERMISSION_DENIED: Permission denied to enable service [compute.googleapis.com]; Help Token: AVzH8v0NCN6UR5g5Xtu_gFde3SeZCmToYDDOlz7hp5HiVvGHKX8aeJ-kn0N0n72nMovbuw4ksm8MB0OifqPrdxlc6lWwJJKi0CsIJon1a7SSlF_H; - '@type': type.googleapis.com/google.rpc.PreconditionFailure; violations:; - subject: ?error_code=110002&service=serviceusage.googleapis.com&permission=serviceusage.services.enable&resource=notmyproject; type: googleapis.com; - '@type': type.googleapis.com/google.rpc.ErrorInfo; domain: serviceusage.googleapis.com; metadata:; permission: serviceusage.services.enable; resource: notmyproject; service: serviceusage.googleapis.com; reason: AUTH_PERMISSION_DENIED; (base) dking@wm28c-761 ~ % echo $?; 1; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1743472268:182,echo,echo,182,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1743472268,7,"['ERROR', 'Error', 'echo']","['ERROR', 'ErrorInfo', 'echo']"
Availability,"Are you sure that you installed all the necessary packages listed here: https://hail.is/docs/0.2/install/linux.html ? In particular this kind of error can happen if you did not install openblas. In the future, please use https://discuss.hail.is for support questions, we don't monitor GitHub issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939#issuecomment-786236682:145,error,error,145,https://hail.is,https://github.com/hail-is/hail/issues/9939#issuecomment-786236682,1,['error'],['error']
Availability,"Argh, sorry, it's actually the parens, not the spaces:; ```; /bin/sh: -c: line 9: syntax error near unexpected token `('; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5759#issuecomment-479599142:89,error,error,89,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599142,1,['error'],['error']
Availability,"As a data point, we saw this error for IR sizes beyond 20000000 in our 0.2.132ish deployment too, when using ServiceBackend. PR #14567 was present in 0.2.132, so unless something weird was going on with our installation (certainly a possibility) I think this means #14567 wasn't effective for us either. We're very much looking forward to giving #14750 a try.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749#issuecomment-2458392840:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/14749#issuecomment-2458392840,1,['error'],['error']
Availability,"As written here, this feature is incompatible with `overwrite=True`. The checkpoint file makes it possible to recover progress from a partially written file at `url`, so I'm not sure what overwriting would mean in this context. Is there a particular use case you have in mind? This was built for a specific application, and I am eager to hear others.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215#issuecomment-822958412:73,checkpoint,checkpoint,73,https://hail.is,https://github.com/hail-is/hail/pull/10215#issuecomment-822958412,2,"['checkpoint', 'recover']","['checkpoint', 'recover']"
Availability,Assertion error in RVB,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7693#issuecomment-563585896:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/7693#issuecomment-563585896,1,['error'],['error']
Availability,"Assigned to Patrick, but anyone should feel free to approve, small change to add better error messages on `MakeNDArray`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10095#issuecomment-784448517:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/pull/10095#issuecomment-784448517,1,['error'],['error']
Availability,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278:632,down,downloading,632,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278,8,"['down', 'error', 'failure']","['download', 'downloading', 'error', 'failure']"
Availability,"Awesome. God, how long was this in coming. No for this PR, but I observe this seems a bit error prone:. > --vep; > actual = hl.vep(expected.select_rows(), 'gs://hail-common/vep/vep/vep85-loftee-gcloud.json', csq=csq). We should probably think a bit more about the vep/cloudtools interface after this. I'm thinking `cloudtools --vep --vep-version=85 --vep-assembly=GRCh37 --loffee-version=beta ...` and then just `hl.vep(foo, csq=csq)` where the properties file defaults to something set up by cloudtools. @konradjk?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422170789:90,error,error,90,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422170789,1,['error'],['error']
Availability,"Back to you. I didn't do the octal changes or the desired error handling, but I made an issue to do that when I have a little more time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/468#issuecomment-234586874:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/pull/468#issuecomment-234586874,1,['error'],['error']
Availability,"Back to you. I made some inline comments before starting review, see both. Test failure is due to not changing high_kin to related_pairs on the right hand side, etc. in doc example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2148#issuecomment-326106858:80,failure,failure,80,https://hail.is,https://github.com/hail-is/hail/pull/2148#issuecomment-326106858,1,['failure'],['failure']
Availability,"Back to you. I've re-pushed to accommodate space-delimited .fam again (will throw error if some sample name has space too). While you look at the rest, I'll work in another branch on .fam importer to annotations which will give user a delimiter option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/234#issuecomment-204439315:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/234#issuecomment-204439315,1,['error'],['error']
Availability,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:274,toler,toleration,274,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683,8,"['down', 'downtime', 'toler']","['downsizing', 'downtime', 'toleration']"
Availability,"Because that won't work at all, because the input file name expected by `gatk IndexFeatureFile` won't exist. But perhaps you meant to connect the unrelated filenames via something like. ```; …; bgzip -c {j.tsv_counts} > {j.counts['tsv.gz']}; …; ```. Reasons for not doing that would include:. 1. Filename extensions are significant to GATK, so I would not trust `SubCommand` to write the output file in the right format if the filename did not have the expected extension. (This could be ameliorated via `j.tsv_counts.set_extension`.). 2. Using bgzip with `-c … >` is not its natural mode of operation, so is more likely to encounter bugs than the more typical `bgzip filename` invocation. (For example, plain gzip burns the input filename into the compressed file's header, so the redirection version produces different results from the typical invocation for gzip; bgzip does not embed the filename but the change may have other effects. For example, bgzip's error checking (e.g. in disk full situations) may well be more complete in the typical invocation than when writing to standard output.). It is also less clear than the straightforward invocation, so using this would be a hack. 4. The resulting code is IMHO overall less clear than the original version in which the resource group models the relationship between all three filenames. If Hail Batch is a well-rounded orthogonal API, then that code ought to work too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599478181:961,error,error,961,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599478181,2,['error'],['error']
Availability,"Because we're running the bash script that is generated with +e so it will fail on the first error. To try and implement always_run would require implementing something more complicated in the local backend and not just building up a script. Because you'd have to check whether the parents succeeded for each task. It's not out of the question, but I don't think it's worth doing right now. Hence, the NotImplementedError. Same reasoning for timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8258#issuecomment-596109210:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/pull/8258#issuecomment-596109210,1,['error'],['error']
Availability,Ben confirmed this was a user error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10645#issuecomment-875824135:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/10645#issuecomment-875824135,1,['error'],['error']
Availability,"Both these fail with the same error:; ```; broken_ht = hl.import_table('../data/bikes.csv'); broken_ht = hl.import_table('../data/bikes.csv', delimiter=';'); ```. I think this is an encoding issue. This file is encoded with latin-1 and contains French diacritics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5221#issuecomment-459112810:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/5221#issuecomment-459112810,1,['error'],['error']
Availability,"Btw the docs say `If `mt` contains an entry field `GT` of type :py:data:`.tcall`, then the following fields are computed:` but the code would error out if it didn't have it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3629#issuecomment-392932165:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/pull/3629#issuecomment-392932165,1,['error'],['error']
Availability,"Bump, deadlock errors are absolutely hammering the cluster today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985#issuecomment-949085569:15,error,errors,15,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-949085569,1,['error'],['errors']
Availability,"Bumping this PR, I'd like it to land so I can nail down exactly why my flags PR is causing BN tests to fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12139#issuecomment-1317326725:51,down,down,51,https://hail.is,https://github.com/hail-is/hail/pull/12139#issuecomment-1317326725,1,['down'],['down']
Availability,"Bundling working well now:. For instance, the addition of this file, which handles the auth0 callback/sets cookie, adds only *501B* despite importing Auth and react-easy-state :tada:. The entirety of Auth dependency, react-easy-state (just observable JS properties for easy event notification), js-cookie to simplify cookie management, all other imports that are used at least 2x, poly fills for IE11 compat (promises, object.assign) + React + React-Dom is 99KB, and served in parallel with the page, so initial render doesn't incur the cost. Not bad; we can get this down a bit by removing js-cookie (2KB). ```jsx; // TODO: Replace Loading component without Material UI; import { Component } from 'react';; import Router from 'next/router';; import { view } from 'react-easy-state';; import Auth from '../lib/Auth';. class Callback extends Component {; componentDidMount() {; Auth.handleAuthenticationAsync(err => {; // TODO: notify in modal if error; if (err) {; console.error('ERROR in callback!', err);; }. Router.push('/');; });; }. render() {; return !Auth.isAuthenticated() ? <div>Loading</div> : <div>Hello</div>;; }; }. export default view(Callback);; ```. <img width=""353"" alt=""screen shot 2018-12-19 at 5 06 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50251076-ad695680-03b0-11e9-88f2-28d3ff7daa33.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682:568,down,down,568,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448761682,7,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"By using Double and converting to Long:; 1) there may be off-by-one errors from what is expected (both 0.99999999 and -0.99999999 go to 0); 2) you can only represent positive integers up to 2^53 = 9e15 exactly, rather than 2^63:; https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64. So question is whether to create a Long version of aggregator for both Int and Long, analogous to Double for Float and Double.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1051#issuecomment-259964811:68,error,errors,68,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-259964811,1,['error'],['errors']
Availability,"C1). ```; hailctl dataproc start ; --autoscaling-policy={autoscaling_policy}; --worker-machine-type {worker_machine_type}; --region {region}; --project {gcs_project}; --service-account {account}; --num-master-local-ssds 1; --num-worker-local-ssds 1 ; --max-idle=60m; --max-age=1440m; --subnet=projects/{gcs_project}/regions/{region}/subnetworks/subnetwork; {cluster_name}; ```. </details>. I have the driver node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: low memory! at or below SIGTERM limits: mem 0.12%, swap 1.00%; Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM to process 8421 uid 0 ""java"": badness 1852, VmRSS 40578 MiB; ```. Indeed, the VmRSS is the memory in use from the kernel's perspe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:1393,down,down,1393,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,1,['down'],['down']
Availability,CI deploy is broken due to checkout failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8577#issuecomment-616211238:36,failure,failure,36,https://hail.is,https://github.com/hail-is/hail/pull/8577#issuecomment-616211238,1,['failure'],['failure']
Availability,"CI had a hiccup. I hope this will finally pass, unless the latest master-merge introduced more issues. Needs a careful walk through tomorrow to ensure everything is in place and no unnecessary slow downs were added.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3414#issuecomment-386497740:198,down,downs,198,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-386497740,1,['down'],['downs']
Availability,CI recovered once the PR was closed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6377#issuecomment-503584053:3,recover,recovered,3,https://hail.is,https://github.com/hail-is/hail/pull/6377#issuecomment-503584053,1,['recover'],['recovered']
Availability,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865,3,['failure'],"['failure', 'failures']"
Availability,CRIU/checkpoint-restore using Eclipse OpenJ9 JVM https://blog.openj9.org/2022/09/26/getting-started-with-openj9-criu-support/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13675#issuecomment-1728033312:5,checkpoint,checkpoint-restore,5,https://hail.is,https://github.com/hail-is/hail/issues/13675#issuecomment-1728033312,1,['checkpoint'],['checkpoint-restore']
Availability,CS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:14827,echo,echo,14827,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"Calling that the ""right"" error is a little bit bold, but you certainly fixed the bug!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1502#issuecomment-284850456:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/1502#issuecomment-284850456,1,['error'],['error']
Availability,"Can confirm that this ""fixes"" it, since current master results in:; ```; Hail version: devel-f4fc571b4570; Error summary: AssertionError: assertion failed: type mismatch:; name: global; actual: +Struct{}; expect: Struct{}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412775955:107,Error,Error,107,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412775955,1,['Error'],['Error']
Availability,"Can we follow up with a test that creates one job which echos its IP and port and then listens, then the test will try to curl the public IP and port?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11250#issuecomment-1021554196:56,echo,echos,56,https://hail.is,https://github.com/hail-is/hail/pull/11250#issuecomment-1021554196,1,['echo'],['echos']
Availability,"Can we make a ""backup"" of what the data source settings are that are working now before we redeploy this in default? I want to make sure we can recover if these changes put us in a bad state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-905883214:144,recover,recover,144,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-905883214,1,['recover'],['recover']
Availability,Can you fix the following pylint formatting errors?. ```; batch/front_end/front_end.py:1455:1: W293 blank line contains whitespace; batch/front_end/front_end.py:1462:1: W293 blank line contains whitespace; batch/front_end/front_end.py:1463:13: W291 trailing whitespace; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10800#issuecomment-902867954:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/pull/10800#issuecomment-902867954,1,['error'],['errors']
Availability,Can you post the output of:; ```; cat /proc/cpuinfo | grep flags; ```. I suspect this error might arise if your processor doesn't support the pop count intrinsic. If this is the case it's my mistake for not including a back up implementation in terms of bit shifting. I can fix this tonight if that's the issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1520#issuecomment-285803343:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/issues/1520#issuecomment-285803343,1,['error'],['error']
Availability,"Can you take a look at the new structure and see if it's better? If so, then I'll test everything again making sure there's no errors in the logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-949956416:127,error,errors,127,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-949956416,1,['error'],['errors']
Availability,Can you take another look? I'm getting test errors in other PRs that I hope this PR fixes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10696#issuecomment-887830765:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/pull/10696#issuecomment-887830765,1,['error'],['errors']
Availability,"Can you verify; 1. Batch has access to the hail-query bucket; 2. Our terraform correctly grants permissions for that? And if it currently doesn’t, we should ping AUS to make sure they’re aware of this change",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138729334:157,ping,ping,157,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138729334,1,['ping'],['ping']
Availability,Can't figure out the error. Some segmentation fault. Can trigger on regressionTestUnifyBug. Looks to be triggered in Compile(MakeTuple.ordered),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7733#issuecomment-566223349:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/7733#issuecomment-566223349,2,"['error', 'fault']","['error', 'fault']"
Availability,"Can't quite figure out the failure in LDPruneSuite, will follow up tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7687#issuecomment-563878450:27,failure,failure,27,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-563878450,1,['failure'],['failure']
Availability,Certainly appears to have fixed my issue. Pipeline that went from 11 to 25 mins is now back down to 11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4028#issuecomment-410786746:92,down,down,92,https://hail.is,https://github.com/hail-is/hail/issues/4028#issuecomment-410786746,1,['down'],['down']
Availability,Change to `echo $SPARK_HOME/python/lib/py4j*-src.zip`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1573#issuecomment-287743256:11,echo,echo,11,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287743256,1,['echo'],['echo']
Availability,"Changes since last review:; - Method now takes expressions for call and (optionally) scores.; - Block matrix and table of scores annotated and collected from source.cols() sent to Python, processed using int indices, column names restored on python side (thanks @tpoterba); - Fixed bug that silently dropped `n_samples / block_size` proportion of pairs, Python test checks it; - Extended Python tests to compare k and scores paths, test counts, min_kinship, maf, block_size; - Tuned tolerances on comparison with R from Python; - Extended to general column key, removing unique key check, noted in docs; - MEMORY_AND_DISK caching as default (thanks @konradjk) on Scala side; - The diagonal fix meant phi is computed with parallelism up to the number of diagonal blocks, rather than parallelism 1. But that's still likely a bottleneck as phi requires computing and point-wise dividing two big gram matrices. I now write phi to disk and read it back in, which squares the parallelism up to the number of blocks in phi. I think this should also improve the stability of the many downstream calculations derived from phi, esp. if pre-emptibles are used. No longer cacheing phi, but I left caching on the other matrices. @konradjk let us know how this version compares next time you run it.; - Noted in FIXME room for further improvement when fusing blocks: `replace join with zipPartitions, throw away lower triangular blocks sooner, avoid the nulls`; - Updated docs accordingly; - Deleted a bunch of code in PCRelate and PCRelateSuite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104:483,toler,tolerances,483,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104,2,"['down', 'toler']","['downstream', 'tolerances']"
Availability,Changing the chromosome to 2 3 or 4 preserves the error. Chromosome 5 eliminates the error. fails:; ```; 	chromosome	locus	gene; 0	chr4	6109351	RNF207; 119111	chr8	51749536	PXDNL; ```; succeeds; ```; 	chromosome	locus	gene; 0	chr5	1	RNF207; 119111	chr8	51749536	PXDNL; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13339#issuecomment-1660524651:50,error,error,50,https://hail.is,https://github.com/hail-is/hail/issues/13339#issuecomment-1660524651,2,['error'],['error']
Availability,Checked that PartitioningSuite doesn't get an out of memory error on bgen branch and ExprSuite.testImpexes takes less than a second now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1748#issuecomment-298948635:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298948635,1,['error'],['error']
Availability,"Checkpointing the VCF made it, if anything, slower.; ```; 254.81s call hail/methods/relatedness/test_pc_relate.py::test_pc_relate_against_R_truth; 189.52s call hail/methods/test_pca.py::test_spectra_2[triplet0]; 95.37s call hail/vds/test_vds.py::test_truncate_reference_blocks; 92.80s call hail/methods/test_qc.py::Tests::test_vep_grch38_against_dataproc; 86.83s call hail/backend/test_service_backend.py::test_tiny_driver_has_tiny_memory; 68.86s call hail/matrixtable/test_matrix_table.py::test_read_write_balding_nichols_model; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1564883408:0,Checkpoint,Checkpointing,0,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1564883408,1,['Checkpoint'],['Checkpointing']
Availability,"Clarifying for future devs: the Batch.read_input is probably just causing the script to grow large enough that we start using scripts which need to be uploaded separately from the job. Attempting to replicate with; ```. In [6]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```. triggers https://github.com/hail-is/hail/issues/14051. I'll try to fix both.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13940#issuecomment-1834357495:370,echo,echo,370,https://hail.is,https://github.com/hail-is/hail/issues/13940#issuecomment-1834357495,1,['echo'],['echo']
Availability,Closing as we're going down a different path.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5117#issuecomment-455789526:23,down,down,23,https://hail.is,https://github.com/hail-is/hail/pull/5117#issuecomment-455789526,1,['down'],['down']
Availability,Closing due to lack of reproduction. Please feel free to comment and ping me if you can provide more details!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717#issuecomment-1467001174:69,ping,ping,69,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1467001174,1,['ping'],['ping']
Availability,Closing this for now until we have a plan for versioning cloudtools. This change would require not using the hard coded init_notebook.py available on GCS right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6118#issuecomment-493502443:137,avail,available,137,https://hail.is,https://github.com/hail-is/hail/pull/6118#issuecomment-493502443,1,['avail'],['available']
Availability,Closing while I address test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1548#issuecomment-286622901:29,failure,failure,29,https://hail.is,https://github.com/hail-is/hail/pull/1548#issuecomment-286622901,1,['failure'],['failure']
Availability,Closing while I resolve test failures and do some clean up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2254#issuecomment-332244947:29,failure,failures,29,https://hail.is,https://github.com/hail-is/hail/pull/2254#issuecomment-332244947,1,['failure'],['failures']
Availability,Closing. The latter issue will be addressed with better handing of categorical covariates down the line.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1156#issuecomment-422471680:90,down,down,90,https://hail.is,https://github.com/hail-is/hail/issues/1156#issuecomment-422471680,1,['down'],['down']
Availability,"Comments addressed. You asked about scalars so I've rounded that out with support for any combination of scalar and block matrix, as well as unary + and -, testing in notebook along the way. I've marked the class with experimental.rst until I've stabilized the interface with robust testing of all operations in subsequent broadcasting PR. I fixed the process_joins bug as noted, but stopped there in this PR since just switching to select_entries will end up calling the expression machinery twice. The right solution requires simultaneous changes on the Scala side. I'll make a PR to check if `entry_expr` is a field, if not to use `select_entries` to make it one, and then change `MatrixTable.writeBlockMatrix` to take a field rather than an expression. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3072#issuecomment-370569833:276,robust,robust,276,https://hail.is,https://github.com/hail-is/hail/pull/3072#issuecomment-370569833,1,['robust'],['robust']
Availability,Compilation errors,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9648#issuecomment-717421895:12,error,errors,12,https://hail.is,https://github.com/hail-is/hail/pull/9648#issuecomment-717421895,1,['error'],['errors']
Availability,Confirmed that the hailctl bit works:; ```; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 1234s; Error: bad value '1234s' for parameter <ConfigVariable.HTTP_TIMEOUT_IN_SECONDS: 'http/timeout_in_seconds'> should be a float or an int like 42.42 ; or 42; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 42 ; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 42.0; (base) dking@wm28c-761 hail % hailctl config set http/timeout_in_seconds 60 ; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query-ger0g/jars/dking/uk4prwgezgva/5fc88d5a4b614454004226f5c77ea72efee1e38f.jar. [batch]; remote_tmpdir = gs://1-day/; billing_project = hail; backend = service. [aiocloudflare]. [global]; domain = hail.is. [gcs_requester_pays]; project = broad-ctsa. [http]; timeout_in_seconds = 60. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14206#issuecomment-1915030577:124,Error,Error,124,https://hail.is,https://github.com/hail-is/hail/pull/14206#issuecomment-1915030577,1,['Error'],['Error']
Availability,"Confirmed, this failure is not happening on local. . ```sh; (base) alex:~/projects/hail/hail:$ ./gradlew test --tests is.hail.expr.ir.ForwardLetsSuite.testForwardingOps; :checkSettings; check: seed = 1, size = 1000, count = 10; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; c++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin Hadoop.cpp -MG -M -MF build/Hadoop.d -MT build/Hadoop.o; c++ -o build/Region.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:16,failure,failure,16,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925,1,['failure'],['failure']
Availability,"Connector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environme",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4922,AVAIL,AVAILABLE,4922,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"Consider three alleles. ```; GT: 1/2; GQ: 10. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2; ```. Want to preserve:; - PL(GT) = 0. ### Algorithms. Suppose we remove allele 2. There are two options:; - (_minning_) in the PL array convert occurences of 2 to 0 (""downcode to ref"") and take minimums where there are multiple likelihoods for a single genotype; also downcode GT to ref; - (_subsetting_) subset the PL array (i.e. remove entries with 2's); set GT to the genotype with the minimum likelihood. ### Interpretations. The qualitative interpretation of _minning_ is a belief that the alternate is real but we want to shift the probability mass to 0 (thus changing our interpretation of 0 from ""reference"" to ""reference or something not listed""). The qualitative interpretation of _subsetting_ is a belief that the alternate is not-real and we want to discard any probability mass associated with the alternate. ### Results. The _minning_ algorithm produces. ```; GT: 0/1; GQ: 10. 0 | 20; 1 | 0 10; +-----------; 0 1; ```. The _subsetting_ algorithm produces. ```; GT: 1/1; GQ: 990. 0 | 990; 1 | 990 0; +----------; 0 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240823343:278,down,downcode,278,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240823343,2,['down'],['downcode']
Availability,Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?access_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6Ik16YzNRekpFUXpWRk5VSXdPRE0yTmpJMFF6VkZPVVk1TkRZME9UZzJOa00xUkRBek1ERTJOZyJ9.eyJpc3MiOiJodHRwczovL2hhaWwuYXV0aDAuY29tLyIsInN1YiI6Imdvb2dsZS1vYXV0aDJ8MTEwNzI2NTIxOTIxMjQ5NDQzNzYwIiwiYXVkIjpbImhhaWwiLCJodHRwczovL2hhaWwuYXV0aDAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTU0OTIzMDI2MSwiZXhwIjoxNTQ5MjM3NDYxLCJhenAiOiJURDc4azIzQ2NkTTRwTVdvWVp3WXdLSmJRUEJqMDZqWSIsInNjb3BlIjoib3BlbmlkIHByb2ZpbGUifQ.p3HjkP5t3xrGMGOG8kkCocRCg6BRSrGiO_ymwjqQt-omgk55KnObZCJXFX20BM6n6azzNvF_8EpruB3iSRAFiuhwVvHyabwvRpSZAy3giOpYyxgnj4mPlphdAF9c0yduIU-VpLA6ifaqF9Tj69pfMlFfdjo5ku1tkJnRIkysWYB58bXCqRp9dYSYxZZ45X52YOoP_VrnyyIWX4AvZnp-1Cy9nssFV6l6j2PJmvqkMPLR0suS-lR6NK6PMRRiOessKZy3SXwLJv1oLhJW7qFFEb8kP9pG7zoW0v-TpP9f-XBH0UE9WNaIyur0QOU80qsUa7CmjMdLoi7klDBqdfx-Mg&token=ef8707c52ba1439e9b7ebf78e136075d (10.32.13.94) 0.85ms; alexkotlar:~/projects/hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:1871,down,down,1871,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942,2,['down'],['down']
Availability,"Cotton -- I fixed the changes you suggested and it should be ready to be merged. For the multiarray of size 0, I tested that you can create the object, but using the apply for (0,0) throws an error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/79#issuecomment-161677715:192,error,error,192,https://hail.is,https://github.com/hail-is/hail/pull/79#issuecomment-161677715,1,['error'],['error']
Availability,"Could you give some context on the two different places in the driver we're checking the number of attempts? What are those two places, and why do we need two?. Also, have you done anything to test how much adding joins with the `attempts` table slows down those queries?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14682#issuecomment-2356218496:252,down,down,252,https://hail.is,https://github.com/hail-is/hail/pull/14682#issuecomment-2356218496,1,['down'],['down']
Availability,Could you please fix the error messages? See above.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3234#issuecomment-376664624:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/3234#issuecomment-376664624,1,['error'],['error']
Availability,"Current Plan:. - input pod; exit 0 if any files exist in /io/; if pod is preempted, delete the pvc and start a new pod. - main pod; init container touches /io/some_file_main; if /io/some_file_main exists, exit 0; If pod is preempted, delete pvc and start a new input pod (rollback). - output pod; init container checks /io/some_file_output; if /io/some_file_output exists, exit 0; touch /io/some_file_output at the end of running the output pod successfully",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6494#issuecomment-506534967:272,rollback,rollback,272,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506534967,1,['rollback'],['rollback']
Availability,"Current master on representative dataset:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: running: count; [Stage 2:======================================================>(661 + 2) / 663]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 5.760s; filtervariants intervals: 386.191ms; count: 32.617s; total: 38.763s; ```. and new:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: pruned 0 redundant intervals; hail: info: interval filter loaded 67 of 663 partitions; hail: info: running: count; [Stage 2:=======================================================> (65 + 2) / 67]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 9.911s; filtervariants intervals: 445.193ms; count: 3.356s; total: 13.712s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200:715,redundant,redundant,715,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200,1,['redundant'],['redundant']
Availability,Currently the linear SKAT routine is implemented to be optimal for the case of (genetic variants) k < n (genetic samples). Implementation will process sets of variants associated to the same gene in such a way that there is no redundant computation in the algorithm. . cases handled:; hard call genetic data; dosage genetic data; k << n - (Cannot explicitly form a matrix containing all the genotype data) . ran on chromosome 22 1kgDataset with approximately 100 intervals and the program runs in about 3-4 minutes with 2 workers and 12 pre-emptibles with 8 cores each.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1973#issuecomment-320358482:227,redundant,redundant,227,https://hail.is,https://github.com/hail-is/hail/pull/1973#issuecomment-320358482,1,['redundant'],['redundant']
Availability,"Currently the` nvidia-container-toolkit` is installed both in the vm startup script and in the worker docker image. The toolkit must be installed in the startup script to be able to configure docker with the command `nvidia-ctk runtime configure --runtime=docker`. This command cannot be run from Dockerfile.worker because it gets the error `""unable to flush config: unable to open /etc/docker/daemon.json for writing: open /etc/docker/daemon.json: no such file or directory""`.; The toolkit also has to be installed in Dockerfile.worker since that is where crun is invoked from. To execute the nvidia hook, the toolkit needs to be installed in that container. We could probably find a workaround to this if you would like but it only increased the worker image from 1.47Gb to 1.55Gb so it seems pretty small.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481:335,error,error,335,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481,1,['error'],['error']
Availability,"Currently there are some tests failures, but they are stemming from me running more tests than I expect to it would seem (i.e. trying to run the NDArray write tests in JVM byte code world). General review of the byt ecode generation stuff would still be appreciated, I'll debug the testing stuff when I get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062:31,failure,failures,31,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062,1,['failure'],['failures']
Availability,"Dan's OOO this week. It's a Python lint failure:; ```; + cd /io/repo; + make check-hail; make -C hail/python check; make[1]: Entering directory '/io/repo/hail/python'; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; hailtop/aiogoogle/auth/credentials.py:43:1: W293 blank line contains whitespace; hailtop/aiogoogle/auth/credentials.py:47:5: E303 too many blank lines (2); hailtop/aiogoogle/auth/credentials.py:105:1: E302 expected 2 blank lines, found 1; make[1]: Leaving directory '/io/repo/hail/python'; make[1]: *** [Makefile:12: check] Error 1; make: *** [Makefile:13: check-hail] Error 2; Status; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10647#issuecomment-876439207:40,failure,failure,40,https://hail.is,https://github.com/hail-is/hail/pull/10647#issuecomment-876439207,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:12,down,down,12,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293,1,['down'],['down']
Availability,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:166,failure,failure,166,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904,3,"['down', 'failure']","['down', 'failure', 'failures']"
Availability,Did this finish today? It is the same error as the last issue: it actually succeeded and shouldn't happen again for new jobs. ```; -rw-r--r-- 2 aganna supergroup 0 2016-04-19 00:00 /user/aganna/exac_noCANCER.split.onlygeno.vep.NEWHAIL.vds/rdd.parquet/_SUCCESS; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/309#issuecomment-211989236:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/issues/309#issuecomment-211989236,1,['error'],['error']
Availability,Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532254006:128,failure,failure,128,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532254006,2,"['down', 'failure']","['down', 'failure']"
Availability,"Discussion in a different forum sounds good. This came up with the GPU branch as there was a question on whether we can trust what we are parsing as a resource or did we need to hardcode the SKUs explicitly. This was my proposed solution for us at least knowing that something has changed with lots of driver error messages and to not try and do any updates if the invariants of the ""SKU"" don't hold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579:309,error,error,309,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579,1,['error'],['error']
Availability,Do I need to add resource requests to the build.yaml file and CI build.py? I'm worried about things getting out of memory errors now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7583#issuecomment-557524946:122,error,errors,122,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557524946,1,['error'],['errors']
Availability,"Do I need to do something on my end in order to test copying from my computer rather than through CI? I keep getting a 400 regardless of whether I use my bucket `hail-jigold` or the test bucket `hail-test-dmk9z`. The only information I can get is ""Bad Request"" which I think is from this:. > The request cannot be completed based on your current Cloud Storage settings. For example, you cannot lock a retention policy if the requested bucket doesn't have a retention policy, and you cannot set ACLs if the requested bucket has Bucket Policy Only enabled. I don't think this is the real error as we know copying works fine with the Hail test bucket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10218#issuecomment-813514852:586,error,error,586,https://hail.is,https://github.com/hail-is/hail/pull/10218#issuecomment-813514852,1,['error'],['error']
Availability,Do we need to manually delete the stopped but not deleted instances for batch to recover?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614:81,recover,recover,81,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614,1,['recover'],['recover']
Availability,"Do you have an error message other than that failed post? I'm not seeing why that would stop a deploy. In #13115, the PR healing code is after the `_heal_deploy` code, so I don't know why an exception when healing a PR would stop a deploy from occurring. This POST error is also occurring on GCP. Definitely something to fix but I'm not sure why it's related. > This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. Unfortunately I'm not sure if this is relevant in Azure. Azure CI thinks about merge candidate when it comes to testing PRs, but it doesn't merge any PRs and whether or not it does a deploy just depends if there's a new commit on `main`, it shouldn't have to do with PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065,2,['error'],['error']
Availability,"Do you mean when showing the log in the UI or the final upload to blob storage?. I think that's a great idea for the UI, where we show a truncated view in the page and the download button provides a way to stream the log file without loading it into memory on the front-end. In terms of the final upload, I'm a little wary about making a breaking change like that. It's probably true that if you're spewing tons of logs as a user you probably want to not do that. But if we move later to hosting logs in user-provided buckets instead of our own bucket there's no reason why they shouldn't be able to write large logs if they want to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1570846197:172,down,download,172,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1570846197,1,['down'],['download']
Availability,Do you want me to adopt this and make sure it doesn't break anything / see if the error messages disappear?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1710276555:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1710276555,1,['error'],['error']
Availability,"Does the version of spark matter? such as apache spark 2.0.2 and the cloudera spark?; We use the cloudera hadoop,but for hail, the cloudera'spark can't work,so in the configuration we replaced the cloudera spark with the apache spark2.0.2,and this works in local mode,but have errors in cluster mode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969:277,error,errors,277,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969,2,['error'],['errors']
Availability,Doing and administrator merge override because I'm afraid CI won't be alive long enough to successfully test this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4572#issuecomment-430813198:70,alive,alive,70,https://hail.is,https://github.com/hail-is/hail/pull/4572#issuecomment-430813198,1,['alive'],['alive']
Availability,"Don't commit this yet. I intentionally introduced an error to make sure it fails. Although it failed for a different reason, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/540#issuecomment-237322879:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/540#issuecomment-237322879,1,['error'],['error']
Availability,"Don't know what's going on with `test_linreg` failing. It passes locally, and the error ""java.io.FileNotFoundException: /tmp/blockmgr-0a5af284-ccc3-4893-bfcc-bf840e7b1973/1e/broadcast_3344 (No space left on device)"" looks like a CI issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4189#issuecomment-415088122:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/pull/4189#issuecomment-415088122,1,['error'],['error']
Availability,Don't know why the diff looks so weird -- I just dropped benchmark down one level into `benchmark/run`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521:67,down,down,67,https://hail.is,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521,1,['down'],['down']
Availability,"Done!. Thanks Tim!. On Wed, Feb 1, 2017 at 8:24 AM, Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* commented on this pull request.; >; > Need just one tiny change to the py/j connector. Looks great!; > ------------------------------; >; > In python/hail/dataset.py; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>:; >; > > @@ -2336,6 +2336,22 @@ def mendel_errors(self, output, fam):; > pargs = ['mendelerrors', '-o', output, '-f', fam]; > self.hc._run_command(self, pargs); >; > + def min_rep(self):; > + """"""; > + Gives minimal, left-aligned representation of alleles. Note that this can change the variant position.; > +; > + ** Examples **; > + 1) Simple trimming of a multi-allelic site, no change in variant position; > + `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`; > +; > + 2) Trimming of a bi-allelic site leading to a change in position; > + `1:10000:AATAA,AAGAA` => `1:10002:T:G`; > +; > + """"""; > + jvds = self._jvds.minrep(); >; > add in the try: / except: here, following the other methods in dataset.py.; >; > The default py4j errors look horrible, so calling our wrapper method helps; > a lot.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/1324#pullrequestreview-19546823>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADVxgaQoXMxYMPE_V-RMRgYp5mvNSf-Pks5rYIePgaJpZM4LzbBv>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883:1077,error,errors,1077,https://hail.is,https://github.com/hail-is/hail/pull/1324#issuecomment-276663883,1,['error'],['errors']
Availability,Double check that the test is what you want. I made the bounds pretty wide to make sure we never had an unlucky sporadic test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10748#issuecomment-893816262:126,failure,failure,126,https://hail.is,https://github.com/hail-is/hail/pull/10748#issuecomment-893816262,1,['failure'],['failure']
Availability,Down with Travis. Up with Jenkins!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/118#issuecomment-208690564:0,Down,Down,0,https://hail.is,https://github.com/hail-is/hail/issues/118#issuecomment-208690564,1,['Down'],['Down']
Availability,"Downgrading from high priority to normal priority because konrad isn't actually blocked by this. The issue is probably caused by hail downloading the FASTA file once per task. Consider:; ```; import hail as hl; rg = hl.get_reference('GRCh38'); rg.add_sequence('gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz',; 'gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai'); ht = hl.read_table('gs://konradk/liftover_test/gnomad_exomes.ht'); ht.annotate(context=ht.locus.sequence_context(before=1, after=1))._force_count(); ```. `gnomad_exomes.ht` has about 10000 partitions. If you execute this on a 100 node cluster, you'll see that workers will have many copies of the FASTA file:. ```; dking@dk-sw-sczv:~$ du -sh /tmp/hail.*/*.fasta; 3.1G	/tmp/hail.iNLnbdai1pJe/00000.fasta; 3.1G	/tmp/hail.Psc430xLLmdE/00000.fasta; 3.1G	/tmp/hail.RNWZxuNSm6h2/00000.fasta; 3.1G	/tmp/hail.rxwJfyieiIie/00000.fasta; 3.1G	/tmp/hail.w79BrNc7RXOz/00000.fasta; 3.1G	/tmp/hail.yqgUhdCe5I6I/00000.fasta; ```. I think the issue is that a ReferenceGenome is allocated once per shipped JVM bytecode pack. A ReferenceGenome has a FASTAReader which has a SerializableReferenceSequenceFile. That roughly means we allocate one SerializableReferenceSequenceFile per-task. As the tasks:worker ratio gets large, this becomes infeasible. If we move the reference genome management to some broadcasted object, we can ensure it's once per-JVM (in fact one per-JVM for a whole slew of tasks). I'll look into this more at some point soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703:0,Down,Downgrading,0,https://hail.is,https://github.com/hail-is/hail/issues/5371#issuecomment-464225703,2,"['Down', 'down']","['Downgrading', 'downloading']"
Availability,"EDIT2:. OK, so, I'm not sure when this behavior changed but Make 4.0 wants a `\` to indicate that the recipe continues on the next line *but also* passes that backslash and newline to the shell. In Make 3.81, the `\` was also required but the newline and backslash *are not passed* to the shell. In other words: in 3.81, backslash-newline is always replaced with a space and in 4.0, backslash-newline is replaced with a space *except on recipe lines in which case it is necessary to indicate the recipe continues but it is also passed literally to the shell*. The docs page you linked directly addresses our use case and suggests we put the command inside of a make variable (thus triggering normal backslash-newline rules rather than the special recipe line rules). > Sometimes you want to split a long line inside of single quotes, but you don’t want the backslash/newline to appear in the quoted content. This is often the case when passing scripts to languages such as Perl, where extraneous backslashes inside the script can change its meaning or even be a syntax error. One simple way of handling this is to place the quoted string, or even the entire command, into a make variable then use the variable in the recipe. In this situation the newline quoting rules for makefiles will be used, and the backslash/newline will be removed. If we rewrite our example above using this method:; > ; > ```; > HELLO = 'hello \; > world'; > ; > all : ; @echo $(HELLO); > ```; > ; > we will get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less leg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:1069,error,error,1069,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,2,['error'],['error']
Availability,ETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2451,echo,echo,2451,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"E_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resour",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1097,echo,echo,1097,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['echo'],['echo']
Availability,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:347,down,downsampled,347,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307,15,['down'],"['downed', 'downsample', 'downsampled']"
Availability,Either is fine with me -- just ping me when you are ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9934#issuecomment-776937414:31,ping,ping,31,https://hail.is,https://github.com/hail-is/hail/pull/9934#issuecomment-776937414,1,['ping'],['ping']
Availability,Either sphinx cached incorrectly or it's not writing the added text: https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocs/18456:id/www/hail/expr/hail.expr.TInt.html#hail.expr.TInt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806:99,down,download,99,https://hail.is,https://github.com/hail-is/hail/pull/1691#issuecomment-295269806,1,['down'],['download']
Availability,Er sorry needs to be a nicer error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6062#issuecomment-490092137:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/6062#issuecomment-490092137,1,['error'],['error']
Availability,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"Error occurs in 0.2.18 and 0.2.17, but not in 0.2.16",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6634#issuecomment-511214882:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/6634#issuecomment-511214882,1,['Error'],['Error']
Availability,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:0,Error,Error,0,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738,3,"['Error', 'error']","['Error', 'errors']"
Availability,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:192,ERROR,ERROR,192,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417,2,['ERROR'],['ERROR']
Availability,"Example VCF parse error now looks like:. ```; Error summary: HailException: sample.vcf:column 1862: invalid character 'x' in integer literal; ... :80,0:80:13:0,13,2219 0/1:65,19:94:9x9:233,0,1732 0/0:34,3:45:74:0,74,12 ...; ^; offending line: 20	10273694	.	CT	C	29059.60	VQSRTrancheINDEL97.00to99.00	HWP...; see the Hail log for the full offending line; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2421#issuecomment-343983297:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/2421#issuecomment-343983297,2,"['Error', 'error']","['Error', 'error']"
Availability,"Example batch job created (1 burn in, 1 iteration, 1 job per benchmark). Failures are legit test failures.; https://batch.hail.is/batches/8181721",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14565#issuecomment-2142867787:73,Failure,Failures,73,https://hail.is,https://github.com/hail-is/hail/pull/14565#issuecomment-2142867787,2,"['Failure', 'failure']","['Failures', 'failures']"
Availability,Example error:. ```; Caused by: is.hail.relocated.org.json4s.MappingException: No usable value for value_parameter_names; No usable value for str; Did not find value which can be converted into java.lang.String; 	at is.hail.relocated.org.json4s.reflect.package$.fail(package.scala:53); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:638); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:689); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder$$anonfun$3.applyOrElse(Extraction.scala:688); 	at scala.PartialFunction.$anonfun$runWith$1$adapted(PartialFunction.scala:145); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at scala.collection.TraversableLike.collect(TraversableLike.scala:407); 	at scala.collection.TraversableLike.collect$(TraversableLike.scala:405); 	at scala.collection.AbstractTraversable.collect(Traversable.scala:108); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.instantiate(Extraction.scala:688); 	at is.hail.relocated.org.json4s.Extraction$ClassInstanceBuilder.result(Extraction.scala:767); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$extract$10(Extraction.scala:462); 	at is.hail.relocated.org.json4s.Extraction$.$anonfun$customOrElse$1(Extraction.scala:780); 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127); 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126); 	at scala.PartialFunction$$anon$1.applyOrElse(PartialFunction.scala:257); 	at is.hail.relocated.org.json4s.Extraction$.customOrElse(Extraction.scala:780); 	at is.hail.relocated.org.json4s.Extraction$.extract(Extraction.scala:454); 	at is.hail.relocated.org.json4s.Extraction$.org$json4s$Extraction$$extractDete,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/pull/14579#issuecomment-2163457890,2,['error'],['error']
Availability,Example failure: https://ci.hail.is/batches/202357/jobs/104,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10153#issuecomment-792776867:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/10153#issuecomment-792776867,1,['failure'],['failure']
Availability,"ExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9645,failure,failure,9645,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['failure'],['failure']
Availability,"F=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:18859,Toler,Tolerations,18859,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['Toler'],['Tolerations']
Availability,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:85,error,error,85,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197,2,['error'],['error']
Availability,"FWIW, cutting it down to 2446 aggregators work (sat in code generation for about a minute but then ran in 18 seconds)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4516#issuecomment-428425318:17,down,down,17,https://hail.is,https://github.com/hail-is/hail/issues/4516#issuecomment-428425318,1,['down'],['down']
Availability,"FWIW, in my current run I've successfully run 14440 jobs and seen 36 deadlock errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568581842:78,error,errors,78,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568581842,1,['error'],['errors']
Availability,"FWIW, it is currently available at www.hail.is but we'll fix the top level domain",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14616#issuecomment-2230929637:22,avail,available,22,https://hail.is,https://github.com/hail-is/hail/issues/14616#issuecomment-2230929637,1,['avail'],['available']
Availability,"FWIW, the per-row copy was super painful. Once I filtered down to the only the things I needed, I saw ~10X speedups in some cases. Will be good to get rid of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3421#issuecomment-383730398:58,down,down,58,https://hail.is,https://github.com/hail-is/hail/pull/3421#issuecomment-383730398,1,['down'],['down']
Availability,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:14,echo,echo,14,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414,2,['echo'],['echo']
Availability,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:172,avail,available,172,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706,1,['avail'],['available']
Availability,"FYI I've made 1000 genomes phase 3 MTs publicly available here: ; ```; gs://hail-datasets/hail-data/1000_genomes_phase3_{autosomes,chrX,chrY,chrMT}.GRCh37.mt; gs://hail-datasets/hail-data/1000_genomes_phase3_{autosomes,chrX,chrY,chrMT}.GRCh38.liftover.mt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4160#issuecomment-413959206:48,avail,available,48,https://hail.is,https://github.com/hail-is/hail/issues/4160#issuecomment-413959206,1,['avail'],['available']
Availability,"FYI It failed due to transient connection reset by peer, I added ECONNRESET to the list of transient OS errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7631#issuecomment-559284193:104,error,errors,104,https://hail.is,https://github.com/hail-is/hail/pull/7631#issuecomment-559284193,1,['error'],['errors']
Availability,FYI The last build failed due to a transient error (aiohttp.ServerDisconnectedError). I added it to the retry logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7605#issuecomment-557905128:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/7605#issuecomment-557905128,1,['error'],['error']
Availability,"FYI, I was seeing some intermittent test_batch failures where the service wasn't quite up yet in spite of the deployment being available. I added a wait for service option which just hits the /healtcheck endpoint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769:47,failure,failures,47,https://hail.is,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769,2,"['avail', 'failure']","['available', 'failures']"
Availability,"FYI, git clone can clone a single branch: https://stackoverflow.com/questions/1778088/how-do-i-clone-a-single-branch-in-git. > k run. Might be worth benchmarking in batch2, since that is where it will run. (k run! Shame on you!). > But the download drops from 4.7 to ~1.5. I don't understand this. Drops compared to what? Where'd the 4.7 come from?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560446660:240,down,download,240,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560446660,1,['down'],['download']
Availability,"FYI, includes missing dependencies for batch_deploy and ci_deploy on create_accounts that can cause race condition failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460:115,failure,failures,115,https://hail.is,https://github.com/hail-is/hail/pull/6193#issuecomment-496358460,1,['failure'],['failures']
Availability,"Failed due to intermittent dataproc failure (sign), this just needs a bump.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5985#issuecomment-489738406:36,failure,failure,36,https://hail.is,https://github.com/hail-is/hail/pull/5985#issuecomment-489738406,1,['failure'],['failure']
Availability,Failed with a 500 error from batch 2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287#issuecomment-542216720:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-542216720,1,['error'],['error']
Availability,Failing due to some mypy silliness but only in the pip installed images. Need to somehow modify this to make sure we have all appropriate stubs available and/or ignore missing stubs?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1059677772:144,avail,available,144,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1059677772,1,['avail'],['available']
Availability,"Fails with:; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 491 in stage 1.0 failed 20 times, most recent failure: Lost task 491.19 in stage 1.0 (TID 17951, exomes-sw-gf9k.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:95,failure,failure,95,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,2,['failure'],['failure']
Availability,"Failure in image_fetcher_image:. ```; Traceback (most recent call last):; File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 279, in run; await docker_call_retry(self.container.start); File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 86, in docker_call_retry; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start; data=kwargs; File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'read unix @->@/containerd-shim/moby/7f911041e4fcc5ea78b6b3979d1232d0954193bced0e1e85acd2133b80cc463e/shim.sock: read: connection reset by peer: unknown'); ```. I will add this to the retry list. Another reason to drop docker ASAP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320,1,['Failure'],['Failure']
Availability,Failure seems to be a spurious cluster create error:. ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'; ```. Is this common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064:0,Failure,Failure,0,https://hail.is,https://github.com/hail-is/hail/pull/6008#issuecomment-488185064,4,"['ERROR', 'Failure', 'error']","['ERROR', 'Failure', 'error']"
Availability,Failures:; - a Java test I created that should fail to make sure the memory checks are happening; - Python test_ld_score; - A docs test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858:0,Failure,Failures,0,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858,1,['Failure'],['Failures']
Availability,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:576,avail,available,576,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,2,"['avail', 'error']","['available', 'error']"
Availability,Finally tracked down on the failures. This should be ready to go now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5139#issuecomment-455036052:16,down,down,16,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-455036052,2,"['down', 'failure']","['down', 'failures']"
Availability,"Finally working! Ugh, that was painful. Changes I made since I closed:; - You can't broadcast an object which has a reference to its own broadcast (e.g. ReferenceGenome => locusType => rgBc). I made locusType transient and recompute after serialization.; - Removed BroadcastSerializable. I can't figure out how to check ReferenceGenome/RVDPartitioner are only serialized during partitioning. This is basically a failure of the Kryo interface. I might try again sometime when I'm feeling beat down by serialization.; - Removed removeReference. This just isn't something we can support (except in isolated situations like tests, and I fixed those.) Now, if you add a reference, it only throws an error if an existing reference exists by that name and is incompatible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362:412,failure,failure,412,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-473088362,3,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,"First, this actually succeeded:. ```; $ hdfs dfs -ls /user/aganna/CANCER.vep.vds/rdd.parquet/_SUCCESS; -rw-r--r-- 2 aganna supergroup 0 2016-04-18 20:14 /user/aganna/CANCER.vep.vds/rdd.parquet/_SUCCESS; ```. It crashed while the program was shutting down. I know what caused it and it has been fixed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/303#issuecomment-211716001:250,down,down,250,https://hail.is,https://github.com/hail-is/hail/issues/303#issuecomment-211716001,1,['down'],['down']
Availability,Fix the `(`. Mention in docs the three available distributions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1318#issuecomment-276210949:39,avail,available,39,https://hail.is,https://github.com/hail-is/hail/pull/1318#issuecomment-276210949,1,['avail'],['available']
Availability,"Fixed a rebase failure, deleted some unnecessary whitespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6333#issuecomment-501729022:15,failure,failure,15,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501729022,1,['failure'],['failure']
Availability,"Fixed error message in 4d38cca, new issue supercedes in #376",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-218033798:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-218033798,1,['error'],['error']
Availability,"Fixes #1368 . Yes, TArray is IndexSeq, I've verified it fixes the error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1369#issuecomment-278473040:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/1369#issuecomment-278473040,1,['error'],['error']
Availability,"Fixes this error msg:. ```; jinja2.exceptions.TemplateSyntaxError: Encountered unknown tag 'endfor'. You probably made a nesting mistake. Jinja is expecting this tag, but currently looking for 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6414#issuecomment-504040126:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/pull/6414#issuecomment-504040126,1,['error'],['error']
Availability,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396:676,avail,available,676,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396,2,['avail'],['available']
Availability,"For now, downgrade:; ```; pip3 install 'ipython<8.17'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300:9,down,downgrade,9,https://hail.is,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300,1,['down'],['downgrade']
Availability,For some reason this is timing out after a minute in my dev deploy even though I've removed the heartbeat and it's working on a local server I have running. Need to investigate further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047:96,heartbeat,heartbeat,96,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047,1,['heartbeat'],['heartbeat']
Availability,"For your consideration… We often have a `pathlib.Path` or `cloudpathlib.CloudPath` that we've built up by parts, which is then the path to be used as an input resource:. ```python; res = mybatch.read_input(str(mycloudpath)); ```. Periodically we accidentally omit the `str(…)`, which leads to a semi-obscure error message and an extra editing round-trip. There is a point of view that `read_input()` and `read_input_group()` could also accept `os.PathLike` objects directly, and have Hail convert them to `str` itself, e.g. in `_new_input_resource_file()` which underlies both methods, as per this PR. The difficulty is how to do that conversion: `str(…)` does the trick for [`pathlib.Path`](https://docs.python.org/3.12/library/pathlib.html#operators) and [`cloudpathlib.CloudPath`](https://cloudpathlib.drivendata.org/stable/api-reference/cloudpath/), returning the path and URL, respectively, as a string. But it looks like in theory there might be [`os.PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) subclasses that don't define `__str__()` to produce a usable path/URL. The official conversion method appears to be [`os.fspath()`](https://docs.python.org/3/library/os.html#os.fspath), but that does not do the right thing for `cloudpath.CloudPath` — there it downloads the remote file and returns a local path — which is not at all what Hail needs. However probably this is a theoretical concern and `str(…)` will be fine…",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965:308,error,error,308,https://hail.is,https://github.com/hail-is/hail/pull/14544#issuecomment-2105616965,4,"['down', 'error']","['downloads', 'error']"
Availability,Force pushed to resolve errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1775#issuecomment-302124342:24,error,errors,24,https://hail.is,https://github.com/hail-is/hail/pull/1775#issuecomment-302124342,1,['error'],['errors']
Availability,"Friendly ping, as merging this would help keep our fork in sync.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1143064464:9,ping,ping,9,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1143064464,1,['ping'],['ping']
Availability,"From a fresh clone, the above (modified with `rm -f`) fails with: ; ```bash; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; /home/edmund/.local/src/hail/.venv/bin/python3: Error while finding module specification for 'hailtop.aiotools.copy' (ModuleNotFoundError: No module named 'hailtop'); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```. I'll try again with `hailtop` installed - just wanted to point out the dependency failure in `Makefile`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777:821,Error,Error,821,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"From triage discussion. What this means: Instance destruction is slower than it needs to be, this can impact throughput but does not really impact reliability, and is uncommon enough to not be high priority at this time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14380#issuecomment-1977202541:147,reliab,reliability,147,https://hail.is,https://github.com/hail-is/hail/issues/14380#issuecomment-1977202541,1,['reliab'],['reliability']
Availability,Funny I actually did this a few weeks ago and then realized it was logged further down. It's around line 76 in a random log I just looked at. I could move it to log from python early on?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747:82,down,down,82,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747,1,['down'],['down']
Availability,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:656,error,error,656,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,2,['error'],['error']
Availability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:811,error,error,811,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,1,['error'],['error']
Availability,"GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; retur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1931,Error,Error,1931,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['Error'],['Error']
Availability,Getting more failures :(. Let's sit down tomorrow and I'll show you how to build docs locally,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-529649760:13,failure,failures,13,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-529649760,2,"['down', 'failure']","['down', 'failures']"
Availability,Getting some weird CI errors: the hail module isn't found,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-669870559:22,error,errors,22,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-669870559,1,['error'],['errors']
Availability,"Good comments, thanks. I think I addressed everything. There were some pending bugs from moving things to workshop.hail.is. My namespace should be up to date now, let me know if you're still seeing 500 errors. I'm still working on the remaining todo items, but those are independent and I think this is ready to go in (when you approve).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-535317771:202,error,errors,202,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535317771,1,['error'],['errors']
Availability,"Good idea, I'll check. I feel like I initially found this in deep in a redhat tutorial, but ultimately found it again at the bottom of the [man page](https://man7.org/linux/man-pages/man8/xfs_quota.8.html). I was following this example:; ```; Enabling project quota on an XFS filesystem (restrict files in; log file directories to only using 1 gigabyte of space). # mount -o prjquota /dev/xvm/var /var; # echo 42:/var/log >> /etc/projects; # echo logfiles:42 >> /etc/projid; # xfs_quota -x -c 'project -s logfiles' /var; # xfs_quota -x -c 'limit -p bhard=1g logfiles' /var. Same as above without a need for configuration files. # rm -f /etc/projects /etc/projid; # mount -o prjquota /dev/xvm/var /var; # xfs_quota -x -c 'project -s -p /var/log 42' /var; # xfs_quota -x -c 'limit -p bhard=1g 42' /var; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396:405,echo,echo,405,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396,2,['echo'],['echo']
Availability,Good work hunting this down; these messages are always a pain to find.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10713#issuecomment-891277817:23,down,down,23,https://hail.is,https://github.com/hail-is/hail/pull/10713#issuecomment-891277817,1,['down'],['down']
Availability,Got it. +1 for a more specific error message if possible.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3760#issuecomment-397472784:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/issues/3760#issuecomment-397472784,1,['error'],['error']
Availability,"Gotcha. Yes, your test works. I'll throw an error for now, since this sounds like not a use case we want to support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7384#issuecomment-546397181:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546397181,1,['error'],['error']
Availability,"Great change. Still an error related to hail_pip_version in the tests, though. Can you make a discuss post when this goes in to alert people compiling their own builds?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-457196817:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-457196817,1,['error'],['error']
Availability,"Great question! I have not thought to run black on query and agree with your point about it maybe not being a good match. The intention here was just for formatting internal python services, not necessarily as part of the CI but for ease of development and not having to fix `pylint` format errors by hand. Even so I don't love quite how restrictive black is, but it does seem like the most popular formatter out there now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-769082082:291,error,errors,291,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-769082082,1,['error'],['errors']
Availability,Great that error's resolved! but you'll need to compile Hail against the version of Spark you downloaded:. ```; ./gradlew -Dspark.version=2.1.1 shadowJar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302838096:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302838096,2,"['down', 'error']","['downloaded', 'error']"
Availability,"Great, perfect, I just wanted to make sure there wasn't another higher level issue to fix. This is great for now, but I just want to reiterate need for Error state and reason_for_error in the batch schema.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6249#issuecomment-498796054:152,Error,Error,152,https://hail.is,https://github.com/hail-is/hail/pull/6249#issuecomment-498796054,1,['Error'],['Error']
Availability,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:104,down,download,104,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090,3,['down'],"['download', 'downstream']"
Availability,"Had to chase down a latent bug in `StreamAgg`, but I think everything should be sorted now (pun intended).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405#issuecomment-1673830234:13,down,down,13,https://hail.is,https://github.com/hail-is/hail/pull/13405#issuecomment-1673830234,1,['down'],['down']
Availability,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-240550289:311,error,error,311,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289,4,"['error', 'reliab']","['error', 'reliable', 'reliably']"
Availability,"Handler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Ve",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13392,AVAIL,AVAILABLE,13392,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"Handler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6600,AVAIL,AVAILABLE,6600,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"Happy to commit this if it passes tests. Looks like you've got a rebase error, though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5485#issuecomment-468401938:72,error,error,72,https://hail.is,https://github.com/hail-is/hail/pull/5485#issuecomment-468401938,1,['error'],['error']
Availability,"Has the math never rendered on the CI server? I see MathJax.js downloaded successfully but math isn't rendered, for example, in `lmmreg`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2145#issuecomment-324995288:63,down,downloaded,63,https://hail.is,https://github.com/hail-is/hail/pull/2145#issuecomment-324995288,1,['down'],['downloaded']
Availability,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:47,error,error,47,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734,2,['error'],['error']
Availability,"Heh, I separately discovered the test failure and fixed at https://github.com/hail-is/hail/pull/13477",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13470#issuecomment-1688883263:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/13470#issuecomment-1688883263,1,['failure'],['failure']
Availability,"Heh, so turns out that `test_weird_urls` is missing the `@pytest.mark.asyncio` decorator, and so it was getting skipped with a warning this whole time. The pytest upgrade added auto-detection of async tests and so it ran this broken test for the first time. I'm PR'ing to treat most warnings as errors in #12322.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498:295,error,errors,295,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498,1,['error'],['errors']
Availability,"Heh, this is all my fault: https://github.com/hail-is/hail/pull/5078/files",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5692#issuecomment-479642895:20,fault,fault,20,https://hail.is,https://github.com/hail-is/hail/pull/5692#issuecomment-479642895,1,['fault'],['fault']
Availability,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:94,Error,Error,94,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:364,error,error,364,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374,1,['error'],['error']
Availability,"Here are the alternatives that I see:. The original design:; ```scala; object LowerArrayToStream {; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]); streamified = ToArray(streamified). if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]); ToStream(node); else; node; }; }; }. private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; }; }. def apply(node: IR): IR = boundary(node); }; ````. the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. This can be fixed using TContainer instead of TArray. But as soon as you do this, you need to make sure you're never generating ToArray(ToStream(something of type TDict or TSet)), which means you need the if check in the present PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586598280:1009,error,errors,1009,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598280,1,['error'],['errors']
Availability,Here is the problematic command:. `annotateglobal table \; -i file:///humgen/atgu1/fs03/wip/aganna/HCSCORE/genelists/all_scores.scores \; -r global.all_scores \; annotateglobal expr -c 'global.GWAS_height = global.all_scores.filter(x => x.GWAS_HEIGHT == '1').map(x => x.V1)' \; annotatevariants expr -c 'va.andrea.test = global.GWAS_height.toSet.contains(va.andrea.genename)' \`. The shell was eliding the single quote and we were comparing a String and an Int. That should be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/388#issuecomment-219845067:480,error,error,480,https://hail.is,https://github.com/hail-is/hail/issues/388#issuecomment-219845067,1,['error'],['error']
Availability,"Here is what I get when invoking pyspark. $ pyspark; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/02 13:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/02 13:56:47 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.13 (default, Jul 18 2017 09:16:53); SparkSession available as 'spark'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996:1396,avail,available,1396,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996,1,['avail'],['available']
Availability,Here's a clear instance of buffer corruption after a transient error (in this case an SSLException). https://batch.hail.is/batches/7996481/jobs/182741; ```; 2023-09-13 16:37:36.612 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 2: /batch/1c00c7157d4d41bcbf508f12d75329b1; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 3: /batch/1c00c7157d4d41bcbf508f12d75329b1/log; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 4: gs://hail-query-ger0g/jars/be9d88a80695b04a2a9eb5826361e0897d94c042.jar; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 5: worker; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 6: gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho=; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 7: 38854; 2023-09-13 16:37:36.613 JVMEntryway: INFO: 8: 47960; 2023-09-13 16:37:36.613 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-13 16:37:36.614 Worker$: INFO: is.hail.backend.service.Worker be9d88a80695b04a2a9eb5826361e0897d94c042; 2023-09-13 16:37:36.614 Worker$: INFO: running job 38854/47960 at root gs://gnomad-tmp-4day/parallelizeAndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:63,error,error,63,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['error'],['error']
Availability,"Here's a typical interaction for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:126,FAILURE,FAILURE,126,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,2,['FAILURE'],['FAILURE']
Availability,"Here's my proposed interface (names to be changed, I'm terrible at those). ```; case class WithSource[T](value: T, source: InputSource) {; def map[U](f: T => U): WithSource[U] = {; try {; copy[U](value = f(value)); } catch {; case e: Exception => source.wrapError(e); }; }; }. abstract class InputSource {; def wrapError(e: Exception): Nothing; }. case class TextSource(line: String, file: String, position: Option[Int]) extends InputSource {; def wrapError(e: Exception): Nothing = {; val msg = e match {; case _: FatalException => e.getMessage; case _ => s""caught $e""; }; val lineToPrint =; if (line.length > 62); line.take(59) + ""...""; else; line. log.error(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $line"""""".stripMargin); fatal(; s""""""; |$file${position.map(ln => "":"" + (ln + 1)).getOrElse("""")}: $msg; | offending line: $lineToPrint"""""".stripMargin); }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233012302:655,error,error,655,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233012302,1,['error'],['error']
Availability,Here's one of the easier solutions: http://discuss.hail.is/t/python-error-importerror-no-module-named-decorator/131/4,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1459#issuecomment-283822337:68,error,error-importerror-no-module-named-decorator,68,https://hail.is,https://github.com/hail-is/hail/issues/1459#issuecomment-283822337,1,['error'],['error-importerror-no-module-named-decorator']
Availability,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:296,redundant,redundant,296,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,2,['redundant'],['redundant']
Availability,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-248645143:242,error,errors,242,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143,6,"['down', 'error']","['download', 'downloaded', 'errors']"
Availability,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/446#issuecomment-234642054:498,error,error,498,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054,2,['error'],['error']
Availability,"Hey @danking,; Thanks so much for the advice. Your team has been very helpful and responsive. - I made the adjustment to my `create_intervals` function; - Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. I tried your suggestion of writing the data to a table first, but my `cols` field doesn't contain the computed HWE values. These are contained within the `entries` field. Table description is below. I tried modifying the code to what is shown below but I'm still having the same issue. Also tried increasing the RAM to max available per CPU. One thing I noticed is the `mt_hwe_vals` variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. ```; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'ancestry': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'filters': set<str>; 'variant_qc': struct {; gq_stats: struct {; mean: float64, ; stdev: float64, ; min: float64, ; max: float64; }, ; call_rate: float64, ; n_called: int64, ; n_not_called: int64, ; n_filtered: int64, ; n_het: int64, ; n_non_ref: int64, ; het_freq_hwe: float64, ; p_value_hwe: float64, ; p_value_excess_het: float64; }; 'info': struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>; }; 'a_index': int32; 'was_split': bool; ----------------------------------------; Entry fields:; 'hwe': struct {; het_freq_hwe: float64, ; p_value: float64; }; ----------------------------------------; Column key: ['ancestry']; Row key: ['locus', 'alleles']; ----------------------------------------; ```. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)). # T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492:589,avail,available,589,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674895492,2,['avail'],['available']
Availability,"Hey @dlcotter ! Thanks for the report. I anticipate a fix in the next version of Hail. For now, I think you can fix with `pip3 install 'parsimonious>=0.9'` or downgrading to Python 3.10",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443:159,down,downgrading,159,https://hail.is,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443,1,['down'],['downgrading']
Availability,"Hey @iris-garden !. This was a PR for a new tutorial that our summer intern, Aleisha, worked on during her internship. Could you do a review for me? The goal here is to make sure there's no typos and that each cell executes without error. Can you also build the docs and make sure the rendered notebook looks OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12097#issuecomment-1266105755:232,error,error,232,https://hail.is,https://github.com/hail-is/hail/pull/12097#issuecomment-1266105755,1,['error'],['error']
Availability,Hey @mhebrard !. I'm really sorry Hail has been such a pain to install. This looks to me like a Scala version incompatibility. In your Makefile you specified this:; ```; ... SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; ```; [EMR's docs](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-691-release.html) confirms that Scala 2.12.15 should be installed. I think my next questions are:; 1. Which `spark-shell` is that?; 2. What latent JVMs are around?; 3. What's the class path and what's on it?; 4. What scala executables are around?. ```; which spark-shell; spark-shell --version; which java; java -version; which scala; scala -version; echo $CLASSPATH; ```. That `SettingsOps` is an implicit nested class of the `MutableSettings` object. It is definitely present in [2.13](https://github.com/scala/scala/blob/2.13.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L70-L88) and [2.12](https://github.com/scala/scala/blob/2.12.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L83-L94). It appears to be missing in [2.11](https://github.com/scala/scala/blob/2.11.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L64-L68). It appears to have arrived in [2.12.14](https://github.com/scala/scala/commit/3bd24299fc34e5c3a480206c9798c055ca3a3439).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525:642,echo,echo,642,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525,1,['echo'],['echo']
Availability,"Hey @natestockham,. I'm glad you resolved the breeze issue. I imagine you were encountering a situation where the native libraries were not extant / not where expected / not the correct architecture. Three of the newly failing tests are related to plink. The output included in `tests.zip` indicates that you're using a fairly old version of plink,; ```; PLINK v1.90b1b 64-bit (20 May 2014); ```; Our testing server uses versions of plink from 2016. It's possible these tests are over constrained and need to be relaxed. I will investigate the precision required to pass the two tests in `IBDSuite`. However, part of one failure in the `IBDSuite` and the failure in the `ImputeSexSuite` are both caused by plink failing to produce output on certain input files. I strongly suspect these are bugs in plink version `1.90b1b` because plink `1.90b3.38` (from 2016, the version used on our test server) does not err on such files. This leaves one final test: `LinearMixedRegressionSuite.genAndFitLMM`. This is the test I have been writing about above and I can confirm that this is a bug (or, perhaps, overly precise test) **on our end** that we are actively investigating. Hail is usable even though the tests do not pass (you can run `./gradlew shadowJar` to produce a working jar), but I will advise you against relying on the results of `lmmreg` until we can confirm why this test is failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771:621,failure,failure,621,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281861771,4,['failure'],['failure']
Availability,"Hey @poneill !. `-O`/`PYTHONOPTIMIZE` is explicitly defined as the ""turn off asserts"" option in [the docs](https://docs.python.org/3/using/cmdline.html#cmdoption-O). If you disable asserts, you'll get even more inscrutable errors. I recommend against doing that. If you see any `assert(x, y)` in the code base, please file a PR or a bug. We'll fix it. We will not replace asserts with if-raise. ---. As to the bug you've found: yes this is a bug in Hail. We incorrectly assume that if there is at least one dataset with the right version and at least one dataset with the right reference genome that there's a dataset with the right version *and* reference genome. That logic is obviously false. I'll have someone fix this in the next couple weeks. As to the root issue: the Hail annotation database doesn't have a GRCh38 version of `gnomad_pca_variant_loadings` version 2.1. This is because [gnomAD](https://gnomad.broadinstitute.org/downloads#v2-liftover) hasn't published a GRCh38 version of their 2.1 variant loadings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952#issuecomment-1530239230:223,error,errors,223,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1530239230,2,"['down', 'error']","['downloads', 'errors']"
Availability,"Hey @seanjosephjurgens !. Sorry you're running into trouble. This error message is bad. See https://github.com/hail-is/hail/issues/13346 for that bug. The real issue here is VCF INFO fields like:; ```; AS_BaseQRankSum=0.000,.,0.100,0.500; ```; The VCF spec doesn't explicitly permit missing values as elements of INFO or FORMAT fields. It does permit the whole field to be missing a la `FIELD=.` but `FIELD=1,.,1` or `FIELD=.,.,.` are not explicitly permitted. In particular, `FIELD=.` could mean ""this field is missing"" or ""this field is not-missing, it is a one-element array containing one missing value"". The fix is to use `hl.import_vcf(..., array_elements_required=False)`. When that is true, Hail will parse `1,.,1` as `[1, NA, 1]`. Be forewarned: Hail treats `FIELD=.` as a missing field, not an array with one missing element.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14102#issuecomment-1860989511:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/issues/14102#issuecomment-1860989511,1,['error'],['error']
Availability,"Hey @tpoterba I tried, but am getting ; `remote: Permission to broadinstitute/hail.git denied to bw2.; fatal: unable to access 'https://github.com/broadinstitute/hail/': The requested URL returned error: 403`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248072411:197,error,error,197,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248072411,1,['error'],['error']
Availability,Hey Tim! Just curious what the status on this bug is. I also just got this error (I did update to the newest version incase it was fixed).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721,1,['error'],['error']
Availability,"Hi @Sun-shan,. First, I should note that we do not currently test hail against Spark version 2.2.0, I recommend using Spark 2.1.1 or 2.0.2. Spark versions aside, the error you encountered is unrelated to Spark, as far as I know. What version of the `decorator` package is installed on your machine? `decorator` version 4.0.10 should work correctly. Unfortunately, we are still looking for a python dependency management solution. My apologies that you've run into this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534,1,['error'],['error']
Availability,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-240446097:97,down,downloaded,97,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097,5,"['down', 'error']","['download', 'downloaded', 'errors']"
Availability,"Hi @danking, I see the initial CI result failed, but I'm unable to login and see what the failure is. I signed in to google with my popgen account, and get 504 Gateway Time-out on the `auth.hail.is/oauth2callback`, I imagine because I don't have an account there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-784542735:90,failure,failure,90,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-784542735,1,['failure'],['failure']
Availability,"Hi @danking, sorry this took me a little to test. I think there's a problem with the latest changes, in my dev-deploy, it failed on the '`create_certs` and `create_accounts`, with the error:. ```; FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/hailtop/hail_version'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814:184,error,error,184,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814,1,['error'],['error']
Availability,"Hi @danking, thanks for this. On the topic of asserts, there are really two interacting issues:. 1. Asserts are intended to ensure invariants, i.e. conditions that should always be true. In correct code, assertions should never raise so disabling them should have no consequences at runtime. In practice, however, they are often casually used to catch value errors, which can be expected to occur if a user-facing method receives bad/nonsensical inputs (e.g. here: https://github.com/hail-is/hail/blob/1940547d35ddddb084ad52684e36153c1e03a331/hail/python/hailtop/hailctl/dataproc/diagnose.py#L62); 2. Python's language design allows anyone calling your code to disable asserts for optimization purposes, because disabling asserts should never change the semantics of the program. Putting these two features together, you can arrive at a situation where a user thinks they're turning off asserts (which should never raise anyway) and instead stops catching value errors (whose absence can never be guaranteed). All that said, if the final answer is: ""if you invoke `-O` you deserve what's coming"", I'm happy to drop it :). Thanks for taking a look at the example. If I understand you correctly, it sounds like I passed the wrong inputs to the function, in which case it might be clearer to raise a ValueError instead of an AssertionError in the end. On a closer look, it seems like most of the instances of `assert(x, y)` are actually in scala code-- my mistake. Thanks again for looking into this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665:358,error,errors,358,https://hail.is,https://github.com/hail-is/hail/issues/12952#issuecomment-1531675665,4,['error'],['errors']
Availability,"Hi @eric-czech,. The errors you're running into with aggregating sorted arrays should be fixed in versions 0.2.31 and later; if upgrading the version works for you, please let me know and I will close this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076#issuecomment-585938806:21,error,errors,21,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-585938806,1,['error'],['errors']
Availability,"Hi @jmarshall, the team talked about this issue in our standup today. We had some concerns about appropriateness of using this table as a long term storage area for larger metadata, and the likely developer effort and system downtime to perform the migration. So we currently don't plan on prioritizing this in the immediate future, but do let us know if you have any concerns about that - or if it ends up being impossible for you to work around this - and we might be able to reconsider (or maybe come up with alternative solutions). Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14702#issuecomment-2384128647:225,downtime,downtime,225,https://hail.is,https://github.com/hail-is/hail/issues/14702#issuecomment-2384128647,1,['downtime'],['downtime']
Availability,"Hi @konradjk Did you ever determine a cause of this? I am seeing very, very similar errors in an unrelated project (but also executing in GCP).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419655426:84,error,errors,84,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419655426,1,['error'],['errors']
Availability,"Hi @mgeaghan, it looks like in this [change from 14 Feb](https://github.com/hail-is/hail/commit/6eacd66b3979bf27982b4d15b5f17c60474148cb) data for US, UK and Europe was moved to regional buckets, however it appears that the Australian data was not moved. At the moment `gs://hail-australia-southeast1-vep` does not exist, yet `gs://hail-aus-sydney-vep` DOES exist. You see the opposite pattern if you try UK, US or EU equivalents. @danking - is this a reasonable explanation? Are you able to move the Australian data, or is there someone else (or a different forum) that we should ask for this?. also pinging @patrick-schultz and @chrisvittal as you both were cc'd in the change referred to above :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14513#issuecomment-2092036876:601,ping,pinging,601,https://hail.is,https://github.com/hail-is/hail/issues/14513#issuecomment-2092036876,1,['ping'],['pinging']
Availability,"Hi @rmporsch,. I'm sorry you're experiencing this error. I've resolved this in https://github.com/hail-is/hail/pull/3589 . After that PR is merged, it should be available in the GS bucket no more than an hour later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3585#issuecomment-389245636:50,error,error,50,https://hail.is,https://github.com/hail-is/hail/issues/3585#issuecomment-389245636,2,"['avail', 'error']","['available', 'error']"
Availability,"Hi @tushu1232, can you post the output of:. ```; gcc --version ; g++ --version; ```. We strongly suggest using GCC 4.7 to compile hail because it has [relatively complete feature support](https://gcc.gnu.org/gcc-4.7/cxx0x_status.html) for C++11. If you must use an older version of C++ you can try changing that argument to `-std=c++0x`. If you have a version of GCC >4.7 and are still seeing this issue, I am happy to help determine why you're getting this error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-276986028:458,error,error,458,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276986028,1,['error'],['error']
Availability,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:865,error,error,865,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['error'],['error']
Availability,"Hi Jerome, this `AnnotationPathException` issue is something we've seen before. It seems to be caused sporadically by gradle's build caching, and can usually be fixed by running `gradle clean`. The tests that failed are probably the ones that require external tools available on the command line:; FisherExactSuite (requires Rscript); ImportPlinkSuite (requires plink 1.9); ExportPlinkSuite (requires plink 1.9); LoadBgenSuite (requires qctool)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240384398:266,avail,available,266,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240384398,1,['avail'],['available']
Availability,"Hi TJ! @tpoterba tells me he told you to make an Issue, but he forgot that we're trying to limit Issues to bug reports. Would you mind reposting this feature request on the forum and we'll follow up there?. http://discuss.hail.is/c/features. Can you also spell out a bit more what information you'd like for each parent-proband trio and how this information can be useful? We think the forum will be an easier place to get community feedback to nail down the best spec for all. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456:450,down,down,450,https://hail.is,https://github.com/hail-is/hail/issues/1055#issuecomment-258299456,2,['down'],['down']
Availability,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:357,error,error,357,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['error'],['error']
Availability,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:166,error,error,166,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['error'],['error']
Availability,"Hi all,. Here's the error message that I get when I go to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-eb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['error'],['error']
Availability,"Hi guys,; I've cloned the master and rebuilt, but unfortunately I get the same error when annotating the VCF just read.; ```; [Stage 0:====================================================>(2111 + 1) / 2112]hail: info: Coerced sorted dataset. ------------Annotate the VCF file-------------; [Stage 1:> (0 + 160) / 2112]; [Stage 1:> (0 + 160) / 2112]found fatal is.hail.utils.package$FatalExcepti; on: swe.vcf.bgz: invalid AD field `24,0,0': expected 2 values, but got 3.; offending line: 1	65684548	.	G	A	3524.14	VQSRTrancheSNP99.60to99.80	AC=1;AF=...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1415#issuecomment-282545923:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282545923,1,['error'],['error']
Availability,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250746848:264,error,errors,264,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848,2,['error'],['errors']
Availability,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:402,down,download,402,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724,1,['down'],['download']
Availability,"Hi!!!. - I was just delivered a wheel of Hail 0.2.128 loaded with this method from Chris, available at: gs://hail-30-day/hailctl/dataproc/cvittal-dev/0.2.128-2863e0eb192c/hail-0.2.128-py3-none-any.whl . Very helpful for removing those temp methods in the PR!. - it looks like the code will have a home inside of something that looks like the following: ; ```; qc = hl.struct(; **{; ann: hl.agg.filter(; expr,; vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt.variant_ac,; variant_atypes=vmt.variant_atypes,; dp=vmt.DP,; ),; ); for ann, expr in argument_dictionary.items(); }; ); ```; inside of a hl.agg.filter call, and where argument_dictionary looks like {'vep_name':ht.vep.field , ""pass_all_vqc"": hl.len(vmt.filters) == 0} and so on. Just to give an idea what the usage is going to be.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1979150641:90,avail,available,90,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1979150641,1,['avail'],['available']
Availability,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561#issuecomment-238502640:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640,1,['error'],['error']
Availability,"Hi, @danking ,. Thanks for the fast and detailed answer.; No worry, it didn't disrupt my workflow, but confused me since the pages are still in the search results but not available and I didn't see the note for moving to VDS in the changelogs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13377#issuecomment-1667580110:171,avail,available,171,https://hail.is,https://github.com/hail-is/hail/issues/13377#issuecomment-1667580110,1,['avail'],['available']
Availability,"Hi, @danking ; I reconfigurated the spark cluster, with the cloudera spark : version 2.2.0.cloudera1; But I can't import hail this time, How can I fix it?. The test:; ```; >>> spark.sparkContext.master; u'yarn'. bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> spark.sparkContext.master; u'yarn'; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/hail/python/hail/__init__.py"", line 1, in <module>; import hail.expr; File ""/opt/Software/hail/python/hail/expr.py"", line 3, in <module>; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; File ""/opt/Software/hail/python/hail/representation/__init__.py"", line 1, in <module>; from hail.representation.variant import Variant, Locus, AltAllele; File ""/opt/Software/hail/python/hail/representation/variant.py"", line 2, in <module>; from hail.typecheck import *; File ""/opt/Software/hail/python/hail/typecheck/__init__.py"", line 1, in <module>; from check import *; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 1, in <module>; from decorator import decorator, getargspec; ImportError: cannot import name getargspec; >>> ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336722486:960,avail,available,960,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336722486,1,['avail'],['available']
Availability,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229750270:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270,1,['error'],['error']
Availability,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. （1）HDFS file path ：/user/hail/hail.log， have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; （2）log file：local PATH， hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252404979:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979,2,['error'],['error']
Availability,"Hi, not sure if this is the right avenue, but I'd also like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:330,error,error,330,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,2,"['avail', 'error']","['available', 'error']"
Availability,"Hi,; When I remove plink from my path I get a bit of a different error. Can you rerun `gradle check` with the `--info` argument? It'll vomit a bunch of details, but the output from those tests should tell us what's going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230192783:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230192783,1,['error'],['error']
Availability,"High level and the tests look great. I'll try to take a closer look tonight, but I'm basically ready to approve once the tests pass. I thought about throwing an error on hl.agg.filter if there isn't an aggregator inside, and I think I agree with you now. At least, if you use an hl.agg.filter, etc. inside an aggregation, like hl.agg.sum(hl.agg.filter(...)), that should given error rather than doing nothing. This is important given that this was the old syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109:161,error,error,161,https://hail.is,https://github.com/hail-is/hail/pull/4555#issuecomment-430448109,2,['error'],['error']
Availability,"Hi，cseed @cseed , I configured the java related to the Spark cluster, as follows：. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command， use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors：; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250697347:871,error,errors,871,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347,7,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"Hm, weird. When I try these tests out against default I get:. ```; FatalError: batch id was 2271614; HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; is.hail.utils.HailException: Hail off-heap memory exceeded maximum threshold: limit 2.25 GiB, allocated 3.35 GiB; Report: 3.4G allocated (192.0K blocks / 3.4G chunks), regions.size = 3, 0 current java objects, thread 9: pool-1-thread-2; 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:78); 	at is.hail.annotations.RegionPool.closeAndThrow(RegionPool.scala:58); 	at is.hail.annotations.RegionPool.incrementAllocatedBytes(RegionPool.scala:73); 	at is.hail.annotations.ChunkCache.newChunk(ChunkCache.scala:75); 	at is.hail.annotations.ChunkCache.getChunk(ChunkCache.scala:130); 	at is.hail.annotations.RegionPool.getChunk(RegionPool.scala:96); 	at is.hail.annotations.RegionMemory.allocateBigChunk(RegionMemory.scala:62); 	at is.hail.annotations.RegionMemory.allocate(RegionMemory.scala:96); 	at is.hail.annotations.Region.allocate(Region.scala:332); 	at __C35collect_distributed_array.__m61split_ToArray(Unknown Source); 	at __C35collect_distributed_array.__m54split_StreamFor(Unknown Source); 	at __C35collect_distributed_array.__m49begin_group_0(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at __C35collect_distributed_array.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); 	at is.hail.utils.package$.using(package.scala:638); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:142); 	at scala.ru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573:579,Error,ErrorHandling,579,https://hail.is,https://github.com/hail-is/hail/pull/11777#issuecomment-1110147573,4,['Error'],['ErrorHandling']
Availability,"Hmm, I realized one problem with this - the error message will return that of coalesce rather than or_else and might be a little confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355,1,['error'],['error']
Availability,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:280,avail,available,280,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145,1,['avail'],['available']
Availability,"Hmm, idempotency is a bit hard to talk about here. This change makes it impossible to not ""cleanup"" the chunks iterator if you hit an exception midway through the chunks iterator. In particular, this now works:; ```; try:; with chunks(...) as data:; raise ValueError(); except ValueError:; pass; with chunks(...) as data:; ... use data ...; ```. In the current code, that does not work. The second call to `chunks` raises an error unless chunks is empty. ---. But you're probably asking about the code that uses chunks? In the Google case it is idempotent: lines 206-215 construct a new request before iterating chunks. The PUT request includes the specific range of bytes we want to write to, so even if we partially succeeded with a previous PUT, this subsequent PUT should overwrite (or, more likely, error). In practice, I don't think we can partially succeed. I think either we write fully or we terminate the connection early and google drops the data. Summary: I think Google is fine. As for Azure, we use a randomly generated block_id. If we error while inside `stage_block` that block_id is never added to `self.block_ids`. As a result, we can safely make a second attempt to upload the block with a new id.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12492#issuecomment-1332686868:425,error,error,425,https://hail.is,https://github.com/hail-is/hail/pull/12492#issuecomment-1332686868,3,['error'],['error']
Availability,"Hmm, so I've been using a branch to run some 10k and 100k scale tests of /bin/true https://github.com/hail-is/hail/pull/7783 and I've found deadlocks to be rather rare?. In that PR, I only changed the known deadlocking calls to be deadlock resilient. However, deadlock errors seem to be a feature of mysql and it seems were always intended to retry them, so I think this PR (7782) is the right solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582:240,resilien,resilient,240,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582,2,"['error', 'resilien']","['errors', 'resilient']"
Availability,"Hmm, so, I see that the py4j backend doesn't error if you `get_flags` for a flag that you haven't previously set. I think it gives you the default value?. The service backend is a bit screwy when it comes to flags because I don't want to run a batch job just to figure out what the Scala-side flags are. I think the right fix is to move all the flag information into Python. What do you think of that @tpoterba ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12307#issuecomment-1276964160:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/12307#issuecomment-1276964160,1,['error'],['error']
Availability,"Hmm. I think in the main pod file exists case, the init container actually needs to exit non-zero. Batch then needs to distinguish initContainer failure from main container failure. If the initContainer exits 0, the main container will run. For output pod, I think we should just use a normal container, not an init container, so that the above issue isn't present.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6494#issuecomment-506730632:145,failure,failure,145,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506730632,2,['failure'],['failure']
Availability,"Hmm. I trust the code now. I test against several R SKAT runs. I'm not sure I understand how we derive that Q is generalized chi-squared distributed. We use the residual phenotypes in the calculation of Q, but those are inverse-logit transformed normal variables. The derivation for the linear case doesn't apply, as far as I can tell. I assume the residuals are Bernoulli distributed? Maybe not. I guess the phenotypes are Bernoulli but the errors aren't? I'm not sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599:442,error,errors,442,https://hail.is,https://github.com/hail-is/hail/pull/12643#issuecomment-1419295599,1,['error'],['errors']
Availability,"Hmm. I'm having a real hard time figuring out how the fast path has affected this new test. Checkpointing before the group by makes this pass. Removing the union/filter_partitions line makes it pass.; ```; t = hl.utils.range_table(8, n_partitions=8); t = t._filter_partitions([7]).union(t._filter_partitions([7], keep=False)); t = t.group_by(_key=t.idx).aggregate(t_value=hl.agg.collect(t.row_value)); expected = [; hl.Struct(_key=0, t_value=[hl.Struct()]),; hl.Struct(_key=1, t_value=[hl.Struct()]),; hl.Struct(_key=2, t_value=[hl.Struct()]),; hl.Struct(_key=3, t_value=[hl.Struct()]),; hl.Struct(_key=4, t_value=[hl.Struct()]),; hl.Struct(_key=5, t_value=[hl.Struct()]),; hl.Struct(_key=6, t_value=[hl.Struct()]),; hl.Struct(_key=7, t_value=[hl.Struct()]); ]; actual = t.collect(); assert actual == expected. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11785#issuecomment-1106757367:92,Checkpoint,Checkpointing,92,https://hail.is,https://github.com/hail-is/hail/pull/11785#issuecomment-1106757367,1,['Checkpoint'],['Checkpointing']
Availability,Hmm. Not reliably.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834378001:9,reliab,reliably,9,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834378001,1,['reliab'],['reliably']
Availability,"Hmm. This means every dev deploy will generate a new root key. I'm worried about the derived keys and trust lists. After this runs, any service which was not dev deployed needs to know to reload the trust list and start using the new key. For example, if you dev deploy batch, then separately dev deploy query, the new query will get cert errors when talking to batch, I think. I will give some thought this week to the right long-term certificate strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199:339,error,errors,339,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199,1,['error'],['errors']
Availability,"Hmm... shadowJar is building at about 2 minutes on my computer, 1 min 20s of which is compileScala. Master is building at about 1 min 40s (not sure how much scala compile is taking, forgot to check), and I managed to get it down to like 1 min 45s by not bundling some of the dependencies that we weren't bundling before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903:224,down,down,224,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498742903,1,['down'],['down']
Availability,Hmmm... we do give these labels to disks. Maybe that's the error when no orphaned disks are found with any labels,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13719#issuecomment-1737802239:59,error,error,59,https://hail.is,https://github.com/hail-is/hail/pull/13719#issuecomment-1737802239,1,['error'],['error']
Availability,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733,2,['error'],['error']
Availability,Hopefully down to ~2.5 minutes now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5582#issuecomment-471740579:10,down,down,10,https://hail.is,https://github.com/hail-is/hail/pull/5582#issuecomment-471740579,1,['down'],['down']
Availability,"Hopefully you don't mind this unrequested review, I noticed this PR and had some thoughts. As a developer, I like the new way of defining available parameters and the greater clarity between group and command options. As a user, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--asyn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:138,avail,available,138,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,1,['avail'],['available']
Availability,How does the history slow it down? It's just a wrapper.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2209#issuecomment-328142662:29,down,down,29,https://hail.is,https://github.com/hail-is/hail/pull/2209#issuecomment-328142662,1,['down'],['down']
Availability,"However, a `NameError` is surprising here: I would've thought that this would be an attribute error instead. Let us know any updates!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642:94,error,error,94,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642,1,['error'],['error']
Availability,"Huh! I seem to have reliably broken unicode ordering and/or en/de-coding. I've marked as fails for now. Hopefully all the remaining tests will now pass.; ```; _________________________ Tests.test_unicode_ordering __________________________; [gw1] linux -- Python 3.7.12 /usr/bin/python3. self = <test.hail.table.test_table.Tests testMethod=test_unicode_ordering>. def test_unicode_ordering(self):; a = hl.literal([""é"", ""e""]); ht = hl.utils.range_table(1, 1); ht = ht.annotate(fd=hl.sorted(a)); > assert ht.fd.collect()[0] == [""e"", ""é""]; E AssertionError: assert ['?', 'e'] == ['e', 'é']; E At index 0 diff: '?' != 'e'; E Full diff:; E - ['e', 'é']; E + ['?', 'e']. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1034968369:20,reliab,reliably,20,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1034968369,1,['reliab'],['reliably']
Availability,"Huh, checkpointing `sample.vcf` after the filter clears up the issue. This may be something to do with the result of import_vcf being used directly with `_same`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1061867500:5,checkpoint,checkpointing,5,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1061867500,2,['checkpoint'],['checkpointing']
Availability,Huh. I added you back to broad-ctsa. I'm curious to nail down the requester pays issues. Mmm. The billing monitoring is a bit of a mess. The BQ table should really be in hail-vdc and terraform should be able to create the necessary billing sinks to dump billing data into BQ. I'm not exactly sure what the equivalent tools are in Azure to get billing information.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964686794:57,down,down,57,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964686794,1,['down'],['down']
Availability,"Huh. Well, this is a terrible error message, but the short answer is that Hail doesn't support reading directly from an HTTP(S) server. You can either download that file or use a dataset that is available in a cloud storage bucket. In general, you'll want to convert to Hail's native MatrixTable format before you do further analysis anyway. I'll fix this to give a more reasonable error message, but, in general, not all HTTP(S) servers support the Range header which means Hail can't efficiently read from all HTTP(S) servers. If you're looking for public datasets to experiment with, I strongly recommend using the Dense Hail MatrixTable of the HGDP+1KG dataset hosted for free by the three major clouds https://gnomad.broadinstitute.org/downloads#v3-hgdp-1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/12280#issuecomment-1270722614,5,"['avail', 'down', 'error']","['available', 'download', 'downloads', 'error']"
Availability,I accidentally introduced this in #10172 while removing a templating error that inadvertently masked this problem.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10239#issuecomment-809615637:69,error,error,69,https://hail.is,https://github.com/hail-is/hail/pull/10239#issuecomment-809615637,2,"['error', 'mask']","['error', 'masked']"
Availability,I added an optimization so we prefer the default location if we haven't had many failures creating instances. This will resolve my concerns about unnecessarily charging users more with this change without giving them a way to prefer a cheaper region.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1170374342:81,failure,failures,81,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1170374342,1,['failure'],['failures']
Availability,I added the MatrixRead as an input and changed range to take into account the dropRows and dropCols push down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3487#issuecomment-386750335:105,down,down,105,https://hail.is,https://github.com/hail-is/hail/pull/3487#issuecomment-386750335,1,['down'],['down']
Availability,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:576,error,error,576,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['error'],['error']
Availability,"I added the error for each container to the logs output. I think that's fine, but maybe we need separate section for errors?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10545#issuecomment-853153867:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/pull/10545#issuecomment-853153867,2,['error'],"['error', 'errors']"
Availability,"I addressed comments apart from improving the tests on VSM. There are two options regarding plan for writing out a Spark IRM:; 1) just delete it; 2) keep it, pass partStarts through for efficiency, and cut down on code duplication. I tried the latter, creating KeyedIndexedRowMatrix as abstraction to handle both PCA and writing, and pushing common structure to an object WriteBlocksRDD.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2559#issuecomment-351252274:206,down,down,206,https://hail.is,https://github.com/hail-is/hail/pull/2559#issuecomment-351252274,1,['down'],['down']
Availability,"I addressed most of your comments. I also fixed the `downcastToPK` problem by getting rid of it, instead adding a `KeyedOrderedRVD` which has a join key in addition to an ordering key.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3159#issuecomment-374660245:53,down,downcastToPK,53,https://hail.is,https://github.com/hail-is/hail/pull/3159#issuecomment-374660245,1,['down'],['downcastToPK']
Availability,"I agree this is the most compelling critique. I think the Pythonistas would make two points: a) KeyError is the one specific error you get in this case and b) it should be written like this:; ```python3; try:; persisted_bm = self._persisted_locations[bm]; except KeyError as err:; raise ValueError(f'{bm} is not persisted') from err; persisted_bm.__exit__(None, None, None); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12864#issuecomment-1516511437:125,error,error,125,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516511437,1,['error'],['error']
Availability,"I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:. > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to untitled-folder. That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482:556,error,error,556,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482,1,['error'],['error']
Availability,"I agree. And parsing strings to figure out what's going on is insanity. Here's a middle-ground: retry unknown 500 errors once. If they really are rare, they won't reoccur (p^2 very small).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7783#issuecomment-568585850:114,error,errors,114,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568585850,1,['error'],['errors']
Availability,"I agree. Rate limiting is an expected scenario, not really an error. Do you know by chance if all the rate limiting you're seeing are in fact 429s? I hope that covers most scenarios, but I feel like I've seen some rate limiting use other more generic http error codes, or just reset the connection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14595#issuecomment-2200997075:62,error,error,62,https://hail.is,https://github.com/hail-is/hail/issues/14595#issuecomment-2200997075,2,['error'],['error']
Availability,I also checked and made sure the `$(dir)` function was available on make 3.81.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6940#issuecomment-524914202:55,avail,available,55,https://hail.is,https://github.com/hail-is/hail/pull/6940#issuecomment-524914202,1,['avail'],['available']
Availability,"I also created a Starlette branch; which may be preferable, as Sanic brings with it a bit of controversy and a bunch of errors generate on Techempower benchmarks. I took a brief look at the bench source didn't see an immediate issue, so worry a bit about. Sanic. Starlette is a light layer on top of Uvicorn, one of the leading ASGI web servers. Similar to Sanic/Flaks interface:. https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune&l=zijzen-1. Branch here, can issue a separate pr and close this one: https://github.com/akotlar/hail/tree/scorecard-starlette",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115:120,error,errors,120,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461188115,1,['error'],['errors']
Availability,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045:112,down,down,112,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045,2,['down'],['down']
Availability,I also switched the `api` argument to `BatchClient` to `default_api` rather than `None`. I wonder if that was somehow creating a muffled error message. It should have erred as soon as someone tried to make a request with `api=None`. Not sure why it looped forever instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373:137,error,error,137,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434412373,1,['error'],['error']
Availability,I am convinced this is actually just a bad error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/issues/4668#issuecomment-434267326,1,['error'],['error']
Availability,I am having the same error on Mac OS 10.12.4 with gcc 4.9.3. How do I move over to Xcode cc?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-295648585:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-295648585,1,['error'],['error']
Availability,I am having this problem when writing to `HDFS` with `write.parquet()` function.; ```java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN.```. Spark 1.6.2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-319521657:175,error,error,175,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319521657,1,['error'],['error']
Availability,I am so glad I added those requester pays tests. They changed the exception type for requester pays failures and that broke our try-catch. The requester pays situation in GCP is so harebrained.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12555#issuecomment-1397345833:100,failure,failures,100,https://hail.is,https://github.com/hail-is/hail/pull/12555#issuecomment-1397345833,1,['failure'],['failures']
Availability,I am still getting the same error when I take the type explicitly from the table I am trying to transform. Updated code is here:; https://github.com/chrisvittal/hail/blob/404cbd2b3255fc58656801febccce6ed98e594b9/hail/python/hail/experimental/vcf_combiner.py#L13-L59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354,1,['error'],['error']
Availability,"I apologize for not responding sooner to this. I've been mulling over what to do here as it's been over 4 years since I wrote the first interface. I think your changes are fine, but I need to go through the tests again and figure out what `_mentioned` was originally intended for to make sure this change doesn't break anything subtle. I'm going to have our CI run this SHA so I can see what the failures are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13192#issuecomment-1603274953:396,failure,failures,396,https://hail.is,https://github.com/hail-is/hail/pull/13192#issuecomment-1603274953,1,['failure'],['failures']
Availability,I believe I've addressed all of your comments now. The test failure is some spurious batch thing. I'll rerun when it's approved.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9487#issuecomment-697554339:60,failure,failure,60,https://hail.is,https://github.com/hail-is/hail/pull/9487#issuecomment-697554339,1,['failure'],['failure']
Availability,I believe that even a local cluster (2+ jvms) would be sufficient to reproduce this error. I just have no idea how to configure such a thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449493275,1,['error'],['error']
Availability,"I believe that regardless of chunking, the total body of the request is not to exceed aiohttp's [client_max_size](https://docs.aiohttp.org/en/stable/web_reference.html?highlight=client_max_size#application). Internal-gateway will stream transparently to aiohttp which will buffer the chunks [here](https://github.com/hail-is/hail/blob/235d2bcba1d4594a27a3dea6947c91cc4043de72/memory/memory/memory.py#L61) and blow up. It could be that adding chunking will help java to better see the 429 instead of erroring with `Connection closed` or hanging, but that is just wishful thinking, it could be happening at a deeper level of the network stack.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12691#issuecomment-1432381359:499,error,erroring,499,https://hail.is,https://github.com/hail-is/hail/pull/12691#issuecomment-1432381359,1,['error'],['erroring']
Availability,"I believe the code is behaving as designed. The error message, however, leaves a lot to be desired. A few take-aways:; 1. Hail doesn't support heterogeneous arrays. In situations like these, using an array of tuples has the desired outcome.; 2. The variable `x` in `lambda x:` is already a hail expression and so you don't need to explictly capture it as a `literal`.; a. While support for using hail expressions with `literal` was added in https://github.com/hail-is/hail/pull/4086 (see the issue for motivation), it can only be used when that expression is self-contained (ie it's not dependent on another hail expression, eg referencing an element of a hail array expression or tuple expression etc).; b. Our evaluation strategy is to `eval` the expression, then broadcast the result in a `literal`.; c. `eval` correctly complains that that expression has free variables and so can't be evaluated.; d. This error is ugly and has little to do with what the user wanted to achieve. Off the top of my head, a couple of ways to proceed:; 1. The hardest (but backwards compatible) fix is to somehow provide a good error message that the `x` in `lambda x:` in this particular context is a hail expression containing a reference that you should not use with `literal`.; 2. Remove support for using hail expressions with literal.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624335413,3,['error'],['error']
Availability,I can reproduce the error in the current master with:. ```; ./build/install/hail/bin/hail importvcf ~/sample2.vcf splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-241806708:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241806708,1,['error'],['error']
Availability,"I can write an RFC for how to do this with regards to billing updates and the database. I don't think it's too difficult, but it will take a bit of work to add some new metadata that says whether a resources is `by_time` or `by_unit` and compute usage accordingly per billing update. If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. If we have to track by IP address, I don't know how to do that and would have to look into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482:328,down,downloaded,328,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482,1,['down'],['downloaded']
Availability,"I can't figure out why I'm getting an error in one test. But I also am not sure what to do with the `/batches` endpoint. I want the UI default to only show you your batches with the default query string 'user:jigold`. However, what should the REST endpoint be? All batches in all billing projects you have access for?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9954#issuecomment-770063101:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/9954#issuecomment-770063101,1,['error'],['error']
Availability,"I can't figure out why my out of memory test isn't working. It's reporting exit code 0 and no out of memory error even though when I do the same thing locally on my computer with docker run or on an instance in the cluster, I get exit code 137 and out of memory. I'm limiting the docker run command to the same amount of bytes that the docker command in the worker should be limiting it to (looked at the docker output in the worker logs). I think the next step is to try using curl to run docker containers rather than the docker cli.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216,1,['error'],['error']
Availability,"I can't make create idempotent, it returns a fresh batch id. I did make `jobs/create` idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I only retry transient errors, but I allow the user to say they don't want that. A missing internet connection is apparently considered ""transient"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574394825:241,error,errors,241,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574394825,1,['error'],['errors']
Availability,I can’t help you without an error message or description of what didn’t work. I recommend waiting for the next release which should come out today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992,1,['error'],['error']
Availability,I changed PCA and toIndexRowMatrix to take a field. Now these all use select_entries so no need to analyze keys or process joins. But I still check that the expression has a matrix table source with good error message using `matrix_table_source`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3262#issuecomment-377599353:204,error,error,204,https://hail.is,https://github.com/hail-is/hail/pull/3262#issuecomment-377599353,1,['error'],['error']
Availability,I changed my PR so the tests will run. I narrowed down the change in behavior occurs when removing just the commit #3426. Should we look into this further?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3417#issuecomment-385512355:50,down,down,50,https://hail.is,https://github.com/hail-is/hail/pull/3417#issuecomment-385512355,1,['down'],['down']
Availability,"I changed the block_size to default (4096) because a 1024 means 4x the number of open files per task for `BlockMatrix.write_from_entry_expr`. On UKBB, that's about 500 open files, each with its own write buffer (64MB on by default on GCP) so Hadoop fails spectacularly. Liam reports that `ld_prune` succeeds with 50k samples (and ~280k markers), and succeeds but with lots of failures with 75k samples, and fails completely with 100k samples. He just tried again with 459k samples reducing the write buffer size to 1MB, and it still failed there, but I'm hopeful the larger block size will help. We'll try 100k again soon.; ` --properties 'core:fs.gs.io.buffersize.write=1048576,core:fs.gs.io.buffersize=8388608' \`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697#issuecomment-393889443:376,failure,failures,376,https://hail.is,https://github.com/hail-is/hail/pull/3697#issuecomment-393889443,1,['failure'],['failures']
Availability,"I checked the [logs for this PR](https://console.cloud.google.com/logs/query;query=logName:%22worker%22%0Alabels.namespace:%22pr-10467%22;timeRange=PT6H;cursorTimestamp=2021-05-07T19:35:43.101282634Z?project=hail-vdc&folder=true&organizationId=548622027621&query=%0A), both normal logs and just ERRORs and didn't see anything abnormal. If there's a particular stress test you'd like me to try out I'm happy to test it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834787593:295,ERROR,ERRORs,295,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834787593,1,['ERROR'],['ERRORs']
Availability,I checked the database and was surprised to see the SKUs weren't necessarily unique to a specific region. But it makes sense when I looked at their API here: https://cloud.google.com/billing/docs/reference/rest/v1/services.skus/list#sku. I think we should put this in and address what happens if they change the SKU of a particular region if that occurs in the future. We'll just get a bunch of error messages with no price updates and it shouldn't impact users. ~~I will also manually check this in Azure.~~ I checked in both GCP and Azure and the updates looked fine with no errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597:395,error,error,395,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597,2,['error'],"['error', 'errors']"
Availability,I checked the test failure should be fixed by #8131,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592584656:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592584656,1,['failure'],['failure']
Availability,I commented on Zulip about how to make this error the same for every backend. I think it should be a simple change to use `parallelizeAndComputeWithIndex`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10819#issuecomment-906833242:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/10819#issuecomment-906833242,2,['error'],['error']
Availability,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:319,down,download,319,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927,8,['down'],['download']
Availability,"I copied all the secrets from batch-pods into default that (1) didn't already exist (by name) in default, and (2) weren't k8s service account tokens (which are batch-pods specific). I also fixed the remaining test failures so this should be ready to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9718#issuecomment-729993692:214,failure,failures,214,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-729993692,1,['failure'],['failures']
Availability,I couldn't replicate the error locally. It seems to be transient because at least one of the CI builds succeeded.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1834495397:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1834495397,1,['error'],['error']
Availability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:164,error,errors,164,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,2,['error'],"['error', 'errors']"
Availability,I dev deployed this and it still is working fine. It's still pretty slow and I was getting rate limit exceeded errors still trying to attch/detach 64 disks. Average operation time was still 15 seconds. I think part of the problem might be the delay starts at 0.1 for retry_transient_errors. We can consider making this a parameter and setting it to a higher number for this use case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838:111,error,errors,111,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838,1,['error'],['errors']
Availability,"I did debug this, though. The failing test (which succeeds in Spark, but fails in local backend) collects a table and asserts that the result is equal to a list of expected rows. The failure is caused by an ordering issue - the rows are the same, but the order is slightly different between the Spark backend (which produces the expected output) and the local backend. However, I think that actually *both* orders are valid under Hail's guarantees. I'll bring this to our next team meeting for group discussion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408:183,failure,failure,183,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408,1,['failure'],['failure']
Availability,"I did make sure it renders as I intended, and the round trip test means it produces valid type grammar. but I'm hesitant to add a test for exact characters, since if we want to change spacing or something cosmetic then we have to change the test. I feel the same way about assertRaisesRegex checks -- we should be able to make error messages nicer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2998#issuecomment-368885849:327,error,error,327,https://hail.is,https://github.com/hail-is/hail/pull/2998#issuecomment-368885849,1,['error'],['error']
Availability,I did try it out and the error messages are WAY better!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2916#issuecomment-366339845:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/2916#issuecomment-366339845,1,['error'],['error']
Availability,"I don't believe I have access to look at the test failures. If you let me know what failed, I'll do my best to fix it!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13129#issuecomment-1581212490:50,failure,failures,50,https://hail.is,https://github.com/hail-is/hail/pull/13129#issuecomment-1581212490,1,['failure'],['failures']
Availability,"I don't disagree. However, we need toString on (scalar) Type because they are used for error messages all over. MatrixTable used to have a bunch of separate types, now it just has a MatrixType. I think some error messages could now use the matrix type. Python also has some matrix type printing logic, these should probably get unified. Once I have printing for the user, it seemed easier to write a (admittedly small) parser than a separate to/from JSON. I admit, apart from user error messages, JSON is natural since that's what we're storing in the metadata file. Do you have a concrete suggestion? I'm not sure quite what to do that's better than this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2825#issuecomment-362082473:87,error,error,87,https://hail.is,https://github.com/hail-is/hail/pull/2825#issuecomment-362082473,3,['error'],['error']
Availability,"I don't know how to fix the deadlock errors. But for context, the next change Daniel had in mind is to have a table with the n_jobs counts (n_pending, n_failed, etc.) as well as time_completed and then use aggregation queries when the batch state is needed in the Python services. The time the batch is completed is just taken as the latest timestamp from the tokenized table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039620995:37,error,errors,37,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039620995,1,['error'],['errors']
Availability,I don't know why I'm getting pylint errors for Pipeline. The only thing I can think of is I changed the PR build environment with the new docker image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362:36,error,errors,36,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362,1,['error'],['errors']
Availability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:170,fault,fault,170,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,1,['fault'],['fault']
Availability,"I don't really put this in the same category as other external libraries. It's just bindings to the C API that `iptables` itself uses. In general, I think we should prefer just directly calling C libraries instead of shelling out and parsing strings, but I doubt this will have a major impact on worker speed. I worry a bit about the reliability of parsing text, but `iptables` seems like the sort of core utility that would be machine-parsable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12542#issuecomment-1371317794:334,reliab,reliability,334,https://hail.is,https://github.com/hail-is/hail/pull/12542#issuecomment-1371317794,1,['reliab'],['reliability']
Availability,I don't really understand the failure. Seems like the stack trace is missing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4243#issuecomment-417655757:30,failure,failure,30,https://hail.is,https://github.com/hail-is/hail/pull/4243#issuecomment-417655757,1,['failure'],['failure']
Availability,"I don't recall why that `isPrimitive` was added to be honest. I remember sitting down with you to write `checkedConvertFrom` and we decided we needed it then, but it needs to go away and be dealt with.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128#issuecomment-663024852:81,down,down,81,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-663024852,1,['down'],['down']
Availability,"I don't see the utility in creating an unnecessary stack trace to see 'method ""variant QC"" requires a split dataset'. I think there is value in having clear, short, stack-trace-free error messages when it's clear what the problem is and what the user needs to do. I think that printing unnecessary stack traces will cause users to view hail even more as a tool in development, and they will be more inclined to ask us about errors rather than try to figure out how whether they made a simple mistake using the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290:182,error,error,182,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287149290,4,['error'],"['error', 'errors']"
Availability,"I don't think I addressed all of your comments, but I'd like you to take another look. I rewrote the `io.py` component and fixed the parallelism and context managers. I also figure out where to write files to based on the checkpoint_dir argument. We need to `results.checkpoint()` at the end because I decided to treat the raw SAIGE output as temp files (or checkpoints) and the results as a hail table is the thing people want to save to an output directory. I can make the output directory optional as well and not checkpoint that. The issue is that if we write the raw SAIGE results to a temp dir, when the context manager exits, the results HailTable won't be able to read the data once it's returned back to the user.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13804#issuecomment-1779917553:267,checkpoint,checkpoint,267,https://hail.is,https://github.com/hail-is/hail/pull/13804#issuecomment-1779917553,3,['checkpoint'],"['checkpoint', 'checkpoints']"
Availability,"I don't think it makes sense to check explicitly for ""Chromosome"" and not impute that as `Int`. I also don't particularly like implicitly converting `Int` to `String` in the variant constructor -- this could lead to much more indecipherable errors like ""NumberFormatException: cannot convert 'X' to Int in column Chr"". I'm not sure what to do here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/520#issuecomment-236406850:241,error,errors,241,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236406850,1,['error'],['errors']
Availability,"I don't think so. The change is clearly fixes an issue and is an improvement. That said, write failures are rare and I just want to flush out any other rare errors so the tests are reliable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666:95,failure,failures,95,https://hail.is,https://github.com/hail-is/hail/pull/10023#issuecomment-776868666,6,"['error', 'failure', 'reliab']","['errors', 'failures', 'reliable']"
Availability,"I don't think the browser error should pose a problem, it just failed to open a browser. Jupyter should still be running. I run it with `--no-browser`. I'm not sure how Dan's image managed that without the option, I will investigate. You can't connect to my image because it is binding localhost. You can pass `--ip 0.0.0.0` or I can figure out how Dan's image is doing it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460095986:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460095986,1,['error'],['error']
Availability,"I don't think this is going to get fixed especially soon. There's not an easy way to warn / error given our current infrastructure. If you can convince Laurent to PR a fix for downcode to work when PLs are missing (which is reasonable, I think) that's probably the best short-term bet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1283#issuecomment-318495458:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-318495458,2,"['down', 'error']","['downcode', 'error']"
Availability,"I don't think this is the correct solution. As you say, we're incorrectly conflating null to mean two things: the value is legitimately null and it has been filtered out. I see two ways to fix this correctly:. - Values being aggregated indicate having been filtered by making them Option[Any] or storing an extra boolean somewhere to indicate they've been filtered out. This has the downside of more allocations.; - Make value being aggregated an Iterator. This makes aggregables pull-based rather than push-based. It allows aggregators to terminate early (e.g. take(5)). On the other hand, it means the genotype stream will be decoded multiple times, once for each aggregator. That's slow. I think the solution is to unpack the genotype stream into an array before running the aggregators. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1209#issuecomment-268822041:383,down,downside,383,https://hail.is,https://github.com/hail-is/hail/pull/1209#issuecomment-268822041,1,['down'],['downside']
Availability,"I don't understand how this PR can have the following test failures!. ```; =================================== FAILURES ===================================; ____________________________ Test.test_list_batches ____________________________. self = <test.test_batch.Test testMethod=test_list_batches>. def test_list_batches(self):; tag = secrets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:59,failure,failures,59,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506,3,"['FAILURE', 'echo', 'failure']","['FAILURES', 'echo', 'failures']"
Availability,"I don't understand why the Python tests are failing yet. They are failing on master as well, and at least some of the errors are identical (NullPointerException in HailContext.apply). Scala tests are passing. Wondering if this is due to py4j version mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123:118,error,errors,118,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-493192123,1,['error'],['errors']
Availability,I downloaded from the artifact url ; ```; gsutil cp gs://hail-ci-nt3qc/build/ec2176d5842192a57afea55fe102e32c/www.tar.gz .; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789#issuecomment-517703256:2,down,downloaded,2,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-517703256,1,['down'],['downloaded']
Availability,I ended up posting on the forum. I did update to the newest version. It did not generate an error this time. The job ran much further but hung with 4 tasks left. . John,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896,1,['error'],['error']
Availability,I feel like rebinding an argument should be a syntax error in scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6984#issuecomment-527605115:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/6984#issuecomment-527605115,1,['error'],['error']
Availability,"I find it compelling that this fixed the downloads. But I'd also like to understand why this changed worked. I will approve it to see if it unblocks Lindo while we keep discussing. > We needed to await cancelled tasks to handle the error that was raised inside the task. Right. We want to cancel the task and wait for it to finish, but we don't want any exceptions to be raised out. Your code appears to do that, but so does the previous code. Nothing in the documentation for `asyncio.wait` indicates it will raise exceptions: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait. I also tested a short example:. ```; import asyncio; import sys. async def foo():; try:; print('A'); await asyncio.sleep(5); print('B'); return 5; finally:; print(sys.exc_info()). async def async_main():; print('creating task...'); t = asyncio.ensure_future(foo()); # wait for foo to sleep; await asyncio.sleep(1). # cancel foo in sleep; print('cancelling task...'); t.cancel(). print('waiting for task...'); await asyncio.wait([t]). print('done.'). asyncio.run(async_main()); ```. which prints:. ```; $ python3 foo.py; creating task...; A; cancelling task...; waiting for task...; (<class 'concurrent.futures._base.CancelledError'>, CancelledError(), <traceback object at 0x7f8cdef65e10>); done.; ```. The task is cancelled, and CancelledError is raised, but not propagated out. > 75% of his jobs would fail with this error. I'm actually confused where the cancellation error is coming from in the first place. If the code you're changing is the issue (and I think it is, too) then we only cancel if some other exception was raised, either by a task or in `__aexit__`. What's that exception? Can we print it out (enable more logging) in your test setup?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534#issuecomment-853115655:41,down,downloads,41,https://hail.is,https://github.com/hail-is/hail/pull/10534#issuecomment-853115655,4,"['down', 'error']","['downloads', 'error']"
Availability,"I fixed some of these with hacks that defer errors beyond the MatrixReader/TableReader constructors, but some remain (native reader, VCF, etc)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5030#issuecomment-462493288:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/issues/5030#issuecomment-462493288,1,['error'],['errors']
Availability,"I fixed the SQL query. I had to use a feature available in MySQL v8.0.14 and later called lateral joins. I believe what this does is it scans the first table for rows that match, then scans the second lateral table for that matching row and applies the filter. So basically we have an iterator of the two tables joined together rather than a realized temporary table with the results of the entire subquery.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12748#issuecomment-1583061748:46,avail,available,46,https://hail.is,https://github.com/hail-is/hail/pull/12748#issuecomment-1583061748,1,['avail'],['available']
Availability,I fixed the client not closed errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684971261:30,error,errors,30,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684971261,1,['error'],['errors']
Availability,"I fixed the part where the behavior of locus_windows was changed, and now the behavior should be consistent with the previous version. (Some of the error types were changed, but I don't really see that as a breaking interface change).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260:148,error,error,148,https://hail.is,https://github.com/hail-is/hail/pull/5690#issuecomment-477306260,1,['error'],['error']
Availability,I forgot I turned off the syslog so we won't see errors there from while the worker is running. Up to you on whether you think this change needs more scrutiny.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769117923:49,error,errors,49,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769117923,1,['error'],['errors']
Availability,"I forgot that we still had cron jobs running gcr-cleaner daily. This could have been conflicting with the new cleanup policy deletion settings. Let's reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:828,echo,echo,828,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['echo'],['echo']
Availability,I get these errors locally with mypy 0.931,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11734#issuecomment-1090453169:12,error,errors,12,https://hail.is,https://github.com/hail-is/hail/pull/11734#issuecomment-1090453169,1,['error'],['errors']
Availability,I get this error trying to install GWASTools,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080,1,['error'],['error']
Availability,"I got a bad local variable compilation error without the fields for the bindings. I suspect this is a locals/fields problem elsewhere but don't want to debug right now, so reverted.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641,1,['error'],['error']
Availability,I got a timeout!; ```; SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:106,Error,Error,106,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['Error'],['Error']
Availability,I had done this a while ago in a throwaway after talking to Tim. This causes more failures than just the new test that I added. I'm at a loss for how to proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-434111128:82,failure,failures,82,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434111128,1,['failure'],['failures']
Availability,"I had to choose rather small upper bounds to avoid a variety of overflow errors in `Genotype`. I imagine these are practically not a problem, but I didn't take a particularly principled approach to choosing upper bounds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/649#issuecomment-241846815:73,error,errors,73,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241846815,1,['error'],['errors']
Availability,I had to put the tolerations back to get it to pass. I'll remove them in a second pass after I remove the preemptible pool taint.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560172025:17,toler,tolerations,17,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560172025,1,['toler'],['tolerations']
Availability,I handled the merge conflicts and a syntax error. Hopefully we can get this merged post-haste!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1335949595:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1335949595,1,['error'],['error']
Availability,I have a subsequent PR that uses it in build.yaml. Using it in batch is going to take a bit more thinking because we need an image with Hail available.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11072#issuecomment-972380804:141,avail,available,141,https://hail.is,https://github.com/hail-is/hail/pull/11072#issuecomment-972380804,1,['avail'],['available']
Availability,"I have a thing today until 12:30, someone else should look at it. Quick fix: revert changes. 404 is an error anyway",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4948#issuecomment-446587917:103,error,error,103,https://hail.is,https://github.com/hail-is/hail/issues/4948#issuecomment-446587917,1,['error'],['error']
Availability,I have an error in the SQL,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13399#issuecomment-1672022806:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/13399#issuecomment-1672022806,1,['error'],['error']
Availability,"I have commit `a451e1aaa5d1dd4cc055f8e7c1e261aa59eabeca`, I built the jar as `cd hail && ./gradlew shadowJar`. I have this file:; ```; (foo) # cat /tmp/failure.R ; data(mtcars); hail_jar <- ""/Users/bking/projects/hail/hail/build/libs/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:152,failure,failure,152,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,1,['failure'],['failure']
Availability,"I have looked at https://github.com/agronholm/sphinx-autodoc-typehints, yes. One of the major problems with it right now is that it doesn't work with forward references, like this in matrixtable:; ```python; @typecheck_method(exprs=oneof(str, Expression),; named_exprs=expr_any); def group_rows_by(self,; *exprs: Tuple[Union[Expression, str]],; **named_exprs: NamedExprs) -> 'GroupedMatrixTable':; ```. The reason it doesn't work is the decorator. The forward reference strings are evaluated in the module of the function after the module is fully imported, and in this case the module is the module of the _decorator_, not _group_rows_by_. So GroupedMatrixTable is a name error. There are a few solutions:; - Don't use decorators anywhere. This requires a lot of work to fully port over typechecking to typecheck2, which I'm now not totally sure is even the right thing.; - A bit hacky: import `hail` in the typecheck module and use fully clarified paths in forward references.; - see if we can eagerly evaluate the hints in the typecheck module and set them on the decorated function.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3138#issuecomment-373043373:673,error,error,673,https://hail.is,https://github.com/hail-is/hail/pull/3138#issuecomment-373043373,1,['error'],['error']
Availability,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:404,Down,Downloading,404,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700,1,['Down'],['Downloading']
Availability,"I have no idea what this means. ```; Caused by: is.hail.shadedazure.com.azure.storage.blob.models.BlobStorageException: If you are using a StorageSharedKeyCredential, and the server returned an error message that says 'Signature did not match', you can compare the string to sign with the one generated by the SDK. To log the string to sign, pass in the context key value pair 'Azure-Storage-Log-String-To-Sign': true to the appropriate method call.; If you are using a SAS token, and the server returned an error message that says 'Signature did not match', you can compare the string to sign with the one generated by the SDK. To log the string to sign, pass in the context key value pair 'Azure-Storage-Log-String-To-Sign': true to the appropriate generateSas method call.; Please remember to disable 'Azure-Storage-Log-String-To-Sign' before going to production as this string can potentially contain PII.; Status code 403, (empty body); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906073:194,error,error,194,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906073,2,['error'],['error']
Availability,I have rerun one of scripts. It ran fine without any errors. Thanks for the fix!!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-698963629:53,error,errors,53,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-698963629,1,['error'],['errors']
Availability,"I have some reorganization and better error checking I want to do, but I'll accept this and make that in a separate pull request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/310#issuecomment-212577261:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/310#issuecomment-212577261,1,['error'],['error']
Availability,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:95,error,errors,95,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990,8,['error'],"['error', 'errors']"
Availability,"I haven't tested this error message, as I'm not sure how to replicate the bug scenario, but I think it should work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11321#issuecomment-1031552946:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/11321#issuecomment-1031552946,1,['error'],['error']
Availability,I hope this breaks so I can use up some of that downtime budget! Totally worth it to get rid of batch-pods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9718#issuecomment-730020693:48,downtime,downtime,48,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-730020693,1,['downtime'],['downtime']
Availability,"I just got the same error running my [slightly altered] code again, but I stupidly didn't save the error output. I'm running it again and will let you know if I encounter another error!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387561493:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387561493,3,['error'],['error']
Availability,"I just tried cloned locally and `gradle installDist` worked on the first try. We've gotten this compiler error sporadically for a few months, and every time it's been resolved by rebuilding after `gradle clean`. I'll continue to investigate, and see if I can find the source of the problem (could be a compiler bug).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229752430:105,error,error,105,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229752430,1,['error'],['error']
Availability,"I just wanted to let you know that the issue was fixed by running python through Jupyter notebook on my Mac where the HailContext could be created without problems. I never managed to run this through the command line, but as I wanted to use the notebook, anyway, this was ok for me. . However, I encountered a problem within your tutorial with the 'data/1kg.vds', which throws a fatal error ; ``HailException: Invalid VDS: old version [4]; Recreate VDS with current version of Hail.``; However, as this is a separate problem, I'll open up a separate issue for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320149912:386,error,error,386,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320149912,1,['error'],['error']
Availability,"I kinda think this should be an error, actually.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7418#issuecomment-548483578:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548483578,1,['error'],['error']
Availability,"I know I said I thought this was a reasonable approach a while ago, but I’ve been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something we’re trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that I’m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers — right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration — they’ve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:256,down,down,256,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249,1,['down'],['down']
Availability,"I let it go at the end of a long string of commands overnight and it looked to get stuck in the same place, still at (0 + 25) / 25 after what I estimate was about three hours on the grm. A glance at the log shows the same error. I killed it to free up the cluster. . Log here: humgen/atgu1/fs03/satterst/DBS_v2.3/hail.kryo.log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/321#issuecomment-212885679:222,error,error,222,https://hail.is,https://github.com/hail-is/hail/issues/321#issuecomment-212885679,1,['error'],['error']
Availability,"I like the idea that, in a future PR, we investigate a slightly higher initial back off for disk creation failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872569078:106,failure,failures,106,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872569078,1,['failure'],['failures']
Availability,"I like this in concept, but I'm worried about people messing themselves up by changing upstream code and expecting checkpoint to rerun.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5599#issuecomment-472976356:115,checkpoint,checkpoint,115,https://hail.is,https://github.com/hail-is/hail/issues/5599#issuecomment-472976356,1,['checkpoint'],['checkpoint']
Availability,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:202,down,downstream,202,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739,1,['down'],['downstream']
Availability,"I like this table much better! However, it's too wide. I don't know exactly the best way to shrink it down, but here's a few off the cuff thoughts:. I don't think I care about ""Live"", I can do that math myself (it's pending + active, right?). Can we shorten Instances to ""I"" and Cores to ""C"" with abbr tags a la `<abbr title=""Instances"">`?. I don't think I care about schedulable instances, for scheduling I really care about cores. I don't think I care about the cores column, right? ~~Isn't that a synonym for ""active cores""?~~ Ah versioning matters. Hmm. Can we maybe just do `XX / YY` and `ZZ%` columns? It's just too wide to quickly scan this table. I think the most important super-heading is ""Schedulable"", what do you think of putting that at the far left of the table?. If we swap ""Spot"" for ""Preemptible"" that will also shrink the width of the table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13943#issuecomment-1789625920:102,down,down,102,https://hail.is,https://github.com/hail-is/hail/pull/13943#issuecomment-1789625920,1,['down'],['down']
Availability,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['error'],['error']
Availability,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:261,echo,echo,261,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,2,['echo'],['echo']
Availability,"I looked at this again and I think we can do this with `online: true`. It's a quick enough migration where it shouldn't impact the driver for too long that it's trying to query the long tables. If there's a problem and the driver can't make forward progress once the database migration is done, we can just shut down the deployment to 0 replicas. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153:312,down,down,312,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153,1,['down'],['down']
Availability,I lowered the tolerance,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5305#issuecomment-462478878:14,toler,tolerance,14,https://hail.is,https://github.com/hail-is/hail/pull/5305#issuecomment-462478878,1,['toler'],['tolerance']
Availability,"I merged main and was forced to fix the pyright errors, which was great! I found two bugs with these new types:. 1. It's possible for a spec file to be missing which means the spec is None which would fail our get job front end code when it tries to fix up the resources.; 2. In the aioclient, we assumed the attributes was present and a dict but that key can be completely missing from the dict. I also noticed `cost_str` has had the wrong type since always.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13500#issuecomment-1701592782:48,error,errors,48,https://hail.is,https://github.com/hail-is/hail/pull/13500#issuecomment-1701592782,1,['error'],['errors']
Availability,I met with Lily and Ruchit. They have more work they need to do to figure out what their use case is and how they want downstream analysts to use the data to figure out how to key their datasets. They'll get back to me once they're ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13864#issuecomment-1785898859:119,down,downstream,119,https://hail.is,https://github.com/hail-is/hail/issues/13864#issuecomment-1785898859,1,['down'],['downstream']
Availability,"I modified the artifacts to include the two uber jars:. ```; +:build/docs => docs; +:build/libs/hail-all-spark.jar; +:build/libs/hail-all-spark-test.jar; ```. The latest successful master build of `hail-all-spark.jar` is available at:. https://ci.hail.is/httpAuth/app/rest/builds/buildType:(id:HailSourceCode_HailCi),count:1,status:SUCCESS/artifacts/content/hail-all-spark.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/754#issuecomment-245692697:221,avail,available,221,https://hail.is,https://github.com/hail-is/hail/issues/754#issuecomment-245692697,1,['avail'],['available']
Availability,"I mostly rewrote things to fit this interface, but found a pretty significant problem with it -- we lose our informative error messages. Once we've got an `RDD[Annotation]`, we've lost our line context. This new variant expression interface will be different and challenging with our users, and if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context, or a match error from a `Chr:Pos:Ref:Alt` with too few colon-split fields.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232824524:121,error,error,121,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232824524,2,['error'],['error']
Availability,"I moved the changes to the aggregator into #6076, and added error bounds that apply to all quantiles simultaneously, which is what you really want for the pdf.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039#issuecomment-492307141:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-492307141,1,['error'],['error']
Availability,I moved to TBoolean and simplified the error messages. Back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/511#issuecomment-236389770:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/511#issuecomment-236389770,2,['error'],['error']
Availability,I need to fix the pipeline pylint errors in this branch as well...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743:34,error,errors,34,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743,1,['error'],['errors']
Availability,"I need to investigate further, but now I see a segfault and I think it's coming from the LMM tests. I need to fix the `test-gcp.sh` script so that it looks for the coredump file in the case of a seg fault. ```; [Stage 2225:==========================================> (3 + 1) / 4]2017-08-28 21:47:32 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:32 Hail: INFO: Ordering unsorted dataset with network shuffle; 2017-08-28 21:47:33 Hail: WARN: called redundant split on an already split VDS; 2017-08-28 21:47:33 Hail: INFO: using 2 trios for transmission analysis; [Stage 2229:==========================================> (3 + 1) / 4]2017-08-28 21:47:35 Hail: INFO: while writing:; file:/tmp/hail.16cpq9RzwI7a/out.00000.txt; merge time: 65.459ms; 2017-08-28 21:47:35 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:35 Hail: INFO: Ordering unsorted dataset with network shuffle; [Stage 2234:============================================> (4 + 1) / 5]2017-08-28 21:47:37 Hail: WARN: Found 2 samples with missing sex information (not 1 or 2).; Missing sex identifiers: [ 0 ]; 2017-08-28 21:47:37 Hail: WARN: 2 samples discarded from .fam: sex of child is missing.; 2017-08-28 21:47:38 Hail: INFO: Found 250 samples in fam file.; 2017-08-28 21:47:38 Hail: INFO: Found 2000 variants in bim file.; 2017-08-28 21:47:38 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:38 Hail: INFO: Modified the genotype schema with annotateGenotypesExpr.; Original: Struct{GT:Call}; New: Genotype; 2017-08-28 21:47:38 Hail: INFO: Reading table to impute column types; [Stage 2258:============================> (1 + 1) / 2]2017-08-28 21:47:40 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: Reading table to impute column types; 2017-08-28 21:47:41 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading colu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:199,fault,fault,199,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,2,"['fault', 'redundant']","['fault', 'redundant']"
Availability,I need to make this more robustly ignore migration files,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11231#issuecomment-1016834010:25,robust,robustly,25,https://hail.is,https://github.com/hail-is/hail/pull/11231#issuecomment-1016834010,1,['robust'],['robustly']
Availability,"I now pass the scores_table through as a Table rather than localizing and passing through colKeys, colKeyType, and scores annotations. The column key can now be any type. Both string and integer keys are tested from Python. However, `requireUniqueSamples` still requires a single string ID (this was the remaining problem of going generic), so I've removed this check and would appreciate feedback on the best approach to checking uniqueness, preferably on the localized `keys` in PCRelate so as not to trigger additional actions. I could use keyType.valuesSimilar to compare any two elements...it's a bit weird to have a tolerance on floats here. As noted, I'm also a bit wary that I'm relying on `scores` from `pca` to be in the same order as the columns on the matrix table. This is currently true, but could change. @danking I think the joins in `fuse` should also be zipPartitions, I've noted it in a FIXME. I'm also concerned that the number of diagonal blocks is an upper bound on parallelism for the matrix multiply. We should be able to fix that by immediately writing and then reading phi.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065:622,toler,tolerance,622,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376385065,2,['toler'],['tolerance']
Availability,"I observed this in my namespace while testing fixes for a different transient error.; https://internal.hail.is/dking/batch/batches/5/jobs/7034 (this URL will obviously not last long). The worker actually successfully wrote the correct output to GCS, but it received this error anyway which caused the job to fail. Since QoB checks for the results files first, it didn't even notice that the worker job had failed. That seems not great. We should probably fail if an worker job fails, even if we find output because that output could be corrupt. ```; (base) dking@wm28c-761 hail % hexdump -C foo ; 00000000 01 82 00 00 00 7e 00 00 00 67 73 3a 2f 2f 31 2d |.....~...gs://1-|; 00000010 64 61 79 2f 74 6d 70 2f 68 61 69 6c 2f 54 53 4f |day/tmp/hail/TSO|; 00000020 66 4f 72 67 5a 55 6d 62 56 69 78 6e 52 69 4b 57 |fOrgZUmbVixnRiKW|; 00000030 51 46 42 2f 61 67 67 72 65 67 61 74 65 5f 69 6e |QFB/aggregate_in|; 00000040 74 65 72 6d 65 64 69 61 74 65 73 2f 2d 50 74 33 |termediates/-Pt3|; 00000050 67 4e 74 51 57 35 57 6f 42 64 43 54 44 50 51 69 |gNtQW5WoBdCTDPQi|; 00000060 77 48 64 61 39 63 32 36 35 66 32 2d 66 62 64 38 |wHda9c265f2-fbd8|; 00000070 2d 34 66 31 62 2d 62 63 64 65 2d 66 62 66 32 39 |-4f1b-bcde-fbf29|; 00000080 31 38 30 63 33 34 37 00 00 00 00 |180c347....|; 0000008b; ```. code:; ```ipython3; In [1]: import hail as hl; ...: import gnomad.utils.sparse_mt; ...: ; ...: ; ...: tmp_dir = 'gs://danking/tmp/'; ...: vds_file = 'gs://neale-bge/bge-wave-1.vds'; ...: out = 'gs://danking/foo.vcf.bgz'; ...: ; ...: vds = hl.vds.read_vds(vds_file); ...: mt = hl.vds.to_dense_mt(vds); ...: t = gnomad.utils.sparse_mt.default_compute_info(mt); ...: t = t.annotate(info=t.info.drop('AS_SB_TABLE')); ...: t = t.annotate(info = t.info.drop(; ...: 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; ...: )); ...: t = t.drop('AS_lowqual'); ...: ; ...: hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```; worker failure:; ```; 2023-09-27 16:43:10.389 JVMEntryway: INFO: is.hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:78,error,error,78,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,2,['error'],['error']
Availability,I particularly like that a `PropertySuite` would reduce the possibility of human error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/649#issuecomment-241139977:81,error,error,81,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241139977,1,['error'],['error']
Availability,"I pushed a commit that will error on resource warning. Hopefully we can figure out where we're leaking, fix the leaks, and stop the hangs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12731#issuecomment-1518368590:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1518368590,1,['error'],['error']
Availability,"I pushed the logic #2861 down so I can remove this copy. I'll add that change to that PR once this is merged (good catch) since I may need to rebase anyway. See here:; https://github.com/hail-is/hail/pull/2861/files#diff-912e03c9c34a874ecdc0e520a13cb572R133. This avoids the copy if the BDM is compact, which blocks always are. If the BDM is not compact, then we could add logic to stream out the bytes without an intermediate compactification but I don't want to add that complexity now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2848#issuecomment-363553946:25,down,down,25,https://hail.is,https://github.com/hail-is/hail/pull/2848#issuecomment-363553946,1,['down'],['down']
Availability,I put the WIP tag on this. I don't have the energy to debug any failures today. Will merge it on Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071:64,failure,failures,64,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071,1,['failure'],['failures']
Availability,"I read somewhere that kube-system pods tolerate any taint because, e.g. the kube-proxy, is necessary to even participate in k8s. These all exist:; ```; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-0msn 1/1 Running 0 13d; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-fqkt 1/1 Running 0 13d; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-g3z7 1/1 Running 0 60m; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-pdlv 1/1 Running 0 14h; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-td40 1/1 Running 0 3d20h; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-4fkb 1/1 Running 0 12m; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-51jf 1/1 Running 0 100m; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-cr11 1/1 Running 0 2d22h; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-x6cl 1/1 Running 0 18h; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7784#issuecomment-571218268:39,toler,tolerate,39,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-571218268,1,['toler'],['tolerate']
Availability,"I registered the functions inside and outside the actual aggregation to bring the IR size all the way down to about what it was before. It was maybe 140 before I did that, 30 of which belonged to the string concatenation in the error messages :(. . This also hit a bug that I'm fixing in #6740, so it won't be able to go in before that does.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365:102,down,down,102,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365,2,"['down', 'error']","['down', 'error']"
Availability,I remember you said this was hitting some error/segfault -- anything you need from me to unblock?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8952#issuecomment-666412422:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-666412422,1,['error'],['error']
Availability,"I replicated the issue with this:; ```; In [1]: grch37 = hl.get_reference('GRCh37'). In [2]: grch37.add_liftover('src/test/resources/grch37_to_grch38_chr20.over.chain.gz', 'GRCh38'). In [3]: i = hl.parse_locus_interval('1:10000-10000'). In [4]: hl.eval(hl.liftover(i)); ```. The issue is this interval is `Interval(10000, 10000, includesStart=True, includesEnd=False)` which has a length of zero. @patrick-schultz Should this be a valid interval? i.e. start==end and includesStart = True and includesEnd = False. Otherwise, if it is a valid Hail interval, then I'll throw a nicer error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308:580,error,error,580,https://hail.is,https://github.com/hail-is/hail/issues/5174#issuecomment-455713308,1,['error'],['error']
Availability,"I retried Azure. It ran into the exact same problem with Azure Blob Storage flakiness https://ci.azure.hail.is/batches/4126780/jobs/136 . We gotta fix our interaction with Azure Blob Storage, it's the only way to get CI to be stable again. It's possible https://github.com/hail-is/hail/pull/13325 will help, but it doesn't address the root cause of the SDK giving us these mysterious errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13306#issuecomment-1656089547:384,error,errors,384,https://hail.is,https://github.com/hail-is/hail/pull/13306#issuecomment-1656089547,1,['error'],['errors']
Availability,I saw different errors if a `show()` vs `_force_count_rows()` was the last line.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325#issuecomment-603568619:16,error,errors,16,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603568619,1,['error'],['errors']
Availability,"I saw this again today in a fairly simple and isolated test. I'm beginning to wonder if this is just a new form of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:125,error,error,125,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['error'],['error']
Availability,"I search the google logs for the pull request in question. For example,. ```; (resource.labels.namespace_name=""pr-11471-default-fn1xhr9ahy9v"" AND; labels.""k8s-pod/app"":""batch""; ) OR (; labels.""compute.googleapis.com/resource_name"":""batch-worker-pr-11471-default-fn1xhr9ahy9v"" AND; logName:""worker.log""); ```. That PR seems to have a worker stuck alive: `batch-worker-pr-11471-default-fn1xhr9ahy9v-standard-ux0sp`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059448548:346,alive,alive,346,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059448548,1,['alive'],['alive']
Availability,"I searched [here](https://portal.azure.com#@d6c9f2ea-d3bb-4ca9-8b14-231bac999aa6/blade/Microsoft_OperationsManagementSuite_Workspace/Logs.ReactView/resourceId/%2Fsubscriptions%2F22cd45fe-f996-4c51-af67-ef329d977519%2Fresourcegroups%2Fhaildev%2Fproviders%2Fmicrosoft.operationalinsights%2Fworkspaces%2Fhaildev-logs/source/LogsBlade.AnalyticsShareLinkToQuery/q/H4sIAAAAAAAAA21Sy27CMBC88xVbLiRSqNprUSpVgFpUVCraOzLxJhgcO7LNI2r7710nKRBoDokzmp2dnbVEB6%252FbJb5rPtWZhRiCQnPFciwMpuLwYJ0RKovAQ7ZgCR4RqTOJO5QNEMJXB%252BiRpDjUyjGh0Ez4VFhHqk2PidqhctqUFfUb9is0CG%252BkDdYx4%252BxeuBUdTcJc20gE3X43bJURTaIKTs1G4ePdlXBlGuL4NEFD4eRMqOTc7SgCHwRbooxOxRU4KWpkUFUfSyi0Rg4PDhUHGs%252BUccGMxcXaahUQY%252ByxsM0jbcsyjCv%252BbfPXMn9mC4SC4CLUdha1jKV9GOFKP%252B7fev6h4SFZCJVquElqTQvdj5enbkOVWm%252B2BWyE4rHE1OmtQ3PdH7Q699gUF0avMXF9tmclTHIaKqo%252Fn4zuzBwLbYW%252FAHWaEXyKHGfpUOc5U%252FxCw6DfmU%252Ffc2P%252Fuu%252F8DDpnFzboJaIXQY9jyrbS%252BeN4Pp%252FNeyHpaMPJ%252BLKsmjwj2WQOOXC0yS9DxlTe%252BAIAAA%253D%253D/timespan/2023-04-16T20%3A37%3A28.000Z%2F2023-04-28T20%3A37%3A28.000Z) and couldn't really find anything insightful. It does feel odd that we went that long with so *few* error messages, so maybe some silent error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561910142:1150,error,error,1150,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561910142,2,['error'],['error']
Availability,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:635,error,error,635,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395,1,['error'],['error']
Availability,"I see, it wasn't doing redundant work, just generating redundant IR by regenerating the IR to load covariates per set of phenotypes. This would only affect chained linear regression, since that's the only time there's more than one `one_y_field_name_set in y_field_names`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871:23,redundant,redundant,23,https://hail.is,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871,2,['redundant'],['redundant']
Availability,I see. It's a bug in that the debug information if the test had failed would have been wrong / thrown an error. But the actual test right now was testing the right thing. Is this correct?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11203#issuecomment-1006756945:105,error,error,105,https://hail.is,https://github.com/hail-is/hail/pull/11203#issuecomment-1006756945,1,['error'],['error']
Availability,"I see.. if there's a place where it's easy to check for this, I could add an error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639:77,error,error,77,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301753639,1,['error'],['error']
Availability,"I set it to Running at the end of `_create_pod`. Maybe that's not correct. It should probably be this then:. Pending -> Ready -> (Error, Created -> Running -> (Failed, Success))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499331021:130,Error,Error,130,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499331021,1,['Error'],['Error']
Availability,"I started looking into the test failures last week, but I can't reproduce them locally; I'm very confused. Anyways, I'm working on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7134#issuecomment-542281084:32,failure,failures,32,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-542281084,1,['failure'],['failures']
Availability,I still don't understand how the error you posted above relates to the changes in this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592590862:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592590862,1,['error'],['error']
Availability,"I still have the index error, so it must be inherent to my branch. Let's close and go with your implementation instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13807#issuecomment-1766743037:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/13807#issuecomment-1766743037,1,['error'],['error']
Availability,"I still haven't looked at the code carefully, but one thing I'd say with database migrations is to make sure that your incremental change won't make the next change you want to make in the future more difficult or impossible. I don't think that's the case here, but keep in mind that this migration is O(n_batches). Anything that is O(n_jobs) right now is not great on hail-vdc as it could take hours to complete. How big of a lift is it to the next change you wanted to try out? Where is the new deadlock error coming from?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036214712:506,error,error,506,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036214712,1,['error'],['error']
Availability,I still haven't managed to set it up to work from that. I hit errors while installing the Genesis stuff,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312:62,error,errors,62,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312,1,['error'],['errors']
Availability,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:587,Error,Error,587,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751,1,['Error'],['Error']
Availability,"I tested this by adding an assertion error that would get hit just before the while loop.With the old code, we always hit the assertion, with the new code, we do not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635:37,error,error,37,https://hail.is,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635,1,['error'],['error']
Availability,"I tested this by submitting 64 true, 10 Gi 0.25 core jobs on one node with a local SSD. It took 7 minutes for the whole batch to finish. This is a bit disappointing, but at least there were no errors. We can potentially revisit using get instead of wait for the polling loop, but the default sleep backoff method we have quickly blew our quota.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-871705329:193,error,errors,193,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-871705329,1,['error'],['errors']
Availability,I tested this locally by downloading the HTML for the UI page and tinkering.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6430#issuecomment-504241795:25,down,downloading,25,https://hail.is,https://github.com/hail-is/hail/pull/6430#issuecomment-504241795,1,['down'],['downloading']
Availability,"I tested this on my branch that had a bunch of deadlock errors and those were replaced with CallError in schedule job because the job was running, cancelled, or instance not active.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288:56,error,errors,56,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288,1,['error'],['errors']
Availability,"I tested this with a hard-hitting batch that used a bunch of storage, looked through the UI and didn't get any 500s, and checked the logs on both the k8s pods and the instances for errors. I also commented out each part of the garbage collection loops and made sure everything got cleaned up. For example, commenting out the activity logs loop or the monitor instances loop with the deactivate API point not doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-942374466:181,error,errors,181,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-942374466,1,['error'],['errors']
Availability,I think GitHub doesn't show that far up from the diff but there is a comment describing the error code already on line 27. I tried to keep them close together.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14251#issuecomment-1927987394:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/pull/14251#issuecomment-1927987394,1,['error'],['error']
Availability,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230288836:109,fault,fault,109,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836,1,['fault'],['fault']
Availability,"I think I squashed all the bugs, failure is from the unrelated test_ci issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8335#issuecomment-602304418:33,failure,failure,33,https://hail.is,https://github.com/hail-is/hail/pull/8335#issuecomment-602304418,1,['failure'],['failure']
Availability,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488:362,down,down,362,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488,2,['down'],['down']
Availability,"I think fixing this will mask an issue where a 1 CPU job blocks the scheduling of smaller jobs. I'm gonna debug that first, then remove WIP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067:25,mask,mask,25,https://hail.is,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067,1,['mask'],['mask']
Availability,"I think in an ideal world, every project including ours, our dependencies, and our end users, maintain public wide-open requirements files and private fully-pinned lockfiles. Our users know we need X and Y and they pin whatever versions are compatible with the union of our code and theirs. If we don't want our end users to have to do that, we need to compromise by narrowing the window and accepting the faulted but practical middle-ground. @jmarshall, out of curiosity does your group fully pin your dependencies / do you have any thoughts? It would be interesting to hear from the perspectives of end users but also third-party collaborators.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520506773:406,fault,faulted,406,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520506773,1,['fault'],['faulted']
Availability,"I think it was wrong -- the buffered thing probably already implements it in terms of write. I didn't even define flush on the java side, so it wasn't getting called in my tests (or it would have errored)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1645#issuecomment-294269949:196,error,errored,196,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-294269949,1,['error'],['errored']
Availability,I think it's passing fine now. Just a transient failure in the latest one: https://ci.hail.is/batches/8118694,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14224#issuecomment-1922387328:48,failure,failure,48,https://hail.is,https://github.com/hail-is/hail/pull/14224#issuecomment-1922387328,1,['failure'],['failure']
Availability,"I think it's useful if you want to write something like:. ```; hl.if_else(x < y, ...., hl.die(""Error: x must be less than y"")); ```. Otherwise you can't do input validation on expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584:95,Error,Error,95,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584,1,['Error'],['Error']
Availability,"I think maybe `tolerations` is not currently indented to the same level as `volumes`, causing the error you're getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6664#issuecomment-512843380:15,toler,tolerations,15,https://hail.is,https://github.com/hail-is/hail/pull/6664#issuecomment-512843380,2,"['error', 'toler']","['error', 'tolerations']"
Availability,I think no. The frequency of failures is a bit lower than it was.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11265#issuecomment-1027174857:29,failure,failures,29,https://hail.is,https://github.com/hail-is/hail/pull/11265#issuecomment-1027174857,1,['failure'],['failures']
Availability,I think one answer to this would be to just cut out the Java parts and show just the summary. But I also wrote up some further thoughts for how to improve here: https://dev.hail.is/t/better-python-error-messages-idea/201,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952:197,error,error-messages-idea,197,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952,1,['error'],['error-messages-idea']
Availability,"I think that would probably be fine---I just got worried since `_error_from_cdf` had some non-working code and wasn't being called anywhere explicitly (it's imported in `plots`, but not used in the error calculation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6039#issuecomment-490256479:198,error,error,198,https://hail.is,https://github.com/hail-is/hail/pull/6039#issuecomment-490256479,1,['error'],['error']
Availability,I think the best thing to do is to throw an error here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1587#issuecomment-288400774:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/issues/1587#issuecomment-288400774,1,['error'],['error']
Availability,I think the primary question is:. > Is the difference between `0.84` and `1.31` a sign of an error or is this a reasonable value for the search to find for `delta`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281827437:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281827437,1,['error'],['error']
Availability,"I think this PR is just about as good as it's going to get for now. From looking at the Grafana API metrics, I think I was hitting the maximum scheduler throughput. The get running cancellable jobs is around 40ms each call for 5000 jobs while the getting the job head queue is 123ms. If the 40ms becomes a problem, then we can pull less records (see explanation below) or we can not do a json array agg and figure out the regions using bit shifting. When we did the load tests yesterday getting the job head queue was around 1-2 seconds with us each having 20k records. I think we just have to keep an eye on it. I did some further optimization of the scheduler by allowing it to pull up to 10000 jobs from the database to try and schedule before it hits its fair share of jobs scheduled. This helps a lot with efficiency to use the existing capacity if there are jobs further down the queue that are schedulable. I know it's a bit of a departure from what we've done in the past, but I think since we're going in order of fair share now and pulling more jobs from the database isn't that expensive, then this is fine. Happy to make this number 1000 even. 300 was too small though. Jobs at the front of the queue will eventually be able to run because the next iteration of the autoscaler will create the correct instances for those jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928:877,down,down,877,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276546928,1,['down'],['down']
Availability,I think this is a bad error message but should still be an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/4118#issuecomment-411796377,2,['error'],['error']
Availability,"I think this is a good change but the partitioner hint won't fix Xiao's problem. Here's why:. His OOM error comes from the way we do ordered joins. His workflow was basically annotatevariants table x10, so each partition of the left ended up pulling 10 128M (compressed, so really more) chunks into memory, and boom goes the dataflow. Each table was sorted, so the partitioner hint is never applied. . I'm not sure how we can fix this without a query optimizer. It certainly seems like our current model is dangerous. If I were hand-optimizing his workflow, I might want to shuffle small text files against the vds partitioner regardless of sortedness",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248592567:102,error,error,102,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248592567,1,['error'],['error']
Availability,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-210903100:293,error,errors,293,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100,1,['error'],['errors']
Availability,I think this is a very similar bug: https://github.com/aio-libs/aiomysql/issues/539. Turning off uvloop seemed to get rid of these errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13863#issuecomment-1788047416:131,error,errors,131,https://hail.is,https://github.com/hail-is/hail/issues/13863#issuecomment-1788047416,1,['error'],['errors']
Availability,"I think this is good to go, once MatrixIR.scala comments pertaining to execute are removed. I would like to know, as an aside, more about execute. Coverage of modified join functionality seems good!. Breaking line 1505, using; ```python; join_table = src.rows(); ```. generates a test error in; test/hail/matrixtable/test_matrix_table.py:490. Breaking line 1529, using; ```python; joiner = lambda left: 1; ```; triggers an error in test/hail/matrixtable/test_matrix_table.py:905",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974:285,error,error,285,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-453375974,2,['error'],['error']
Availability,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:546,alive,alive,546,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,4,"['alive', 'avail']","['alive', 'available']"
Availability,"I think this is just pip not honoring retries everywhere. The crash appears to occur while downloading a dependency. Sure, I can add a retry to apt as well, though apt seems to be more careful about retrying on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767034755:91,down,downloading,91,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767034755,1,['down'],['downloading']
Availability,"I think this issue is going to arise often with type imputation. As Tim wrote, you can also use --impute and say str(contig) or give it type string which will overwrite the imputation...but it's going to confuse folks. We could expand on the error message, or maybe we should implicitly convert Int to String for contig in the Variant constructor.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/520#issuecomment-236401286:242,error,error,242,https://hail.is,https://github.com/hail-is/hail/issues/520#issuecomment-236401286,1,['error'],['error']
Availability,"I think this just isn't something you should do. You should use a newer version of hail than 0.2.60. We admittedly had a slightly rough transition into supporting multiple spark versions. It does seem like that message is a warning not an error, so you may be able to remove ` ""-Xfatal-warnings"", ` option from build.gradle and get things to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831#issuecomment-914325862:239,error,error,239,https://hail.is,https://github.com/hail-is/hail/issues/10831#issuecomment-914325862,1,['error'],['error']
Availability,"I think this needs to be rewritten to be robust to non-primitive elements. We can't just use copyFrom -- We need to loop and use constructAtAddressFromValue, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128#issuecomment-663025712:41,robust,robust,41,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-663025712,1,['robust'],['robust']
Availability,I think those failures are due to legacy tables not being keyed. See the workaround in the interpreted path here:; https://github.com/hail-is/hail/blob/bb7f847d9b3af888d4a59376fdd70900265bdeda/hail/src/main/scala/is/hail/expr/ir/TableIR.scala#L877-L881,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10379#issuecomment-829606450:14,failure,failures,14,https://hail.is,https://github.com/hail-is/hail/pull/10379#issuecomment-829606450,1,['failure'],['failures']
Availability,I think we need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. The only parts that would be referencing the wrong tables are in the `Canceller` and `notify_batch_complete`. I think scheduling and MJC would just work because we update those stored procedures and don't change the child code. We can shut batch down though for the migration. Seems safest although more of a pain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477:57,toler,tolerate,57,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477,2,"['down', 'toler']","['down', 'tolerate']"
Availability,"I think we should continue with another review and then a load test. I'm still a bit hesitant about the query change, but we can keep an eye on it. I'm still get errors with the typing:. ```; (venv) jigold@wm349-8c4 hail % make -C hail/python check; python3 -m flake8 --config ../../setup.cfg hail; python3 -m flake8 --config ../../setup.cfg hailtop; python3 -m pylint --rcfile ../../pylintrc hailtop --score=n; python3 -m mypy --config-file ../../setup.cfg hailtop; hailtop/batch/backend.py:481: error: Incompatible types in assignment (expression has type ""Union[str, List[str], None]"", variable has type ""Optional[List[str]]""); Found 1 error in 1 file (checked 146 source files); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464:162,error,errors,162,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1271807464,3,['error'],"['error', 'errors']"
Availability,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:1013,Down,Download,1013,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887,1,['Down'],['Download']
Availability,"I think we're okay in terms of not having errors currently. This was a backwards compatibility code path. I will double check again, but I think we first create the config on the driver `GCPSlimInstanceConfig.create()`. This config is sent to the worker which deserializes it, but it's on the last part of that if/else and runs `resources = [gcp_resource_from_dict(data) for data in resources]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13539#issuecomment-1703188825:42,error,errors,42,https://hail.is,https://github.com/hail-is/hail/pull/13539#issuecomment-1703188825,1,['error'],['errors']
Availability,I think what we want is an error code on apply nodes! There are lots of functions that can throw errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10119#issuecomment-786836520:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/pull/10119#issuecomment-786836520,2,['error'],"['error', 'errors']"
Availability,"I think you'll still have the doctest failure. It just returns list of lists now ,right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8265#issuecomment-601433785:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601433785,1,['failure'],['failure']
Availability,I think you're getting an error because you need to add this to `hl.experimental.__init__.__all__`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4052#issuecomment-409700497:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409700497,1,['error'],['error']
Availability,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522,2,"['Error', 'error']","['Error', 'error']"
Availability,I timed creating the new indices. Confirmed the operations are not blocking and they took 50 minutes on a 4 core database. We'd want to change the database size to 6 or 8 cores before doing this as it used about 70-80% of the available CPU on a 4 core machine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12748#issuecomment-1468431358:226,avail,available,226,https://hail.is,https://github.com/hail-is/hail/pull/12748#issuecomment-1468431358,1,['avail'],['available']
Availability,"I tossed this up in my namespace and this is what seems to be the issue:; ```. Job Step	Image Pulling Time (s)	Running Time (s)	Error Type	State; main	0.135	30.011	timed out	error; Logs; Main; Log; executable file `sleep 5` not found in $PATH: No such file or directory; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 680, in _run; raise ContainerTimeoutError(f'timed out after {self.timeout}s'); ContainerTimeoutError: timed out after Nones",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11397#issuecomment-1076793811:128,Error,Error,128,https://hail.is,https://github.com/hail-is/hail/pull/11397#issuecomment-1076793811,3,"['Error', 'error']","['Error', 'error']"
Availability,I tracked down the failure. sc.textFile with a glob is non-deterministic (unlike the sc.union it replaced). So we need to either go back to the old code or reorder the partitions a la partitioned parquet reader. I'll deal with it tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248515508:10,down,down,10,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248515508,2,"['down', 'failure']","['down', 'failure']"
Availability,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,2,['error'],['error']
Availability,"I tried to benchmark 100M rows against Spark:. ```; $ spark-shell; scala> val df = spark.range(100000000); df: org.apache.spark.sql.Dataset[Long] = [id: bigint]. scala> val df2 = df.select(df.col(""id""), functions.rand().as(""x"")); df2: org.apache.spark.sql.DataFrame = [id: bigint, x: double]. scala> df2.write.parquet(""df2.parquet""); 18/07/29 13:47:09 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2); Caused by: java.lang.OutOfMemoryError: Java heap space; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4030#issuecomment-408697268:352,ERROR,ERROR,352,https://hail.is,https://github.com/hail-is/hail/pull/4030#issuecomment-408697268,1,['ERROR'],['ERROR']
Availability,"I tried to look at this, but libsimdpp is completely spamming the diff visualizer. I'm back to thinking we shouldn't include this in the repo. We should either assume it is installed or download it during the build process. I'm inclined to do the former for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579:186,down,download,186,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579,1,['down'],['download']
Availability,"I updated the ""before attempts"" trigger because there was a bug where the start and end time on error (i.e. create fails) are both None and then when the instance gets deactivated, the reason is overwritten to deactivated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520:96,error,error,96,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520,1,['error'],['error']
Availability,I use this:; ```; dking@wmb16-359 # echo $SPARK_HOME; /Users/dking/borg/spark-2.0.2-bin-hadoop2.7/; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319704034:36,echo,echo,36,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319704034,1,['echo'],['echo']
Availability,"I used filters for the following images when I've run the Azure cleanup script, but we should double check these make sense still in light of changing how we use ""cache"" and there aren't any additional images or ones that we don't want to delete that are in this list:. ```; --filter 'auth:.*' \; --filter 'base:.*' \; --filter 'base_spark_3_2:.*' \; --filter 'batch:.*' \; --filter 'batch-driver-nginx:.*' \; --filter 'batch-worker:.*' \; --filter 'benchmark:.*' \; --filter 'blog_nginx:.*' \; --filter 'ci:.*' \; --filter 'ci-intermediate:.*' \; --filter 'ci-utils:.*' \; --filter 'create_certs_image:.*' \; --filter 'echo:.*' \; --filter 'grafana:.*' \; --filter 'hail-base:.*' \; --filter 'hail-build:.*' \; --filter 'hail-buildkit:.*' \; --filter 'hail-run:.*' \; --filter 'hail-run-tests:.*' \; --filter 'hail-pip-installed-python37:.*' \; --filter 'hail-pip-installed-python38:.*' \; --filter 'hail-ubuntu:.*' \; --filter 'memory:.*' \; --filter 'monitoring:.*' \; --filter 'notebook:.*' \; --filter 'notebook_nginx:.*' \; --filter 'prometheus:.*' \; --filter 'service-base:.*' \; --filter 'service-java-run-base:.*' \; --filter 'test-ci:.*' \; --filter 'test-monitoring:.*' \; --filter 'test-benchmark:.*' \; --filter 'website:.*' \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349:620,echo,echo,620,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349,1,['echo'],['echo']
Availability,I used pip install - currently struggling to install the pyspark 2.0.2 version after downgrading to spark 2.0.2 . ``$SPARK_HOME is /Users/ih/languages/spark-2.0.2-bin-hadoop2.7/bin``,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678:85,down,downgrading,85,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678,1,['down'],['downgrading']
Availability,I used the gsutil storage bandwidth tool and confirmed we get 1.2 Gibit / second upload and download speeds from within a 1 core job and 10 Gi storage. Adding more cores didn't change anything. I ran a test job with the copy tool on a 10 Gi random file and matched 1.2 Gibit / second. I'm wondering if the problem is actually workload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write thr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:92,down,download,92,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['down'],['download']
Availability,"I verified this now works and also verified it fails on current main:; ```; In [2]: from hailtop.hail_logging import *; ...: import logging; ...: configure_logging(); ...: logging.getLogger('foo').info(""hello!""); ...: ; ...: try:; ...: raise ValueError('boom!'); ...: except:; ...: logging.getLogger('foo').exception(""hello!""); {""severity"":""INFO"",""levelname"":""INFO"",""asctime"":""2023-05-10 09:54:36,474"",""filename"":""<ipython-input-2-740eb5422cd6>"",""funcNameAndLine"":""<module>:4"",""message"":""hello!"",""hail_log"":1}; {""severity"":""ERROR"",""levelname"":""ERROR"",""asctime"":""2023-05-10 09:54:36,474"",""filename"":""<ipython-input-2-740eb5422cd6>"",""funcNameAndLine"":""<module>:9"",""message"":""hello!"",""exc_info"":""Traceback (most recent call last):\n File \""<ipython-input-2-740eb5422cd6>\"", line 7, in <module>\n raise ValueError('boom!')\nValueError: boom!"",""hail_log"":1}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13023#issuecomment-1542260535:524,ERROR,ERROR,524,https://hail.is,https://github.com/hail-is/hail/pull/13023#issuecomment-1542260535,2,['ERROR'],['ERROR']
Availability,"I want to get it to not only omit the Java, but also show the desirable python stack trace that points to the source of the error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9226#issuecomment-672876115:124,error,error,124,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672876115,1,['error'],['error']
Availability,"I wanted to chime in on this briefly because I think it a good example use case and its design will influence many future methods, so it is important to get the design right. Thoughts:. - the underscore stuff is a non-starter in my opinion, and too clever by half. A lot of my feedback on your stuff is guided by the general heuristic that you should start by writing down the code you want, and then decide how to implement. You'd never want to write this _ stuff if you didn't have to. - I'm still not quite sure what tablify does (in part because the name is too clever by half and in part because it doesn't appear to always return tables). - But I think the idea of tablify is something we want, which is to convert (possibly indexed expressions) back into relational objects (Table, MatrixTable) because the latter support a wider set of operations and don't have the ""source mismatch problem"". Tim and I discussed this yesterday and we suggest the following interface:. ```; t = build_table(); .set_globals(x = 5, batch = batch); .set_rows(locus = locus, aaf = aaf); .build(); ```. and. ```; mt = build_table_matix(); .set_globals(dataset = dataset); .set_rows(locus = locus, aaf = aaf); .set_entries(GT = GT); .build(); ```. where the input expressions for each part must all come from the same source (or be compatible, e.g., constants) and the resulting (matrix) table inherits the keys from the original table. I think there is an unresolved question about how to handle potential name conflicts (e.g. a column key named locus).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964:368,down,down,368,https://hail.is,https://github.com/hail-is/hail/pull/2852#issuecomment-363841964,2,['down'],['down']
Availability,"I was able to figure out how to remove all the ""network shuffle""s and ""coerced sorted dataset""s and that improved the time down to 73 seconds, so a big improvement! I would still hope to improve performance a bit more, being reliably under a minute would be helpful. Here are the logs from that search, let me know what else I can do to help improve the performance or to help you figure it out: ; [hail-search.log](https://github.com/hail-is/hail/files/13310449/hail-search.log). PR is here if you are interested: https://github.com/broadinstitute/seqr/pull/3717",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1804152779:123,down,down,123,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1804152779,2,"['down', 'reliab']","['down', 'reliably']"
Availability,"I was able to narrow this down a bit further. The issue appears due to this statement: https://github.com/broadinstitute/gnomad-browser/blob/80430090645ce087aa54d67688a4f0920ad1c8fd/data-pipeline/src/data_pipeline/datasets/gnomad_v3/gnomad_v3_variants.py#L127-L143. `subsets` contains 8 elements: `{'non_cancer', 'tgp', 'controls_and_biobanks', 'non_neuro', None, 'non_topmed', 'hgdp', 'non_v2'}`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609:26,down,down,26,https://hail.is,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609,1,['down'],['down']
Availability,"I was having trouble figuring out how to handle the token and the attributes in hailtop.batch_client.aioclient.Batch. When we create an update from a Batch that already existed perhaps in a different process, we don't have the attributes and token. I made a contract where `commit_update` always returns the token and attributes regardless of whether the BatchBuilder already has that infromation. However, we could also get that information available lazily and cache the result. In addition, the `n_jobs` returned to the client are the number of jobs that are committed and not the same as the `n_jobs` in the batches table. Things to do before merging:; 1. Get rid of the batch updates additions to the UI2. ; 2. Double check the GCP LogsExplorer to make sure there are no silent error messages especially with regards to cancellation.; 3. Have @danking look over the SQL stored procedure for `commit_batch_update` to make sure that query is going to perform as good as what is possible given the complexity of the check.; 4. Run a test batch with the old client (I just checked out the current version of main). You need to make sure both create and create-fast are accounted for and succeed. I've been using the following script to make sure we're using the slow path in addition to the fast path with a regular small test job:. ```python3; from hailtop.batch import ServiceBackend, Batch; import secrets. backend = ServiceBackend(billing_project='hail'); b = Batch(backend=backend); # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; for i in range(8):; j1 = b.new_job(); long_str = secrets.token_urlsafe(256 * 1024); j1.command(f'echo ""{long_str}"" > /dev/null'); batch = b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347:442,avail,available,442,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347,3,"['avail', 'echo', 'error']","['available', 'echo', 'error']"
Availability,"I was surprised to see this didn't fail by fixing some fails_local_backend tests. It turns out that we're writing out invalid files with the lowered matrix writer:; ```; def test_indexed_read(self):; mt = hl.utils.range_matrix_table(2000, 100, 10); f = new_temp_file(extension='mt'); mt.write(f); mt2 = hl.read_matrix_table(f, _intervals=[; hl.Interval(start=150, end=250, includes_start=True, includes_end=False),; hl.Interval(start=250, end=500, includes_start=True, includes_end=False),; ]); self.assertEqual(mt2.n_partitions(), 2); self.assertTrue(mt.filter_rows((mt.row_idx >= 150) & (mt.row_idx < 500))._same(mt2)). mt2 = hl.read_matrix_table(f, _intervals=[; hl.Interval(start=150, end=250, includes_start=True, includes_end=False),; hl.Interval(start=250, end=500, includes_start=True, includes_end=False),; ], _filter_intervals=True); self.assertEqual(mt2.n_partitions(), 3); self.assertTrue(mt.filter_rows((mt.row_idx >= 150) & (mt.row_idx < 500))._same(mt2)). E Java stack trace:; E is.hail.utils.HailException: `intervals` specified on an unindexed matrix table.; E This matrix table was written using an older version of hail; E rewrite the matrix in order to create an index to proceed; E 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:11); E 	at is.hail.utils.package$.fatal(package.scala:77); E 	at is.hail.expr.ir.MatrixNativeReader$.apply(MatrixIR.scala:166); E 	at is.hail.expr.ir.MatrixNativeReader$.fromJValue(MatrixIR.scala:184); ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10111#issuecomment-790698282:1221,Error,ErrorHandling,1221,https://hail.is,https://github.com/hail-is/hail/pull/10111#issuecomment-790698282,2,['Error'],['ErrorHandling']
Availability,I went down that route once before and the main issue is how to trigger the change in batch state to completed. I couldn't figure out how to do that correctly and efficiently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594:7,down,down,7,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039624594,1,['down'],['down']
Availability,"I would also suggest the following as necessary for an upcoming release:. * #13728. Google's gcsfuse APT repository currently produces 502 Bad Gateway errors when accessed via http, which shows no sign of being resolved any time soon. I've commented on #13728 noting how this PR can work around the problem. At present (since early October), the `batch_worker_image` job always fails with 502 during a hail deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602:151,error,errors,151,https://hail.is,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602,1,['error'],['errors']
Availability,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195:175,error,erroring,175,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195,2,['error'],['erroring']
Availability,"I'd be ok with taking it off the nav bar if there was some other link to it that was reasonable and findable. Python is a good example. They always show newest thing, but they have a drop down at the top that lets you switch: https://docs.python.org/3/library/re.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8210#issuecomment-593504954:188,down,down,188,https://hail.is,https://github.com/hail-is/hail/pull/8210#issuecomment-593504954,1,['down'],['down']
Availability,"I'd probably suggest a better error message if possible, but if you don't want to, go ahead and close",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-386372161:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-386372161,1,['error'],['error']
Availability,"I'll digest over the next day or so, then let's sit down and go through some of the design choices + trajectory together.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074#issuecomment-320641116:52,down,down,52,https://hail.is,https://github.com/hail-is/hail/pull/2074#issuecomment-320641116,1,['down'],['down']
Availability,"I'll do a performance test, but there's still foreign key constraints on these rows. They're just redundant. We don't need a check on both `batches` and `attempts`. The rows in `attempts` wouldn't have been inserted without the check in `batches`. All of these proposed changes don't change anything about data integrity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811:98,redundant,redundant,98,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811,1,['redundant'],['redundant']
Availability,"I'll leave the issue open for now, but this isn't really feasible. You'll get the same problem with GNU pipes: . ``` bash; wm9f1-8cf:tmp tpoterba$ echo ""hello"" > test; wm9f1-8cf:tmp tpoterba$ cat test; hello; wm9f1-8cf:tmp tpoterba$ cat test > test; wm9f1-8cf:tmp tpoterba$ cat test; wm9f1-8cf:tmp tpoterba$; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/747#issuecomment-256325883:147,echo,echo,147,https://hail.is,https://github.com/hail-is/hail/issues/747#issuecomment-256325883,1,['echo'],['echo']
Availability,"I'll rerun CI now. In case it's helpful, here are the failures from the last run, all in the doctests:; ```; FAILED usr/local/lib/python3.9/dist-packages/hail/matrixtable.py::hail.matrixtable.MatrixTable.from_parts; FAILED usr/local/lib/python3.9/dist-packages/hail/matrixtable.py::hail.matrixtable.MatrixTable.localize_entries; FAILED usr/local/lib/python3.9/dist-packages/hail/table.py::hail.table.Table.to_matrix_table; FAILED usr/local/lib/python3.9/dist-packages/hail/table.py::hail.table.Table.to_matrix_table_row_major; FAILED usr/local/lib/python3.9/dist-packages/hail/experimental/full_outer_join_mt.py::hail.experimental.full_outer_join_mt.full_outer_join_mt; FAILED usr/local/lib/python3.9/dist-packages/hail/methods/impex.py::hail.methods.impex.import_vcf; FAILED usr/local/lib/python3.9/dist-packages/hail/methods/statgen.py::hail.methods.statgen.balding_nichols_model; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14250#issuecomment-1991409946:54,failure,failures,54,https://hail.is,https://github.com/hail-is/hail/pull/14250#issuecomment-1991409946,1,['failure'],['failures']
Availability,"I'll stew on this a little further and I have yet to look closely at the queries themselves, but my first two questions are:. 1. I'm not opposed to adding tokens to the `batches_n_jobs_in_complete_states` table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. > (C) The new server code deploys with the new mark_batch_complete code that runs periodically. Eventually the newly completed batches since the migration will get set to ""complete"". It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, *then* remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412:867,redundant,redundant,867,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412,1,['redundant'],['redundant']
Availability,I'll take another look. Looks like you're hitting some failures in Python now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3414#issuecomment-385520903:55,failure,failures,55,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-385520903,1,['failure'],['failures']
Availability,I'm a little puzzled by this solution. I feel that we shouldn't be modifying class composition in method calls; the structure of our code seems somewhat upside down. If anything the StagedArrayBuilder should only take a class builder.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13939#issuecomment-1785433230:160,down,down,160,https://hail.is,https://github.com/hail-is/hail/pull/13939#issuecomment-1785433230,1,['down'],['down']
Availability,I'm confused by this failure. It looks the CI didn't merge with the latest master. Maybe rebase?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1127#issuecomment-263737204:21,failure,failure,21,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263737204,1,['failure'],['failure']
Availability,"I'm curious how you would test this. I've manually tested and confirmed it works with the currently commented out test. That test is kind of slow and heavy though, and it's hard to write a good version if I don't know how much memory someone has available. Do you think it's worth adding a new CI job to run certain tests in constrained memory environment to verify they stay below a prescribed limit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10233#issuecomment-808504566:246,avail,available,246,https://hail.is,https://github.com/hail-is/hail/pull/10233#issuecomment-808504566,1,['avail'],['available']
Availability,"I'm currently running this branch of CI on a pull request of itself on my own fork of hail, and it nearly passes all tests except for hailtop_batch_* because of requester pays permissions issues and monitoring, because I don't have a service account in my project with all the permissions for broad-ctsa. So unfortunately haven't fully validated that it will _not_ merge a passing PR, but this seemed good enough that we can push it through for azure (since both of these errors are gcp-dependent). If this goes through I can put in a follow-up PR that mirrors the infra resources that CI needs in azure (blob storage, acr permissions, etc.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539:472,error,errors,472,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539,1,['error'],['errors']
Availability,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:840,down,down,840,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['down'],['down']
Availability,"I'm fine removing copy-paste-tokens. They were for a prototype with Terra. We obviously are pursuing a different approach now. Hmm. I suppose old versions of hailctl have no way to know that the fix is to upgrade to a newer version of hailctl? Like, the server can't send a message in the auth failure? We can just ask our local users to upgrade. As long as there's a stable & robust version of query that they can rely on, I think they're happy to upgrade. Which version of hailctl is compatible with new auth?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024:294,failure,failure,294,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024,2,"['failure', 'robust']","['failure', 'robust']"
Availability,"I'm fine with calling it whatever. `die` could be mistaken for something that will shut down hail, at the same time, that's what the IR node is called.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865#issuecomment-634292347:88,down,down,88,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-634292347,1,['down'],['down']
Availability,"I'm fine with this, though I think frequency of failures is still relatively high. I have been hitting retry on PRs when i notice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11265#issuecomment-1027179803:48,failure,failures,48,https://hail.is,https://github.com/hail-is/hail/pull/11265#issuecomment-1027179803,1,['failure'],['failures']
Availability,"I'm going to close for now. If we realize this solves some of the deadlock errors, then we can reopen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7853#issuecomment-581597915:75,error,errors,75,https://hail.is,https://github.com/hail-is/hail/pull/7853#issuecomment-581597915,1,['error'],['errors']
Availability,I'm going to close this issue because I feel like we've moved past the sticking point that this issue is referring to. @konradjk if downstream operations are still having problems please feel free to open issues for them?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5320#issuecomment-479696405:132,down,downstream,132,https://hail.is,https://github.com/hail-is/hail/issues/5320#issuecomment-479696405,1,['down'],['downstream']
Availability,I'm going to close this. The problem is more systemic -- got other errors with the types not being correct for `Die` and `In`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4784#issuecomment-439439892:67,error,errors,67,https://hail.is,https://github.com/hail-is/hail/pull/4784#issuecomment-439439892,1,['error'],['errors']
Availability,I'm going to work on making the pvc failure not throw an exception and have a polling loop that tries to recreate the jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6075#issuecomment-490967834:36,failure,failure,36,https://hail.is,https://github.com/hail-is/hail/pull/6075#issuecomment-490967834,1,['failure'],['failure']
Availability,"I'm gonna push a change that puts a hard 2 minute limit on all tests, we'll see which ones timeout, then I'll mark the ones that are legitimately slow with a per-test timeout. Hopefully this will isolate us down to both the test and particular portion of code that's getting stuck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13122#issuecomment-1568417144:207,down,down,207,https://hail.is,https://github.com/hail-is/hail/pull/13122#issuecomment-1568417144,1,['down'],['down']
Availability,"I'm good with this, but I want my service PR to merge first so that we can adjust the test failure annotations accordingly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10421#issuecomment-831299029:91,failure,failure,91,https://hail.is,https://github.com/hail-is/hail/pull/10421#issuecomment-831299029,1,['failure'],['failure']
Availability,"I'm good with waiting, ping here when ready",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-526623710:23,ping,ping,23,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-526623710,1,['ping'],['ping']
Availability,I'm having second thoughts on echo. let's let @danking tie break.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1573#issuecomment-287745455:30,echo,echo,30,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287745455,1,['echo'],['echo']
Availability,"I'm looking at `worker.py` now and it looks like you worked around this with the addition of `ignore_job_deletion`, which maybe Dan wasn't aware of but is still a workaround since Timings just shouldn't care about deletion in the first place. Without that flag you'd get this:. 1. Running step would start; 2. Job is cancelled, so `Job.deleted` would be set to `True`.; 3. Job would set the Container's `deleted_event`, which would abort the run function inside `run_until_done_or_deleted`; 4. Container would jump to the uploading logs step, which would raise a job deleted error before running `upload_log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054613946:575,error,error,575,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054613946,1,['error'],['error']
Availability,"I'm not 100% sure, but I did this locally. I'd like to test it with aiodocker as well to make sure and confirm on the worker before merging. I couldn't find anything online that 100% confirmed, but my understanding of associated volumes was those created by the container create command based on this behavior. ```; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; (base) wmecc-475:ci jigold$ docker volume create foo; foo; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; (base) wmecc-475:ci jigold$ docker create -v foo:/foo google/cloud-sdk:237.0.0-alpine echo hello; 1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local b4b0706c4dfd3ed1907c1fd3325303578f4805a626b88ecbc4935852440577aa; local foo; (base) wmecc-475:ci jigold$ curl --unix-socket /var/run/docker.sock -H ""Content-Type: application/json"" -X DELETE http:/v1.40/containers/1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7?v=true; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7359#issuecomment-545193658:604,echo,echo,604,https://hail.is,https://github.com/hail-is/hail/pull/7359#issuecomment-545193658,1,['echo'],['echo']
Availability,I'm not aware of an existing problem. We produce an error if sample IDs are not unique and user can rename as needed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/78#issuecomment-316223517:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/issues/78#issuecomment-316223517,1,['error'],['error']
Availability,I'm not sure how I feel about the warning / error suggestion when always run jobs have inputs that aren't always copied out. Cleanup jobs shouldn't care whether the outputs don't get copied out on failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11884#issuecomment-1165813547:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1165813547,2,"['error', 'failure']","['error', 'failure']"
Availability,"I'm not sure quite what we want to do about the DLLs under prebuilt. In the world of dynamic-generated C++, you're going to be linking C++ code compiled on; the master node against libhail. And that isn't going to work reliably if the libhail is; prebuilt, with no guarantees about which compiler/version is used either for the libhail,; or for the dynamic-generated code. The current kludge is that src/main/c/Makefile copies newly-built libraries into prebuilt -; so that *if* you've built from source, then those will be compiled with your compiler.; But it's going to be a crapshoot if you haven't built from source, because the prebuilt; libraries may not work a) against whatever libstdc++.so you have, and b) against your; fresh dynamic-compiled C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-407205066:219,reliab,reliably,219,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-407205066,1,['reliab'],['reliably']
Availability,I'm not sure this will make such a performance difference in the common case -- the genotype-level downcode/subset operation will dominate runtime,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484:99,down,downcode,99,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268333484,1,['down'],['downcode']
Availability,I'm not sure what to do about the invalid block id transient error...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1836784929:61,error,error,61,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1836784929,1,['error'],['error']
Availability,I'm preparing a change to a `-Wall` and `-Werror` to the `Makefile` as well so we catch these as build failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1342#issuecomment-277010916:103,failure,failures,103,https://hail.is,https://github.com/hail-is/hail/pull/1342#issuecomment-277010916,1,['failure'],['failures']
Availability,"I'm pretty sure all the failures are from a bug I fixed in #8564. Was waiting for that to merge, but I can rebase to double check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8565#issuecomment-615229346:24,failure,failures,24,https://hail.is,https://github.com/hail-is/hail/pull/8565#issuecomment-615229346,1,['failure'],['failures']
Availability,"I'm still at a loss as to the source of this specific error, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319677878:54,error,error,54,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319677878,1,['error'],['error']
Availability,"I'm still looking, but I could only find the logs for PR 13509 as PR 13458 is too old. There are no batch worker logs at all for these two instances, but there are a bunch of sys logs. I didn't see an obvious error message, but there's 1000s of sys log messages in there. https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0Alabels.%22compute.googleapis.com%2Fresource_name%22:%22batch-worker-pr-13509-default-p2aogbaogrsp-highmem-np-zx6w4%22;summaryFields=:false:32:beginning;cursorTimestamp=2023-08-29T20:39:28Z;aroundTime=2023-08-29T20:16:33.950Z;duration=PT24H?project=hail-vdc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1736282516:209,error,error,209,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1736282516,1,['error'],['error']
Availability,"I'm sure this is fine, but I would like to track down all places where .init() (or its side effects) are called, since otherwise I would be approving something I didn't fully understand; we could add these to the note. Currently having some spark version mismatch that I'm working through.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4987#issuecomment-447929316:49,down,down,49,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-447929316,1,['down'],['down']
Availability,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:270,down,downloaded,270,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246,2,"['down', 'error']","['downloaded', 'error']"
Availability,"I've added a second commit that fixes the remainder of #13191, marking the individual JobResourceFiles within the ResourceGroup as `_mentioned` and hence preventing the “undefined resource” BatchException previously observed. (Some of the tests in _hail/python/test/hailtop/batch/test_batch.py_ would also need adjusting to account for the re-imagined `_mentioned`.). Having now studied f6fe19c085a9d9ebee23866961cb582a713cc1ad, which introduced `_mentioned` and this error message hint, IMHO this is a reasonable fix. Apart from the code in _backend.py_ to do with `symlink_input_resource_group`, which I haven't looked at, `_mentioned` is maintained solely to decide whether to emit this BatchException hinting to the user that the resource ought to be defined if you're going to use it in `write_output`. In this case, because the filenames are related, `foo.gz.tbi` may well have been created even though only `foo.gz` appears explicitly in the command text, so it may be a false positive (as in #13191's case) to raise the exception. So the conservative thing to do is to suppress the message in these `declare_resource_group` cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13192#issuecomment-1600755633:468,error,error,468,https://hail.is,https://github.com/hail-is/hail/pull/13192#issuecomment-1600755633,1,['error'],['error']
Availability,"I've addressed the comments, but I can't run the benchmark anymore, I get weird errors about partitions being empty that I expected to be non-empty. I will continue to investigate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2253#issuecomment-332664586:80,error,errors,80,https://hail.is,https://github.com/hail-is/hail/pull/2253#issuecomment-332664586,1,['error'],['errors']
Availability,"I've addressed the two comments: now using an `entry_fields` parameter and throwing an error if 'dosage' is requested and any variant is multi-allelic. Docs updated accordingly. I considered setting dosage on multi-allelics to missing rather than throwing an error, but I think error is safest since I could imagine the missingness leading to QC confusion, and if users want dosage in the presence of multi-allelics than they should either use a custom expression or split and then use `hl.gp_dosage`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913:87,error,error,87,https://hail.is,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913,3,['error'],['error']
Availability,"I've done some extensive remodeling of Pedigree and MendelErrors, shorter and conceptually cleaner now, got to delete a bunch of code. But I'm having a serialization issue, which may be related to changing MendelError to include the CompleteTrio rather than the sample. For example, if I replace ""implicatedSample"" by pasting the body in the closure instead, then the serialization error at that point goes away. but there are a bunch of other ones from the toLine below. ```; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:382,error,error,382,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['error'],['error']
Availability,"I've eventually reproduced this. I was a bit thrown that the error message in this issue is different to the one in the op. Here's my work so far:; ```python; # create `variants` heterogeneous array as described in the op; variants = [[""10"", 123, ""G"", ""C""], [""10"", 456, ""T"", ""A""]]. # not sure how the `mt` was created, but not sure it's important for the; # purposes of reproducing the failure; mt = hl.struct(; locus=hl.locus(contig='10', pos=60515, reference_genome='GRCh37'),; alleles=['C', 'T']; ). expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2:])),; variants; ). hl.eval(expr); ```; This fails in the call to `any` with the following: ; ```; Traceback (most recent call last):; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/expression_typecheck.py"", line 78, in check; return self.coerce(to_expr(x)); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 275, in to_expr; return cast_expr(e, dtype, partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 281, in cast_expr; dtype = impute_type(e, partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 129, in impute_type; t = _impute_type(x, partial_type=partial_type); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 179, in _impute_type; ts = {_impute_type(element, partial_type.element_type) for element in x}; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 179, in <setcomp>; ts = {_impute_type(element, partial_type.element_type) for element in x}; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/base_expression.py"", line 182, in _impute_type; raise ExpressionException(""Hail does not support heterogeneous arrays: ""; hail.expr.expressions.base_e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:61,error,error,61,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,2,"['error', 'failure']","['error', 'failure']"
Availability,"I've merged a stack of changes to this branch including:; - Cleaned up tests, including refactoring, making Balding-Nichols covariates deterministic and removing lots of extra test code; - Reorder args in Scala to match Python, related bug fixes; - Improved large N performance by using single array D rather than A and B; - Moved dense versus sparse matching outside of loop; - Improved Python docs and Scala remarks; - Debugged test failure only occurring in Spark 2.1.0, which turned out to be related to accuracy of Davies. I've increased accuracy to 1e-8 which is enough to make current tests pass. Once this goes in, I'll make PRs to:; - Allow users to set accuracy and iterations on Davies, will use same defaults as R: 1e-6 and 10k.; - Add number of variants per key as column.; - Fix behavior to finish running even if some groups are too big upper bound, or if Cholesky fails. Document this behavior. Less urgently, but to keep in mind:; - If bottleneck, improve performance of Gramian computation in large N case using blocking; - Improve Davies C code",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707:435,failure,failure,435,https://hail.is,https://github.com/hail-is/hail/pull/2153#issuecomment-325388707,1,['failure'],['failure']
Availability,"I've merged fixes to two old bugs and one new bug:; - `typecheck_method` => `typecheck`; - an extra `rvb.startStruct()`; - match error for `alleles: Array[String]` to `IndexedSeq[_]` in `RegionValueBuilder.addAnnotation`. (apologies, I meant to PR rather rather than merge directly but forgot to change cseed to origin). Adding; ```; --init gs://hail-common/nirvana/nirvana-init-GRCh37.sh; ```; at cluster startup and running; ```; import hail as hl; mt = hl.import_vcf(path='gs://jbloom/profile225.vcf.bgz'); mt = mt.filter_rows(hl.len(mt.alleles) == 2); ht = hl.nirvana(mt, config='/nirvana/nirvana-cloud-GRCh37.properties', block_size=10000).rows(); (ht.filter((ht.locus.position > 24430000) & (ht.locus.position < 24580000)); .export(output='gs://jbloom/nirvana_cabin1_3.tsv')); ```; yields the same output as before modulo superficial changes to our representation of variant and flattening of `va` to `rsid	qual	filters	info	nirvana`. Here's an examplar now:; ```; locus	alleles	rsid	qual	filters	info	nirvana; 22:24468386	[""G"",""A""]	NA	3.8351e+04	NA	{""AC"":[306],""AF"":[0.061],""AN"":5018,""BaseQRankSum"":26.807,""ClippingRankSum"":-0.538,""DP"":22432,""DS"":null,""FS"":1.203,""HaplotypeScore"":null,""InbreedingCoeff"":0.0335,""MLEAC"":[309],""MLEAF"":[0.062],""MQ"":59.13,""MQ0"":0,""MQRankSum"":16.406,""QD"":14.9,""ReadPosRankSum"":-0.637,""set"":null}	{""chromosome"":""22"",""refAllele"":""G"",""position"":24468386,""altAlleles"":[""A""],""cytogeneticBand"":""22q11.23"",""quality"":null,""filters"":null,""jointSomaticNormalQuality"":null,""copyNumber"":null,""strandBias"":null,""recalibratedQuality"":null,""variants"":[{""altAllele"":""A"",""refAllele"":""G"",""chromosome"":""22"",""begin"":24468386,""end"":24468386,""phylopScore"":3.457,""isReferenceMinor"":null,""variantType"":""SNV"",""vid"":""22:24468386:A"",""isRecomposed"":null,""regulatoryRegions"":null,""clinvar"":null,""cosmic"":[{""id"":""COSM3759087"",""isAlleleSpecific"":true,""refAllele"":""G"",""altAllele"":""A"",""gene"":""CABIN1"",""sampleCount"":2,""studies"":[{""id"":376,""histology"":""carcinoma"",""primarySite"":""large intestine""},{""id",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3122#issuecomment-372137231:129,error,error,129,https://hail.is,https://github.com/hail-is/hail/pull/3122#issuecomment-372137231,1,['error'],['error']
Availability,"I've narrowed down where the problem is. `agg.explode` is not treating missing values correctly. This is true if both PL is missing or the entry is missing (filtered out). `agg.explode` is equivalent to `flatMap` aggregator in Scala. This will pass:; `agg.counter(agg.explode(hl.empty_array(hl.tint32)))`. This will fail:; `agg.counter(agg.explode(hl.null(tarray(tint32))))`. This will pass:; ```; e = mt.entries().select('locus', 'alleles', 's', 'PL'); e = e.filter(hl.is_defined(e.PL)); e.aggregate(hl.agg.counter(hl.agg.explode(e.PL))); ```. I tried looking at the `flatMap` function registry function. It looks like the non-aggregator version has `flattenOrNull`, but not the aggregator version. @danking @catoverdrive can you look at the `flatMap` code both in the function registry and the IR to make sure missing values are handled correctly?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3276#issuecomment-380222316:14,down,down,14,https://hail.is,https://github.com/hail-is/hail/issues/3276#issuecomment-380222316,1,['down'],['down']
Availability,"I've replicated the issue. invocation:; ```bash; ./pyhail-submit cluster-2 foo.py; ```; `foo.py`:; ```python; #!/usr/bin/python. from pyhail import *. hc = HailContext(log=""/tmp/hail.log""). (hc.read(<andrea's file here>); .write('gs://hail-1kg/trash.vds')); ```; first failure:; ```; 2016-12-15 19:05:43 ERROR Utils:91 - Uncaught exception in thread task-result-getter-1; java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:269,failure,failure,269,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"I've reviewed most of this and it looks good, just waiting for Chris to finish squashing this tail of FS errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1072729431:105,error,errors,105,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1072729431,1,['error'],['errors']
Availability,"I've reworked the docs, breaking the description into three stages and describing strategies for dealing with the Hadoop write error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697#issuecomment-394003822:127,error,error,127,https://hail.is,https://github.com/hail-is/hail/pull/3697#issuecomment-394003822,1,['error'],['error']
Availability,I've simplified / improved the test to show both modes of failure that indeed occur on master.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583:58,failure,failure,58,https://hail.is,https://github.com/hail-is/hail/pull/3749#issuecomment-396754583,2,['failure'],['failure']
Availability,"I've taking a similar approach to try to remove unwanted elements before exploding. Sadly, I haven't seen any noticable improvement. I'm also not sure about correctness as I couldn't get your changes in [2e45403](https://github.com/broadinstitute/seqr/commit/2e45403efc159b58cec723f86e6de7653d64cf5f) to work (got errors about missing `gene_ids`). I saw ~20 fewer results in the second query with the code below. Here's what I wrote based off master. ```python; def _filter_compound_hets(self):; ch_ht = self._ht; if self._is_recessive_search:; ch_ht = ch_ht.filter(ch_ht.comp_het_family_entries.any(hl.is_defined)). # Get possible pairs of variants within the same gene; ch_ht = ch_ht.annotate(gene_ids=self._gene_ids_expr(ch_ht, comp_het=True)); ch_ht = ch_ht.explode(ch_ht.gene_ids). # Filter allowed transcripts to the grouped gene; transcript_annotations = {; k: ch_ht[k].filter(lambda t: t.gene_id == ch_ht.gene_ids); for k in [ALLOWED_TRANSCRIPTS, ALLOWED_SECONDARY_TRANSCRIPTS] if k in ch_ht.row; }; if transcript_annotations:; ch_ht = ch_ht.annotate(**transcript_annotations); primary_filters = self._get_annotation_filters(ch_ht); secondary_filters = self._get_annotation_filters(ch_ht, is_secondary=True). self.unfiltered_comp_het_ht = ch_ht.filter(hl.any(primary_filters + secondary_filters)); if self._has_secondary_annotations and not (primary_filters and secondary_filters):; # In cases where comp het pairs must have different data types, there are no single data type results; return None. primary_variants = hl.agg.filter(hl.any(primary_filters), hl.agg.collect(ch_ht.row)); if secondary_filters:; row_agg = ch_ht.row; if ALLOWED_TRANSCRIPTS in row_agg and ALLOWED_SECONDARY_TRANSCRIPTS in row_agg:; # Ensure main transcripts are properly selected for primary/secondary annotations in variant pairs; row_agg = row_agg.annotate(**{ALLOWED_TRANSCRIPTS: row_agg[ALLOWED_SECONDARY_TRANSCRIPTS]}); secondary_variants = hl.agg.filter(hl.any(secondary_filters), hl.agg.collect(row_agg)); el",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1828689808:314,error,errors,314,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1828689808,1,['error'],['errors']
Availability,ICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/pr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10041,echo,echo,10041,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/gi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3799,echo,echo,3799,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c H,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9275,echo,echo,9275,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + ec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:11730,echo,echo,11730,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,IP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'REMOTE is unset or empty'; REMOTE is unset or empty; + exit 1; make: *** [release] Error 1. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:16084,echo,echo,16084,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,3,"['Error', 'echo']","['Error', 'echo']"
Availability,"IRSuite passes, but all tests do not. From test_docs:. ```. Java stack trace:; is.hail.utils.HailException: not a streamable IR: (ToArray; (ArrayMap __iruid_226; (ToStream; (ToArray; (GetTupleElement 0; (Ref __iruid_225)))); (MakeTuple (0 1); (GetField key; (Ref __iruid_226)); (GetTupleElement 0; (GetField value; (Ref __iruid_226)))))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:851); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191:357,Error,ErrorHandling,357,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191,2,['Error'],['ErrorHandling']
Availability,"If it's an important VCF, it shouldn't be corrupted... My solution to this error message will be to add something like `requirement failed: ref was equal to alt` or something like that",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-216535838:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535838,1,['error'],['error']
Availability,"If there's no requester pays, is it just impossible to have ""public"" data in Azure storage safely? Like anything in there could be downloaded infinity times to drive up a bill?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1004205518:131,down,downloaded,131,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1004205518,1,['down'],['downloaded']
Availability,"If we feel confident the APIs make sense, then that's a great idea!. I'm a bit worried that localize_entries was an API mistake (my fault :/) that we use because we lack better tools. I'm especially curious to see how its used and whether there's a better API for that kind of work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9121#issuecomment-662711391:132,fault,fault,132,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662711391,1,['fault'],['fault']
Availability,"If with my changes it is still slower, then let's abandon. My bad for heading us down this route. Seems natural to use binary, but given that pandas probably has native code for parsing TSVs, maybe we're fighting a losing battle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11368#issuecomment-1057594484:81,down,down,81,https://hail.is,https://github.com/hail-is/hail/pull/11368#issuecomment-1057594484,1,['down'],['down']
Availability,"If you look at the driver logs, there should be a bunch of errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956513975:59,error,errors,59,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956513975,1,['error'],['errors']
Availability,"If you start a HailContext with no arguments:; ```python; HailContext(); ```; then Hail does not require you to set any Spark variables. This error:; ```; hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4; ```; is caused by a failure in your Spark cluster. I suggested investigating `spark.cleaner.ttl` due to [Spark bug 5594](https://issues.apache.org/jira/browse/SPARK-5594). This also seems to happen when [you're running more than one spark context at once](https://github.com/spark-jobserver/spark-jobserver/issues/147). You might also be encountering [Spark bug 116599](https://issues.apache.org/jira/browse/SPARK-16599). I think the most productive use of your time is to:; 1. restart your spark cluster; 2. ensure there are no pending jobs and no one will submit any jobs while you run the next steps; 3. start a fresh `pyspark` session; 4. execute your hail commands. If this _still_ fails, then I suspect your Spark cluster is misconfigured in some way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338716796:142,error,error,142,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338716796,2,"['error', 'failure']","['error', 'failure']"
Availability,ImportError: No module named hailjwt; Makefile:22: recipe for target 'test/jwt-test-user-token' failed; make: *** [test/jwt-test-user-token] Error 1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083:141,Error,Error,141,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483899083,1,['Error'],['Error']
Availability,"In InferPType, first 2 I looked at: ArrayRef, Coalesce (in getNestedElementPTypesOfSameType). ##### Experiment with ToStream removed in the case _ => condition in streamify:. ```scala; // as above, but catch all condition in streamify:; private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren); x; }; }; ```. results in many errors in IRSuite, one of which is:. > is.hail.utils.HailException: not a streamable IR: (Literal Array[Int32] ""[3,null,7]""); > ...; > at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ##### With `ToStream(x)` as the return of `case _ =>` and the rest same as above. 100 errors in IRSuite, first one I looked at:. > Caused by: java.lang.ClassCastException: is.hail.expr.types.virtual.TInterval cannot be cast to is.hail.expr.types.virtual.TIterable; > at is.hail.expr.ir.InferType$.apply(InferType.scala:95)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586599051:521,error,errors,521,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586599051,2,['error'],['errors']
Availability,"In Order Theory, this operation is called ""the upper/downward closure of x"" https://en.wikipedia.org/wiki/Upper_set . `groups_affected_by` maybe? `group_self_and_ancestors`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475#issuecomment-1760381169:53,down,downward,53,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1760381169,1,['down'],['downward']
Availability,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-239686959:887,error,error,887,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959,1,['error'],['error']
Availability,"In particular, isn't it possible that you have the n-1th and nth job racing to complete. Everyone else is already done. Call the n-1th job's transaction T1 and the nth job's transaction T2. Both race down to this statement in MJC:; ```; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id AND n_completed = batches.n_jobs;; ```. That will now need to have a sum(n_completed) over all tokens. The isolation level is repeatable read. Assume T1 and T2 generate non equal tokens. T1 and T2 may both snapshot the state of the database before either T1 or T2 executes. T1 and T2 will necessarily see the changes they've made (which affect distinct rows because they have distinct tokens), but neither is required to see the changes the other has made. I think the only way to guarantee that at least one of T1 or T2 sees the database with sum(n_completed) == n_jobs is for both of them to LOCK IN SHARE MODE when doing the sum(n_completed). That will cause lock contention. Maybe that's OK? In the worst case you could have this happen:. 1. Job 1 executes all the way to just before the sum(n_completed).; 2. Job 2 executes all the way to modifying the volatile state.; 3. Job 1 blocks waiting for Job 2 to modify the volatile state.; 4. Job 3 executes all the way to modifying the volatile state.; 5. Job 1 and 2 now wait for Job 3 to modifying the volatile state.; 6. ...; 7. Job 1, 2, 3, n-1 now all wait for Job n to modify the volatile state.; 8. Job 1...n finally execute the sum, all in parallel. I guess that's not terrible, it just means that the latency of Job 1 is extended as long as other jobs can race in before it grabs a shared lock on all the rows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916:200,down,down,200,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916,1,['down'],['down']
Availability,"In the latter two cases, the error does not come from zstd decompression. It comes later during region allocation and using isHet on a Call with ploidy 3. When zstd does notice a decompression issue, it's always immediately after a read. In this case, immediately after a read of the entries data, but in the past we've seen reads of other MTs/HTs. Note that the entries are the bulk of the bytes, so if there's something that's rare in terms of bytes processed, we're just much more likely to see it in the entries.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1843765049:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1843765049,1,['error'],['error']
Availability,"In the short term, a fix which makes the UI usable again for these kinds of jobs is to check blob size, if it's over some threshold, show no log and instruct the user to download it. Then fix the download to use aiohttp's StreamResponse. We should maybe split this issue into a frontend-side and worker-side.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936:170,down,download,170,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1653991936,4,['down'],['download']
Availability,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:1652,Error,Error,1652,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733,1,['Error'],['Error']
Availability,"Increasing the executor memory per core to 20G/core seemed to help get by this memory error. . It would be useful to have some rule of thumbs for estimating memory requirements based on number of samples and variants. spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 12\; --executor-cores 4\; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 80G\; --driver-memory 60g\; --driver-cores 1\; --name ""$1"" \; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303:86,error,error,86,https://hail.is,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303,2,"['error', 'heartbeat']","['error', 'heartbeatInterval']"
Availability,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:340,redundant,redundant,340,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511,2,['redundant'],['redundant']
Availability,"Indeed, if i do `hl.literal()`, I get the right error message:; ```; hail.expr.expressions.base_expression.ExpressionException: Hail does not support heterogeneous dicts: found dict with values of types [dtype('str'), dtype('int32')]; ```; But fixing the value types upstream removes the requirement for `hl.literal()` and works",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3886#issuecomment-401931653:48,error,error,48,https://hail.is,https://github.com/hail-is/hail/issues/3886#issuecomment-401931653,1,['error'],['error']
Availability,"Interesting, this directly contradicts the [k8s documentation on pod lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/):; > Running | The Pod has been bound to a node, and all of the Containers have been created. At least one Container is still running, or is in the process of starting or restarting. However, given the available statuses, Running seems like the most reasonable one to describe a pod in the process of shutting down. Shall we close the issue now that we've understood the semantics or is there further action for us to take?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5486#issuecomment-469299860:348,avail,available,348,https://hail.is,https://github.com/hail-is/hail/issues/5486#issuecomment-469299860,2,"['avail', 'down']","['available', 'down']"
Availability,Interestingly I was have a consistent test failure here but never got the chance to diagnose it. Would like to come back to it once the glut of PRs is through. Would you rather keep it open or closed?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11931#issuecomment-1266120559:43,failure,failure,43,https://hail.is,https://github.com/hail-is/hail/pull/11931#issuecomment-1266120559,1,['failure'],['failure']
Availability,Is there currently a debugging version of Region (one that checks bounds and throws an exception)? Dan and I have both uses that profitably in the past to isolate bugs like this quickly. That might be the first step to tracking this down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522#issuecomment-429179210:233,down,down,233,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429179210,1,['down'],['down']
Availability,Is this as easy as d38896d5b3e5160d50070103f7948158af5a5ea1? The commit gives this error instead:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: ; Index Expressions: int32; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6663#issuecomment-514769232:83,error,error,83,https://hail.is,https://github.com/hail-is/hail/issues/6663#issuecomment-514769232,1,['error'],['error']
Availability,"It doesn't seem like a bad change. I suspect it's rare for this range to be more than 2 long, and I'd hope that the InsertFields avoids copying the entire row, so I'd be surprised if this ever made a significant difference. But I also doubt adding memory management would slow it down much, so better to be safe I guess.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12037#issuecomment-1187948315:280,down,down,280,https://hail.is,https://github.com/hail-is/hail/pull/12037#issuecomment-1187948315,1,['down'],['down']
Availability,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:903,down,down,903,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942,2,['down'],['down']
Availability,It is my intention to eventually document it when when local mode is feature complete and reliable. I'm not sure what the standard process is for this. I'd be OK to rename it when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9596#issuecomment-707933032:90,reliab,reliable,90,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707933032,1,['reliab'],['reliable']
Availability,"It is possible there is a race condition, though I have not witnessed this before. In fact, it seems rather reasonable that GitHub had some intermittent slow down that delayed repository creation or ability to find said repository temporarily.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153:158,down,down,158,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429025153,1,['down'],['down']
Availability,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:169,down,downloading,169,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082,1,['down'],['downloading']
Availability,"It looks like permissions for deleting disks and VMs are broken for the `delete_batch_instances` CI step. This job also hung for a long time and then got restarted. There's some other wonky things about this PR, but it just seems like the main issue was the Batch deployment was cancelled mid-run and the driver didn't have time to cleanup those 2 VMs that weren't responding before being shut off. Then the cleanup step isn't actually working so they didn't get cleaned up. The only remaining question I have is why these VMs weren't starting up correctly. There were at least 5 in this one PR that didn't start up in time before the driver was shut down. https://batch.hail.is/batches/7908998/jobs/207",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569:651,down,down,651,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569,1,['down'],['down']
Availability,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240306249:308,Down,Download,308,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249,3,"['Down', 'down']","['Download', 'download']"
Availability,"It seems the test failures are due to:; 1. TemporaryDirectory (and TemporaryFilename); 2. `hailtop.batch.backend.ServiceBackend` absolutely should not use sync `BatchClient`, the async one is right there!; 3. `hailctl batch submit` is broken because of (2); 4. `test_callback` should use async `BatchClient` b/c it is async. TemporaryDirectory & TemporaryFilename use `hailtop.fs`, which is sync. This is nearly the async FS API except:; 1. `isfile` vs `is_file`; 2. `isdir` vs `is_dir`; 3. `stat` returns a `FileListEntry` instead of a `FileStatus`.; 4. `listfiles` vs `ls`. `hailtop.fs.router_fs.RouterFS` is a sync shim between these APIs. So there's basically sync-vs-async and Python-vs-Hail FS APIs. We have:; 1. sync, Python: `hailtop.fs.FS`.; 2. async, Python: does not exist.; 3. async, Hail: `hailtop.aiotools.fs.FS`.; 4. sync, Hail: `hailtop.fs.router_fs.RouterFS`. If we had (2), we could write an async version of TemporaryDirectory and TemporaryFilename and use those in async methods (in particular, in `hail.backend.ServiceBackend`). The high-level need is that we gotta be careful about not interleaving async-sync-async. Your PR reveals that we were inadvertently violating that rule. It seems best to follow the rule and only use `nest_asyncio` when we're in a Jupyter Notebook.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1743171933:18,failure,failures,18,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1743171933,1,['failure'],['failures']
Availability,"It seems to only occur when I use bp.read_input(..) to localize many files per job. ; Above a certain number of input files (many thousands), I started getting this error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13940#issuecomment-1786375579:165,error,error,165,https://hail.is,https://github.com/hail-is/hail/issues/13940#issuecomment-1786375579,1,['error'],['error']
Availability,"It should never not be in the database. That's why it should be an error if we get an event for an instance we've never created before. The reason we keep getting these events is because when we run this query:. ```; filter = f'''; logName=""projects/{PROJECT}/logs/cloudaudit.googleapis.com%2Factivity"" AND; resource.type=gce_instance AND; protoPayload.resourceName:""{self.machine_name_prefix}"" AND; timestamp >= ""{mark}""; '''; ```. We have `timestamp >= {mark}`. This means if `mark` doesn't change each time the event polling loop reruns, then we'll always keep getting the same event as the last one processed. We need that `>=` though because different events can have the same timestamp. So the driver could have been shut down in the middle of processing multiple events with the same timestamp. So we do the conservative thing and try to reprocess the event again until the mark changes to a different timestamp. Does that make sense?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10325#issuecomment-819729595:67,error,error,67,https://hail.is,https://github.com/hail-is/hail/pull/10325#issuecomment-819729595,2,"['down', 'error']","['down', 'error']"
Availability,It was an old worker that didn't have the idempotent updates. The error was in create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922,1,['error'],['error']
Availability,It would be a new class to help express what you're trying to do that doesn't exist yet. To `localize` means to download the file to a VM and `delocalize` is to upload a file back to the cloud.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599635933:112,down,download,112,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599635933,1,['down'],['download']
Availability,"It would be easy to implement TDT, de novo, and mendel errors in expr using this, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2374#issuecomment-340212371:55,error,errors,55,https://hail.is,https://github.com/hail-is/hail/pull/2374#issuecomment-340212371,1,['error'],['errors']
Availability,"It's a bit confusing, but on the CI page you can navigate to the Artifacts tab from which you can open the [build report's index.html](https://ci.hail.is/repository/download/HailSourceCode_HailCi/33165:id/build/reports/tests/index.html). The error looks to me like a Hail problem, not a you problem. I'll investigate further.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320333635:165,down,download,165,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320333635,2,"['down', 'error']","['download', 'error']"
Availability,It's as if it was just spinning on `exit $BUILD_EXIT`. It had to have finished the last for loop because all the logs open in my browser instead of downloading.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4541#issuecomment-429915864:148,down,downloading,148,https://hail.is,https://github.com/hail-is/hail/issues/4541#issuecomment-429915864,1,['down'],['downloading']
Availability,"It's getting a forbidden error when trying to download the secret. I think I know why. The service account being used is the `batch2` service account from the default namespace. I think we need to do what I did with CI where the service account lives in the default namespace, but it can read secrets, service accounts in the batch pods namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7470#issuecomment-550495837:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-550495837,2,"['down', 'error']","['download', 'error']"
Availability,"It's hard to go through every line to look for bugs, but the structure looks great. You've got a couple test failures (code cast errors)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9926#issuecomment-767796221:109,failure,failures,109,https://hail.is,https://github.com/hail-is/hail/pull/9926#issuecomment-767796221,2,"['error', 'failure']","['errors', 'failures']"
Availability,"It's not so much so that **we** can check out a tagged release, as we have already worked around the problem. But I would expect that you and any other installations will also run into the same `batch_worker_image` failure. We have been running our production instance using gcsfuse 1.2.0 for about a week now, and I think @illusional will agree with me that we haven't seen any problems from it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622:215,failure,failure,215,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622,1,['failure'],['failure']
Availability,"It's possible to get this error:; ```; ----> 5 hc.import_vcf('src/test/resources/sample.vcf').write('sample.vds'). /hail/python/hail/java.py in function_wrapper(*args, **kwargs); 92 except Py4JJavaError as e:; 93 msg = env.jutils.getMinimalMessage(e.java_exception); ---> 94 raise FatalError(msg); 95 except Py4JError as e:; 96 env.jutils.log().error('hail: caught python exception: ' + str(e)). FatalError: UnsupportedClassVersionError: htsjdk/tribble/TribbleException : Unsupported major.minor version 52.0; ```. I figure this change might be nicer, but am happy to hear input",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286:26,error,error,26,https://hail.is,https://github.com/hail-is/hail/pull/1406#issuecomment-280786286,2,['error'],['error']
Availability,"I’ll look for closely once I get to the retreat, but first impression is that centering and normalizing are redundant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054:108,redundant,redundant,108,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054,1,['redundant'],['redundant']
Availability,"I’m out today. I’m good with the PR if Tim is. On Friday, November 9, 2018, Christopher Vittal <notifications@github.com>; wrote:. > @tpoterba <https://github.com/tpoterba> @jigold; > <https://github.com/jigold> ping; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/4720#issuecomment-437412681>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/ABnWpLgmNZwI4lVd3I0vKAkwVhicd-4_ks5utawIgaJpZM4YLr0h>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4720#issuecomment-437417490:212,ping,ping,212,https://hail.is,https://github.com/hail-is/hail/pull/4720#issuecomment-437417490,1,['ping'],['ping']
Availability,"Julia has regenerated the file with the correct extension, and it is available at `gs://gnomad-public/papers/2019-tx-annotation/pre_computed/all.possible.snvs.tx_annotated.021520.ht`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9955#issuecomment-791633393:69,avail,available,69,https://hail.is,https://github.com/hail-is/hail/pull/9955#issuecomment-791633393,1,['avail'],['available']
Availability,"Just a few comments. Looking good! Looking forward to tests. Also, need to abstract out the code you share with Mendel errors as we discussed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/109#issuecomment-169135201:119,error,errors,119,https://hail.is,https://github.com/hail-is/hail/pull/109#issuecomment-169135201,1,['error'],['errors']
Availability,Just fix the error message in AST and I'm happy with it!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242153148:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242153148,1,['error'],['error']
Availability,Just linting failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13163#issuecomment-1585078013:13,failure,failures,13,https://hail.is,https://github.com/hail-is/hail/pull/13163#issuecomment-1585078013,1,['failure'],['failures']
Availability,"Just skimmed the discussion. >> I've been working on an R interface to Hail through the sparklyr package; >; > this also sounds awesome. woah, hell yes. I'll look tomorrow. Our build situation is a bit messed up right now. I'll try to isolate your issue and fix it. Moreover, I should be fixing the build situation for good soon. Can you share a full executor log for an executor that fails? That should have some information about why the spark context got shut down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868:463,down,down,463,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430451868,1,['down'],['down']
Availability,"Just so I understand correctly (and sorry if this is obvious), the current job logs interface is still the same. But if you want a container's logs, then you'll get bytes which the user will have to decode themselves. How does that affect the file download button in the UI and the hailctl batch logs functionality you have? Will you see text or a random byte string?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1426340756:248,down,download,248,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1426340756,1,['down'],['download']
Availability,"Just to be clear, I'm proposing:. Pending -> Ready; Ready -> Error, Running; Running -> Ready, Error, Failed, Success",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906:61,Error,Error,61,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339906,2,['Error'],['Error']
Availability,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:134,error,error,134,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150,2,['error'],['error']
Availability,"Just to be sure, I checked the logs for errors and found none. I manually logged into the database and watched the compaction happen for both tables. I don't think we can do any more due diligence with this. Let's plan on merging on Tuesday when you're back from vacation. ```; mysql> select * from aggregated_billing_project_user_resources_v3 where resource_id = 6 limit 100;; +----------------------------------+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:40,error,errors,40,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['error'],['errors']
Availability,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:463,avail,available,463,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569,2,['avail'],['available']
Availability,"Known facts:; - the last CI k8s deployment that started the deploy=1 batch on April 27th was active since at least April 25th.; - For a small window of time that I looked at on April 25th, it kept getting errors when trying to get the Github status for possible merge candidates: 12848, 12849, 12547. There might be other PRs at later dates. I saw at least the same errors for 12848 on April 27th. I'm going to throw out a hypothesis. I merged the dedup attempt resources PR on April 19th. The PRs that were stacked on previous commits of that PR now have merge conflicts with the set of commits that actually got merged. This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. CI didn't get restarted at least from the 25th to the 27th so the merge candidate never would have been refreshed. We know that there's less GKE node turnover in Azure, so not unexpected that the ci pod wouldn't get redeployed on its own. I'm thinking it's possible that I merged the database trigger fix on April 27th in response to the excessive deadlocks we noticed and then rebased the subsequent stacked PRs that had merge conflicts, thus unblocking CI, but I'm not sure (it's really hard to get what I want from the Azure log analytics system). I think the ""bug fix"" here is to reassess the code in CI and possibly harden it where we select the merge candidate and try to get the status so it doesn't block deployments. I have a screenshot from April 25th below in case it's helpful. The log analytics query that is helpful is:. ```; ContainerLog; | where ContainerID == ""273584134970cdae08cf0d412461862e2a0e558888a52c91870ca46a146cbb8a""; | order by TimeGenerated; ```. <img width=""1085"" alt=""Screen Shot 2023-05-24 at 12 58 18 PM"" src=""https://github.com/hail-is/hail/assets/1693348/e2da08b6-5982-46cb-9e2c-2178a19f2f86"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011:205,error,errors,205,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011,2,['error'],['errors']
Availability,Konrad and Beryl have seen this error before trying to use Spark 2.2:; http://discuss.hail.is/t/typeerror-javapackage-object-is-not-callable/250,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319708208:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319708208,1,['error'],['error']
Availability,"Kudu 0.9.0 was released a few days ago, and it has a re-written Spark library so we don't need the `org.kududb.spark` package any more. It also fixes bugs, like the one @cseed saw with the context not being shut down. Annotations still don't work though - is there a way to get their schema early on so we can create a database table for them?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-226535225:212,down,down,212,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-226535225,1,['down'],['down']
Availability,"LOL yeah right u l33t. My fault, should have caught that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468828579:26,fault,fault,26,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468828579,1,['fault'],['fault']
Availability,"Latest build for spark failing. -- Performing Test CAN_COMPILE_POWER_ALTIVEC - Failed; -- Configuring done; -- Generating done; -- Build files have been written to: /gpfs/home/tpathare/hail_new/hail/src/main/c/libsimdpp-2.0-rc2; mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 ibs.cpp -o lib/linux-x86-64/libibs.so; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [lib/linux-x86-64/libibs.so] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 12.153 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635:410,error,error,410,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-276938635,3,"['Error', 'FAILURE', 'error']","['Error', 'FAILURE', 'error']"
Availability,Let me do some experiments and see whether there is a performance cost. The third post down seemed to think the way I wrote it there shouldn't be a performance penalty and it should still use the index. I'm not sure whether I believe that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12275#issuecomment-1268740914:87,down,down,87,https://hail.is,https://github.com/hail-is/hail/pull/12275#issuecomment-1268740914,1,['down'],['down']
Availability,Let's close until we see another reproduction. Transient error handling has improved a bit since this ticket was opened.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980#issuecomment-1551538732:57,error,error,57,https://hail.is,https://github.com/hail-is/hail/issues/12980#issuecomment-1551538732,1,['error'],['error']
Availability,"Let's compare 8093951-8854 to 8093977-8854. The latter is a failed task (partition 6914) the former is successful. We'll download the logs and make toss away some debug info that changed between the experiments. ```; cat log | rg StreamBlockInputBuffer: | sed 's/bytes.*//' > newlog; ```. Since the latter failed, the log obviously ends earlier, but there are *no differences* (besides timestamps) in the size of the blocks read from GCS. Since these block sizes are read from the input stream, this is pretty good evidence that the bytes aren't corrupted up until now. ```; # git diff --no-index --word-diff good bad ; ...; 2023-12-06 [-19:47:11.500-]{+21:39:00.885+} StreamBlockInputBuffer: INFO: reading 2081[-2023-12-06 19:47:11.531 StreamBlockInputBuffer: INFO: reading 2499-]; ```. The decompressed data size is the same: 65536. It's worth noting this is a relatively small compressed buffer after a series of much larger compressed buffers. This one is 2081 and the immediately previous one is 14675. Most of the ones before this are also in the 14k range. ---. Same experiment on job 7157 again shows no differences in bytes read before the exception occurs. ```; 2023-12-06 [-19:45:18.693-]{+21:36:52.116+} StreamBlockInputBuffer: INFO: reading 17923 ; 2023-12-06 [-19:45:18.809-]{+21:36:52.388+} StreamBlockInputBuffer: INFO: reading 17843[-2023-12-06 19:45:18.810 StreamBlockInputBuffer: INFO: reading 17657-]; [-2023-12-06 19:45:18.811 StreamBlockInputBuffer: INFO: reading 17646-]; ```. The network reads are identical other than the size of the first read. That first read is the serialized function. I'm not that surprised it differs in size between different commits of Hail. The byte counting is done in our code. If we're counting bytes correctly, then it seems like we're reading the same series of chunks from GCS. . ```; GoogleStorageFS$: INFO: read 1755052 (0 of 1755052) oldbb(0, 8388608) newbb(0, 1755052); GoogleStorageFS$: INFO: read 8388608 (62604 of 58870664) oldbb(0, 8388",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1843799744:121,down,download,121,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1843799744,1,['down'],['download']
Availability,Let's merge this so people don't get horrible error messages for now. I'll make an issue to make typecheck more powerful.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652:46,error,error,46,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301973652,1,['error'],['error']
Availability,"Let's see if #13969 fixes this. If we don't see these errors again over the next week, let's close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13863#issuecomment-1792532420:54,error,errors,54,https://hail.is,https://github.com/hail-is/hail/issues/13863#issuecomment-1792532420,1,['error'],['errors']
Availability,Lindo reports an error with a side-length of 177860. Cal reports a side length of 544768. The square of both is larger than 2^32.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734193173:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734193173,1,['error'],['error']
Availability,Look into the test failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1632#issuecomment-290947359:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290947359,1,['failure'],['failure']
Availability,"Looking at the `…/Packages` URL in the previous comment, 1.2.0 is now available (and 1.1.0 does not appear to be there). In our recent local hail update deployment, the `batch_worker_image` job failed repeatedly due to GoogleCloudPlatform/gcsfuse#1424. We worked around this as initially suggested on that issue with populationgenomics/hail@607408bee752dabca48d9a2732b14d32813ace9f, but later comments on the issue suggest that the better approach would be this PR with an additional change to access the apt repo via https:. ```diff; - echo ""deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; + echo ""deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502:70,avail,available,70,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502,3,"['avail', 'echo']","['available', 'echo']"
Availability,"Looking at the auth image, it is down from 2.76GB in main to 674MB. The hail-ubuntu image underneath it has stayed basically the same at about half the new auth image. Nearly all of the 674MB is split evenly between the layer that installs python in hail-ubuntu and the layer that installs the pip dependencies in the auth image. I've not yet inspected the hail-ubuntu layer, but for the pip dependencies the main offenders are:. ```; 77M	/usr/local/lib/python3.7/dist-packages/googleapiclient; 76M	/usr/local/lib/python3.7/dist-packages/botocore; 33M	/usr/local/lib/python3.7/dist-packages/_sass.abi3.so; 29M	/usr/local/lib/python3.7/dist-packages/kubernetes_asyncio; 20M	/usr/local/lib/python3.7/dist-packages/uvloop; 14M	/usr/local/lib/python3.7/dist-packages/pip; 14M	/usr/local/lib/python3.7/dist-packages/cryptography; 8.9M	/usr/local/lib/python3.7/dist-packages/google; 7.9M	/usr/local/lib/python3.7/dist-packages/pygments; 7.0M	/usr/local/lib/python3.7/dist-packages/azure; 5.0M	/usr/local/lib/python3.7/dist-packages/setuptools; 4.2M	/usr/local/lib/python3.7/dist-packages/aiohttp; 2.5M	/usr/local/lib/python3.7/dist-packages/googlecloudprofiler; 2.2M	/usr/local/lib/python3.7/dist-packages/yaml; 2.2M	/usr/local/lib/python3.7/dist-packages/hailtop; 2.0M	/usr/local/lib/python3.7/dist-packages/rich; 1.6M	/usr/local/lib/python3.7/dist-packages/pyasn1_modules; 1.5M	/usr/local/lib/python3.7/dist-packages/boto3; 1.4M	/usr/local/lib/python3.7/dist-packages/pkg_resources; 1.4M	/usr/local/lib/python3.7/dist-packages/oauthlib; 1.1M	/usr/local/lib/python3.7/dist-packages/pycparser; ```. Most of a gigabyte still feels annoyingly bloated but might just have to do for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308:33,down,down,33,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308,1,['down'],['down']
Availability,Looks good other than a couple nits on the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/1005#issuecomment-256483820,1,['error'],['error']
Availability,"Looks great, aside from Value.fromLIR stuff which I think is causing test failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10791#issuecomment-901427655:74,failure,failures,74,https://hail.is,https://github.com/hail-is/hail/pull/10791#issuecomment-901427655,1,['failure'],['failures']
Availability,Looks like a compilation error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8282#issuecomment-611255794:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-611255794,1,['error'],['error']
Availability,"Looks like it was opened and closed here #12381. I haven't taken a look at this yet, but there are two footguns that should be avoided as much as possible:. - A job waiting on a batch that it is a part of. We add the batch id into the container so we should be able to throw an error here; - Waiting on a batch in general is really wonky when there is more than 1 entity controlling it. I don't know of a good way to control this, so it might just be a ""be sure you know what you're doing thing"", but worth thinking about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1334441229:278,error,error,278,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1334441229,1,['error'],['error']
Availability,"Looks like some Python failures. It will take me a little while to track them down, but they all look minor and of two forms I understand:; - RVDType being constructed with the row type being optional,; - and incorrect type signatures when building emit methods due to mismatched missingness.; Go ahead and review while I fix them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8371#issuecomment-607958840:23,failure,failures,23,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-607958840,2,"['down', 'failure']","['down', 'failures']"
Availability,Looks like test failure is due to needing to wrap phenotype in array in this line; `top_5_pvals = (vds.linreg('sa.metadata.CaffeineConsumption')`; of; https://github.com/hail-is/hail/blob/master/python/hail/docs/tutorials/expression-language-part-2.ipynb,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2042#issuecomment-318833001:16,failure,failure,16,https://hail.is,https://github.com/hail-is/hail/pull/2042#issuecomment-318833001,1,['failure'],['failure']
Availability,Looks like there's quite a few rebase errors to touch up as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1304186719:38,error,errors,38,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1304186719,1,['error'],['errors']
Availability,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172:20,failure,failures,20,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172,2,['failure'],['failures']
Availability,"Looks like we're getting some intermittent failures, monitoring.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11623#issuecomment-1074129063:43,failure,failures,43,https://hail.is,https://github.com/hail-is/hail/pull/11623#issuecomment-1074129063,1,['failure'],['failures']
Availability,Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175:190,error,error,190,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175,1,['error'],['error']
Availability,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662:12,robust,robust,12,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662,2,['robust'],['robust']
Availability,"Man, you got some *weird* errors in this PR. Why is asyncio giving us a `None` for a loop?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11098#issuecomment-983797608:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/11098#issuecomment-983797608,1,['error'],['errors']
Availability,"Maryam,; I ran this command in Unix:. ```; gunzip -c <file> | cut -f4 | sort | uniq -c; 20709505 A; 20934670 C; 20968049 G; 20693812 T; 25 alt; ```. I think the problem is that the headers from all the files were included in the one file. I'm running another grep now to be sure. I'll fix the error message though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/317#issuecomment-212477402:293,error,error,293,https://hail.is,https://github.com/hail-is/hail/issues/317#issuecomment-212477402,1,['error'],['error']
Availability,"Maybe more damning is the scaling of the error with n:; ```; In [35]: t = hl.utils.range_table(10**4). In [36]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [37]: t.aggregate(hl.agg.stats(t.x)); Out[37]: Struct(mean=10000000.012717757, stdev=1.043072384832424, min=9999996.160907382, max=10000003.396893706, n=10000, sum=100000000127.17758). In [38]: t = hl.utils.range_table(10**5). In [39]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [40]: t.aggregate(hl.agg.stats(t.x)); Out[40]: Struct(mean=10000000.000944939, stdev=1.0017584539199058, min=9999994.870601522, max=10000004.903980266, n=100000, sum=1000000000094.4939). In [41]: t = hl.utils.range_table(10**6). In [42]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [43]: t.aggregate(hl.agg.stats(t.x)); Out[43]: Struct(mean=10000000.000245763, stdev=0.9050966799187808, min=9999995.23730167, max=10000004.54308704, n=1000000, sum=10000000000245.764). In [44]: t = hl.utils.range_table(10**7). In [45]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [46]: t.aggregate(hl.agg.stats(t.x)); Out[46]: Struct(mean=9999999.999657989, stdev=nan, min=9999994.645387085, max=10000005.969974454, n=10000000, sum=99999999996579.89). In [47]: t = hl.utils.range_table(10**7). In [48]: t = t.annotate(x = 10**7 + hl.rand_norm()). In [49]: t.aggregate(hl.agg.stats(t.x)); Out[49]: Struct(mean=9999999.999756364, stdev=1.5274428303540528, min=9999995.005864669, max=10000005.419492438, n=10000000, sum=99999999997563.64); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10132#issuecomment-788973494:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/10132#issuecomment-788973494,1,['error'],['error']
Availability,"Maybe no longer relevant, but zeroing missings *after* centering is; equivalent to using non-missing terms only rather than mean imputing,; provided you then use N_nonmissing for the final normalization. On Tue, Dec 10, 2019 at 8:50 AM Jon Bloom <notifications@github.com> wrote:. > I’ll look for closely once I get to the retreat, but first impression is; > that centering and normalizing are redundant.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/7653?email_source=notifications&email_token=ACC577VJUORGGYMDZUE72IDQX6NDNA5CNFSM4JVAFXT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGPJKXQ#issuecomment-564041054>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC577UWWGRAMQHAOV7TGADQX6NDNANCNFSM4JVAFXTQ>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326:394,redundant,redundant,394,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326,1,['redundant'],['redundant']
Availability,Maybe we make the name change for success and put the failure conditions in a separate PR? I wonder if there are any sneaky bits with those additions,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365#issuecomment-1289351886:54,failure,failure,54,https://hail.is,https://github.com/hail-is/hail/pull/12365#issuecomment-1289351886,1,['failure'],['failure']
Availability,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877,2,['error'],['error']
Availability,"Maybe? We have an assertion in `TextMatrixReader.parseOptionalValue` that the missing values are not empty strings. I didn't trace through why, I just moved the error up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11078#issuecomment-975610252:161,error,error,161,https://hail.is,https://github.com/hail-is/hail/pull/11078#issuecomment-975610252,1,['error'],['error']
Availability,"Mendel errors should stay a separate pass for various reasons. Perhaps this can be revisited when we start working on a higher-level, plink-like tool.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/148#issuecomment-240001350:7,error,errors,7,https://hail.is,https://github.com/hail-is/hail/issues/148#issuecomment-240001350,1,['error'],['errors']
Availability,Merging in https://github.com/hail-is/hail/pull/14233 causes the failure in `test_union_rows1`. Some strangeness with these new dependencies - running without this commit and everything works fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14231#issuecomment-1924903341:65,failure,failure,65,https://hail.is,https://github.com/hail-is/hail/pull/14231#issuecomment-1924903341,1,['failure'],['failure']
Availability,Mmm. Yes. I need a more robust IR testing plan. I think testing these individually will be more painful than testing them in the context of IR expressions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2514#issuecomment-349092532:24,robust,robust,24,https://hail.is,https://github.com/hail-is/hail/pull/2514#issuecomment-349092532,1,['robust'],['robust']
Availability,"Most of the failures were coming from `SUnreachableValue`s inheriting from both `SValue` and `SCode`, which has caused me problems before too. Since `SCode` will be going away, I just gave in and duplicated them all for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10848#issuecomment-914601076:12,failure,failures,12,https://hail.is,https://github.com/hail-is/hail/pull/10848#issuecomment-914601076,1,['failure'],['failures']
Availability,"Mostly me not knowing how this code works, so, my apologies for slowing you down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13385#issuecomment-1668220060:76,down,down,76,https://hail.is,https://github.com/hail-is/hail/pull/13385#issuecomment-1668220060,1,['down'],['down']
Availability,"NOTE: This issue is **only** about the error message. We can definitely produce a more insightful error message (perhaps suggesting the use of `.rows()[key_field1, key_field2]`) without also addressing the confusing syntax.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14237#issuecomment-1921963399:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/issues/14237#issuecomment-1921963399,2,['error'],['error']
Availability,Need more information. Please re-open when more information is available!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13993#issuecomment-1969996616:63,avail,available,63,https://hail.is,https://github.com/hail-is/hail/issues/13993#issuecomment-1969996616,1,['avail'],['available']
Availability,"Needs a `python3 -m black batch --line-length=120 --skip-string-normalization`; ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/driver/main.py	2023-04-05 14:40:12.638902 +0000; +++ batch/driver/main.py	2023-04-05 14:44:25.172615 +0000; @@ -1226,11 +1226,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, `user`, resource_id, token, `usage`); SELECT billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; async def compact_agg_billing_project_users_by_date_table(app):; @@ -1254,11 +1255,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_by_date_v3 (billing_date, billing_project, `user`, resource_id, token, `usage`); SELECT billing_date, billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; USER_CORES = pc.Gauge('batch_user_cores', 'Batch user cores (i.e. total in-use cores)', ['state', 'user', 'inst_coll']); would reformat batch/driver/main.py. Oh no! 💥 💔 💥; 1 file would be reformatted, 93 files would be left unchanged.; make[1]: *** [Makefile:18: check] Error 1; make[1]: Leaving directory '/io/repo/batch'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925:1442,Error,Error,1442,https://hail.is,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925,1,['Error'],['Error']
Availability,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114,1,['failure'],['failure']
Availability,New test failure mode:. ERROR: (gcloud.beta.dataproc.clusters.create) Operation [projects/broad-ctsa/regions/global/operations/07164d0e-6c27-35d9-8132-9960b0db6d43] failed: Internal server error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429176963,3,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417,2,['failure'],"['failure', 'failures']"
Availability,"Nice! I think we should add some error checking code that verifies the name is non-empty when we, for example, `create`. It's unlikely to happen but would create some confusing situations if a user creates a file with an empty name. I'm not even sure if it's allowed in the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13435#issuecomment-1679764031:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/pull/13435#issuecomment-1679764031,1,['error'],['error']
Availability,"Nice, `test_paired_elementwise_ops` down to under a minute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12347#issuecomment-1282578133:36,down,down,36,https://hail.is,https://github.com/hail-is/hail/pull/12347#issuecomment-1282578133,1,['down'],['down']
Availability,"Nice. This is looking better. I don’t care so much about fluctuations in the handful of seconds but do we believe these big changes to pc relate, block matrix, and range MT sum? Can you re-run just those five or so slow ones and confirm they really slowed down?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1564333638:256,down,down,256,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1564333638,1,['down'],['down']
Availability,"No, the input strings are all on `gs://` but in the error I get:. ```; subprocess.CalledProcessError: Command '#!/bin/bash; # change cd to tmp directory; cd /tmp//pipeline-dc5b53d50f45/. cp /Users/konradk/Dropbox (Partners HealthCare)/src/python/gnomad_hail/gs:/phenotype...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017,1,['error'],['error']
Availability,"No. There's one test that only fails on Azure and I haven't figured out why yet. ```. =================================== FAILURES ===================================; ___________________________ test_dir_outside_curdir ____________________________. runner = <typer.testing.CliRunner object at 0x7f95b3d14b50>. def test_dir_outside_curdir(runner: CliRunner):; with tempfile.TemporaryDirectory() as dir:; os.mkdir(f'{dir}/working_dir'); os.chdir(f'{dir}/working_dir'); write_hello(f'{dir}/hello1.txt'); write_hello(f'{dir}/hello2.txt'); write_script(dir, '/hello1.txt'); res = runner.invoke(cli.app, ['submit', '--files', f'{dir}/:/', '../test_job.py']); > assert res.exit_code == 0; E AssertionError: assert 1 == 0; E + where 1 = <Result HttpResponseError('The specified block list is invalid.\nRequestId:86424c6a-d01e-004a-272b-0b6b10000000\nTime:2023-10-30T12:21:01.7415144Z\nErrorCode:InvalidBlockList')>.exit_code; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13785#issuecomment-1785325401:122,FAILURE,FAILURES,122,https://hail.is,https://github.com/hail-is/hail/issues/13785#issuecomment-1785325401,1,['FAILURE'],['FAILURES']
Availability,"Nope, seeing the same error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301759591:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301759591,1,['error'],['error']
Availability,"Not really sure why this is coming up now (I don't see anything that changed in this PR, but the scala isn't compiling because of a redundant function definition in is.hail.expr.types and is.hail.expr.ir (coerce)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089:132,redundant,redundant,132,https://hail.is,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089,1,['redundant'],['redundant']
Availability,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820,3,"['Error', 'error']","['Error', 'ErrorHandling', 'error']"
Availability,Not sure. It doesn't throw an error on my version of anaconda. Which I've been meaning to update for awhile because all of the make files fail with my version of conda...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999,1,['error'],['error']
Availability,"Note some highlights from the log:; ```; #12 42.27 ./Bio/tmp/Bio-DB-HTS-2.9 - moving files to ./biodbhts; #12 42.27 - making Bio::DB:HTS; #12 42.40 Checking prerequisites...; #12 42.40 requires:; #12 42.40 ! Bio::Root::Version is not installed; #12 42.40 ; #12 42.40 ERRORS/WARNINGS FOUND IN PREREQUISITES. You may wish to install the versions; #12 42.40 of the modules indicated above before proceeding with this installation; #12 42.40 ; #12 42.40 Run 'Build installdeps' to install missing prerequisites.; ```; ```; #13 138.3 Building and testing Test2-Suite-0.000152 ... ! Installing Test2::V0 failed. See /root/.cpanm/work/1682614674.13506/build.log for details. Retry with --force to force install it.; #13 150.9 FAIL; #13 150.9 --> Working on FFI::CheckLib; #13 150.9 Fetching http://www.cpan.org/authors/id/P/PL/PLICEASE/FFI-CheckLib-0.31.tar.gz ... OK; #13 150.9 Configuring FFI-CheckLib-0.31 ... OK; #13 151.1 ==> Found dependencies: Test2::V0, Test2::Require::EnvVar, Test2::Require::Module; #13 151.1 ! Installing the dependencies failed: Module 'Test2::Require::EnvVar' is not installed, Module 'Test2::V0' is not installed, Module 'Test2::Require::Module' is not installed; #13 151.1 ! Bailing out the installation for FFI-CheckLib-0.31. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230:267,ERROR,ERRORS,267,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230,1,['ERROR'],['ERRORS']
Availability,"Note that pandas 2.0.0 [removes the deprecated `DataFrame.iteritems()`](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes), which is used by bokeh-1.4.0. That particular old version of bokeh is listed in _hail/python/requirements.txt_ but it is thus incompatible with pandas 2; so one or the other of these pinnings probably needs to be revisited. (This incompatibility has caused the [large_cohort unit test failure](https://github.com/populationgenomics/production-pipelines/actions/runs/4782280056/jobs/8501466504?pr=354#step:5:134) in populationgenomics/production-pipelines#354.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581:457,failure,failure,457,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581,1,['failure'],['failure']
Availability,"Note that we never see trisomy 22, but do see trisomy 21 (down)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9085#issuecomment-658826248:58,down,down,58,https://hail.is,https://github.com/hail-is/hail/pull/9085#issuecomment-658826248,1,['down'],['down']
Availability,Note to self to check the logs for error messages before merging!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1254026870:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1254026870,1,['error'],['error']
Availability,"Note, this means migrations for batch will shut batch down, but batch won't be running to restart itself (!) so this will have to be done manually. The UI will also be down, so we'll have to watch the database to verify the migration has been applied to know when to restart batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573659765:54,down,down,54,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573659765,2,['down'],['down']
Availability,"Note, when I made this change, I ran into a bug where the label in whileLoop got laid down twice. I found the two cuprits:; - one was && and ||, which duplicated the code on different code paths so was conceptually correct but could lead to code explosion. Either way, I rewrote them.; - and the other was in checkedConvertFrom which actually executed its input twice. I will propose some code changes to deal with this reuse issue. In general, I want the picture that Code[_] cannot be placed in multiple locations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8207#issuecomment-593374823:86,down,down,86,https://hail.is,https://github.com/hail-is/hail/pull/8207#issuecomment-593374823,1,['down'],['down']
Availability,"Noted. I understand, I will switch to a newer version of Hail.; I had some other trouble with Hail 0.2.74, but that is for another ticket. There is an error as you can see in the last line ; ```; one error found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10831#issuecomment-918748397:151,error,error,151,https://hail.is,https://github.com/hail-is/hail/issues/10831#issuecomment-918748397,2,['error'],['error']
Availability,"Notes: ; #### 1st & 3rd set of errors. 1st and 3rd set identical, except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,4,['error'],"['errorNum', 'errors']"
Availability,"Notice that this script is working with spark 1.6, the error appears with spark 2. ```; exac_vds_split = hc.read(root + 'andrea_subset_splitmulti_hard.vds'); dbNSFP_vds = hc.read(root + 'dbNSFP_3.2a_variant.filtered.allhg19_nodup.vds'); discovEHR_vds = hc.read(root + 'discoverEHR.vds'); exacV2_vds = hc.read(root + 'exacV2_split_variants_non_in_andrea_subset.vds'); fin_vds = hc.read(root + 'finnish_noexac_subset.vds'). (exac_vds_split; # .filter_variants_expr('v.contig==""7"" && v.start > 75013221 && v.start < 76253221', keep=True); # .filter_variants_expr('v.contig==""20""', keep=True); .annotate_variants_expr('va = drop(va, vep)'); .vep(config='/vep/vep-gcloud.properties', root='va.vep', force=True); .write(stroot + '/andrea_subset_splitmulti_hard_vep.vds', overwrite=True)). exac_vds_split_vep = hc.read(stroot + 'andrea_subset_splitmulti_hard_vep.vds'). (exac_vds_split_vep; .annotate_global_list(root + 'all_scores_reduced.scores', root='global.allgenes'); .annotate_global_expr_by_sample('global.allgenes = global.allgenes.map(x => x.split(""\\t""))'); .variant_qc(); .annotate_variants_vds(discovEHR_vds,root='va.EHR'); .annotate_variants_vds(exacV2_vds,root='va.EXACV2'); .annotate_variants_vds(fin_vds,root='va.FIN'); .annotate_variants_table(root + 'clinvar_clean.txt','Variant(Variant)', root='va.clinvar'); .annotate_variants_table(root + 'lethal_clean.txt','Variant(Variant)', root='va.lethal'); .annotate_variants_expr(; 	""""""; 	va.clinvar.yes=if(isMissing(va.clinvar.Variant)) 0 else 1,; 	va.lethal.yes=if(isMissing(va.lethal.Variant)) 0 else 1,; 	va.nNonRef = gs.filter(g => g.isCalledNonRef).count(); 	""""""); .annotate_variants_expr(; 	""""""; 	va.freq.AF01 = (va.qc.AF < 0.01),; 	va.freq.AF001 = (va.qc.AF < 0.001),; 	va.freq.DOUBLE = (va.qc.AC == 2),; 	va.freq.SING = (va.nNonRef == 1),; 	va.freq.URVEXACV2 = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC)),; 	va.freq.URVEXACV2EHR = (va.nNonRef == 1 && isMissing(va.EXACV2.qc.AC) && isMissing(va.EHR.info.AF) && isMissing(va.FIN.qc.AC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541:55,error,error,55,https://hail.is,https://github.com/hail-is/hail/issues/1284#issuecomment-274678541,1,['error'],['error']
Availability,Now `hc.import_vcf('/Users/jbloom/data/bgz_error/sample_plain.vcf.bgz')` on mislabeled plaintext file gives:. ```; FatalError: ZipException: File does not conform to block gzip format. Java stack trace:; java.util.zip.ZipException: File does not conform to block gzip format.; 	at is.hail.io.compress.BGzipInputStream$BGzipHeader.<init>(BGzipInputStream.java:35); ```. This error message is closer to that thrown in the .gz case when reading a plain text file:. ```; FatalError: IOException: not a gzip file. Java stack trace:; java.io.IOException: not a gzip file; 	at org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2129#issuecomment-323611362:374,error,error,374,https://hail.is,https://github.com/hail-is/hail/pull/2129#issuecomment-323611362,1,['error'],['error']
Availability,"Now that we have the SQL query monitoring, I would love to also see just the simple comparison of the total number of queries we perform over a 1-minute period under high load. We have the tools now to see just what chunk of overall database communication we are cutting down on, which is an achievement in itself. Just need to run the test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520:271,down,down,271,https://hail.is,https://github.com/hail-is/hail/pull/11346#issuecomment-1036284520,2,['down'],['down']
Availability,"OK, I can see why you didn't enable PLW2901 but it did discover one bug in a CLI command. It's kind of annoying and invalidates reasonable programs but it seems really valuable for catching errors that are common amongst new programmers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12967#issuecomment-1535590575:190,error,errors,190,https://hail.is,https://github.com/hail-is/hail/pull/12967#issuecomment-1535590575,1,['error'],['errors']
Availability,"OK, I eliminated a bunch of `log.exception` that are either retried or re-raised. I completely eliminated some of them and in other cases I downgraded them to warning.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9980#issuecomment-775247626:140,down,downgraded,140,https://hail.is,https://github.com/hail-is/hail/pull/9980#issuecomment-775247626,1,['down'],['downgraded']
Availability,"OK, I improved the tests two ways:. 1. I allocate a random amount of memory in the region to start so things don't always start at offset 0. 2. I test addRegionValue adding a value at the top level and and a nested level (by allocating a non-unsafe Row when t == TStruct) so it calls through to RVB.addRow. I verified it would have caught the previous errors, and it caught another error (toOff was wrong in addRegionValue because we called currentOffset before allocateRoot). Hopefully good to go now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521:352,error,errors,352,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521,2,['error'],"['error', 'errors']"
Availability,"OK, I moved the file format test changes to https://github.com/hail-is/hail/pull/11906. This change can go in independently, but #11906 will even out the test job times and make developer experience better. Service backend tests on #11904 which should be representative of a normal PR:. id | name | state | exit_code | duration; -- | -- | -- | -- | --; 118 | test_hail_python_service_backend_0 | Success | Success 🎉 | 24 minutes; 119 | test_hail_python_service_backend_1 | Success | Success 🎉 | 27 minutes; 120 | test_hail_python_service_backend_2 | Success | Success 🎉 | 24 minutes; 121 | test_hail_python_service_backend_3 | Success | Success 🎉 | 41 minutes; 122 | test_hail_python_service_backend_4 | Success | Success 🎉 | 21 minutes. Service backend tests on this PR (albeit with #11906 which evens out test times):. id | name | state | exit_code | duration; -- | -- | -- | -- | --; 118 | test_hail_python_service_backend_0 | Failed | Failure 🤷‍♀️ (1) | 31 minutes; 119 | test_hail_python_service_backend_1 | Success | Success 🎉 | 31 minutes; 120 | test_hail_python_service_backend_2 | Success | Success 🎉 | 28 minutes; 121 | test_hail_python_service_backend_3 | Success | Success 🎉 | 33 minutes; 122 | test_hail_python_service_backend_4 | Success | Success 🎉 | 26 minutes. I think there is almost no effect on service backend test times! We should really see if there's a way to improve the autoscaler & schedule to achieve this on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11902#issuecomment-1152508508:939,Failure,Failure,939,https://hail.is,https://github.com/hail-is/hail/pull/11902#issuecomment-1152508508,1,['Failure'],['Failure']
Availability,"OK, I overcame my irrational fear of taits, and changed everything to use tolerations. If I can't get things to work like this, I will try node selectors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-562079366:74,toler,tolerations,74,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-562079366,1,['toler'],['tolerations']
Availability,"OK, I reimplemented the sync-er in Python. This works well enough though it would benefit from something that waited for changes to settle down and did one copy-restart. Currently, you can queue up a bunch of changes and it sometimes take as long as 5 seconds for the whole system to settle down enough that you can refresh and get the new page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089:139,down,down,139,https://hail.is,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089,2,['down'],['down']
Availability,"OK, I seem to have resolved this error, but now another transient error has dramatically increased; its frequency. I included my test code which was reliably reproducing this error approximately once per run. I ran; this three times using a commit very similar to `main` [1]. All three runs failed:. 1. In run 1, three partitions had this error.; 2. In run 2, one partition had a different error (#13721 to be exact).; 3. In run 3, two partitions had this error. After my fix [2] for this issues bug, the #13721 bug became super common! I saw it 50 times in my first run:; ```; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; ```. Luckily, that one is actually trivial to fix, we just need to [update to the latest GCS client; library](https://github.com/hail-is/hail/issues/13721#issuecomment-1737924344). # Test Code. ```python3; import hail as hl; import gnomad.utils.sparse_mt. tmp_dir = 'gs://danking/tmp/'; vds_file = 'gs://neale-bge/bge-wave-1.vds'; out = 'gs://danking/foo.vcf.bgz'. vds = hl.vds.read_vds(vds_file); mt = hl.vds.to_dense_mt(vds); t = gnomad.utils.sparse_mt.default_compute_info(mt); t = t.annotate(info=t.info.drop('AS_SB_TABLE')); t = t.annotate(info = t.info.drop(; 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; )); t = t.drop('AS_lowqual'). hl.methods.export_vcf(dataset = t,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184:33,error,error,33,https://hail.is,https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184,7,"['error', 'reliab']","['error', 'reliably']"
Availability,"OK, I tested my branch twice with stress. No error logs. Many warnings due to known deadlock errors. Otherwise it looks clean. I've pushed those changes onto this branch. Let's merge tomorrow first thing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956662015:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956662015,2,['error'],"['error', 'errors']"
Availability,"OK, I think I addressed all the comments. I'm going to make the change to not stat the destination if we set treat_dest_as=Transfer.TARGET_FILE. Then we just need to decide if we want to throw an error on file:// for a non-existent Transfer.TARGET_DIR. I say no.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822#issuecomment-764847097:196,error,error,196,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764847097,1,['error'],['error']
Availability,"OK, I think I've addressed all the comments. I made some additional changes:. - pipe input file directly into tar instead of writing to disk (writing to SSDs, I get ~100MB/s/core download saturating gs://),; - report records read,; - directly create OrderedRVD instead of coercing,; - updated GenomicsDB to latest: 0.9.2-proto-3.0.0-beta-1+ed318f7e815 which involved revising GenomicsDBFeatureReader ctor call",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861:179,down,download,179,https://hail.is,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861,1,['down'],['download']
Availability,"OK, I think this is actually ready for a real review. Almost everything was spurious (I marked as such, so hopefully we won't have to do that on every PR). There were a few real things:; 1. use integrity checks for CDN javascript libraries; 2. don't let edits to the search textbox modify the URL arbitrarily; 3. don't let the target pages of anchor tags mutate the source page's DOM (wtf, how is this the default behavior???); 4. don't send the IntegrityError from mysql back to the users. I think this is basically safe because of how restrictive we are with which error is printed, but it's also not necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860:567,error,error,567,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860,1,['error'],['error']
Availability,"OK, I will PR improved error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470,1,['error'],['error']
Availability,"OK, I won't be able to fix this. @ehigham @patrick-schultz @daniel-goldstein some combo of you three can probably figure it out. The local backend tests that hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloud",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1005,error,errors,1005,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['error'],['errors']
Availability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777:93,avail,available,93,https://hail.is,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777,8,['avail'],['available']
Availability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:93,avail,available,93,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,255,['avail'],['available']
Availability,"OK, I'm not sure how to fix this but the work is to explain to the GCS Hadoop Connector which credentials we want it to use. See the failure here: https://batch.hail.is/batches/8136069/jobs/49 . It uses CI's credentials instead of the test credentials. We use core-site.xml to do this in Spark <3.5, but the GCS connector is different in Spark 3.5 and it uses different configuration parameters. My most recent change did not successfully configure it. Daniel G can help you a bit with credentials in Batch if that's necessary but the real work is to figure out how to tell the GCS Hadoop Connector to use the /gsa-key/key.json file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844:133,failure,failure,133,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844,1,['failure'],['failure']
Availability,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:632,error,error,632,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,4,['error'],['error']
Availability,"OK, I've moved it and made the interface as close as I could to the previous `scatter`. One thing is the default value for `n_divisions`. It was 500 before, now I've set it to `None` (i.e. no downsampling). I'm fine either way, but it seems somewhat more intuitive to me for the default to be no downsampling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696:192,down,downsampling,192,https://hail.is,https://github.com/hail-is/hail/pull/5601#issuecomment-473338696,4,['down'],['downsampling']
Availability,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240309173:114,down,download,114,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173,1,['down'],['download']
Availability,"OK, alright. I think you're right. I'm just reliving how frustrated I am by this situation. Can you modify aggregate_cols to include the same warning from `cols()`?; ```; In [6]: import hail as hl; ...: mt = hl.utils.range_matrix_table(3, 3); ...: mt = mt.choose_cols([2, 1, 0]); ...: mt = mt.checkpoint('/tmp/foo.mt', overwrite=True); ...: mt.col_idx.show(); ...: print(mt.aggregate_cols(hl.agg.collect(mt.col_idx))); ...: mt = mt.key_cols_by(); ...: print(mt.aggregate_cols(hl.agg.collect(mt.col_idx))); 2023-08-10 14:43:14.286 Hail: INFO: wrote matrix table with 3 rows and 3 columns in 3 partitions to /tmp/foo.mt; +---------+; | col_idx |; +---------+; | int32 |; +---------+; | 2 |; | 1 |; | 0 |; +---------+; 2023-08-10 14:43:16.338 Hail: INFO: Coerced sorted dataset; [0, 1, 2]; [2, 1, 0]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405#issuecomment-1673731948:293,checkpoint,checkpoint,293,https://hail.is,https://github.com/hail-is/hail/pull/13405#issuecomment-1673731948,1,['checkpoint'],['checkpoint']
Availability,"OK, but I'm not sure it's the right change to make. Now some jobs will fail silently.; I think the right thing to do would be to change how benchmark jobs are run and always collect results, regardless of job outcome (making it resiliant to some benchmark files not being in their expected locations, which we'll have to do anyway). That way, you'd still see the failures in the batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527578232:363,failure,failures,363,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527578232,1,['failure'],['failures']
Availability,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385:27,failure,failure,27,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385,10,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"OK, let's always mount it. People download and run software on their laptops which can read their gcloud and hail credentials awnyway!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-768637760:34,down,download,34,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-768637760,1,['down'],['download']
Availability,"OK, should be resolved now. I had a lint error and needed to mount the global config into hello and website.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12380#issuecomment-1297763402:41,error,error,41,https://hail.is,https://github.com/hail-is/hail/pull/12380#issuecomment-1297763402,1,['error'],['error']
Availability,"OK, so the big insight is that ""InstanceConfig"" is really just ""ResourcesForAParticularInstance"" (well, and, sometimes, ""ResourcesOfARepresentativeInstance""). I trimmed the InstanceConfig down *significantly* removing the ""vm_config"". Now the InstanceConfig is cheap and easy to create and there's no circularity between vm_config and instance config. I pushed that through everywhere and then abstracted the common create_instance logic for pool and job-private into InstanceCollection. With both of those changes, I was able to modify the ResourceManager's API to expose methods for constructing instance configs. However, the instance config isn't critical to the operation of the ResourceManager. It's just an interface for communicating an instance's resources to the rest of the code base.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279:188,down,down,188,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279,1,['down'],['down']
Availability,"OK, so, I took the GRCh38 file that we test against and named it `bar`. I downloaded the gist and named it `foo`. | header | footer | success? |; |---|---|---|; |foo|bar|success|; |bar|bar|success|; |bar|foo|failure|; |foo|foo|failure|. So clearly the issue is the variants. Here's an example of running on just the first handful of variants: https://batch.hail.is/batches/8089052. ```; chr1	1339585	.	G	A	.	.	.; chr1	24907372	.	C	T	.	.	.; chr1	36859143	.	G	T	.	.	.; chr1	37969436	.	T	C	.	.	.; chr1	40416828	.	G	A	.	.	.; chr1	41581842	.	G	A	.	.	.; chr1	43920822	.	T	C	.	.	.; chr1	45327881	.	G	A	.	.	.; chr1	46817055	.	CT	C	.	.	.; chr1	54999203	.	C	T	.	.	.; chr1	65218884	.	C	T	.	.	.; chr1	102962250	.	G	T	.	.	.; chr1	111756087	.	G	C	.	.	.; chr1	113881802	.	G	A	.	.	.; chr1	117920205	.	G	A	.	.	.; chr1	151408784	.	G	C	.	.	.; chr1	151428261	.	C	T	.	.	.; chr1	152305539	.	G	C	.	.	.; chr1	152884596	.	C	A	.	.	.; chr1	153933240	.	C	T	.	.	.; chr1	156624012	.	G	A	.	.	.; chr1	159205821	.	CT	C	.	.	.; chr1	173803162	.	G	T	.	.	.; chr1	179813831	.	G	A	.	.	.; chr1	179917551	.	T	C	.	.	.; chr1	180935962	.	G	C	.	.	.; chr1	180941229	.	G	A	.	.	.; chr1	186893053	.	C	A	.	.	.; chr1	201363319	.	G	A	.	.	.; chr1	223749094	.	A	G	.	.	.; chr1	224294328	.	G	A	.	.	.; chr1	235809337	.	G	A	.	.	.; chr1	241592073	.	G	T	.	.	.; chr2	9376947	.	G	A	.	.	.; chr2	11618532	.	C	T	.	.	.; ```. We can see the characteristic super high memory use.; <img width=""570"" alt=""Screenshot 2023-11-28 at 16 35 26"" src=""https://github.com/hail-is/hail/assets/106194/e5dfa586-5c77-479b-8050-9b0b7d2fe319"">. ---. If we use the same header, but just one variant, it succeeds, but notice that the RAM use grows rapidly. https://batch.hail.is/batches/8089064/jobs/3; ```; chr1	241592073	.	G	T	.	.	.; ```; <img width=""577"" alt=""Screenshot 2023-11-28 at 16 37 39"" src=""https://github.com/hail-is/hail/assets/106194/90c5ab45-9ca4-43e0-9a97-bf6032863f32"">. ---. If we use the same header with this variant from our (successful) test VCF, the RAM use grows",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344:74,down,downloaded,74,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344,6,"['down', 'failure']","['downloaded', 'failure']"
Availability,"OK, so, this really feels like bad data. We just merged https://github.com/hail-is/hail/commit/98adcce1d07001995b0819fd6afe161bf34ba840 which fixed https://github.com/hail-is/hail/issues/13979 . Google Cloud Storage's Java library was very rarely returning just flat-out bad data. The frequency of occurrence on one particularly large pipeline appears to be 1/30000 tasks (0.003% or 3 in 100,000). The tasks were reading two files, the larger of which was 131MiB. The Java library reads in 8MiB chunks so that's at least 17 network requests per partition. That puts the frequency of this closer to 1 in 1,000,000 requests or 1 in 10TiB of data read. Before we had Zstandard, it seems that this data corruption either (a) was unnoticed (b) caused a rare decoding error or (c) caused segfaults. After we added Zstandard (0.2.119), decompression often failed due to corrupt data. It seems to me that Zstandard more aggressively verifies integrity than LZ4 does. OK, so, when was this bug introduced in Hail? As far as I can tell, this new code path was added in google-cloud-storage 2.17.0 almost one year ago: https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77 . We upgraded to 2.17.1 (😭 ) in Hail 0.2.109 https://github.com/hail-is/hail/commit/fec0cc2263c04c00e02cef5dda8ec46916717152 . All of the attempts above could have been plagued by this rare transient data corruption error. OK, action items:. - [ ] Ask Cal and Lindo to try their pipelines again with the next release of Hail 0.2.127.; - [x] Hail must introduce large-scale testing before releases. We, sadly, cannot assume our underlying storage libraries are reliable. https://github.com/hail-is/hail/issues/14082. Once the first action item is successfully completed, I will close this issue. For the second action item, I have created a separate ticket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114:762,error,error,762,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114,3,"['error', 'reliab']","['error', 'reliable']"
Availability,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:258,down,down,258,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146,6,"['down', 'error']","['down', 'error']"
Availability,"OK, so. The issue seems to be that docker shuts down a connection when an invalid packet is received. https://github.com/moby/libnetwork/issues/1090. Why `git clone` isn't exiting with a non-zero code and thus bailing out of the function, I don't know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8190#issuecomment-600700900:48,down,down,48,https://hail.is,https://github.com/hail-is/hail/pull/8190#issuecomment-600700900,1,['down'],['down']
Availability,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312,1,['error'],['error']
Availability,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:96,avail,available,96,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826,2,['avail'],['available']
Availability,"OK, this PR basically works except we're encountering OOMs somewhat often. I'm trying to track down which tests are triggering the OOMs, but its a bit tricky because the OOMKiller doesn't necessarily kill the test which is using a ton of memory.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1034272076:95,down,down,95,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1034272076,1,['down'],['down']
Availability,"OK, this is now higher priority for me. The Query-on-Batch tests are absolutely hammering the database with huge spikes in deadlock errors during working hours (when deploys trigger tests).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265:132,error,errors,132,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265,1,['error'],['errors']
Availability,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:617,toler,tolerate,617,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072,1,['toler'],['tolerate']
Availability,"OK, this is working and ready for review. I tested manually that on a variety of node types, we both (1) get the expected number of containers (all the cores in the cluster are used) and (2) we get the right OOM error instead of container crashing. Your comment is addressed -- we always call increment before actually allocating, so we won't exceed the threshold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11016#issuecomment-964540617:212,error,error,212,https://hail.is,https://github.com/hail-is/hail/pull/11016#issuecomment-964540617,1,['error'],['error']
Availability,"Oh I see, I actually didn't realize / forgot you could have functions down below that were not present in the summary at the top. Agree we should fix this. I'm going to keep a running list of missing functions in the issue description",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7292#issuecomment-558284093:70,down,down,70,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558284093,1,['down'],['down']
Availability,"Oh yeah, to get my code to work you need to comment out line 778 `gene_id=ch_ht.gene_ids,` in `_annotated_comp_het_variant`. It doesn't break search to be missing that annotation, it just has some downstream display affects that I would need to fix if I actually wanted to use the code, but given the performance hit I wasn't sure it was worth figuring that out as this code may be too slow to use. I was not able to get the code you provided here to run either, but one concern I have with it is that the unique combinations are computed per gene, but if you have a pair of variants that are each in the same 2 genes, you would get the pair twice in the results, one for each gene. The error I get when I run the code you provide is; ```; ""Key type mismatch: cannot index table with given expressions:; Table key: <<<empty key>>>; Index Expressions: locus<GRCh38>, array<str>, set<str>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, array<array<struct{GQ: int32, AB: float64, DP: int32, GT: call, sampleId: str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, struct{z_score: float32}, struct{region_type_ids: array<int32>}, locus<GRCh37>, str, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, str, int64, struct{PHRED: float32}, struct{alleleId: int32, conflictingPathogenicities: array<struct{pathogenicity_id: int32, count: int32}>, goldStars: int32, pathogenicity_id: int32, assertion_ids: array<int32>}, struct{REVEL_score: float32, VEST4_score: float32, MutPred_score: float32, SIFT_pred_id: int32, Polyphen2_HVAR_pred_id: int32, MutationTaster_pred_id: int32, fathmm_MKL_coding_pred_id: int32}, struct{Eigen_phred: float32}, struct{AF_POPMAX: float3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465:197,down,downstream,197,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465,1,['down'],['downstream']
Availability,"Oh, interesting idea Jackie. It has always bothered me that the batches state is duplicated as both the state column and the four job state count columns. @daniel-goldstein , responding specifically to your point about the new deadlock. I agree, that seems strange. I'd bump up the number of tokens and hope that contention for batch_inst_coll_cancellable_resources goes down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039672959:371,down,down,371,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039672959,1,['down'],['down']
Availability,"Oh, it looks like this error isn't from a subprocess call. The thing you added would help debug the first error you posted (No such file or directory) but not this one, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319718587:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319718587,2,['error'],['error']
Availability,"Oh, woah, that test does look wrong. It's concerning that its suddenly failing. I'm not sure I care too much about tracking down exactly which dependency change caused this. We should fix the test obviously. We should add a test that verifies both `?a` and `?` have the expected data (in particular, that we didn't overwrite one with the other!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1276854186:124,down,down,124,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1276854186,1,['down'],['down']
Availability,"Ok I just looked at the scala code, and this must have happened around the sex chromosomes when my dataset shifted to haploid (or more specifically, a mix of haploid and diploid calls for male and female). I'll write in the workaround for my own pipeline, but you might want to have a more explicit error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263:299,error,error,299,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263,1,['error'],['error']
Availability,"Ok, I think I sorted out the make->mill dependency propagation. Any real target which invokes mill to build it now depends on a target `FORCE` which is always out-of-date, so mill is always invoked. But mill will not change the modification time if it doesn't need to, so downstream targets aren't forced to be run. For example, we have targets; ```; FORCE:. SHADOW_JAR := out/assembly.dest/out.jar; $(SHADOW_JAR): FORCE; 	$(mill) assembly. PYTHON_JAR := python/hail/backend/hail-all-spark.jar; $(PYTHON_JAR): $(SHADOW_JAR); 	cp -f $(SHADOW_JAR) $@. .PHONY: python-jar; python-jar: $(PYTHON_JAR); ```. If I remove the python jar and invoke make, it runs mill then copies:; ```; ❯ rm python/hail/backend/hail-all-spark.jar. ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If run again, mill is invoked to check for changes, but as the jar doesn't change it isn't copied again:; ```; ❯ make python-jar; bash millw assembly; [105/110] memory.resources; ```. If I change some scala sources, the jar is updated and copied:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If I change some scala sources in a way that doesn't actually affect the jar, such as modifying comments, mill is smart enough to not change the jar, so it won't be copied again:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 1 Scala source to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [105/110] memory.resources; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793:272,down,downstream,272,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793,1,['down'],['downstream']
Availability,"Ok, I've addressed your comment, corrected requiredness inference for `PartitionNativeWriter` and `SplitPartitionWriter` and revamped things so that the key is not determined based on whether or not there is an index. I've also bumped file version and regenerated files. . I'd be interested to see what you think about testing / whether there's more testing you would do. This is harmless change right now if all the normal tests pass since it's just adding a new metadata field that's unused save for a few tests, but we don't want to start marking files ""distinctlyKeyed"" that aren't and then run into problems down the road when we implement the new join behavior.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11151#issuecomment-1015626695:613,down,down,613,https://hail.is,https://github.com/hail-is/hail/pull/11151#issuecomment-1015626695,1,['down'],['down']
Availability,"Ok, I've downloaded JSON for the batch, default, and CI dashboards (I don't think any of the other ones are really used?), and recorded the configurations for the GCP and prometheus datasources, so I think I should be able to quickly reconfigure grafana if everything is lost.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923:9,down,downloaded,9,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923,1,['down'],['downloaded']
Availability,"Ok, merged to resolve conflicts. Going to now add a few commits to fix errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9228#issuecomment-683803596:71,error,errors,71,https://hail.is,https://github.com/hail-is/hail/pull/9228#issuecomment-683803596,1,['error'],['errors']
Availability,"Ok, so I thought I left a comment on here but I guess I didn't: when I tested this with dev deploy, I didn't see any plots show up, got JS console errors. So I'm not sure this was quite ready to be merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394:147,error,errors,147,https://hail.is,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394,1,['error'],['errors']
Availability,"Ok, so now there's a generic catch all error on `Expression`, and I override it on `StructExpression` to make sure that one works. `CollectionExpression` and `StringExpression` also get overrides to give appropriate examples.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9164#issuecomment-665678053:39,error,error,39,https://hail.is,https://github.com/hail-is/hail/pull/9164#issuecomment-665678053,1,['error'],['error']
Availability,"Ok. I was trying to hide the kubernetes error message from the users because I thought it might be confusing. But if you feel that's ok, then I'll make it just report the original error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066,2,['error'],['error']
Availability,"Ok. This exact scenario is what I was worried about when we merge PRs without checking the logs by hand in a full testing scenario. I want a way to check the PR driver, front-end, and worker logs automatically that they don't have ERROR messages. Like test_invariants. For example, I'm still looking at your change for time_since_last_state_change. When I had the code you wanted, there were errors because time_since_last_state_change was None. The current tests would not have caught that. I think we need either a white list of acceptable front-end/driver errors or some kind of threshold for error types. I'll think about it some more once the batch porting is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956522959:231,ERROR,ERROR,231,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956522959,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"Ok. What's going on here is like a proper pipe, we're consuming vep's standard error on a background thread and outputting it to System.err , then in the failure path, we're attempting to consume it again. This goes poorly. Fix incoming.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8146#issuecomment-590927561:79,error,error,79,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-590927561,2,"['error', 'failure']","['error', 'failure']"
Availability,"Okay, python tests in local mode now have the same number of failures as on main. I just needed to be more careful in preserving the information that determines the subregion relation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9401#issuecomment-690716921:61,failure,failures,61,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-690716921,1,['failure'],['failures']
Availability,"On Oct 21, 2017, at 1:27 PM, Sun-shan <notifications@github.com> wrote:. > hail: info: SparkUI: http://10.131.101.159:4040; > Welcome to; > __ __ <>__; > / // /__ __/ /; > / __ / _ `/ / /; > // //_,/// version 0.1-f69b497; > ; > print sc; > ; > >>> print hc >>> hc.import_vcf() Traceback (most recent call last): File """", line 1, in TypeError: import_vcf() takes at least 2 arguments (1 given) >>> hc.import_vcf('/hail/sample.vcf') [Stage 0:> (0 + 1) / 2]Traceback (most recent call last): File """", line 1, in File """", line 2, in import_vcf File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)) hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. This type of Spark exception seems to be related to the configuration option spark.cleaner.ttl. Have you set that to a value other than the default?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661:614,Error,Error,614,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661,1,['Error'],['Error']
Availability,"On an n1-highmem-8, this is the RAM in use after startup. About 200MiB lower overhead than [the last time I did this](https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790).; ```; Dec 12 15:20:34 dk-m post-hdfs-startup-script[10260]: + echo 'All done'; Dec 12 15:20:34 dk-m post-hdfs-startup-script[10260]: All done; Dec 12 15:20:42 dk-m earlyoom[6529]: mem avail: 42959 of 52223 MiB (82.26%), swap free: 0 of 0 MiB ( 0.00%); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14093#issuecomment-1852287669:251,echo,echo,251,https://hail.is,https://github.com/hail-is/hail/pull/14093#issuecomment-1852287669,2,"['avail', 'echo']","['avail', 'echo']"
Availability,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:655,error,error,655,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842,1,['error'],['error']
Availability,One other question: Is it a breaking change to change the type of error we throw?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9398#issuecomment-685798692:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/9398#issuecomment-685798692,1,['error'],['error']
Availability,"One other snarl I've hit -- I'll need to reboot the jupyter service in the init script in order to use `jgscm` as the content manager, and need to find time to test that and make sure everything continues to work afterwards.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12788#issuecomment-1479689103:41,reboot,reboot,41,https://hail.is,https://github.com/hail-is/hail/pull/12788#issuecomment-1479689103,1,['reboot'],['reboot']
Availability,"Oof, method verification error on one of the lowering tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9637#issuecomment-717239923:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/9637#issuecomment-717239923,1,['error'],['error']
Availability,"Oops, sorry, I misread where the failure was happening. You need `cseed/hail:batch-web`, not `cseed:batch-web`. I created an issue to fix this: https://github.com/hail-is/hail/issues/7074",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532274736:33,failure,failure,33,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532274736,1,['failure'],['failure']
Availability,"Oops, sorry. Although I really blame PruneSuite. It does a bunch of serious work on construction, and basically makes the tests unusable if there are any bugs and testng silently bails with a fatal error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768:198,error,error,198,https://hail.is,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768,1,['error'],['error']
Availability,"Options for nested `forAll`:. ``` scala; toProp(for (; j <- forAll(Gen.choose(0, 10000));; k <- forAll(Gen.choose(0, 10000));; ) yield {; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }).check(); ```. ``` scala; forAll(Gen.choose(0, 10000)) { (j: Int) =>; forAll(Gen.choose(0, 10000)) { (k: Int) =>; val gt = if (j < k) GTPair(j, k) else GTPair(k, j); Genotype.gtPair(Genotype.gtIndex(gt)) == gt; }; }.check(); ```. I think I can ditch the `toProp` on the do notation with an implicit conversion. I might be able to support either syntax in a unified way, but I haven't found the time to think about it. There's a little bit of weirdness because you only want `check` to be callable on things that are `Boolean`-valued. The difference between this monad and the `Gen[T]` monad is that this one is a reader monad, collecting a stack of ""read"" variables that can be used by the inner most `forAll` to generate a useful check-failure message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-244517801:974,failure,failure,974,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-244517801,1,['failure'],['failure']
Availability,Our CI had a sporadic failure here -- you can retest again by pushing an empty commit to the branch:. git commit --allow-empty -m bump && git push,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6264#issuecomment-499467470:22,failure,failure,22,https://hail.is,https://github.com/hail-is/hail/pull/6264#issuecomment-499467470,1,['failure'],['failure']
Availability,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:342,error,error,342,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754,2,"['downtime', 'error']","['downtime', 'error']"
Availability,Our error message on functions that read TSV are much clearer than they used to be. I don't think this needs to be a separate command.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/216#issuecomment-279518768:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/216#issuecomment-279518768,2,['error'],['error']
Availability,"P; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradj",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14765,toler,tolerationSeconds,14765,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['toler'],['tolerationSeconds']
Availability,PR #1437 resolves the `is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM` failure under Spark 2.1.0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-282776933:80,failure,failure,80,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-282776933,1,['failure'],['failure']
Availability,"PartitionedParquetRelation currently does not compile on spark 1.6.0. If compiled with spark 1.5.0 and run on 1.6.0, cryptic errors appear from changed method signatures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/667#issuecomment-242106481:125,error,errors,125,https://hail.is,https://github.com/hail-is/hail/issues/667#issuecomment-242106481,1,['error'],['errors']
Availability,"Passing CI tests for Spark 2.4. Do you have a stack trace for a failure?. Sriram saw issues related to Breeze in #9199, but I think it was the bug you noted above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701358022:64,failure,failure,64,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701358022,1,['failure'],['failure']
Availability,"Pausing for a few hours. Error in testEmitLeftJoinDistinct is a bit strange. IR contains only constants, and appears correctly inferred as having required elements, but a missing element is found at emit time. <img width=""2270"" alt=""Screenshot 2020-02-21 20 38 52"" src=""https://user-images.githubusercontent.com/5543229/75083876-2f3f9d00-54ea-11ea-9bb8-45f9e708a50b.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8142#issuecomment-589902885:25,Error,Error,25,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-589902885,1,['Error'],['Error']
Availability,Ping @chrisvittal. Could you pass this off or review?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263#issuecomment-500844860:0,Ping,Ping,0,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-500844860,1,['Ping'],['Ping']
Availability,"Pipeline failure, needs a bump :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414,1,['failure'],['failure']
Availability,"Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef202da1 Pod spec.containers{main} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef7b3cef Pod spec.containers{cleanup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 7s 7s 1 batch-12728-job-287-742170.15c2403cf8c02f7f Pod spec.containers{cleanup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a5a03fb Pod spec.containers{cleanup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a914c20 Pod spec.containers{keep-alive} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 4s 4s 1 batch-12728-job-287-742170.15c2403d8c542f61 Pod spec.containers{keep-alive} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 4s 4s 1 batch-12728-job-287-742170.15c2403da37c4943 Pod spec.containers{keep-alive} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:2853,alive,alive,2853,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368,3,['alive'],['alive']
Availability,"Popping in here with some context/suggestion but please take it or leave it! Very very few jobs in practice are AlwaysRun, and this ticket originated because of confusion that a cancelled batch still had `Ready` jobs. In a way you can consider AlwaysRun a modifier that affects the job's state machine. Maybe for the page with the jobs table, instead of just exposing the data we could tackle this particular confusion by making the state column for AlwaysRun jobs that are not in completed states (success / failed / error) like `Ready (always run)`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14425#issuecomment-2030160823:518,error,error,518,https://hail.is,https://github.com/hail-is/hail/pull/14425#issuecomment-2030160823,1,['error'],['error']
Availability,"Problem: Currently in the LD Matrix case I compute V, multiply it through the genotype matrix to get U, then subset columns of U. U is probably bigger than V though, so this could limit the number of eigenvectors that can be used. Probably should just subset columns of V to the desired number of eigenvectors for performance and to increase maximum number of eigenvectors available.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-316386605:373,avail,available,373,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-316386605,1,['avail'],['available']
Availability,Proposal: subset behavior should default to downcoding behavior if PLs are missing. Downcode should be fixed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1283#issuecomment-274907004:44,down,downcoding,44,https://hail.is,https://github.com/hail-is/hail/issues/1283#issuecomment-274907004,2,"['Down', 'down']","['Downcode', 'downcoding']"
Availability,"Proposed approach: annotate variants with Map[Int, Set[String]], which maps the error code to the set of trios (child string) with error of that code at that variant. We should still annotate samples with counts that are harder to pull out of this, like count per nuclear family and for the individual overall.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/597#issuecomment-240548537:80,error,error,80,https://hail.is,https://github.com/hail-is/hail/issues/597#issuecomment-240548537,2,['error'],['error']
Availability,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:109,toler,tolerations,109,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109,3,['toler'],"['tolerate', 'toleration', 'tolerations']"
Availability,"Pushed one more fix: a batch test failed on job.wait() where /status threw 500. It was a running job, so batch hit the worker. The job was error, so it threw an exception and the container was being deleted. There was a race condition getting the container status:. ```; if self.container:; ... self.get_container_status() ...; ```. and deleting the container:. ```; if self.container:; ... call self.container.delete(); self.container = None; ```. If the delete happens between the check for self.container being defined and the call to self.container.show inside get_container_status, show throws 404. Thus, I modified get_container_status to return None on 404.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491:139,error,error,139,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491,1,['error'],['error']
Availability,"Pushed two more changes:; - Fixed bug in construction of volumes/volumeMounts in pod spec; - Increased resource limits on some build steps. Note, my change from yesterday set requests = limits (since that's what batch2 currently does), and some steps were requesting less memory than they required. If they landed on low-memory nodes, they would have failed. I will continue to increase as needed based on trial and error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7324#issuecomment-543208801:416,error,error,416,https://hail.is,https://github.com/hail-is/hail/pull/7324#issuecomment-543208801,1,['error'],['error']
Availability,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:169,down,downstream,169,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085,1,['down'],['downstream']
Availability,"Putting you down John, but feel free to reassign.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10820#issuecomment-1074478128:12,down,down,12,https://hail.is,https://github.com/hail-is/hail/pull/10820#issuecomment-1074478128,1,['down'],['down']
Availability,"Rate limiting is a bit too restrictive methinks. That said, there are a lot of deadlocks when the rate limits kick in. Perhaps worth diving more deeply into the deadlocks at some point. The critical path service backend test is #35. Here are its slowest ones. I think we should probably checkpoint the import VCF, but, regardless, pc_relate just needs to be made faster. I know how to double the speed of PCA. It's in a dead branch of mine. Patrick will incorporate those ideas into his rewrite using the new math. If #35 was the same speed as the next slowest, we'd save 3 minutes. I think we can save ~7 minutes by cutting all these slow tests down so that the distribution of runtimes is more uniform. ```; 256.68s call hail/methods/relatedness/test_pc_relate.py::test_pc_relate_against_R_truth; 178.28s call hail/methods/test_pca.py::test_spectra_2[triplet0]; 102.60s call hail/vds/test_vds.py::test_truncate_reference_blocks; 82.86s call hail/backend/test_service_backend.py::test_tiny_driver_has_tiny_memory; ```. f1ac37dbeb3625cbf91f1f9df5399f3723843029 (40 minutes) (https://ci.hail.is/batches/7484187):. <img width=""2032"" alt=""Screen Shot 2023-05-25 at 12 01 55"" src=""https://github.com/hail-is/hail/assets/106194/902a0624-46a0-4beb-ae03-6c419350ca41"">; <img width=""542"" alt=""Screen Shot 2023-05-25 at 12 01 28"" src=""https://github.com/hail-is/hail/assets/106194/3cfa366d-5719-428a-9f4f-5bd07caaf6ca"">; <img width=""521"" alt=""Screen Shot 2023-05-25 at 12 01 07"" src=""https://github.com/hail-is/hail/assets/106194/da778828-9f9e-46cc-b574-68b9835e6589"">; <img width=""522"" alt=""Screen Shot 2023-05-25 at 12 01 03"" src=""https://github.com/hail-is/hail/assets/106194/3074a6f9-06d5-487e-941c-995b47177181"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1563163790:287,checkpoint,checkpoint,287,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1563163790,2,"['checkpoint', 'down']","['checkpoint', 'down']"
Availability,"Re testing, I think the primary error here isn't that the TypeInfo of i2b disagreed with the jvm, it's that `CodeInt.toB` returns a `Code[Byte]`, using an lir instruction with (wrong) TypeInfo Boolean. That should be much easier to catch with some basic typechecking. (Though agreed the fix should go in first.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11328#issuecomment-1032604404:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/11328#issuecomment-1032604404,1,['error'],['error']
Availability,"Re. the questions about the PCA step, I think you'll be beter off modifying `_hwe_normalized_blanczos`. For one thing, this ensures that PC-AiR always returns results in the same form as normal PCA. More importantly, `_hwe_normalized_blanczos` performs the SVD using a ""tall-skinny matrix"" representation, which is just a table of matrices (2d ndarrays). This is more efficient than using block matrices for several reasons that aren't directly relevant here. The result of the SVD is computed as local numpy ndarrays. Given these forms of the data, projecting the related sampled onto the computed PCs should be straightforward and efficient. But once everything is converted to tables and matrixtables, it's much harder and does a lot of redundant work. Let me know if you want to schedule a time to walk through the PCA internals and where you can plug in to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230:740,redundant,redundant,740,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230,1,['redundant'],['redundant']
Availability,"Re: test failure, I'll rebase once the cloud script is in 0.1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2160#issuecomment-325781229:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/2160#issuecomment-325781229,1,['failure'],['failure']
Availability,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:55,avail,available,55,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579,1,['avail'],['available']
Availability,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551:164,error,error,164,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551,4,"['avail', 'error']","['available', 'error']"
Availability,"Related: if you host the pdf within docs, we could keep users within the docs url, while also giving them the use of their native PDF viewer (which will allow download). I think that's neater. Actually, this should be possible with the raw link, not sure why my browser prompts me to immediately download. edit: Ah, this is why: https://webapps.stackexchange.com/questions/48061/can-i-trick-github-into-displaying-the-pdf-in-the-browser-instead-of-downloading. Github places a header to prevent this (content-disposition: attachment). We should just host this file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309:159,down,download,159,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309,3,['down'],"['download', 'downloading']"
Availability,"Remarkable. `hailtop` doesn't even import pandas and yet it triggers this error. I can run pylint on `hail` and it produces many errors, but does not crash.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9819#issuecomment-744582907:74,error,error,74,https://hail.is,https://github.com/hail-is/hail/pull/9819#issuecomment-744582907,2,['error'],"['error', 'errors']"
Availability,"Removing a plural, for example:. movies = movies.explode('genres', name='genre'). I feel like this is natural and will be pretty common so it should be easy. Yes, I agree it is redundant. The alternative is:. movies = movies.explode('genres').rename({'genres': 'genre'}). which duplicates the column being exploded. If we're happy with the latter, I'm happy to close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2985#issuecomment-368348981:177,redundant,redundant,177,https://hail.is,https://github.com/hail-is/hail/pull/2985#issuecomment-368348981,1,['redundant'],['redundant']
Availability,"Reopening, every error will produce a stack trace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190590:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190590,1,['error'],['error']
Availability,Requiredness suite test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10763#issuecomment-895262768:24,failure,failures,24,https://hail.is,https://github.com/hail-is/hail/pull/10763#issuecomment-895262768,1,['failure'],['failures']
Availability,"Rereading the tutorial example reminds me that `{j.counts}` gives me the basename of the resource group. So the following would work:. ```python; j = b.new_job(…). j.declare_resource_group(counts={; 'tsv.gz': '{root}.counts.tsv.gz',; 'tsv.gz.tbi': '{root}.counts.tsv.gz.tbi',; }). j.command(f""""""; gatk SubCommand … --output {j.counts}.counts.tsv; bgzip {j.counts}.counts.tsv; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts, output_base_path); ```. This is perhaps a better use of the `ResourceGroup` as reflecting that I want to delocalise this entire group (of two files). This version is worse in that the command string now contains duplicated hardcoded appearances of the `.counts.tsv` extension in the various commands. This is an invitation to typos that will only be detected as file-not-found errors when the job runs, rather than earlier as invalid-key-for-`j.counts` errors at batch submission time. I may convert the code to use this two-file group. However there is still a benefit in the three-file group version as it avoids this repetition of filenames, so it would still be good in general to address the bug mentioned in the last paragraph of the initial comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599679192:836,error,errors,836,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599679192,2,['error'],['errors']
Availability,Returning null from the JSON parsing was throwing an error a few lines below when we try to parse a variant. This will throw the real error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4465#issuecomment-425114995:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/pull/4465#issuecomment-425114995,2,['error'],['error']
Availability,Right now it will return either a double if it finds one or NA if it doesn't. But it kind of seems like if I'm going to impose a restriction like the one above maybe it should throw an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-305807984:185,error,error,185,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-305807984,1,['error'],['error']
Availability,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:78,error,errors,78,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690,1,['error'],['errors']
Availability,"Right, we had to manually re-deploy CI for our setup as well. Pinging @daniel-goldstein just in case :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318:62,Ping,Pinging,62,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318,1,['Ping'],['Pinging']
Availability,"Ryan Koesterer is reporting a related issue: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/subject/export.20table.20failure. The former causes inexplicable container failures, the latter succeeds.; ```; mt = mt.annotate_cols(phenoFemale =; hl.cond(hl.literal({""female"",""Female"",""f"",""F"",""2""}).contains(mt.pheno[args.sex_col]), True, False)); ```; ```; mt = mt.annotate_cols(; phenoFemale = hl.cond(~ hl.is_missing(mt.pheno[args.sex_col]),; (mt.pheno[args.sex_col] == 'female') |; (mt.pheno[args.sex_col] == 'Female') |; (mt.pheno[args.sex_col] == 'f') |; (mt.pheno[args.sex_col] == 'F') |; (mt.pheno[args.sex_col] == '2'), False)); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4522#issuecomment-429349677:190,failure,failures,190,https://hail.is,https://github.com/hail-is/hail/issues/4522#issuecomment-429349677,1,['failure'],['failures']
Availability,"S_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.196:53938) with ID 21; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21, partition 39, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21, partition 9, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21, partition 19, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q03.scc.bu.edu:36955 with 21.2 GB RAM, BlockManagerId(12, scc-q03.scc.bu.edu, 36955, None); 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21, partition 29, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:169355,ERROR,ERROR,169355,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,Same failure on #7742,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7742#issuecomment-566308517:5,failure,failure,5,https://hail.is,https://github.com/hail-is/hail/pull/7742#issuecomment-566308517,1,['failure'],['failure']
Availability,"Search `.zipWithIndex()` and you'll see five places we used Spark's zipWithIndex, which triggers a job. When partitionCounts are available I think we could avoid that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2547#issuecomment-350560408:129,avail,available,129,https://hail.is,https://github.com/hail-is/hail/pull/2547#issuecomment-350560408,1,['avail'],['available']
Availability,"See for example https://cloudlogging.app.goo.gl/ziaRD9HKxxca8Nd3A. in which ~15 MJCs have to retry because of `ServerDisconnectedError` or `TimeoutError`. With this PR, I think we would have seen just the three ""two errors observed"" warning messages. Here's a possible extension to this PR that fuses the thinking of both PRs (this one and #12505): use the total delay instead of `errors = 2`. We retry really quickly, so two errors could occur in ~500ms which really isn't enough time for batch driver to fix itself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106:216,error,errors,216,https://hail.is,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106,3,['error'],['errors']
Availability,Seeing 500 errors on create (maybe latest not deployed to your namespace),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649:11,error,errors,11,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649,1,['error'],['errors']
Availability,"Seeing sporadic failures, random each run of ci, looking into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9675#issuecomment-730488453:16,failure,failures,16,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-730488453,1,['failure'],['failures']
Availability,Seems like you have one syntax error still.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11444#issuecomment-1067201429:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/pull/11444#issuecomment-1067201429,1,['error'],['error']
Availability,Seems the `testImplementation` doesn't configure the classpath for tests correctly - javatests are currently failing with `Error: Could not find or load main class org.testng.TestNG`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249:123,Error,Error,123,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708671249,1,['Error'],['Error']
Availability,"Shoot, this was branched from the resource-link branch, which hasn't merged yet due to batch failures. Will reissue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7289#issuecomment-541355452:93,failure,failures,93,https://hail.is,https://github.com/hail-is/hail/pull/7289#issuecomment-541355452,1,['failure'],['failures']
Availability,Should we not just crash when someone does this? It feels like this ought to be a python enforced type error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11168#issuecomment-996141311:103,error,error,103,https://hail.is,https://github.com/hail-is/hail/pull/11168#issuecomment-996141311,1,['error'],['error']
Availability,"Simple replication:; ```. In [6]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834356993:176,echo,echo,176,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834356993,1,['echo'],['echo']
Availability,"Since RegionPool cleans its memory via PhantomReferences now, it is not AutoCloseable. In fact, I'm not sure how we avoided double free errors in the past. I made all the tests not use `using`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446:136,error,errors,136,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446,1,['error'],['errors']
Availability,"Since netcdf broke my R installation, I upgraded R. Now to revert to 3.3.1, I'm trying to install from the downloadable tarball and running into a bunch of errors. Is this worth it? Why don't we just test against a static results file?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235:107,down,downloadable,107,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235,2,"['down', 'error']","['downloadable', 'errors']"
Availability,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:23,down,down,23,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674,1,['down'],['down']
Availability,"So I'm moderately unhappy with this for various reasons, and almost didn't PR it. We don't generally respect IR identity (e.g. optimize copies everything), so I'm not sure how useful memoizing partition counts on the IR actually is. We could try to carry this information forward inside copy. In that cases that (MatrixIR child) has the same partition counts as child, we can actually push the computed partition counts down into child. But all of this is getting pretty complicated ... just to optimize count? I wonder if it is worth it. Yes, Count and PartitionCounts have different requirements. I think we either need both, or we need to recognize in (Sum (PartitionCounts child)) that child doesn't need to preserve order. (An analysis pass that determines which IR need to preserve order in general will be better than determining this syntactically from context with a rule like (Count (Unkey child)). How do you optimize (Count (Filter (Unkey ...))?). Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3891#issuecomment-402858809:420,down,down,420,https://hail.is,https://github.com/hail-is/hail/pull/3891#issuecomment-402858809,1,['down'],['down']
Availability,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:3,error,error,3,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584,3,['error'],['error']
Availability,"So the issue is that we used to have `filter` and `explode` inside of aggregations (like `counter` and `collect_as_set`). Now they're placed outside of these operations. There was [a forum post](https://discuss.hail.is/t/breaking-change-redesign-of-aggregator-interface/701) announcing this breaking change. The above to examples should instead be written as:. ```; cut_dict = {'pop': hl.agg.filter(hl.is_defined(mt.meta.pop), hl.agg.counter(mt.meta.pop)),; 'subpop': hl.agg.filter(hl.is_defined(mt.meta.subpop) & hl.is_defined(mt.meta.pop),; hl.agg.collect_as_set(hl.struct(subpop=mt.meta.subpop, pop=mt.meta.pop))); }; ```. The fix for this issue is to change the assertion into an `if` with a `raise` of an error message, probably one that references that discuss post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982:710,error,error,710,https://hail.is,https://github.com/hail-is/hail/issues/4770#issuecomment-452847982,1,['error'],['error']
Availability,"So the reason for that error is this rule is insufficient:. ```scala; case x if x.typ == TVoid =>; x.children.foreach(c => infer(c.asInstanceOf[IR])); PVoid; ```. We need to update the environment as well, in some cases, such as ArrayFor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828,1,['error'],['error']
Availability,"So there appear to be to distinct issues:. - failure to push the initial data to a repository due to a 404 https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/414f3f183bd5f2ec04e1c732522cbc0b8b1fca31/job-log. - 404s due to someone else already bound to port 5000 (maybe `batch`?) https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html; ```; Traceback (most recent call last):; File ""ci/ci.py"", line 372, in <module>; app.run(host='0.0.0.0', threaded=False); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/flask/app.py"", line 943, in run; run_simple(host, port, self, **options); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 814, in run_simple; inner(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 774, in inner; fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 666, in make_server; passthrough_errors, ssl_context, fd=fd); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/site-packages/werkzeug/serving.py"", line 577, in __init__; self.address_family), handler); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 449, in __init__; self.server_bind(); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/http/server.py"", line 137, in server_bind; socketserver.TCPServer.server_bind(self); File ""/home/hail/.conda/envs/hail-ci/lib/python3.7/socketserver.py"", line 463, in server_bind; self.socket.bind(self.server_address); OSError: [Errno 98] Address already in use; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429133941:45,failure,failure,45,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429133941,1,['failure'],['failure']
Availability,"So there's a double regex substitution now in this version. I couldn't figure out how to avoid this without having nice error checking at the exact line there's a problem. For example, `j.command(f'{b}')` right now immediately errors with a nice error message. But if the error checking doesn't come until the massive parallel `_compile` in `Backend.run`, then it will be harder to tell where the error is. I thought about having a `debug_mode` which is on by default that does the double check while the `debug_mode` being off is more efficient.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694,5,['error'],"['error', 'errors']"
Availability,"So this is partially my fault. I'm using parse_known_args, so we don't know where unknown args are placed, and therefore can't give the right usage (even if I can intervene on the usage being printed by argparse). I really only want to allow unknown args in the relevant subcommands, but don't know how to write that argument given the strange behavior I'm seeing with `nargs='*'` and `nargs=argparse.REMAINDER`. . Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9836#issuecomment-747653966:24,fault,fault,24,https://hail.is,https://github.com/hail-is/hail/pull/9836#issuecomment-747653966,1,['fault'],['fault']
Availability,"So what I did to address the comments:; 1. I encoded the task names and the offsets of the data into the response from the worker to the front end and got rid of the MultiPart Reader/Writer.; 2. I reorganized the front end code so it's hopefully clearer what's going on.; 3. I got rid of the periodically_call changes and just did what I wanted directly in the `measure` function in the Resource Manager. Now it should be guaranteed that we do not call the measure function with the write more than once every 5 seconds. I do not want to retry calling this function at all on any error including transient errors. Otherwise, I can't figure out how the set of changes in this PR would use up all the disk space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11750#issuecomment-1137707785:580,error,error,580,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1137707785,4,['error'],"['error', 'errors']"
Availability,"So, there was a test run that got cancelled (probably main branch changed) but which passed the service backend tests. It confirms that this most recent run ran all the tests, but it has some fishy looking error outputs:; ```; [gw2] PASSED [2023-04-21 18:31:07] test/hail/table/test_table.py::Tests::test_from_pandas_missing_and_nans Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; Exception ignored in: <_io.FileIO name=0 mode='rb' closefd=True>; ResourceWarning: unclosed file <_io.TextIOWrapper name=0 mode='r' encoding='UTF-8'>; ```; Are we not cleaning up files somewhere and that's somehow hanging the system?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12731#issuecomment-1518364066:206,error,error,206,https://hail.is,https://github.com/hail-is/hail/pull/12731#issuecomment-1518364066,1,['error'],['error']
Availability,Some Spark tests timed out (e.g. in https://ci.hail.is/batches/7644244/jobs/74 it was `test_spectral_moments_4`) which often crashes the JVM leading to the other errors. Retry and post links into Query Dev to alert them that QoS can timeout on spectral moments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1638682229:162,error,errors,162,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1638682229,1,['error'],['errors']
Availability,"Some documentation-related failures. Also a ""Cloud Test"" failure, although I don't see an error in the log. Am I ok to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147:27,failure,failures,27,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474150147,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,Some kind of batch service account error on this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7729#issuecomment-565757263:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/7729#issuecomment-565757263,1,['error'],['error']
Availability,"Some of the array tests (min, max, etc.) are failing because they rely on the `If` to protect from calling `ArrayRef` on a zero-length array. :-| I'm not sure if you can mitigate this by just not folding along nodes that might reasonably error, like `ArrayRef`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4320#issuecomment-420750223:238,error,error,238,https://hail.is,https://github.com/hail-is/hail/pull/4320#issuecomment-420750223,1,['error'],['error']
Availability,"Some parser failures:. for example:; `Rule 'type' didn't match at 'PCStruct{id:PCString' (line 1, column 1).`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7868#issuecomment-574469050:12,failure,failures,12,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574469050,1,['failure'],['failures']
Availability,"Some progress and new blocker on this topic. I moved to emr-6.11.1 that come with spark 3.3.2 & scala 2.12.15.; I upgraded the environment to get python 3.9 and java 11. * `emr-6.11.1`; * Java: java -version `11.0.20` (/usr/bin/java); * Python: python --version `3.9.18` (/usr/bin/python3); * Hadoop: hadoop version `3.3.3` (/usr/bin/hadoop); * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.15` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. But then once I build hail on this environment, the spark version is downgraded to 2.12.13 and the Java error above come back. ```sh; cd /tmp; git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```. * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.13` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. If I purposly build Hail for scala 2.12.13, the Java error above come back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000:742,down,downgraded,742,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000,3,"['down', 'error']","['downgraded', 'error']"
Availability,"Some strangeness going on with build... ; ```. ------------------------------------------------------------; Gradle 6.8.3; ------------------------------------------------------------. Build time: 2021-02-22 16:13:28 UTC; Revision: 9e26b4a9ebb910eaa1b8da8ff8575e514bc61c78. Kotlin: 1.4.20; Groovy: 2.5.12; Ant: Apache Ant(TM) version 1.10.9 compiled on September 27 2020; JVM: 1.8.0_362 (Private Build 25.362-b09); OS: Linux 5.4.0-1042-gcp amd64. real	0m3.621s; user	0m4.448s; sys	0m0.623s; + retry make jars wheel HAIL_DEBUG_MODE=1; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.0"" which is different from old value """"; printf ""3.3.0"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=' >> src/main/resources/build-info.properties; echo 'revision=e1d86e1908f0911d45b03ef08a694d07e1c4627b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-03-09T23:23:56Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.0' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.110' >> src/main/resources/build-info.properties; HAIL_DEBUG_MODE is set to ""1"" which is different from old value """"; printf ""1"" > env/HAIL_DEBUG_MODE; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; javac -d build/classes/scala/debug -Xlint:all -Werror -XDenableSunApiLintControl -XDignore.symbol.file src/debug/scala/is/hail/annotations/Memory.java; ./gradlew shadowJar -Dscala.version=2.12.13 -Dspark.version=3.3.0 -Delasticsearch.major-version=7; Starting a Gradle Daemon (subsequent builds will be faster); > Task :compileJava NO-SOURCE; > Task :compileScala; [Error] /io/repo/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala:122: value of is not a member of object java.nio.file.Path; [Error] /io/repo/hail/src/main/scala/is/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450:730,echo,echo,730,https://hail.is,https://github.com/hail-is/hail/pull/12773#issuecomment-1463003450,4,['echo'],['echo']
Availability,Something must actually be wrong... Shouldn't get the same error when just adding test files to a directory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5956#issuecomment-487081808:59,error,error,59,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487081808,1,['error'],['error']
Availability,"Sorry @lgruen these errors were transient issues mostly from our click/dependencies breaking style, but nothing to do with the PR. For CI builds, unfortunately your hail account must be a developer account to see the page. Dan would know whether or not we can do that for your account.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11698#issuecomment-1088742898:20,error,errors,20,https://hail.is,https://github.com/hail-is/hail/pull/11698#issuecomment-1088742898,1,['error'],['errors']
Availability,"Sorry I missed your message! The code as written now is plainly wrong: we access a mutable map from two threads without synchronization. We need this change regardless of how it affects error messages. If the tests pass, I'm confident this is fine. Are there components of the system you don't think are well tested by our tests?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471:186,error,error,186,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471,1,['error'],['error']
Availability,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:293,avail,available,293,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,1,['avail'],['available']
Availability,"Sorry for cutoff review line, I remarked on the test failure in a comment. You also need to rebase.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2546#issuecomment-350494085:53,failure,failure,53,https://hail.is,https://github.com/hail-is/hail/pull/2546#issuecomment-350494085,1,['failure'],['failure']
Availability,"Sorry for taking so long with this. There are some changes I think will be needed for this to be production ready, but at this point it's probably best to get it merged and iterate from there. @pwc2 It looks like the tolerance might need to be relaxed in test_pc_relate. And there are some python linter errors. Once those are fixed, I'm ready to approve. Thanks again for doing the work on this, and thanks for your patience!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10873#issuecomment-1146259813:217,toler,tolerance,217,https://hail.is,https://github.com/hail-is/hail/pull/10873#issuecomment-1146259813,2,"['error', 'toler']","['errors', 'tolerance']"
Availability,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:129,error,error-prone,129,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619,1,['error'],['error-prone']
Availability,Sorry this was a lot more broken than I thought. I didn't remember everything I stripped down for the previous PRs and didn't add back in. The commit update and update-fast endpoints need to return the `start_job_id`. Hopefully that's the last of these issues,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276:89,down,down,89,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276,1,['down'],['down']
Availability,"Sorry to bring those troubles, is there anything I should do to locate the error?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321265505:75,error,error,75,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321265505,1,['error'],['error']
Availability,"Sorry! I didn't see this because of the review. The root problem is that `raise_for_status` ignores the response body. This is a [known issue in aiohttp](https://github.com/aio-libs/aiohttp/issues/4600). It will be fixed in 4.0.0, but development on that seems slow. There's a variety of solutions to this problem. Every solution avoids aiohttp's raise_for_status and replaces it with something that includes the response body in the error message. A thorough fix to this is to finish the work I started in `httpx.py`. Instead of returning an `aiohttp.ClientSession` we could return a shim class that wraps `aiohttp.ClientSession` and checks the status code itself and raises an error *with the response body*. A smaller fix that only addresses aiogoogle would be to modify `aiogoogle.auth.Session` to:; 1. Not pass `raise_for_status` on to `aiohttp.ClientSession`.; 2. Store raise_for_status as a field on `aiogoogle.auth.Session`.; 2. In `aiogoogle.auth.Session.request`, if `self.raise_for_status` is true and the response status is greater than or equal to 400, retrieve the response body and raise an exception (maybe `HailHTTPError`) that includes the body.; 3. Ensure `is_transient_error` properly handles whatever exception we raise.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289:434,error,error,434,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289,2,['error'],['error']
Availability,"Sorry, didn't see this earlier -- . this is intentional. We do this so that we don't require users to recompile the C libraries when the install the Python library. Our native library distribution story is definitely a work in progress and this will change (hopefully improve) in the future. Feel free to ping us if this answer isn't sufficient!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152:305,ping,ping,305,https://hail.is,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152,1,['ping'],['ping']
Availability,"Sorry, was actually a docs failure. I'll see if I can figure out what it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287901000:27,failure,failure,27,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287901000,1,['failure'],['failure']
Availability,Sorry. I need to get my head back into this again and I need to do the billing fixes from the redundant resource prices first. Unassigning you for now until it's ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1438856316:94,redundant,redundant,94,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1438856316,1,['redundant'],['redundant']
Availability,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:161,failure,failure,161,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,2,['failure'],['failure']
Availability,"Speaking of not understanding what keys mean, I found what looks to me like a bug, but I'm not sure. `OrderedRVD.downcastToPK` creates an `OrderedRVD` for which `typ.kType` is different from `partitioner.kType`. It's triggering the assert I made in `RepartitionedOrderedRDD2` that says the new key must be a prefix of the old, to ensure that no sorting needs to be done. I want to make join keys parameters of `OrderedRVD.join`, allowing them to be different from the partitioner keys. I was putting that off for a later PR, but now I think I might need to do that to fix this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3159#issuecomment-373773742:113,down,downcastToPK,113,https://hail.is,https://github.com/hail-is/hail/pull/3159#issuecomment-373773742,1,['down'],['downcastToPK']
Availability,Stack overflow error inside of NormalizeNames.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7638#issuecomment-560197965:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-560197965,1,['error'],['error']
Availability,"Stepping back a little bit, there might be a reasonable (if unsatisfying) middle ground. Presumably the operations most at risk are long streams that we always do in chunks anyway, and in that case we can create new downloaders on `AzureReadableStream.read` if the SAS token expires. That would probably solve most of these problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930492732:216,down,downloaders,216,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930492732,1,['down'],['downloaders']
Availability,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:314,error,error,314,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630,1,['error'],['error']
Availability,"Still don't know what's happening with this. I met with Zan a couple weeks ago and gave them some ideas to try to narrow down to a smaller example that reproduces the issue, but I haven't heard from them since.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13689#issuecomment-1759998425:121,down,down,121,https://hail.is,https://github.com/hail-is/hail/issues/13689#issuecomment-1759998425,1,['down'],['down']
Availability,"Still need to add row, col, and table tests. Have pretty robust entry tests, so those should go quick tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993:57,robust,robust,57,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993,1,['robust'],['robust']
Availability,"Still needs a black reformat. You can run `make -C batch/ check` ahead of time to catch these errors or add pre-commit hooks. ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:28:49.199357 +0000; +++ batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:31:14.836500 +0000; @@ -78,15 +78,17 @@; 'automaticRestart': False,; 'onHostMaintenance': 'TERMINATE',; }; ; if preemptible:; - result.update({; - 'provisioningModel': 'SPOT',; - 'instanceTerminationAction': 'DELETE',; - 'preemptible': True,; - }); + result.update(; + {; + 'provisioningModel': 'SPOT',; + 'instanceTerminationAction': 'DELETE',; + 'preemptible': True,; + }; + ); ; return result; ; return {; 'name': machine_name,; would reformat batch/cloud/gcp/driver/create_instance.py; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521:94,error,errors,94,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521,1,['error'],['errors']
Availability,"Still seeing this error in the deploy_batch job:; ```python; utils.py	retry_long_running:923	in delete_prev_cancelled_job_group_cancellable_resources_records	; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; return await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1485, in delete_prev_cancelled_job_group_cancellable_resources_records; async for target in targets:\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 334, in execute_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1054, ""Unknown",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340,1,['error'],['error']
Availability,Still some build errors coming from tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11306#issuecomment-1034147074:17,error,errors,17,https://hail.is,https://github.com/hail-is/hail/pull/11306#issuecomment-1034147074,1,['error'],['errors']
Availability,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:782,error,error,782,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662,2,['error'],['error']
Availability,"Stray ""n"" snuck in, everything failing syntax errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1065413540:46,error,errors,46,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1065413540,1,['error'],['errors']
Availability,Suddenly getting really surprising error here about low level codegen stuff.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9146#issuecomment-663720122:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/9146#issuecomment-663720122,1,['error'],['error']
Availability,"Sure thing, I think you basically want the following in the top-level Makefile instead of docker/Makefile (note that it is not PHONY):. ```make; vep-grch37-image: hail-ubuntu-image; $(eval VEP_GRCH37_IMAGE := $(DOCKER_PREFIX)/hailgenetics/vep-85-grch37:$(TOKEN)); 	python3 ci/jinja2_render.py '{""hail_ubuntu_image"":{""image"":""'$$(cat hail-ubuntu-image)'""}}' docker/vep/Dockerfile.GRCh37 docker/vep/Dockerfile.GRCh37.out; 	./docker-build.sh docker/vep Dockerfile.GRCh37.out $(VEP_GRCH37_IMAGE); 	echo $(VEP_GRCH37_IMAGE) > $@; ```. Side note: I do also agree with Dan's stylistic suggestion of having a directory structure like `docker/hailgenetics/vep/grch37/Dockerfile`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1503376729:494,echo,echo,494,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1503376729,1,['echo'],['echo']
Availability,"Sure, did that, though I kind of want a way to verify that it worked. Watching top doesn't seem super scientific. Linear regression still only takes like 20 seconds on my laptop. Watching top I see that the cpu usage spikes to 300% for a second at the beginning of each of the benchmark iterations, then falls back down to somewhere between 100% and 110% for the duration of the 20 seconds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583451801:315,down,down,315,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583451801,1,['down'],['down']
Availability,TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2072,echo,echo,2072,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"Test failed with this msg. Rerunning. ```; WARNING: Job terminated, but output did not finish streaming.; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [41e21fb2f146423b882503e87af21bcc] failed with error:; Task not found; Submitting to cluster 'ci-test-t3ctuj9rcpmx'...; gcloud command:; gcloud dataproc jobs submit pyspark python/cluster-tests/cluster-read-vcfs-check.py \; --cluster=ci-test-t3ctuj9rcpmx \; --files= \; --py-files= \; --properties=; Traceback (most recent call last):; File ""/usr/local/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/__main__.py"", line 88, in main; submit.main(args); File ""/usr/local/lib/python3.6/dist-packages/cloudtools/submit.py"", line 81, in main; check_call(cmd); File ""/usr/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'python/cluster-tests/cluster-read-vcfs-check.py', '--cluster=ci-test-t3ctuj9rcpmx', '--files=', '--py-files=', '--properties=']' returned non-zero exit status 1. real	4m33.655s; user	0m2.473s; sys	0m0.491s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264:106,ERROR,ERROR,106,https://hail.is,https://github.com/hail-is/hail/pull/6063#issuecomment-490158264,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Test failures seem odd, they don't seem to use FilterSamples?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2564#issuecomment-351422628:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/2564#issuecomment-351422628,1,['failure'],['failures']
Availability,"Tested as follows from a clean environment.; 1. Build the jars and wheel in release mode:; ```bash; HAIL_RELEASE_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:859,echo,echo,859,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,2,['echo'],['echo']
Availability,Tests are passing again! I know there were concerns about error handling in the `vep.py` code. I tried to make it exactly the same as the existing Scala code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1522373710:58,error,error,58,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1522373710,1,['error'],['error']
Availability,Thank @jbloom22 ! Hadn't seen that this had change (was `bool` back in the days). And indeed the error message did not put me on the right track :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088:97,error,error,97,https://hail.is,https://github.com/hail-is/hail/issues/4033#issuecomment-408982088,1,['error'],['error']
Availability,"Thank you for getting back to me. I was using the solution provided by aws (https://github.com/awslabs/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/blob/master/source/GenomicsAnalysisCode/buildhail_buildspec.yml) also the main page for reference (https://docs.aws.amazon.com/solutions/latest/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/welcome.html). In order to use the latest Hail (because we have vcf format 4.3 which is not supported in Hail 0.1), we changed it to ; ```. echo 'Installing pre-reqs'; yum install -y g++ cmake git; yum install -y lz4; yum install -y lz4-devel; git clone $HAIL_REPO; cd hail/hail && git fetch && git checkout main; ./gradlew clean; make install HAIL_COMPILE_NATIVES=1; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.13 SPARK_VERSION=3.1.1; cd build; pip download decorator==4.2.1; aws s3 cp decorator-4.2.1-py2.py3-none-any.whl s3://${RESOURCES_BUCKET}/artifacts/decorator.zip; aws s3 cp distributions/hail-python.zip s3://${RESOURCES_BUCKET}/artifacts/; aws s3 cp libs/hail-all-spark.jar s3://${RESOURCES_BUCKET}/artifacts/; ``` . What else I should change in order to deploy this solution successfully? . Thank you for the help!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181:537,echo,echo,537,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181,2,"['down', 'echo']","['download', 'echo']"
Availability,"Thank you for taking a look!. > 1. I'm not opposed to adding tokens to the batches_n_jobs_in_complete_states table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. The issue is that I started with #13475 and after your insightful comment about keeping the batches and job groups tables in sync, I realized that rather than using the batch_after_update trigger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:930,Error,Error,930,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,['Error'],['Error']
Availability,"Thank you for the above suggestions, I was originally getting the :nativeLib FAILED error. But I resolved it by using the most recent gcc 7.1.0 version. However, even I stumble upon this error while compiling :compileScala step.; `[nroak@compute-0-19 hail]$ ./gradlew shadowJar; Picked up _JAVA_OPTIONS: -Xmx4g; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /mount/pcgp/resources/hail/src/main/c/libsimdpp-2.0-rc2; :compileScala; Picked up _JAVA_OPTIONS: -Xmx4g; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2544: value floorDiv is not a member of object Math; register(""//"", (x: Int, y: Int) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2545: value floorDiv is not a member of object Math; register(""//"", (x: Long, y: Long) => java.lang.Math.floorDiv(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2549: value floorMod is not a member of object Math; register(""%"", (x: Int, y: Int) => java.lang.Math.floorMod(x, y), null); ^; /mount/pcgp/resources/hail/src/main/scala/is/hail/expr/FunctionRegistry.scala:2550: value floorMod is not a member of object Math; register(""%"", (x: Long, y: Long) => java.lang.Math.floorMod(x, y), null); ^; four errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 52.396 secs; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302833404,4,"['FAILURE', 'error']","['FAILURE', 'error', 'errors']"
Availability,"Thank you for the quick response, @tpoterba . I understand, I think we are on the same page. I was mainly curious what would happen if overwrite was True. Whether it would start from scratch or ignore overwrite, or something else. That being said, a possible use case re overwrite could be to actually remove partial files re tasks being recovered. Re a single task, if it would fail, then it would be possible for there to exist a partial task file. Usually if a task fails, then Spark would retry it. And if the task's next attempt passes, then there would exist a full task file plus a partial task file. So, if Hail would recover the task, then I imagine there would also exist a full task file plus a partial task file. I have not seen partial task files affecting downstream processing yet, but I am generally wary of ""duplicate"" files. Just a thought",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215#issuecomment-822979485:338,recover,recovered,338,https://hail.is,https://github.com/hail-is/hail/pull/10215#issuecomment-822979485,3,"['down', 'recover']","['downstream', 'recover', 'recovered']"
Availability,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q ± .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:230,error,error,230,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410,8,['error'],['error']
Availability,"Thanks @tpoterba, the `gradle clean` worked nicely. If it's any use, the test failures are the following:. ```; $ ./gradlew check | grep FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.ExportPlinkSuite.testBiallelic FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > org.broadinstitute.hail.io.LoadBgenSuite.testBgenImportRandom FAILED; ```. Thanks for the help, and please feel free to close this issue whenever suits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240395647:78,failure,failures,78,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240395647,1,['failure'],['failures']
Availability,"Thanks Alex, appreciated. I know it's a big change, but it is needed for a bunch of downstream stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8581#issuecomment-617726597:84,down,downstream,84,https://hail.is,https://github.com/hail-is/hail/pull/8581#issuecomment-617726597,1,['down'],['downstream']
Availability,"Thanks a lot for reporting this error, Tim found the fix, and this detailed report was very helpful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944#issuecomment-665775696:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-665775696,1,['error'],['error']
Availability,"Thanks for clarifying these points and pointing out the errors!. <img width=""1559"" alt=""Screen Shot 2023-02-16 at 18 31 01"" src=""https://user-images.githubusercontent.com/106194/219511417-004e5529-c41c-4068-bc85-d679fca58058.png"">; <img width=""1559"" alt=""Screen Shot 2023-02-16 at 18 31 04"" src=""https://user-images.githubusercontent.com/106194/219511421-ba3600ad-b0a3-4564-862a-78f6b7501467.png"">; <img width=""1559"" alt=""Screen Shot 2023-02-16 at 18 31 09"" src=""https://user-images.githubusercontent.com/106194/219511419-cfa45641-bcf4-4e29-a5f1-bf431981c0c6.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12643#issuecomment-1433881634:56,error,errors,56,https://hail.is,https://github.com/hail-is/hail/pull/12643#issuecomment-1433881634,1,['error'],['errors']
Availability,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:174,down,download,174,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,3,"['avail', 'down']","['available', 'download']"
Availability,"Thanks for sharing the doctest failures. Those were indeed issues, and I think I have fixed them now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14250#issuecomment-1992494034:31,failure,failures,31,https://hail.is,https://github.com/hail-is/hail/pull/14250#issuecomment-1992494034,1,['failure'],['failures']
Availability,"Thanks for the detailed error report! This is a known problem, fix is in PR: #10523",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682#issuecomment-883350719:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/10682#issuecomment-883350719,1,['error'],['error']
Availability,Thanks for the ping!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106#issuecomment-793959393:15,ping,ping,15,https://hail.is,https://github.com/hail-is/hail/pull/10106#issuecomment-793959393,1,['ping'],['ping']
Availability,"Thanks for the report! This is a bug. The `export_vcf` method will currently just error universally on haploid calls, rather than checking if the haploid call is phased. https://github.com/hail-is/hail/blob/026a64d64c8a2905c6125fcd445302c2452fb37b/hail/src/main/scala/is/hail/expr/ir/MatrixWriter.scala#L1012-L1017",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14330#issuecomment-1969442933:82,error,error,82,https://hail.is,https://github.com/hail-is/hail/issues/14330#issuecomment-1969442933,1,['error'],['error']
Availability,"Thanks for the report, @JacobBayer! I don't believe anyone on our team uses Spyder unfortunately, but I don't think this is a Hail issue. According to [this thread](https://community.developers.refinitiv.com/questions/88895/spyder-515-erroreikon-data-api.html?childToView=89408#answer-89408) a Spyder upgrade might resolve the issue if you're on an old version?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309:235,error,erroreikon-data-api,235,https://hail.is,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309,1,['error'],['erroreikon-data-api']
Availability,"Thanks for the review!. The point of this code is to allow optimization across bindings. The `MaximizeLets` pass is ""let lifting"", and is the thing that would push the `ArrayLen` into the body. > Also, what's the point of pushing Lets back down again?. The MinimizeLets pass was what I used to implement single-use let forwarding in a principled way. We could also do it your way, that seems much nicer! I'll rewrite the MinimizeLets pass as `ForwardLets` and write an IR analysis function that asks the right questions. I'd like to talk about implementing a use-def chain. Should that be part of the initial PR, or would you feel OK merging this optimizer pass without that piece?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790:240,down,down,240,https://hail.is,https://github.com/hail-is/hail/pull/5041#issuecomment-453180790,1,['down'],['down']
Availability,Thanks for tracking this down Tim,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9425#issuecomment-689815711:25,down,down,25,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689815711,1,['down'],['down']
Availability,"Thanks so much Daniel!! This is awesome. I can't seem to be able to merge though, probably due to permissions?; <img width=""516"" alt=""Screen Shot 2021-04-22 at 9 58 00 am"" src=""https://user-images.githubusercontent.com/1575412/115636406-3ada5e00-a351-11eb-887d-3882271f6369.png"">. There are also conflicts, but I'm resolving them right now :). UPD: Ah, learned from Leo that the merge button will be available after the tests pass :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697:400,avail,available,400,https://hail.is,https://github.com/hail-is/hail/pull/10347#issuecomment-824439697,2,['avail'],['available']
Availability,Thanks! That helped. There a couple of other issues that came up that required some tinkering. I list them below in case any one runs into them also. 1. curl failed when trying to download the ibsimdpp lib. The workaround was to download it with wget and move it to the Make Directory. ```; wget --no-check-certificate https://storage.googleapis.com/hail-common/libsimdpp-2.0-rc2.tar.gz; mv libsimdpp-2.0-rc2.tar.gz src/main/c; ```. 2. Needed to compile with newer version of gcc; ```; module load gcc/7.2.0; ./gradlew -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar archiveZip. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411:180,down,download,180,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411,2,['down'],['download']
Availability,"Thanks! To clarify your other question, while the local backend segfault with this, I think it is caused by existing memory issues that show up in the Spark backend tests. I conjecture when you fix those, the local backend failure will go away. If not, we can flip to see who debugs it :-)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8957#issuecomment-645097424:223,failure,failure,223,https://hail.is,https://github.com/hail-is/hail/pull/8957#issuecomment-645097424,1,['failure'],['failure']
Availability,"Thanks, and sorry about that! Github's fault.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624157539:39,fault,fault,39,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624157539,1,['fault'],['fault']
Availability,"Thanks, that fixed it. I followed instructions here: https://stackoverflow.com/questions/46513639/how-to-downgrade-java-from-9-to-8-on-a-macos-eclipse-is-not-running-with-java-9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10524#issuecomment-849024403:105,down,downgrade-java-from-,105,https://hail.is,https://github.com/hail-is/hail/issues/10524#issuecomment-849024403,1,['down'],['downgrade-java-from-']
Availability,That definitely makes more sense. Thanks!. > This will hang if we have tasks that were not properly stopped. Do you see this as problematic? I figure a pod would get terminated if it took too long to shut down gracefully and it could more obviously reveal the underlying problem.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9944#issuecomment-769078938:205,down,down,205,https://hail.is,https://github.com/hail-is/hail/pull/9944#issuecomment-769078938,1,['down'],['down']
Availability,"That huge spike in ready jobs has got to be a mistake. Where do we create a 1500 partition dataset?. Service backend [split 35](https://batch.hail.is/batches/7489026/jobs/197) is again the critical path. I tried pushing a commit which checkpoints the VCF. The main problem is probably just that our PC-Relate implementation is slow.; ```; ============================= slowest 50 durations =============================; 240.09s call hail/methods/relatedness/test_pc_relate.py::test_pc_relate_against_R_truth; 168.03s call hail/methods/test_pca.py::test_spectra_2[triplet0]; 93.98s call hail/vds/test_vds.py::test_truncate_reference_blocks; 86.56s call hail/backend/test_service_backend.py::test_tiny_driver_has_tiny_memory; 83.82s call hail/methods/test_qc.py::Tests::test_vep_grch38_against_dataproc; ```. 2a3fbd185e9255ed447dd80b983709d49c7a345e (45 minutes):. <img width=""2032"" alt=""Screen Shot 2023-05-26 at 14 39 28"" src=""https://github.com/hail-is/hail/assets/106194/2f98159e-e60e-4410-af4b-7bca72fc43a8"">. <img width=""305"" alt=""Screen Shot 2023-05-26 at 14 36 58"" src=""https://github.com/hail-is/hail/assets/106194/be2c4b32-db08-484f-a6de-08bc41149e65"">. <img width=""523"" alt=""Screen Shot 2023-05-26 at 14 38 18"" src=""https://github.com/hail-is/hail/assets/106194/981665f6-fe7c-45eb-8659-836d505fa280"">; <img width=""518"" alt=""Screen Shot 2023-05-26 at 14 38 13"" src=""https://github.com/hail-is/hail/assets/106194/feaf826a-57d2-4404-9844-69048e3ade69"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1564787141:235,checkpoint,checkpoints,235,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1564787141,1,['checkpoint'],['checkpoints']
Availability,"That is a good point! I think we could work around that by passing in the CSRF token from Batch in the query parameters as well, and then having the Auth service ping the Batch API with it to verify the token is valid (or perhaps we could just make this UI a single page application instead of a bunch of pages on different subdomains that resemble one) but I think for the short term this is a good fix!. Unrelated process note: I think in order to link the issue to the PR successfully, you have to use a [verb like ""fixes"" or ""closes""](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) in the PR description, rather than ""addresses"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2269566317:162,ping,ping,162,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2269566317,1,['ping'],['ping']
Availability,That's a bit weird. You're pushing to the branch on your fork and still getting that error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248074937:85,error,error,85,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248074937,1,['error'],['error']
Availability,"That's a good point about which fatal is called. I'm OK with removing it. On Wed, Apr 6, 2016 at 10:56 AM, cseed notifications@github.com wrote:. > I argue fatalIf(p, msg) is less readable than if (p) fatal(msg) and not; > any shorter. It also causes an error since you can't control which fatal to; > call, e.g., Utils.fatal vs Line.fatal. @tpoterba; > https://github.com/tpoterba Thoughts?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/hail/issues/262",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/262#issuecomment-206416460:254,error,error,254,https://hail.is,https://github.com/hail-is/hail/issues/262#issuecomment-206416460,1,['error'],['error']
Availability,"That's all part of the same error. If you resolve the file permissions issue, then the HailContext can be successfully initialized. I'll report this confusing error message to the team.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337918138:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337918138,2,['error'],['error']
Availability,That's correct. I saw an intermittent failure in Azure on one of my PRs. That's how I found out the debug info was wrong.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11203#issuecomment-1006815541:38,failure,failure,38,https://hail.is,https://github.com/hail-is/hail/pull/11203#issuecomment-1006815541,1,['failure'],['failure']
Availability,"That's handled in the typechecker on `StringExpression.matches` -- we don't generate `RegexMatch` AST nodes anywhere else. However, there's still a problem that I haven't yet resolved -- it's possible to get parse errors if the string given doesn't parse to a valid 0.1-expr-language string literal. I think there might be additional unescaping going on somewhere in the python/java connector. Still, I think we should merge this in and figure out how to solve this problem later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2637#issuecomment-354891277:214,error,errors,214,https://hail.is,https://github.com/hail-is/hail/pull/2637#issuecomment-354891277,1,['error'],['errors']
Availability,"The ""Arguments refer to no files"" error means that the hadoop file connector may not be working properly, but since you're able to read the file with `textFile` I'm baffled (we call sc.textFile internally to import a VCF!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321255324:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321255324,1,['error'],['error']
Availability,The *really* weird thing is that I think even a write/read/collect doesn't trigger this error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-498871032:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498871032,1,['error'],['error']
Availability,The AddressAndPort pylint failure will be fixed by https://github.com/hail-is/hail/pull/9126,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9129#issuecomment-662812817:26,failure,failure,26,https://hail.is,https://github.com/hail-is/hail/pull/9129#issuecomment-662812817,1,['failure'],['failure']
Availability,"The NativePtr test runs fine as a single test, but it fails when I run all the tests. I'm trying to track; down the problem. Somehow the (anonymous) global data in NativePtr.cpp is getting corrupted.; I can make it pass tests by reinitializing that data, but I really want to figure out how it gets corrupted; because that could cause trouble elsewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3595#issuecomment-390062140:107,down,down,107,https://hail.is,https://github.com/hail-is/hail/pull/3595#issuecomment-390062140,1,['down'],['down']
Availability,"The UI 500'ed, right? I created https://github.com/hail-is/hail/issues/6587 and gave an example from this morning at 5:52. The only explanation for why the heal loop did not recover is that the batch was already completed. If the batch had already completed (whether in success or in failure), then it won't re-run it. Do you recall if the batch had completed? If a batch completes, bumping is the only way to get another batch to run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189:174,recover,recover,174,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189,2,"['failure', 'recover']","['failure', 'recover']"
Availability,"The [`flake8` check job](https://ci.hail.is/batches/536128/jobs/11) is also complaining about whitespace in struct.py on a few lines:; ```; make -C hail/python check; make[1]: Entering directory '/io/repo/hail/python'; python3 -m flake8 --config ../../setup.cfg hail; hail/utils/struct.py:120:1: W293 blank line contains whitespace; hail/utils/struct.py:122:1: W293 blank line contains whitespace; hail/utils/struct.py:124:1: W293 blank line contains whitespace; hail/utils/struct.py:126:1: W293 blank line contains whitespace; hail/utils/struct.py:129:1: W293 blank line contains whitespace; hail/utils/struct.py:131:1: W293 blank line contains whitespace; hail/utils/struct.py:134:1: W293 blank line contains whitespace; hail/utils/struct.py:136:1: W293 blank line contains whitespace; make[1]: *** [Makefile:11: check] Error 1; make[1]: Leaving directory '/io/repo/hail/python'; make: *** [Makefile:13: check-hail] Error 2; ```. I recommend enabling ""show whitespace"" in your editor. That ensure you see these issues before you make a commit. For PyCharm you can [try this](https://intellij-support.jetbrains.com/hc/en-us/community/posts/206349049-How-to-make-show-whitespace-turn-on-as-default-).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11394#issuecomment-1049169544:822,Error,Error,822,https://hail.is,https://github.com/hail-is/hail/pull/11394#issuecomment-1049169544,2,['Error'],['Error']
Availability,"The assertion failure in `mendel_errors` was due to a bug in `TableKeyBy` that was only triggered in this PR because the IR version of `Table.aggregateByKey` maintains an `OrderedRVD`, hence uses the `ordered` branch. The `key_by('s')` after the `group_by('s', 'fam').aggregate(...)` was leaving the partition key as both fields. h/t @patrick-schultz for seeing the faulty `TableKeyBy`, since `nPartitionKeys=None` means that all key fields are partition keys.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3730#issuecomment-396372654:14,failure,failure,14,https://hail.is,https://github.com/hail-is/hail/pull/3730#issuecomment-396372654,2,"['failure', 'fault']","['failure', 'faulty']"
Availability,"The call in the docs is wrong:; ```; [:makeHailDocs] Warning, treated as error:; [13:46:55]	[:makeHailDocs] WARNING: **********************************************************************; [13:46:55]	[:makeHailDocs] File ""hail.VariantDataset.rst"", line 16, in default; [13:46:55]	[:makeHailDocs] Failed example:; [13:46:55]	[:makeHailDocs] linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); [13:46:55]	[:makeHailDocs] .linreg_burden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:73,error,error,73,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,1,['error'],['error']
Availability,"The correct fix is to change this line in `lookup` from `if (tt.xs.size == typ.xs.size)` to `if (tt.xs.size == typ.xs.size && typ.getClass == tt.getClass)`. However, this will cause many test suite failures and we would have to fix 100s of lines of expr language in both the docs and the test suites. In the meantime we could change the AST code for `Select` to `lookupMethodReturnType` on failure of `lookupFieldReturnType`. . The user will not be able to trigger this error once the expression language is implemented in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1983#issuecomment-329826002:198,failure,failures,198,https://hail.is,https://github.com/hail-is/hail/issues/1983#issuecomment-329826002,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"The design of NativeModule's handling of files (which may involve several worker threads; each trying to initialize their own NativeModule referring to the same DLL) assumed that; /bin/mv would do an atomic rename to replace an old DLL with a newer version. But on; MacOS /bin/mv is non-atomic, and leaves a window between deleting the old file and ; replacing it with the new one. I'm working on details of a more robust file-synchronization scheme, once that's ok I'll; backport it here and also address the review comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-408479218:415,robust,robust,415,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-408479218,1,['robust'],['robust']
Availability,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:904,down,down,904,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,4,['down'],['down']
Availability,"The error about a log file is an unfortunate red herring, the main container appears to fail. `/vep/vep` is returning exit code -9 but providing no further debug information. We need to assess who is generating the -9. Based on the contents of `docker/hailgenetics/vep/grch38/95/Dockerfile`, `/vep/vep` appears to be; ```; #!/bin/bash; export PERL5LIB=$PERL5LIB:/vep/ensembl-vep/Plugins/; exec perl /vep/ensembl-vep/vep \""\$@\""""; ```; It seems likely that `/vep/esnembl-vep/vep` is returning exit code -9. We need to determine under what conditions that happens. Main container output:; ```; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,1,['error'],['error']
Availability,"The error in the buildkit image is my fault, I'll address and retry.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11847#issuecomment-1163501994:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/11847#issuecomment-1163501994,2,"['error', 'fault']","['error', 'fault']"
Availability,"The error is here:. ```; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/cloud/gcp/driver/resource_manager.py"", line 158, in create_vm; await self.compute_client.post(f'/zones/{location}/instances', params=params, json=vm_config); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/base_client.py"", line 30, in post; async with await self._session.post(url, **kwargs) as resp:; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 103, in request; return await request_retry_transient_errors(self._http_session, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 770, in request_retry_transient_errors; return await retry_transient_errors(session.request, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 147, in request_and_raise_for_status; body=body; hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('https://compute.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances?requestId=e2555a38-1583-47e2-ab15-c3d7ad84e700') body='{\n ""error"": {\n ""code"": 400,\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""errors"": [\n {\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (defau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728,1,['error'],['error']
Availability,The error mentioned above is cause when container runs out of Memory. Try running with higher Memory parameters.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-333867936:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-333867936,1,['error'],['error']
Availability,"The error message suggests that multiple clients are writing to:; ```; gs://rwalters-hail-tmp/merged_round2_sumstats.fix_lowconf.mt/entries/rows/parts/part-15801-2fde3786-67cb-42ed-8aac-f900cfcc4c00; ```; Which is a Hail Query partition file within a matrix table. This is happening in a flush of GoogleStorageFS after we've filled up the local byte buffer. I don't think this is 0.2.114 because the line numbers don't line up. This appears to be before our recent changes to handle requester pays. A couple thoughts:; 1. We retryTransientErrors on the entire partition. What happens if a partition fails and we automatically retry it? Does the generated code clean up all the output streams when an exception occurs? If not, it's possible that there's still an open connection to GCS when we re-run the partition's generated code gain.; 2. Hail Batch guarantees at least once execution of a job. It's possible that two distinct workers are executing the partition's generated code. Does the Hail Query generated code create a distinct file name for each execution? (I think the answer is yes.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1530268410:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1530268410,1,['error'],['error']
Availability,"The error occurred on a gzipped VCF, and went away after converting to bgzipped.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1806#issuecomment-301357111:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/1806#issuecomment-301357111,1,['error'],['error']
Availability,The error on Azure is unrelated and will post on Zulip.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144089195:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144089195,1,['error'],['error']
Availability,The error should come from logistic_regression_rows and it should complain that row_idx is row-indexed not col-indexed..,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13788#issuecomment-1755689086:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/13788#issuecomment-1755689086,1,['error'],['error']
Availability,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"The failure doesn't appear to be related to my changes. Installing the docker requirements, which has `setuptools>=38.6.0`, is trying to upgrade to the latest setuptools (56.0.0). Another dependency might be forcing the upgrade. However, setuptools was installed via apt, not pip, and that is causing this:. ```; Attempting uninstall: setuptools; Found existing installation: setuptools 45.2.0; Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr; Can't uninstall 'setuptools'. No files were found to uninstall.; ```. So there's two things I don't understand. I'll keep investigating. I glad I PRed this separately!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733,1,['failure'],['failure']
Availability,"The failure here looked intermittent, a jar failed to download for a batch. Ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11811#issuecomment-1119687684:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/11811#issuecomment-1119687684,2,"['down', 'failure']","['download', 'failure']"
Availability,"The failure here:; ```; E org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (hostname-c5956f6f02 executor driver): java.io.EOFException: Invalid seek offset: position value (6) must be between 0 and 6 for 'gs://hail-services-requester-pays/hello'; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.validatePosition(GoogleCloudStorageReadChannel.java:665); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.position(GoogleCloudStorageReadChannel.java:546); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFSInputStream.seek(GoogleHadoopFSInputStream.java:178); E 	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65); ```. I hit this same error in Avro/GVS work recently -- I think the Google Hadoop API connector is wrong in that you cannot seek to the end of a file (N where N is the number of bytes in the file).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700,4,"['error', 'failure']","['error', 'failure']"
Availability,The failure is due to the new memory requirements. Apparently the python script uses a lot more memory than I thought. Trying to find the magic number now. The tests were passing before the memory limits.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7593#issuecomment-558337832:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558337832,1,['failure'],['failure']
Availability,"The failure is in apiserver somehow, probably trivial. Can't id the problem at the moment because ci is giving 401.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6078#issuecomment-494399462:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/6078#issuecomment-494399462,1,['failure'],['failure']
Availability,"The failure was a test_query one, I kicked it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9967#issuecomment-771112056:4,failure,failure,4,https://hail.is,https://github.com/hail-is/hail/pull/9967#issuecomment-771112056,1,['failure'],['failure']
Availability,"The final failures were doctests that called show on tables with duplicated keys, where the order of the duplicate rows, which we don't guarantee, were different. I disabled those tests. Also addressed comments, should be good to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5027#issuecomment-451238849:10,failure,failures,10,https://hail.is,https://github.com/hail-is/hail/pull/5027#issuecomment-451238849,1,['failure'],['failures']
Availability,"The flags PR fixed the test failures!!! I think the last thing is being in agreement on what tests are needed. As painful as it is, I think we should spin up a dataproc cluster, run VEP and save the output into the test_resources folder and use that for the test. Before I do that, is there anything else I need to add?. The last thing for this PR is to modify the cloud run functions for ACR cleanup to cleanup the vep images generated by CI. We can make a follow up PR for Azure once we've transferred the data to the public storage source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1446698641:28,failure,failures,28,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1446698641,1,['failure'],['failures']
Availability,"The flaky test failure can be seen here: https://ci.hail.is/batches/6627486/jobs/105; ```; During handling of the above exception, another exception occurred:. @skip_unless_service_backend(); def test_tiny_driver_has_tiny_memory():; try:; hl.utils.range_table(100_000_000, 50).to_pandas(); except Exception as exc:; # Sometimes the JVM properly OOMs, sometimes it just dies.; > assert (; 'java.lang.OutOfMemoryError: Java heap space' in exc.args[0] or; 'batch.worker.jvm_entryway_protocol.EndOfStream' in exc.args[0]; ); E assert ('java.lang.OutOfMemoryError: Java heap space' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:15,failure,failure,15,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['failure'],['failure']
Availability,The improved error message isn't working because we catch TypeError instead of ExpressionException,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13280#issuecomment-1646024467:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/13280#issuecomment-1646024467,1,['error'],['error']
Availability,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:190,ERROR,ERROR,190,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"The last link has the right answer. For reasons not known to me, you must destroy the service and recreate the service to get correct behavior. You know you have correct behavior when the TCP Load Balancer in the GCP console shows most of your instances as unhealthy (because most of them are not hosting the service in question). This lead to at most 15 minutes of downtime and probably like 10 minutes, which seems unacceptable to me, but 🤷‍♀",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289:366,downtime,downtime,366,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289,1,['downtime'],['downtime']
Availability,"The movie lens dataset (used by some of the tutorials, not hosted by us) failed to download. It happens sometimes. I pushed an empty commit to have it retest. In general, if a PR looks good but the tests are failing, I approve. The robots will handle the tests, so I don't have to. If fixing a bug requires significant changes, or changes not in the spirit of the original PR, as an author, I dismiss the review and request another one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431:83,down,download,83,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-474167431,1,['down'],['download']
Availability,The one test failure was a transient blob Not Found on Azure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1372795141:13,failure,failure,13,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1372795141,1,['failure'],['failure']
Availability,"The only place outside of tests where `combine_gvcfs` is called is `drive_combiner`. And no user would have had the opportunity to use the old parameter, which was a workaround for a bug in GATK that's no longer relevant with the way the combiner output is used by downstream. Also this is still _highly experimental_ functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7247#issuecomment-540747610:265,down,downstream,265,https://hail.is,https://github.com/hail-is/hail/pull/7247#issuecomment-540747610,1,['down'],['downstream']
Availability,The only related problem I can find is a bad error when trying to drop a key field from tables.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673#issuecomment-392912868:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/issues/3673#issuecomment-392912868,1,['error'],['error']
Availability,"The original code now gives a more useful error. I'm closing as wontfix. ```; TypeError: flatten: parameter 'collection': expected expression of type set<set<any>> or array<array<any>>, found <ArrayExpression of type array<ndarray<int32, 1>>>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9128#issuecomment-1145286659:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-1145286659,1,['error'],['error']
Availability,"The original report was about `gnomad.exomes.r2.1.1.sites.liftover_grch38.vcf.bgz`. That's the gnomad v2.1.1 GRCh38 liftover sites table. See [this section of the gnomAD downloads](https://gnomad.broadinstitute.org/downloads#v2-liftover). In particular it is the ""All chromosomes VCF"". That's 85GiB, so I don't want to download it. I believe the chr21 VCF should have just as many row, column, and entry fields, so I downloaded that and tested Hail's ability to import and write it. ```bash; gsutil -m cp \; gs://gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; .; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. With Hail 0.2.108-fc03e9d5dc08 it worked fine. It also worked fine on a recent 0.2.120 development version I had installed. Next I tried running on the first few thousand lines of the full sites table:. ```bash; curl \; https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; | bgzip -d -c\; | head -n 10000 \; | bgzip -c \; > /tmp/head-sites.vcf.bgz; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('/tmp/head-sites.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. This also succeeded with Hail 0.2.108",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525:170,down,downloads,170,https://hail.is,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525,4,['down'],"['download', 'downloaded', 'downloads']"
Availability,"The output of ht.describe():; ```. ----------------------------------------; Global fields:; 'downsamplings': array<int32> ; ----------------------------------------; Row fields:; 'locus': locus<GRCh37> ; 'alleles': array<str> ; 'freq': array<struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>, ; meta: dict<str, str>; }> ; 'project_max': array<struct {; AC: array<int32>, ; AF: array<float64>, ; AN: int32, ; homozygote_count: array<int32>, ; project: str; }> ; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3668#issuecomment-392342812:94,down,downsamplings,94,https://hail.is,https://github.com/hail-is/hail/issues/3668#issuecomment-392342812,1,['down'],['downsamplings']
Availability,The plink website gave us a 503. I retried again and have a PR (https://github.com/hail-is/hail/pull/11649) which fixes our plink download to actually retry (by using curl instead of wget).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11622#issuecomment-1076523299:130,down,download,130,https://hail.is,https://github.com/hail-is/hail/pull/11622#issuecomment-1076523299,1,['down'],['download']
Availability,The previous PR has merged. Could you remove the Stacked PR label and rebase / drop any redundant commits? I thought GitHub wouldn't show the extra diff at this point but *shrug*,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11325#issuecomment-1033961699:88,redundant,redundant,88,https://hail.is,https://github.com/hail-is/hail/pull/11325#issuecomment-1033961699,1,['redundant'],['redundant']
Availability,The problem is that `gsutil` only works with python2. So you'll need a python2 binary available as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419854054:86,avail,available,86,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419854054,1,['avail'],['available']
Availability,"The problem was the eventually consistency of the object store. You could delete a large vds, make a new one at the same path with fewer partitions, and read the new one and see an error because partition 100010 was still there",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/859#issuecomment-317578194:181,error,error,181,https://hail.is,https://github.com/hail-is/hail/issues/859#issuecomment-317578194,1,['error'],['error']
Availability,"The query method is present on the generic `Type`, and overridden by `TStruct`. If you query ""info"", ""AC"", ""Test"", then you'll go to the struct implementation first, correctly identify the field ""info"", pass [""AC"", ""Test""] to that field. That field is also a struct, so you go to the struct implementation again, correctly identify ""AC"", and pass [""Test""] to that field. However, now you're querying ""Test"" on a `TArray`: this has to be an error. We catch AnnotationPathException in VSM.query. This fix is only meant to address a persistent compiler bug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972:440,error,error,440,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257303972,1,['error'],['error']
Availability,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:387,failure,failures,387,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474,3,"['error', 'failure']","['errors', 'failures']"
Availability,The repository in question was created at `2018-10-10T00:32:59Z`. GitHub indicates [no system failures](https://status.github.com/messages) on October the tenth or the ninth.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782:94,failure,failures,94,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429026782,1,['failure'],['failures']
Availability,The script in question is located at: gs://danking/1_Generate_Variant_Stats_NVXvC_v1.py . Ping me if you need access.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1791061741:90,Ping,Ping,90,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1791061741,1,['Ping'],['Ping']
Availability,"The script is running fine on the smaller chromosome 19 to 22 bgen files so far. However, I noticed each were running just 24 cores even though we have 16 nodes * 16 cores each available on the cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439073706:177,avail,available,177,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439073706,1,['avail'],['available']
Availability,"The setuptools thing was actually a red herring, that error isn't fatal, and there was a version conflict elsewhere. I upgraded urllib3 and requests to resolve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422:54,error,error,54,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422,1,['error'],['error']
Availability,The suppressed message is:; ```; 	Suppressed: java.lang.Exception: #block terminated with an error; 		at is.hail.shadedazure.reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:99); 		at is.hail.shadedazure.reactor.core.publisher.Mono.block(Mono.java:1742); 		at is.hail.shadedazure.com.azure.storage.common.implementation.StorageImplUtils.blockWithOptionalTimeout(StorageImplUtils.java:133); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getPropertiesWithResponse(BlobClientBase.java:1379); 		at is.hail.shadedazure.com.azure.storage.blob.specialized.BlobClientBase.getProperties(BlobClientBase.java:1348); 		at is.hail.io.fs.AzureStorageFS.$anonfun$openNoCompression$1(AzureStorageFS.scala:223); 		at is.hail.io.fs.AzureStorageFS.$anonfun$handlePublicAccessError$1(AzureStorageFS.scala:175); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.io.fs.AzureStorageFS.handlePublicAccessError(AzureStorageFS.scala:174); 		at is.hail.io.fs.AzureStorageFS.openNoCompression(AzureStorageFS.scala:220); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:20); 		at is.hail.io.fs.FS.openNoCompression(FS.scala:322); 		at is.hail.io.fs.FS.openNoCompression$(FS.scala:322); 		at is.hail.io.fs.RouterFS.openNoCompression(RouterFS.scala:3); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$3(ServiceBackend.scala:459); 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 		at is.hail.services.package$.retryTransientErrors(package.scala:124); 		at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:459); 		at is.hail.backend.service.Main$.main(Main.scala:15); 		at is.hail.backend.service.Main.main(Main.scala); 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/pull/13032#issuecomment-1542906430,1,['error'],['error']
Availability,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:74,error,error,74,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952,1,['error'],['error']
Availability,"The test failure here is spurious, happening because batch is going through a transition / upgrade and things are a little broken right now. Will make sure this merges when that's resolved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556,1,['failure'],['failure']
Availability,The test failures were unrelated to the proposed changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12365#issuecomment-1300509857:9,failure,failures,9,https://hail.is,https://github.com/hail-is/hail/pull/12365#issuecomment-1300509857,1,['failure'],['failures']
Availability,"The test that lists batches timed out. The main problem is the limit in the aioclient used by the test_batch tests was passing a string rather than an integer. I assumed downstream the function was passing an integer. Therefore, we were doing this:. ```; batch_id < ""137""; ```. and not `batch_id < 137`. So the query was running forever and scanning all batches from the `test` user. I also was missing a tag annotation on the queries, but that was not causing the timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13237#issuecomment-1631270046:170,down,downstream,170,https://hail.is,https://github.com/hail-is/hail/pull/13237#issuecomment-1631270046,1,['down'],['downstream']
Availability,The tests are passing. Failures are from building images.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10497#issuecomment-844462253:23,Failure,Failures,23,https://hail.is,https://github.com/hail-is/hail/pull/10497#issuecomment-844462253,1,['Failure'],['Failures']
Availability,The tests are passing. The error on Azure was cleaning up the base image timed out.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1443661758:27,error,error,27,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1443661758,1,['error'],['error']
Availability,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:222,down,down,222,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500,1,['down'],['down']
Availability,The user on the forum tried a wheel I made for them and they encountered the exact same error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10915#issuecomment-931609566:88,error,error,88,https://hail.is,https://github.com/hail-is/hail/pull/10915#issuecomment-931609566,1,['error'],['error']
Availability,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548:682,down,down,682,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548,2,['down'],['down']
Availability,"There is a [known issue](https://github.com/moby/moby/issues/41792) with the official Docker deb. If you uninstall docker and re-install it later, it might fail to start. The root cause is the `docker.socket` `systemd` unit failing to start because there are ""insufficient file descriptors available"". I think this is confusing verbiage. The socket's name must be `/var/run/docker.sock`. Clearly, if that filename is already in use, we cannot create a new socket at that filename. One of Google's [""Dataproc components""](https://cloud.google.com/dataproc/docs/concepts/components/overview) is Docker. I believe Google installed and then uninstalled docker in this image, thus leaving it in the broken state. For evidence of that:. <details>; <summary> find docker on a worker node of a *non-Hail* Dataproc cluster</summary>. ```; sudo find / -iname '*docker*'; ```. ```; /opt/conda/miniconda3/pkgs/dbus-1.13.6-h5008d03_3/info/recipe/patches/0004-disable-fd-limit-tests-not-supported-in-docker.patch; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/docker",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:290,avail,available,290,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['avail'],['available']
Availability,"There is a kubernetes controlled resource that has been exhausted: CPU. CPU is provided by nodes (`kubectl get nodes`). I agree, users should not see this. The core issue is lack of sufficient CPU. We can resolve this in one of two ways:. 1. add more non-preemptible nodes to the k8s cluster. 2. make the non-preemptible node pool auto-scale. For tutorials, we increase the size of the non-preemptible node pool before the tutorial begins. We shrink it again (to save money) when the tutorial is over. We could enable point two, but we now have two new issues:. 1. AFAIK, the auto-scaler only adds nodes when resources are exhausted. Ergo, we only add more nodes when a pod becomes unscheduleable. When this happens, we must wait for a node to spin up (a couple minutes) to provide sufficient CPU for the pod. There has been work towards teaching the auto-scaler about ""slack"" or ""buffer"" resources, but [the relevant PR was abandoned in Dec 2017](https://github.com/kubernetes/autoscaler/pull/77). 2. Once the node is alive, it needs to download the docker image so it can schedule the pod. We use a `DaemonSet` to ensure that the latest notebook images is always cached on all nodes. Hail's notebook images are very large. They take about two minutes to download. Actions we can take:. 1. Investigate a system for ensuring our cluster always sufficient CPU for another N notebooks. (i.e. have CPU slack); 2. Shrink our notebook images. We need to get off `conda` (for all our projects). Their `jupyter/scipy-notebook` image is 4.16GB. I think this is more of a long-term issue because point 1 is rather tricky and we have only a handful of users right now (so we can set a static cluster size that is sufficiently large).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857:1019,alive,alive,1019,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461152857,3,"['alive', 'down']","['alive', 'download']"
Availability,"There is always a race because `crun` will delete the cgroup once the container completes. Looks like the memory tracking checks for this exact error but not cpu, it would assume it should be both.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13861#issuecomment-1771461424:144,error,error,144,https://hail.is,https://github.com/hail-is/hail/issues/13861#issuecomment-1771461424,1,['error'],['error']
Availability,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:837,avail,available,837,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448,1,['avail'],['available']
Availability,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:772,error,error,772,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['error'],['error']
Availability,There was a series of PRs that addressed most of this but there's still one issue. The cancel thread frequently logs an error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1773562874:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1773562874,1,['error'],['error']
Availability,There was a subsequent error https://github.com/hail-is/hail/pull/8403,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606698835:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606698835,1,['error'],['error']
Availability,There's a new assertion error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3424#issuecomment-383742072:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/pull/3424#issuecomment-383742072,1,['error'],['error']
Availability,There's a seemingly endless stream of errors hidden by not testing impute_sex...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2952#issuecomment-369289579:38,error,errors,38,https://hail.is,https://github.com/hail-is/hail/pull/2952#issuecomment-369289579,1,['error'],['errors']
Availability,"There's not a good reason, this is just how it was originally designed. Whenever a job was cancelled, it would take until the start of the next step for a container's execution to be stopped. I replaced the dependency on this in `Container` with `run_until_done_or_deleted`, but stopped short of deleting the functionality entirely because there were other parts of the worker, specifically `JVMJob` that still relied on it. Hopefully that is no longer the case after the QoB changes, but @danking would know better. We've also both lamented about how it's impossible to use timings currently inside cleanup blocks because it could accidentally re-raise a deleted error. This is a great change, I would just take extra care to test job cancellation to make sure there isn't anywhere that's still relying on this functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054563382:664,error,error,664,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054563382,1,['error'],['error']
Availability,There's some major test errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8968#issuecomment-662453343:24,error,errors,24,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662453343,1,['error'],['errors']
Availability,There's something fishy happening in the service backend. It keeps timing out the same three test jobs. We'll need to do a little grep+diff to figure which tests are getting started but not terminating. I suspect we'll need to either skip something or ensure that it errors early.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11345#issuecomment-1057654251:267,error,errors,267,https://hail.is,https://github.com/hail-is/hail/pull/11345#issuecomment-1057654251,1,['error'],['errors']
Availability,"There's still a compilation error. Otherwise, it's good to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3751#issuecomment-397052753:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/3751#issuecomment-397052753,1,['error'],['error']
Availability,"There's the Python process running the benchmark driver Hail client in Python, and the java process running the hail backend. We want the benchmark driver to return 0 in all cases, with information about whether the hail command finished successfully and how long it took. In current main, this works well for successful runs as well as errors that don't kill the backend (assertion, HailException, etc). It doesn't work for an error that kills the backend (segfault / oom), because while the exception handler in run() works fine, we run stop() unhandled and assume it won't throw (it only throws if the JVM is dead).. In that case, this PR lets the benchmark driver return 0 after writing a file indicating the Hail function failed, just like in an assertion error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12838#issuecomment-1527658197:337,error,errors,337,https://hail.is,https://github.com/hail-is/hail/pull/12838#issuecomment-1527658197,3,['error'],"['error', 'errors']"
Availability,"These changes include a performance regression - instead of allocating memory once and filling in each member of the nested value (srvb) we are using struct constructors that allocate e.g. the locus an extra time and copy it into the container. I do not think it's worth creating constructors right now that prevent this regression -- the region value construction is much slower than the java calls here, and the right solution is coming down teh pike -- constructing containers using SStackStruct emit codes will have exactly the semantics we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644:439,down,down,439,https://hail.is,https://github.com/hail-is/hail/pull/10019#issuecomment-776197644,1,['down'],['down']
Availability,"Things that remain to be done:; - overriding repartition. This isn't super trivial because if you have a huge pipeline that ends in a repartition down to 10 partitions, you're going to get 10 cores for the whole job.; - better ordering process on VCF import; - speed up joins by skipping ahead if the 'right' is behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/555#issuecomment-238118077:146,down,down,146,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-238118077,1,['down'],['down']
Availability,"This ArrayIndexOutOfBounds is caused by a larger design flaw in Tables and ordering, which was previously masked by the horrible coerce-or-shuffle-every-time we were doing. Will try to nail it down today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3637#issuecomment-392746870:106,mask,masked,106,https://hail.is,https://github.com/hail-is/hail/pull/3637#issuecomment-392746870,2,"['down', 'mask']","['down', 'masked']"
Availability,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:478,error,errorTransformer,478,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413,2,['error'],['errorTransformer']
Availability,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279,2,['error'],['error']
Availability,This appears to have cut batch2 test time nearly in half (5-6m down from 11+m),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7543#issuecomment-554780465:63,down,down,63,https://hail.is,https://github.com/hail-is/hail/pull/7543#issuecomment-554780465,1,['down'],['down']
Availability,"This approach to joining doesn't work for me:; ```; In [1]: import hail as hl . In [2]: t = hl.utils.range_table(1) . In [3]: t2 = t.key_by(idx=t.idx, idx2=t.idx) . In [4]: t.annotate(foo=t2[t.key]) ; Traceback (most recent call last):; File ""<ipython-input-4-85e676382c80>"", line 1, in <module>; t.annotate(foo=t2[t.key]); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 368, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32, int32; Index Expressions: int32; ```. And since the annotation db is just built on joins, it wouldn't work in that setting either. Moreover, the annotation db needs to be careful with uniqueness of the key-row relationship. I try to avoid unnecessarily using `all_matches=True` which introduces arrays that complicate downstream work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587:1048,down,downstream,1048,https://hail.is,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587,1,['down'],['downstream']
Availability,"This behavior can produce type errors, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1203#issuecomment-268302819:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/pull/1203#issuecomment-268302819,1,['error'],['errors']
Availability,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:619,error,error,619,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139,6,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,This change uncovered a problem with lowering -- `AggLet` nodes are not tolerated inside MatrixMapCols/MatrixMapRows. I'll follow up with a PR to fix this for MatrixMapRows and add targeted tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6817#issuecomment-522115406:72,toler,tolerated,72,https://hail.is,https://github.com/hail-is/hail/pull/6817#issuecomment-522115406,1,['toler'],['tolerated']
Availability,"This comes up with a number of match errors on Agg IR. Wanted to check that these should be implemented in InferPType, because my impression was that relation IR would not be.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7959#issuecomment-578509253:37,error,errors,37,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-578509253,1,['error'],['errors']
Availability,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:145,down,downloads,145,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965,4,"['Down', 'avail', 'down']","['Downloaded', 'available', 'downloads']"
Availability,"This doesn't look like the same error. This error is found in a genotype call in a biallelic variant with 3 AD values -- a violation of the VCF spec (AD is ""R""-numbered). . We've seen this before on VCFs that were split by certain tools, and since there were enough of them, we added an option `skip_bad_ad` on import_vcf: https://hail.is/hail/hail.HailContext.html#hail.HailContext.import_vcf. You'll want to run with that option set to true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1415#issuecomment-282566840:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/pull/1415#issuecomment-282566840,2,['error'],['error']
Availability,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:205,down,down,205,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707,2,['down'],['down']
Availability,This failed in Azure when compiling the JVM Entryway. ```; > Task :compileScala; [Error] /io/batch/jvm-entryway/src/main/java/is/hail/JVMEntryway.java:126: error: cannot find symbol; [Error] /io/batch/jvm-entryway/src/main/java/is/hail/JVMEntryway.java:153: error: cannot find symbol; javac exited with exit code 1. > Task :compileScala FAILED; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13664#issuecomment-1728404619:82,Error,Error,82,https://hail.is,https://github.com/hail-is/hail/pull/13664#issuecomment-1728404619,4,"['Error', 'error']","['Error', 'error']"
Availability,This fails currently. I want CI and eyes on this. Validation errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175:61,error,errors,61,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175,1,['error'],['errors']
Availability,"This has slipped far enough down my todo list that it isn't gonna get done, at least not by me",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12911#issuecomment-1532088026:28,down,down,28,https://hail.is,https://github.com/hail-is/hail/pull/12911#issuecomment-1532088026,1,['down'],['down']
Availability,This is a compile error. MatrixIR has partitionCounts as an Option[IndexedSeq[String]],MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3877#issuecomment-401561119:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/3877#issuecomment-401561119,1,['error'],['error']
Availability,"This is a little broken and might require some care. I'll look into it and potentially just submit my own PR, but I do want to bump the version if possible because it might help with some of our session cleanup errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10115#issuecomment-786797234:211,error,errors,211,https://hail.is,https://github.com/hail-is/hail/pull/10115#issuecomment-786797234,1,['error'],['errors']
Availability,This is a type error: your input argument doesn't match the declared type of the function. Try `assert transform_row_type == mt.row.dtype` before the application of `transform_row_f`. I'll beef up the error reporting of my define_function prototype so the error gets caught before hitting the JVM.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467643750:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467643750,3,['error'],['error']
Availability,"This is a violation of OOP style -- . ```; class A:; def foo(self):; ... class B(A):; def foo(self, bar):; ... class C(A):; def foo(self, bar, baz):; ...; ```. If I have an `A` and call `foo()` (valid signature for an A!) I'll get an error about calling a function with an incorrect signature. I haven't poked around the CI design enough to understand the intention of the code, but maybe we should just remove the definition from the superclass?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6142#issuecomment-497843719:234,error,error,234,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-497843719,1,['error'],['error']
Availability,"This is an overflow bug that's existed since June 25: https://github.com/hail-is/hail/pull/3833. The implementation was vulnerable to `(a + b) * (c + d) * (b + d) * (a + c)` exceeding max int.; ```; val ad = a * d; val bc = (b * c).toDouble; val det = ad - bc; val chiSquare = (det * det * (a + b + c + d)) / ((a + b) * (c + d) * (b + d) * (a + c)); ```; If so, chiSquare and hence the p-value would be wrong.; The odds ratio was also susceptible to overflow, though less so: if `a * d` or `b * c` exceeded max int. The new implementation converts the ints to floats and is more robust if we switch to allowing float inputs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4192#issuecomment-415137754:579,robust,robust,579,https://hail.is,https://github.com/hail-is/hail/issues/4192#issuecomment-415137754,1,['robust'],['robust']
Availability,"This is better, but I want to go more extreme:. - get rid of TransformedRegionValueAggregator and ZippedRegionValueAggregator. This is a compiler backend. Too much abstraction in your output! Let's compile that shit away.; - ExtractAggregators should return an Array[AggSum]. These are expressions ending in AggSum containing aggregator operations (filter, map, etc.) defined in terms of the aggregated element and associated context.; - Add a function AggSum => RegionValueAggregator. This is the way to generalize: make AggSum into Agg(op: AggOp) where AggOp (like unary and binary op) are all the possible aggregator types, and there is a function that maps the op to the corresponding RegionValueAggregator*; - compiling the Array[AggSum] should product a function that takes the array of aggregators and a single value (with context) of the collection we're aggregating over and updates them with that element. *I think you need an array of arguments to handle things like call_stats which are evaluated in the aggregator scope (the only scope available to evaluate something). Imagine you have `gs.filter(g => g.GT.isHet()).map(g => g.DP).sum() + gs.flatMap(g => g.PL).sum()`. The Array[AggSum] will be. ```; Array(AggSum(AggMap(; AggFilter(AggIn(...), ; ""g"", g.GT.isHet()),; ""g"", (getField (Ref ""g"") ""DP""),; AggSum(AggFlatMap(AggIn(...),; ""g"", (getField (Ref ""g"") ""PL))); ```. The generated function should look like:. ```; def f(aggs: Array[AggSum], region: MemoryBuffer, g: Long, mg: Boolean, ...) {; if (g.GT.isHet()) { // RV-ified, of course; val DP = g.DP // actually fieldOffset; aggs(0).seqOp(DP); }; for (PLi in g.PL) { // actually elementOffset; aggs(1).seqOp(PLi); }; }; ```. This is straight-line and should be fast. It immediately allows you to do common subexpression elimination on aggregator prefixes which is something that is quite common, that is, if you have `gs.filter(g => g.isHet).map(g => g.DP).mean() < 10 gs.filter.map(g => g.GQ).mean() < 50` then in the aggregation fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2555#issuecomment-350868684:1049,avail,available,1049,https://hail.is,https://github.com/hail-is/hail/pull/2555#issuecomment-350868684,1,['avail'],['available']
Availability,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187:163,avail,available,163,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187,2,['avail'],['available']
Availability,This is definitely no longer high-prio -- we'll throw the correct error message now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-500771034:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-500771034,1,['error'],['error']
Availability,This is failing with this message:. ```; + python3 -m pylint --rcfile pylintrc hailtop; ************* Module hailtop.aiotools.weighted_semaphore; /usr/local/lib/python3.7/dist-packages/hailtop/aiotools/weighted_semaphore.py:2:0: E0401: Unable to import 'sortedcontainers' (import-error); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897018931:280,error,error,280,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897018931,1,['error'],['error']
Availability,"This is fine: [x.txt](https://github.com/hail-is/hail/files/12231007/x.txt). ```; In [11]: import hail as hl; ...: gwas = hl.read_table(""gwas_filtered.ht""); ...: loci_to_gene = hl.import_table(""x.txt"",impute=True); ...: locus = hl.locus(loci_to_gene.chromosome, loci_to_gene.locus, ""GRCh38""); ...: loci_to_gene = loci_to_gene.annotate(locus=locus); ...: loci_to_gene = loci_to_gene.key_by(""locus""); ...: loci_to_gene = loci_to_gene.select(""gene""); ...: loci_to_gene = loci_to_gene.filter(loci_to_gene.locus.position == 51749536); ...: loci_to_gene = loci_to_gene.checkpoint('/tmp/foo.ht', overwrite=True); ...: print(gwas.collect()); ...: print(loci_to_gene.collect()); ...: gwas.annotate(gene=loci_to_gene[gwas.locus].gene).collect(); 2023-08-01 10:54:33.166 Hail: INFO: Reading table to impute column types; 2023-08-01 10:54:33.864 Hail: INFO: Finished type imputation; Loading field '' as type int32 (imputed); Loading field 'chromosome' as type str (imputed); Loading field 'locus' as type int32 (imputed); Loading field 'gene' as type str (imputed); 2023-08-01 10:54:34.137 Hail: INFO: Coerced sorted dataset; 2023-08-01 10:54:35.270 Hail: INFO: wrote table with 1 row in 1 partition to /tmp/foo.ht; [Struct(locus=Locus(contig=chr8, position=51749536, reference_genome=GRCh38), alleles=['G', 'T'])]; [Struct(locus=Locus(contig=chr8, position=51749536, reference_genome=GRCh38), gene='PXDNL')]; Out[11]: [Struct(locus=Locus(contig=chr8, position=51749536, reference_genome=GRCh38), alleles=['G', 'T'], gene='PXDNL')]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13339#issuecomment-1660499132:563,checkpoint,checkpoint,563,https://hail.is,https://github.com/hail-is/hail/issues/13339#issuecomment-1660499132,1,['checkpoint'],['checkpoint']
Availability,"This is generating some 500 errors, having a hard time debugging without hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-669818238:28,error,errors,28,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-669818238,1,['error'],['errors']
Availability,This is getting this SQL error in create_batch2_tables:. ERROR 1215 (HY000) at line 70: Cannot add foreign key constraint,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7437#issuecomment-549065979:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/7437#issuecomment-549065979,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This is great! From a downstream utility perspective, should this return BlockMatrix, or a hail table keyed by sample (more like what hl.hwe_normalized_pca returns)? I feel like most analytics would then be plotting PCs, or using them in models, but I might be missing an application that needs BlockMatrix",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1957636454:22,down,downstream,22,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1957636454,1,['down'],['downstream']
Availability,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240346120:324,error,error,324,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120,4,"['FAILURE', 'down', 'error']","['FAILURE', 'down', 'error', 'errors']"
Availability,"This is great, thanks for working on this!. > I have not added an example to the documentation that uses a matrix table yet. (This is an https://github.com/hail-is/hail/issues/13481.) I wanted to get some advice about the best way to do this. I think ideally, the example would have a binary phenotype, an allele to test for association, and some stratifying variable. I tried to search through the existing code to find suitable example matrix tables in the docstrings, but I didn't find anything promising. I would appreciate help here. The code that sets up the doctest environment is [here](https://github.com/hail-is/hail/blob/8a0e8e3375f1fc11efb5a443a350a8b4e8a24950/hail/python/hail/conftest.py#L55). In particular, the matrixtable available in doc examples as `ds` lives at `hail/hail/python/hail/docs/data/example.mt`. It has lots of fields you can use:; ```; In [3]: mt = hl.read_matrix_table('python/hail/docs/data/example.mt'). In [4]: mt.describe(); ----------------------------------------; Global fields:; 'global_field_1': int32; 'global_field_2': int32; 'pli': dict<str, float64>; 'populations': array<str>; ----------------------------------------; Column fields:; 's': str; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; 'is_case': bool; 'pheno': struct {; is_case: bool,; is_female: bool,; age: float64,; height: float64,; blood_pressure: float64,; cohort_name: str; }; 'cov': struct {; PC1: float64; }; 'cov1': float64; 'cov2': float64; 'cohort': str; 'cohorts':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1932814827:739,avail,available,739,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1932814827,1,['avail'],['available']
Availability,This is in response to this failure: https://storage.googleapis.com/hail-ci-0-1/ci/7aa524504b8bafe0a4af859e73bc4f9efdaa052c/39a94649482a2512a7a514e6084c5b84f48b8205/index.html. Which might be mis-diagnosed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428986578:28,failure,failure,28,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428986578,1,['failure'],['failure']
Availability,"This is kind of an annoying problem to have because these pip installed versions are frozen in time. I feel like these steps are either redundant (nothing has changed), or will fail because we've updated the checks to improve on what we had at last release. What do these steps really do for us?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165:136,redundant,redundant,136,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165,1,['redundant'],['redundant']
Availability,"This is my first ever git commit so let me know if there are any changes needed. I tested this functionality by building Hail on my Amazon Linux 2 system before vs after. After removal of the ""sys_platform!='win32'"" the error was no longer happening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1229010436:220,error,error,220,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1229010436,1,['error'],['error']
Availability,This is now available in 4.80.0 through 5.2.0. Work for this issue is:. 1. Upgrade to latest 4.x.x; 2. Encode cleanup policies in terraform.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13504#issuecomment-1773465652:12,avail,available,12,https://hail.is,https://github.com/hail-is/hail/issues/13504#issuecomment-1773465652,1,['avail'],['available']
Availability,"This is ready for final review. Since Konrad is blocked by it, I suggest, unless there are correctness failures or major performance issues, we merge it and I will address comments in follow up PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575711955:103,failure,failures,103,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575711955,1,['failure'],['failures']
Availability,This is ready for review. I keep getting unrelated transient errors in this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12742#issuecomment-1458692668:61,error,errors,61,https://hail.is,https://github.com/hail-is/hail/pull/12742#issuecomment-1458692668,1,['error'],['errors']
Availability,"This is ready to look at. i tried to track down the one failure in test_benchmark, but couldn't do so.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9972#issuecomment-772045985:43,down,down,43,https://hail.is,https://github.com/hail-is/hail/pull/9972#issuecomment-772045985,2,"['down', 'failure']","['down', 'failure']"
Availability,"This is really great. . I have some thoughts below, mostly brain storming. Don't take any of it too seriously. Some thoughts:. 1. I thought you wanted to support f-strings. By making the batch file quote double curly parens, that means if you use them in an f-string, you need to write `f'{{{{foo}}}}'` which is a bit much. But that means that non-batch uses of `{}` need to be double-quoted, so `awk '{{ ... }}'` and `f""awk '{{{{ ... }}}}'""`. Hmm. Maybe using the same escape syntax as f-strings is not ideal. . I don't have a no-brainer suggestion. Happy to brainstorm ideas offline. I think ultimately this is a minor syntactic choice. 2. Inputs seem ... almost redundant, because they also appear in the command strings. What about:. ```; .command('shapeit --bed-file {{<subset.ofile}} --chr ' + contig + ' --out {{>ofile}}'); ```. Then the question becomes, how do associate `subset` with the corresponding Python variable? You could use the task label, but then the user has to maintain two sets of names, which isn't ideal. Hmm, maybe this doesn't work. 3. I like arrays of resources!. > `.command('cat {{files}} >> {{ofile}}')`. I wonder, will we ever want arrays to be formatted other than joined with spaces? I worry the user will want more flexibility in formatting, and we'll want that in Python. What about if the argument is a function, it takes a dictionary from resource names to their string representation, and you can format however you want? Then you could write the last command as:. ```; .command(lambda rs: f'cat {' '.join(rs['files'])} >> {rs['ofile']}'); ```. 4. I was confused by this:. > `p.write_output(merger.ofile + "".haps"", ...)`. What's the left hand side? Why isn't this just `merger.ofile`?. This suggests another issue: what if you want to use `ofile` in a plink command, but plink outputs some files with various extensions with `ofile` as the base? We might need an `outputs` that lists (docker local) output files based on a base path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609:665,redundant,redundant,665,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609,1,['redundant'],['redundant']
Availability,"This is running into the split_multi issue. it *would* be running into this error in the RVD iterator, but we don't perform this check too many places anymore nowadays and it's not hitting this check anywhere after the split happens.; ```; if (localType.kRowOrd.gt(prevK.value, rv)) {; kUR.set(prevK.value); val prevKeyString = kUR.toString(). prevK.setSelect(localType.rowType, localType.kFieldIdx, rv); kUR.set(prevK.value); val currKeyString = kUR.toString(); fatal(; s""""""RVD error! Keys found out of order:; | Current key: $currKeyString; | Previous key: $prevKeyString; |This error can occur after a split_multi if the dataset; |contains both multiallelic variants and duplicated loci.; """""".stripMargin); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292:76,error,error,76,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498870292,3,['error'],['error']
Availability,"This is working except I can't figure out how to delete entries from /etc/projects and /etc/projid when the job finishes. You can't copy or move a file to a bind-mounted file in Docker or you get a ""resource or device busy"" error. This will all work without deleting the entries, but then every time you try and add a new project, you get a directory not found error for previously deleted paths, but the operation will continue successfully ignoring the errors. Any ideas on how to get around this? `sed -i` won't work as well as `mv` and `cp`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9076#issuecomment-657239708:224,error,error,224,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-657239708,3,['error'],"['error', 'errors']"
Availability,"This looks OK to me. The first bit is the stack trace to the retry_transient_errors. The second bit is the stacktfrace for the timeout error. The last part of a Python stack trace is always the name of the exceptional class, e.g.:; ```. In [13]: raise ValueError() ; Traceback (most recent call last):; File ""<ipython-input-13-4954757c312d>"", line 1, in <module>; raise ValueError(); ValueError. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649524:135,error,error,135,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649524,1,['error'],['error']
Availability,This looks a lot like preemption. Are these errors fatal?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635#issuecomment-511343816:44,error,errors,44,https://hail.is,https://github.com/hail-is/hail/issues/6635#issuecomment-511343816,1,['error'],['errors']
Availability,"This looks fine. Could possibly put something in the Notes section of the docs to address changing block size if folks are running into errors with Hadoop, but if most people will be successful with the default, then I guess it's not necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3697#issuecomment-393976813:136,error,errors,136,https://hail.is,https://github.com/hail-is/hail/pull/3697#issuecomment-393976813,1,['error'],['errors']
Availability,This looks good to me. There's a bunch of test failures mainly in TextTableSuite.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5139#issuecomment-454422936:47,failure,failures,47,https://hail.is,https://github.com/hail-is/hail/pull/5139#issuecomment-454422936,1,['failure'],['failures']
Availability,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:425,down,down,425,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731,4,"['down', 'robust']","['down', 'robust']"
Availability,This looks to me like we just have bad long tails. If we can 90% and 99% test times down further maybe we're basically good. Seems feasible for the entire service backend tests to take <20 minutes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1553848763:84,down,down,84,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1553848763,1,['down'],['down']
Availability,"This needs a rebase and has some docs failures, IIRC. I'll push fixes to this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6510#issuecomment-510519521:38,failure,failures,38,https://hail.is,https://github.com/hail-is/hail/pull/6510#issuecomment-510519521,1,['failure'],['failures']
Availability,This needs a rebase. Ping me when it's ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9594#issuecomment-724099162:21,Ping,Ping,21,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-724099162,1,['Ping'],['Ping']
Availability,"This now gives a nice error message:. ```; >>> hl.uniroot(lambda x: x * x + 1, -1, 1).value; Error summary: HailException: sign of endpoints must have opposite signs, got: f(min) = 2.0, f(max) = 2.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1896#issuecomment-408636691:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/1896#issuecomment-408636691,2,"['Error', 'error']","['Error', 'error']"
Availability,This now passes. ping @chrisvittal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6263#issuecomment-501361010:17,ping,ping,17,https://hail.is,https://github.com/hail-is/hail/pull/6263#issuecomment-501361010,1,['ping'],['ping']
Availability,"This one isn't a segfault, it's a test failure where somehow for each ndarray in the table, row i + 1 is getting the data array from row i. Cloud workshop and ndarray aggregator pushed this down my priority list, but I will continue to try and fix it and then get back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8952#issuecomment-667098599:39,failure,failure,39,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-667098599,2,"['down', 'failure']","['down', 'failure']"
Availability,This only yielded a ~20% reduction. Test times are around 16 minutes now down from around 20 minutes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11212#issuecomment-1039677943:73,down,down,73,https://hail.is,https://github.com/hail-is/hail/pull/11212#issuecomment-1039677943,1,['down'],['down']
Availability,"This particular example works for me today in 0.2.27. When I upped M to 350, I instead got a Java stack overflow error like:. ```; java.lang.StackOverflowError: null; 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:98); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:204); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:35); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024:113,error,error,113,https://hail.is,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024,1,['error'],['error']
Availability,"This probably needs more work to return a 'nice' error for user errors and internal errors (for bug reporting), though with the service, we should have visibility into compiler errors from just the batch id.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11623#issuecomment-1073261565:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/pull/11623#issuecomment-1073261565,4,['error'],"['error', 'errors']"
Availability,"This seems fine now (the alternative is to continue using unittest which seems not loved for some reason?). I'll echo patrick that I would prefer we live on a test framework that's maintained. [nose2](http://nose2.readthedocs.io/en/latest/) seems to be a maintained project, but that reddit thread is hot for `pytest`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3096#issuecomment-371837536:113,echo,echo,113,https://hail.is,https://github.com/hail-is/hail/pull/3096#issuecomment-371837536,1,['echo'],['echo']
Availability,This seems like an odd bug?; https://ci.hail.is/repository/download/HailSourceCode_HailMainline_HailCiSpark1/2980:id/build/reports/tests/classes/org.broadinstitute.hail.io.ExportVcfSuite.html#testReadWrite,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1194#issuecomment-268636904:59,down,download,59,https://hail.is,https://github.com/hail-is/hail/pull/1194#issuecomment-268636904,1,['down'],['download']
Availability,"This seems to do it:; ```. In [1]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(32):; ...: j = b.new_job(); ...: j.command(f'cat >/dev/null {"" "".join(b.read_input(""gs://danking/foo.vcf"") for _ in range(1000))}'); ...: b.run(); /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/batch/backend.py:786: UserWarning: Using an image ubuntu:22.04 from Docker Hub. Jobs may fail due to Docker Hub rate limits.; warnings.warn(f'Using an image {image} from Docker Hub. '. https://batch.hail.is/batches/8090821 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 33/33 0:00:00 0:01:37; batch 8090821 complete: success; Out[1]: <hailtop.batch_client.client.Batch at 0x1086d1bd0>. In [2]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```. Perhaps related to creating a fresh service backend?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467:1366,echo,echo,1366,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467,1,['echo'],['echo']
Availability,This seems to have failed due to exactly the error it is supposed to fix. The gear in use by CI is not the gear fixed here. I'll bump.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8237#issuecomment-605073218:45,error,error,45,https://hail.is,https://github.com/hail-is/hail/pull/8237#issuecomment-605073218,1,['error'],['error']
Availability,"This should always produce a valid VCF with respect to: https://samtools.github.io/hts-specs/VCFv4.2.pdf. I've changed the behavior of export_vcf so that, like FORMAT field types, unsupported INFO field types must be explicitly converted to String by the user if the user really wants to export them. With good error messages, I think this will cause less confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2414#issuecomment-343611350:311,error,error,311,https://hail.is,https://github.com/hail-is/hail/pull/2414#issuecomment-343611350,1,['error'],['error']
Availability,This should be a type error. I don't think you should be able to use comparison operators on non-primitive types,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458#issuecomment-505215908:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/6458#issuecomment-505215908,1,['error'],['error']
Availability,This should be all set. I tested backwards compatibility and caught some errors that showed up in the worker logs with regards to iptables lock errors with simultaneous shell calls.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12542#issuecomment-1370274153:73,error,errors,73,https://hail.is,https://github.com/hail-is/hail/pull/12542#issuecomment-1370274153,2,['error'],['errors']
Availability,This should be fixed by #8103 etc. Reopen this issue if you see the error again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053#issuecomment-588390042:68,error,error,68,https://hail.is,https://github.com/hail-is/hail/issues/8053#issuecomment-588390042,1,['error'],['error']
Availability,"This should be functioning (or very close) in GCP but is basically unimplemented in Azure. Azure needs a secret store like how we use Google Secret Manager that workers can access when they start up to load certificates. Azure Key Vault seems reasonable and can be created through terraform. The structure should mirror that in GCP, where we need a client that can upload the certs to Azure in `create_certs` and download them in the azure CloudWorkerAPI. I would leave this unimplemented in TerraAzure until an overall secrets story is established.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14581#issuecomment-2243030552:413,down,download,413,https://hail.is,https://github.com/hail-is/hail/pull/14581#issuecomment-2243030552,1,['down'],['download']
Availability,"This should be good minus tests and double/triple checking the billing is all correct with units etc.. Ideally, there'd be two situations to test:. 1. A storage size that exceeds the unreserved space (i.e. 375Gi); 2. Multiple smaller jobs requesting unreserved space and we need to spin up a disk for one of them. I don't think there's a reliable way to test 2 other than doing it by hand in my dev namespace. Test 1 should be sufficient for checking the new disk and /io works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941:338,reliab,reliable,338,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941,1,['reliab'],['reliable']
Availability,This should be ready for review. Only error was black formatting which hopefully I fixed now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11211#issuecomment-1010359404:38,error,error,38,https://hail.is,https://github.com/hail-is/hail/pull/11211#issuecomment-1010359404,1,['error'],['error']
Availability,"This still causes error messages. `hail -l /mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed.Chr${num}.QC.vds.test.log importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr${num}/TopMed_8k.853.vcf.bgz splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count`. `2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@716: Client environment:host.name=nid00014; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@723: Client environment:os.name=Linux; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-431.el6_1.0000.9051-cray_ari_athena_c_cos; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Thu Jan 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['error'],['error']
Availability,"This test failed in CI, but worked fine for me locally the 3 times I tested it (takes forever to run). Not sure what's going on. ```; =================================== FAILURES ===================================; _______________________ test_input_dependency_directory ________________________. client = <batch.client.BatchClient object at 0x7fee8fbecbe0>; test_user = {'bucket_name': 'hail-user-bucket-batch-tests', 'gsa_email': 'batch-tests@hail-vdc.iam.gserviceaccount.com', 'gsa_key_secret_name': 'gcp-sa-key-batch-tests', 'ksa_name': 'batch-tests'}. def test_input_dependency_directory(client, test_user):; batch = client.create_batch(); head = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'mkdir -p /io/test/; echo head1 > /io/test/data1 ; echo head2 > /io/test/data2'],; output_files=[('/io/test/', f'gs://{test_user[""bucket_name""]}')]); tail = batch.create_job('alpine:3.8',; command=['/bin/sh', '-c', 'cat /io/test/data1 ; cat /io/test/data2'],; input_files=[(f'gs://{test_user[""bucket_name""]}/test', '/io/')],; parent_ids=[head.id]); tail.wait(); > assert tail.log()['main'] == 'head1\nhead2\n'; E KeyError: 'main'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864:170,FAILURE,FAILURES,170,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-485600864,3,"['FAILURE', 'echo']","['FAILURES', 'echo']"
Availability,"This took me an incredibly long time to figure out, but I'm pretty sure the issue is that my last test copies the same file twice. Once explicitly as a single file and the second time as part of the directory to copy recursively. I don't think our copier code interacts correctly with Azure Blob Storage in this case and we end up with invalid block ID list errors. I'm still stewing over which option is the best to address this. I could either not have `submit.py` ask to copy the same file twice or I can modify the copier to have a lock on the create blob writer stream so that we don't have this issue. However, I'm worried about deadlocks with our extensive use of semaphores and parallelism. Thoughts?? I think the right thing to do is fix it for now in `submit.py` so we can get this in and then make an issue to make a more durable fix to the copier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1881894683:358,error,errors,358,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1881894683,1,['error'],['errors']
Availability,This was running locally rather than the spark cluster which I expect relates to the memory error.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-439415237:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-439415237,1,['error'],['error']
Availability,"This will also retry things that are our fault, but that might just be worth it if it avoids most transient errors",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11666#issuecomment-1077884202:41,fault,fault,41,https://hail.is,https://github.com/hail-is/hail/pull/11666#issuecomment-1077884202,2,"['error', 'fault']","['errors', 'fault']"
Availability,"This will enforce even stricter requirements on downstream projects, right? Do we want to do this only for our own builds and use unpinned dependencies in the pip package's setup.py?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1128128100:48,down,downstream,48,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1128128100,1,['down'],['downstream']
Availability,"This would also be slowing down some of the tests on laptops. Not sure about the CI, I don't see `hail.log` published as a CI artifact.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-385266926:27,down,down,27,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-385266926,1,['down'],['down']
Availability,"Tim, . This actually isn't bad. I took a stab at the aggregators, and in the case of those that need InitOps and SeqOps, used the existing AggSignature `toPhysical` instance method, which seems to do about the right thing, with minor modification. Also caught some Inference failures! Still have one more to fix, then check over any missing nodes (RunAgg is missing, have a placeholder comment for that, which I'll fill in now.). Only 1 test failing in IRSuite! That is ArrayFold2, which is just a bug we didn't catch before, and has nothing to do with new nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7959#issuecomment-579992599:275,failure,failures,275,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-579992599,1,['failure'],['failures']
Availability,"Tim, can you shoot me down, too?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469476636:22,down,down,22,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469476636,1,['down'],['down']
Availability,"To clarify, that :-1: is my finger hovering over the button. Not a thumbs down",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2829#issuecomment-361434958:74,down,down,74,https://hail.is,https://github.com/hail-is/hail/pull/2829#issuecomment-361434958,1,['down'],['down']
Availability,"To clarify, you could could say this:. `kt.aggregate('rows.filter(r => r.col1 < r.col2).count()')`. but this would produce a symref error:. `kt.aggregate('col1.filter(c => c < col2).count()')`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1410#issuecomment-282414199:132,error,error,132,https://hail.is,https://github.com/hail-is/hail/issues/1410#issuecomment-282414199,1,['error'],['error']
Availability,"To get rid of the CI errors, then you have to run black on some of your files. ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/inst_coll_config.py	2022-06-01 03:13:40.430014 +0000; +++ batch/inst_coll_config.py	2022-06-01 03:15:40.847020 +0000; @@ -307,13 +307,20 @@; if optimal_cost is None or max_regional_maybe_cost < optimal_cost:; optimal_cost = max_regional_maybe_cost; optimal_result = (pool.name, maybe_cores_mcpu, maybe_memory_bytes, maybe_storage_gib); return optimal_result; ; - def select_pool_from_worker_type(self, cloud, pool_label, worker_type, cores_mcpu, memory_bytes, storage_bytes, preemptible):; + def select_pool_from_worker_type(; + self, cloud, pool_label, worker_type, cores_mcpu, memory_bytes, storage_bytes, preemptible; + ):; for pool in self.name_pool_config.values():; - if pool.cloud == cloud and pool.worker_type == worker_type and pool.preemptible == preemptible and pool.label == pool_label:; + if (; + pool.cloud == cloud; + and pool.worker_type == worker_type; + and pool.preemptible == preemptible; + and pool.label == pool_label; + ):; result = pool.convert_requests_to_resources(cores_mcpu, memory_bytes, storage_bytes); if result:; actual_cores_mcpu, actual_memory_bytes, acutal_storage_gib = result; return (pool.name, actual_cores_mcpu, actual_memory_bytes, acutal_storage_gib); return None; @@ -322,11 +329,19 @@; if self.jpim_config.cloud != cloud:; return None; return self.jpim_config.convert_requests_to_resources(machine_type, storage_bytes); ; def select_inst_coll(; - self, cloud, machine_type, pool_label, preemptible, worker_type, req_cores_mcpu, req_memory_bytes, req_storage_bytes; + self,; + cloud,; + machine_type,; + pool_label,; + preemptible,; + worker_type,; + req_cores_mcpu,; + req_memory_bytes,; + req_storage_bytes,; ):; if worker_type is not None and machine_type is None:; result = self.select_pool_from_worker",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1144135068:21,error,errors,21,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1144135068,1,['error'],['errors']
Availability,Two high-level comments:; - Here is the default documentation:. https://ci.hail.is/repository/download/HailSourceCode_HailCi/846:id/docs/index.html#exportaggregate. Some documentation of the output format and maybe and example or two (with and without `--by-matrix`) would be awesome.; - We need at least some testing. I think a simple aggregation on a small file that you verify by hand would be sufficient. I'll look over the code and let you know if I have additional comments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/837#issuecomment-249244766:94,down,download,94,https://hail.is,https://github.com/hail-is/hail/pull/837#issuecomment-249244766,2,['down'],['download']
Availability,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:49,redundant,redundant,49,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512,1,['redundant'],['redundant']
Availability,"Ugh, and in Azure, the batch-driver was too slow coming alive and it failed the build. Retried.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11622#issuecomment-1076608471:56,alive,alive,56,https://hail.is,https://github.com/hail-is/hail/pull/11622#issuecomment-1076608471,1,['alive'],['alive']
Availability,"Ugh, discovered a problem with race conditions surrounding the `test.cpp` build path. Can cause failures with a naked `make -jN test` on a clean directory. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754:96,failure,failures,96,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188754,1,['failure'],['failures']
Availability,"Ugh, it is failing because movie lens download is timing out again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5082#issuecomment-451988168:38,down,download,38,https://hail.is,https://github.com/hail-is/hail/pull/5082#issuecomment-451988168,1,['down'],['download']
Availability,"Unassigning until I fix errors in CI, apologies for the noise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13745#issuecomment-1739787547:24,error,errors,24,https://hail.is,https://github.com/hail-is/hail/pull/13745#issuecomment-1739787547,1,['error'],['errors']
Availability,"Unfortunately, I can't figure out the verification errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9604#issuecomment-713144112:51,error,errors,51,https://hail.is,https://github.com/hail-is/hail/pull/9604#issuecomment-713144112,1,['error'],['errors']
Availability,"Unfortunately, the CI results are private to protect against inadvertent secret leaks. It looks like the docs are failing. The `Locus` should be `hl.Locus`. I suspect the test will pass with that change. ```; =================================== FAILURES ===================================; __________ [doctest] hail.expr.functions.cochran_mantel_haenszel_test __________; 834 Examples; 835 --------; 836 >>> a = [56, 61, 73, 71]; 837 >>> b = [69, 257, 65, 48]; 838 >>> c = [40, 57, 71, 55]; 839 >>> d = [77, 301, 79, 48]; 840 >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); 841 Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417); 842 ; 843 >>> mt = ds.filter_rows(mt.locus == Locus(20, 10633237)); UNEXPECTED EXCEPTION: NameError(""name 'Locus' is not defined""); Traceback (most recent call last):; File ""/usr/lib/python3.9/doctest.py"", line 1334, in __run; exec(compile(example.source, filename, ""single"",; File ""<doctest hail.expr.functions.cochran_mantel_haenszel_test[5]>"", line 1, in <module>; NameError: name 'Locus' is not defined; /usr/local/lib/python3.9/dist-packages/hail/expr/functions.py:843: UnexpectedException; 835 --------; 836 >>> a = [56, 61, 73, 71]; 837 >>> b = [69, 257, 65, 48]; 838 >>> c = [40, 57, 71, 55]; 839 >>> d = [77, 301, 79, 48]; 840 >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); 841 Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417); 842 ; 843 >>> mt = ds.filter_rows(mt.locus == Locus(20, 10633237)); 844 >>> mt.count_rows(); Expected:; 1; Got:; 9. /usr/local/lib/python3.9/dist-packages/hail/expr/functions.py:844: DocTestFailure; 845 1; 846 >>> a, b, c, d = mt.aggregate_entries(; 847 ... hl.tuple([; 848 ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; 849 ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_wher",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1961918395:245,FAILURE,FAILURES,245,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1961918395,1,['FAILURE'],['FAILURES']
Availability,Unkey the cols seems wrong. It makes the contract of `entries()` much more complicated. I guess a warning or error that makes the user decide between manually unkeying the columns or shuffling would be best? (I guess that means implementing a shuffling option for small data.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4646#issuecomment-433473971:109,error,error,109,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433473971,1,['error'],['error']
Availability,"Update on this. I am getting the same errors when doing `group_cols_by` for another aggregation method. This is a matrixtable with 2 variants and 245k samples. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_gtstats_vals = mt.group_cols_by(mt.ancestry).aggregate(gt_stats_ancestry=hl.agg.call_stats(mt.GT, mt.alleles)); mt_gtstats_vals.gt_stats_ancestry.AF.export(af_ancestry_bucket); ```. ```; [Stage 23:> (0 + 1) / 1]; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /tmp/ipykernel_231/1465831350.py in <module>; ----> 1 mt_gtstats_vals.gt_stats_ancestry.AF.export(af_ancestry_bucket). <decorator-gen-634> in export(self, path, delimiter, missing, header). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in export(self, path, delimiter, missing, header); 1068 **{output_col_name: hl.delimit(column_names, delimiter)}); 1069 file_contents = header_table.union(file_contents); -> 1070 file_contents.export(path, delimiter=delimiter, header=False); 1071 ; 1072 @typecheck_method(n=int, _localize=bool). <decorator-gen-1190> in export(self, output, types_file, header, parallel, delimiter). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:38,error,errors,38,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['error'],['errors']
Availability,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242074915:260,error,error,260,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915,1,['error'],['error']
Availability,"Update to this, tried running the same script with the bgen file as v1.2 instead (was v1.1 in initial posted issue), but it gives the same issue/stack trace:. ```; SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:205,failure,failure,205,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['failure'],['failure']
Availability,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:1446,Error,Error,1446,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174,1,['Error'],['Error']
Availability,"Using the Master branch version and Spark 2.2.1, I am getting the same error. Is Spark 2.2,1 supported? Any suggestions?. ```; /gradlew -Dspark.version=2.2.1 shadowJar archiveZip; fdf130b2f5d4. FAILURE: Build failed with an exception. * Where:; Build file '/restricted/projectnb/genpro/github/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.2.1. Set breeze.version and py4j.version properties for Spark 2.2.1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 6.187 secs; ```; ```; env|grep SPARK; SPARK_HOME=/share/pkg/spark/2.2.1/install; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-375939652:71,error,error,71,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-375939652,2,"['FAILURE', 'error']","['FAILURE', 'error']"
Availability,"Verified that before #3510 was merged into master, if this PR had been added to master, it would have caught the issue. The stack trace directly fingers the EmitPackDecoder, which is exactly what one would hope to happen:. ```; FatalError: AssertionError: assertion failed: PackDecoder compilation should happen on master, but happened on worker. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 1 times, most recent failure: Lost task 3.0 in stage 3.0 (TID 18, localhost, executor driver): java.lang.AssertionError: assertion failed: PackDecoder compilation should happen on master, but happened on worker; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:291); 	at is.hail.io.EmitPackDecoder$.apply(RowStore.scala:637); 	at is.hail.io.PackCodecSpec.buildDecoder(RowStore.scala:110); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:550); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:546); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:282); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:282); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3521#issuecomment-387199916:424,failure,failure,424,https://hail.is,https://github.com/hail-is/hail/pull/3521#issuecomment-387199916,2,['failure'],['failure']
Availability,"VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1318,error,error,1318,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['error'],['error']
Availability,WTF - I'm getting this error in a PR - https://ci.hail.is/jobs/30344/log. also see this message at the bottom: . 2019-05-30 19:47:48 Hail: WARN: struct{idx: int32} has no field row_idx at <root>.<array>.end for value JInt(10),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-497480960,1,['error'],['error']
Availability,"Wait, there's no error. Shouldn't this drop alleles though? Transmute I thought did...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673#issuecomment-392915822:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/3673#issuecomment-392915822,1,['error'],['error']
Availability,"Was getting assertion error when initializing a DB instance in cases where datasets are on only one cloud platform. Specifically with the panUKB Hail tables that are only on AWS and not on GCP, running `db = hl.experimental.DB(region='us', cloud='gcp')` would fail at the assertion to check cloud in `DatasetVersion.from_json()`. To fix, I just made `DatasetVersion.from_json()` return `None` when a version is not available on the specified cloud platform so that it can be filtered out in `Dataset.from_name_and_json()`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10360#issuecomment-825010405:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/10360#issuecomment-825010405,2,"['avail', 'error']","['available', 'error']"
Availability,Watching - AoU is seeing this error with 0.2.126,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1802068296:30,error,error,30,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1802068296,1,['error'],['error']
Availability,"We *should* be handling this error already - there's code that matches this exact message here: https://github.com/hail-is/hail/blob/8362afaefa8829e720f5affc9b482c2568e8299d/hail/src/main/scala/is/hail/services/NettyProxy.scala#L21-L31. Following the logic through, it looks like we retry this kind of exception indefinitely UNLESS the netty `Epoll` is not available...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980#issuecomment-1545975559:29,error,error,29,https://hail.is,https://github.com/hail-is/hail/issues/12980#issuecomment-1545975559,2,"['avail', 'error']","['available', 'error']"
Availability,"We are trying to setup hail `0.2.72` on spark `3.1.2` version. However, we are also facing similar error. * java version: `OpenJDK 64-Bit Server VM, 1.8.0_242`; * scala version: `2.12.10`; * py4j: `0.10.9`; * Python: `3.7.10`. <details>; <summary>Stacktrace</summary>. ```; Py4JJavaError: An error occurred while calling o126.exists.; : java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException; 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2532); 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2497); 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593); 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.exists(FS.scala:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.Ca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:99,error,error,99,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,2,['error'],['error']
Availability,"We are unlikely to support this in the short term, but are planning to refine our VCF parsing and VDS model to include this down the road.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1010#issuecomment-256957523:124,down,down,124,https://hail.is,https://github.com/hail-is/hail/issues/1010#issuecomment-256957523,1,['down'],['down']
Availability,"We can [reserve specific IP addresses](https://cloud.google.com/compute/docs/ip-addresses/reserve-static-internal-ip-address) for use later (like we do with internal gateway), but as far as I can tell the model is you assign a network or subnet an IP address range and that should be available to any resources created in it. I guess the proper way to ensure this would be to then use a range for job IPs that is outside the internal IP address range given to our default network.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11051#issuecomment-965671928:284,avail,available,284,https://hail.is,https://github.com/hail-is/hail/pull/11051#issuecomment-965671928,1,['avail'],['available']
Availability,"We can talk more about how to handle lines with errors, but that won't be part of this fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/361#issuecomment-216535908:48,error,errors,48,https://hail.is,https://github.com/hail-is/hail/issues/361#issuecomment-216535908,1,['error'],['errors']
Availability,We could throw a nicer error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3696#issuecomment-393680136:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/3696#issuecomment-393680136,1,['error'],['error']
Availability,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:608,down,down,608,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761,1,['down'],['down']
Availability,"We don't use near line or cold line storage, so this change should be okay. However, there is a requirement on who can download the files that I am concerned about. > Note that for such uploads, crcmod is required for downloading regardless of whether the parallel composite upload option is on or not. For some distributions this is easy (e.g., it comes pre-installed on macOS), but in other cases some users have found it difficult. Because of this, at present parallel composite uploads are disabled by default. Google is actively working with a number of the Linux distributions to get crcmod included with the stock distribution. Once that is done we will re-enable parallel composite uploads by default in gsutil.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251:119,down,download,119,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251,2,['down'],"['download', 'downloading']"
Availability,"We end up getting exceptions like this, happens every time we get to the end of the file:; ```; RuntimeException: error reading tabix-indexed file src/test/resources/gvcfs/recoding/HG00187.hg38.g.vcf.gz: i=0, curOff=2469855232, expected=2468020224; ```; These virtual offsets correspond to these pairs of absolute|decompressed offset; ```; 2469855232 => 37687|0; 2468020224 => 37659|0; ```; Looking at these, they differ by 28, which is the size of the empty bgzip block at the end. And so when we refresh: we go too far.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-676615090:114,error,error,114,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676615090,1,['error'],['error']
Availability,We fixed all the known issues and install-editable now seems reliable and fast.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13691#issuecomment-1758609203:61,reliab,reliable,61,https://hail.is,https://github.com/hail-is/hail/pull/13691#issuecomment-1758609203,1,['reliab'],['reliable']
Availability,"We have a CI problem leading to random erroneous test failures, and the system isn't designed to run the tests twice for a single commit. Can you add a new commit to the PR with `git commit -m ""bump"" --allow-empty`?; Sorry!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6937#issuecomment-524832178:54,failure,failures,54,https://hail.is,https://github.com/hail-is/hail/pull/6937#issuecomment-524832178,1,['failure'],['failures']
Availability,"We have a difference of opinion about the risks involved in using whatever; compiler happens to show up as $(CXX); to try to compile arbitrarily large auto-generated C++ files, and maybe; about what happens when that fails; and gives an error message about something in the middle of 12000 lines of; code that bears no obvious relationship; to what the user is doing. Or when that compiler takes 15 minutes to; compile it. It's the C++ equivalent of; the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; it but the code gives the wrong answers; because that particular compiler has a bug, and we never tested the; combination of our codegen with *that*; compiler/version. A couple of years ago I was seeing g++ take 40-60 seconds to compile; something that clang did in 2 seconds; (fairly heavily templated code generated for an SQL query, so very much in; the same ballpark as parts of Hail),; which contributes to my concern about this, especially on linux where g++; is the default. So in the long run I expect we'll ship a compiler, or specify a compiler.; But that becomes a problem in itself; if we want the shipped compiler to work on a variety of OS'es. When I did; that before it was all Ubuntu-14.04; and Ubuntu-16.04, and it was manageable to build it for two platforms. On Thu, Aug 2, 2018 at 9:59 PM cseed <notifications@github.com> wrote:. > *@cseed* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207422997>:; >; > > +}; > +; > +std::string strip_suffix(const std::string& s, const char* suffix) {; > + size_t len = s.length();; > + size_t n = strlen(suffix);; > + if ((n > len) || (strncmp(&s[len-n], suffix, n) != 0)) return s;; > + return std::string(s, 0, len-n);; > +}; > +; > +std::string get_cxx_name() {; > + char* p = ::getenv(""CXX"");; > + if (p) return std::string(p);; > + // We prefer clang because it has faster compile; > + auto s = run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709:237,error,error,237,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709,1,['error'],['error']
Availability,"We narrowed it down to star alleles. I think @lfrancioli has a PR for it, but not sure if it's through yet?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1518#issuecomment-287838501:15,down,down,15,https://hail.is,https://github.com/hail-is/hail/issues/1518#issuecomment-287838501,1,['down'],['down']
Availability,"We needed to await cancelled tasks to handle the error that was raised inside the task. Otherwise, I think what would happen is the unhandled cancelled error would cause an exception to be thrown when the main thread finally returned (file finished downloading) because errors in the cancelled tasks weren't ignored. For reference, the original error was this:. ```; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x7fe17e8c4bd0>; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/local/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/site-packages/batch/copy/__main__.py"", line 34, in <module>; asyncio.run(main()); File ""/usr/local/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); concurrent.futures._base.CancelledError; sys:1: RuntimeWarning: coroutine 'retry_transient_errors' was never awaited; RuntimeWarning: Enable tracemalloc to get the object allocation traceback; ```. https://stackoverflow.com/questions/57089430/asyncio-task-cancel-is-is-synchronous. It's possible I'm wrong and this code doesn't do anything. If that's the case, then I need additional help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10534#issuecomment-853098749:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/pull/10534#issuecomment-853098749,5,"['down', 'error']","['downloading', 'error', 'errors']"
Availability,We now mock this test so we've lost the ability to specifically diagnose this error. Hopefully this is useful if a similar bug crops up in the future.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13951#issuecomment-2214404544:78,error,error,78,https://hail.is,https://github.com/hail-is/hail/issues/13951#issuecomment-2214404544,1,['error'],['error']
Availability,"We now use an API token linked to hailgenetics instead of password auth. In order to do the org request, Daniel set up 2FA. There's no way to set up 2FA. There are recovery codes in the ""usual place"". No one besides Daniel can currently log into hailgenetics PyPI account as a result (other than using recovery codes).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13598#issuecomment-1808759767:164,recover,recovery,164,https://hail.is,https://github.com/hail-is/hail/issues/13598#issuecomment-1808759767,2,['recover'],['recovery']
Availability,"We only copy if the image isn't already present. I think this code is not what you want in mirror images. We should copy if the contents change as well. ```; copy_if_not_present() {; src_image=$1; dest_image=$2; if ! skopeo inspect ""docker://docker.io/$1"";; then; echo ""$1 does not exist yet, doing nothing""; elif skopeo inspect ""docker://$2"";; then; echo ""$2 already exists, doing nothing""; else; echo ""$2 does not exist, copying $1 to $2""; copy_image $1 $2; fi; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13536#issuecomment-1703073465:264,echo,echo,264,https://hail.is,https://github.com/hail-is/hail/pull/13536#issuecomment-1703073465,3,['echo'],['echo']
Availability,"We recently encountered jobs that failed due to syntax errors in the shell script generated by Hail, stemming from code such as. ```python; job.command('touch before'); job.command('\n'.join(f'echo {shlex.quote(msg)}' for msg in messages)); job.command('touch after'); ```. Occasionally `messages` is an empty list, so this evaluates to `job.command('')` and the eventual shell script submitted by Hail contains. ```sh/bin/bash' '-c' '; …; {; touch before; }; {. }; {; touch after; }; …; ```. Shell compound commands like `{ … }` must contain at least one command, so this is a syntax error. Empty commands could be rewritten to generate e.g. [`:`](https://pubs.opengroup.org/onlinepubs/9799919799/utilities/V3_chap02.html#tag_19_17) such as. ```sh; …; {; :; }; …; ```. but it seems easier and probably less surprising to just omit them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2372547180:55,error,errors,55,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2372547180,3,"['echo', 'error']","['echo', 'error', 'errors']"
Availability,"We recently put in a couple PRs that improve performance on these searches so thought I would update here. They were mostly changes to things upstream of the portion of code we have been focusing on and change how data is initially read in, but the biggest performance gain we got was adding `hl._set_flags(use_new_shuffle='1')`. A lot of the focus was around how we handle searches in multiple data types which has been out of the scope of this work so far, so for the search we've been profiling here its only came down to like 80 seconds, but figured its worth sharing. Hopefully this does not cause to catastrophic of a merge conflict for you guys. https://github.com/broadinstitute/seqr/pull/3873; https://github.com/broadinstitute/seqr/pull/3876",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1945174111:517,down,down,517,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1945174111,1,['down'],['down']
Availability,"We should add docs that describe how to do this to:; 1. `hl.default_reference`, obviously; 2. Deprecate the `reference_genome` parameter to `hl.init` and instruct users to use `hl.default_reference`. Inform that this parameter has confusing interactions with ReferenceGenome, so we're removing it.; 3. `hl.ReferenceGenome.__init__` should refer users to that. . I think we should also make a separate PR that improves the `hl.import_vcf` error message. If the backend throws an error like; ```; HailException: Invalid locus '1:249367215' found. Position '249367215' is not within the range [1-249250621] for reference genome 'GRCh37'.; ```; `import_vcf` should catch and wrap with another exception that suggests you use a `reference_genome` parameter or `hl.default_reference`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13856#issuecomment-1771234741:438,error,error,438,https://hail.is,https://github.com/hail-is/hail/issues/13856#issuecomment-1771234741,2,['error'],['error']
Availability,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:1476,mainten,maintenance,1476,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073,1,['mainten'],['maintenance']
Availability,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:345,failure,failure,345,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838,2,['failure'],['failure']
Availability,We tried updated to zstd-jni 1.5.5-11 from 1.5.5-2. 4 Failures. [6873](https://batch.hail.is/batches/8093977/jobs/6873) execute(...)_stage2_table_native_writer_job4933	Failed		13s 631ms	$0.0001; [7157](https://batch.hail.is/batches/8093977/jobs/7157) execute(...)_stage2_table_native_writer_job5217	Failed		15s 919ms	$0.0001; [8854](https://batch.hail.is/batches/8093977/jobs/8854) execute(...)_stage2_table_native_writer_job6914	Failed		1 minute 12s	$0.0006; [12795](https://batch.hail.is/batches/8093977/jobs/12795) execute(...)_stage2_table_native_writer_job10855	Failed		21s 305ms	$0.0002,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1843738845:54,Failure,Failures,54,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1843738845,1,['Failure'],['Failures']
Availability,"We used a one off script, an attempt was made to use `Copier.copy`, but that wasn't reliable enough. We also needed to rename destination files beyond what the sync (or copy) tool is capable of.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14601#issuecomment-2214648226:84,reliab,reliable,84,https://hail.is,https://github.com/hail-is/hail/issues/14601#issuecomment-2214648226,1,['reliab'],['reliable']
Availability,"We weren't actually running the tests in QoB previously. I enabled the tests in #14062, but they all still passed even with the error. ```; PASSED test/hail/methods/relatedness/test_identity_by_descent.py::test_ibd_default_arguments; PASSED test/hail/methods/relatedness/test_identity_by_descent.py::test_ibd_does_not_error_with_dummy_maf_float64; PASSED test/hail/methods/relatedness/test_identity_by_descent.py::test_ibd_0_and_1; PASSED test/hail/methods/relatedness/test_identity_by_descent.py::test_ibd_does_not_error_with_dummy_maf_float32; ```. My guess is our test suite isn't robust enough. I don't think we test with any family relationships -- all unrelated samples.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14052#issuecomment-1839163100:128,error,error,128,https://hail.is,https://github.com/hail-is/hail/issues/14052#issuecomment-1839163100,2,"['error', 'robust']","['error', 'robust']"
Availability,We've been seeing an error for a little while which may have some similarities to this: [Zulip](https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/SocketException.20when.20writing.20Table/near/355702095),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12980#issuecomment-1534318489:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/issues/12980#issuecomment-1534318489,1,['error'],['error']
Availability,"We've recently updated from 0.2.126ish to 0.2.130ish, and encountered some teething issues with the new (to us) metadata server. Jobs using `gsutil` failed as their attempts to get credentials from the server resulted in 404. There seems to have been two problems:. 1. As shown (also via `curl`) in [batch 454410](https://batch.hail.populationgenomics.org.au/batches/454410/jobs/1), our `gsutil` queried for `http://169.254.169.254/computeMetadata/v1/instance/service-accounts` (without a final `/`) which resulted in a 404. I don't know if there's a more elegant way for the server to accept both, rather than just adding a route with and without. 2. With that fixed, [batch 454418](https://batch.hail.populationgenomics.org.au/batches/454418/jobs/1) shows a failure within `GetInstanceScopes()`. This is failing because the metadata server does not implement the `…/scopes` endpoint. PR #14019 implemented only so much as is needed for `hail` and `gcloud` to get access tokens for hail GSAs so they can then make API calls to GCS or Hail Batch, but we seem to have needed a bit more. Not sure why you didn't encounter this yourselves: possibly sufficiently different versions of `gsutil` or the cloud SDK, or perhaps you are better at remembering to use `gcloud` rather than `gsutil` than we are!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331:760,failure,failure,760,https://hail.is,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331,1,['failure'],['failure']
Availability,We've seen this error before on other deployments with no easy fix. I'll continue to investigate over the next couple days and get back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-256194596:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-256194596,1,['error'],['error']
Availability,"Weird, I can't replicate the `testShuffleIR` failure any more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664632027:45,failure,failure,45,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664632027,1,['failure'],['failure']
Availability,"Welcome @ryerobinson, and thanks for the Pull Request! Could you report exactly the error that you saw when building hail? `sys_platform!='win32'` is necessary to install the dependencies on Windows, so there is probably an alternative solution to the error that works for all platforms.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1230330783:84,error,error,84,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1230330783,2,['error'],['error']
Availability,"Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816:113,error,error,113,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816,1,['error'],['error']
Availability,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:145,error,error,145,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668,1,['error'],['error']
Availability,"Were you using the old copier or the new (not yet merged) `hailctl fs sync`? I had hoped the latter was finally robust enough for real use. `hailtop.aiotools.copy` is indeed not very reliable. Regardless, using the rewrite action when the source and destination agree is the correct move.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14601#issuecomment-2214474849:112,robust,robust,112,https://hail.is,https://github.com/hail-is/hail/issues/14601#issuecomment-2214474849,2,"['reliab', 'robust']","['reliable', 'robust']"
Availability,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:23,mainten,maintenance,23,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338,4,"['down', 'error', 'failure', 'mainten']","['down', 'error', 'failure', 'maintenance']"
Availability,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233007740:89,error,error-catching,89,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740,1,['error'],['error-catching']
Availability,"What do you think about converting `Job.regions()` to `Job.tolerations(regions=['us-central1'], pools=['standard', 'highmem'])` where we don't support the `pools` option yet. I'm not in love with this, but it is a different option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1269917803:59,toler,tolerations,59,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1269917803,1,['toler'],['tolerations']
Availability,What does the error look like in Python? Can we add that to transient_errors?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11329#issuecomment-1032033995:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/11329#issuecomment-1032033995,1,['error'],['error']
Availability,"What errors?. I'll run CI on this, but I don't expect the spark 2 stuff to have any problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701068245:5,error,errors,5,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701068245,1,['error'],['errors']
Availability,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:338,avail,available,338,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425,2,['avail'],['available']
Availability,What is the output of this script?. ```; echo $HAIL_HOME; echo $PYTHONPATH; echo $SPARK_CLASSPATH; ```. This might be caused by an incorrect compilation of hail. The output of the above script will tell us more about what to check next.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337352607:41,echo,echo,41,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337352607,3,['echo'],['echo']
Availability,What were the errors? It should be OK to hold the job object around and e.g. use it to ask for logs even if the job is deleted.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475664544:14,error,errors,14,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475664544,1,['error'],['errors']
Availability,Whatever is failing here is likely different from the interval pipeline failures seen in https://github.com/hail-is/hail/issues/13748 and related tickets because GVS team has confirmed that 0.2.126 reduces peak RAM usage from >50GB to 11GB.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1791064886:72,failure,failures,72,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1791064886,1,['failure'],['failures']
Availability,"When CI is testing itself, it will start pods in `test`, but `test` is missing the user secret. ```; (hail) dking@wmb16-359 # k get secrets user-jwt-vkqfw -n test ; Error from server (NotFound): secrets ""user-jwt-vkqfw"" not found; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046:165,Error,Error,165,https://hail.is,https://github.com/hail-is/hail/pull/5897#issuecomment-484122046,1,['Error'],['Error']
Availability,"When I attempt to rewrite `transform` to use a function, I'm getting a parser error. ```; is.hail.utils.HailException: no conversion found for __uid_1(struct{locus: locus<GRCh38>, alleles: array<str>, rsid: str, qual: float64, filters: set<str>, info: struct{BaseQRankSum: float64, ClippingRankSum: float64, DP: int32, END: int32, ExcessHet: float64, MQ: float64, MQRankSum: float64, MQ_DP: int32, QUALapprox: int32, RAW_MQ: float64, ReadPosRankSum: float64, VarDP: int32}, __entries: array<struct{AD: array<int32>, DP: int32, GQ: int32, GT: call, MIN_DP: int32, PGT: call, PID: str, PL: array<int32>, SB: array<int32>}>}); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.expr.ir.package$.invoke(package.scala:76); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:751); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:718); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.named_value_ir(Parser.scala:500); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$$anonfun$named_value_irs$1.apply(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.hail.expr.ir.IRParser$.named_value_irs(Parser.scala:495); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:714); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:943); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$$anonfun$table_ir_children$1.apply(Parser.scala:846); 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:282); 	at is.ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:78,error,error,78,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718,3,"['Error', 'error']","['ErrorHandling', 'error']"
Availability,"When I run the current GRM with BlockMatrix locally on profile225.hardcalls.vds (2535 samples, 225k variants), I get:. ```; java.lang.ArrayIndexOutOfBoundsException: 1048578; ```. When I run on profile.hardcalls.vds (only 25k variants), I get:. ```; java.lang.OutOfMemoryError: Java heap space; ```. When I cut profile down to only 10k variants, grm takes about 66s:. ```; read: 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:319,down,down,319,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,2,['down'],['down']
Availability,"When I try applying this patch:. ```diff; diff --git a/batch/batch/worker/worker.py b/batch/batch/worker/worker.py; index 9d3d89498b..a9106b7a60 100644; --- a/batch/batch/worker/worker.py; +++ b/batch/batch/worker/worker.py; @@ -1228,22 +1228,28 @@ class Container:; return config; ; async def _get_in_container_user(self) -> Tuple[int, int]:; + # https://docs.docker.com/engine/reference/builder/#user; assert self.image.image_config; user = self.image.image_config['Config']['User']; if not user:; return 0, 0; if "":"" in user:; - uid, gid = user.split("":""); - else:; - uid, gid = await self._read_user_from_rootfs(user); - return int(uid), int(gid); + user, group = user.split("":""); + try:; + return int(user), int(group); + except ValueError:; + return await self._read_uid_gid_from_rootfs(user); + try:; + return int(user), 0; + except ValueError:; + return await self._read_uid_gid_from_rootfs(user); ; - async def _read_user_from_rootfs(self, user) -> Tuple[str, str]:; + async def _read_uid_gid_from_rootfs(self, user: str) -> Tuple[int, int]:; with open(f'{self.image.rootfs_path}/etc/passwd', 'r', encoding='utf-8') as passwd:; for record in passwd:; if record.startswith(user):; _, _, uid, gid, _, _, _ = record.split("":""); - return uid, gid; + return int(uid), int(gid); raise ValueError(""Container user not found in image's /etc/passwd""); ; def _mounts(self, uid: int, gid: int) -> List[MountSpecification]:; ```. I can run the container successfully but it fails with the following error:. ```; mkdir: cannot create directory '/io/batch': Permission denied; ```. Which is caused by the following line inserted by the `hailtop.batch` client library:. ```; mkdir -p /io/batch/4c8107/NjztN; ```. so images with non-existent users would fail to run on batch regardless of whether we were using crun / docker / podman, because Batch assumes that the user in the container will have permission to write to `/io`, regardless of whether the job requires input/output files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13679#issuecomment-1728424448:1495,error,error,1495,https://hail.is,https://github.com/hail-is/hail/issues/13679#issuecomment-1728424448,1,['error'],['error']
Availability,"When I was writing `hailctl batch init`, I used that method to parse the remote_tmpdir to check if the bucket existed and to add permissions. I kept getting errors because my remote_tmpdir was `gs://hail-jigold-random-token` and I didn't specify a path name beyond the bucket name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13435#issuecomment-1679759856:157,error,errors,157,https://hail.is,https://github.com/hail-is/hail/pull/13435#issuecomment-1679759856,1,['error'],['errors']
Availability,"When the kube event loop fails, it should be restarted by `run_forever`. Understanding why we lose connection to k8s seems fruitful and important, but does not explain why batch becomes unstable / non-communicative after a kube event loop failure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450915405:239,failure,failure,239,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450915405,1,['failure'],['failure']
Availability,When will it be available to use?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14375#issuecomment-1972078374:16,avail,available,16,https://hail.is,https://github.com/hail-is/hail/pull/14375#issuecomment-1972078374,1,['avail'],['available']
Availability,"Which exception was being masked?. We currently use this `deserialize` function to construct reader/writer classes, like MatrixVCFReader. This class does a bunch of work on construction, including throwing user-facing errors. Wrapping these errors in a `MappingException` (which becomes the top-level error, and the one in the summary in Python) is wrong. Obscuring full stack traces is wrong too. The correct thing is to stop doing a bunch of work on class construction, but until we make that change, I think that right now, we should continue peeling off the mapping exception",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361:26,mask,masked,26,https://hail.is,https://github.com/hail-is/hail/pull/6156#issuecomment-494809361,4,"['error', 'mask']","['error', 'errors', 'masked']"
Availability,"While this seems to currently be a functioning improvement on what we had before, it's still slower than I'd like it to be for Jacob's use case and getting unexpected OOM errors, working on it now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6723#issuecomment-515481261:171,error,errors,171,https://hail.is,https://github.com/hail-is/hail/pull/6723#issuecomment-515481261,1,['error'],['errors']
Availability,"Whoa, it worked. I included one change that might have warranted re-review. I was getting errors becomes some Jobs, on which delete had been called, were still being used. I tracked it down to a recent cancel => delete change in `PR.update_from_completed_batch_job`. If look at that function, it is clear delete is not OK because in several cases the build object keep a handle to the job. I reverted it, and now clear all the fields of Job when it is deleted. https://github.com/hail-is/hail/pull/5655/files#diff-433f83d97fa8a526a3f8cff52590e422R479; https://github.com/hail-is/hail/pull/5655/files#diff-0c1f876ad25335b076837f768f727566R59",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180:90,error,errors,90,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475474180,4,"['down', 'error']","['down', 'errors']"
Availability,Why did you need to add back `downcastToPK` and `upcast`? I tried to make those unnecessary with `KeyedOrderedRVD`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3186#issuecomment-375789138:30,down,downcastToPK,30,https://hail.is,https://github.com/hail-is/hail/pull/3186#issuecomment-375789138,1,['down'],['downcastToPK']
Availability,Why do delete and query throw errors whenever the path is non-empty? Is there some magic recursion happening to prune to move to the end of the path?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1007#issuecomment-257220862:30,error,errors,30,https://hail.is,https://github.com/hail-is/hail/pull/1007#issuecomment-257220862,1,['error'],['errors']
Availability,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:27,toler,tolerations,27,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209,5,['toler'],"['tolerate', 'tolerates', 'toleration', 'tolerations']"
Availability,Why would we want the behavior to be that a user has to explicitly cancel batches on a failure? This code already does that:. ```python3; async def async_result_or_cancel_all(future):; try:; return await future.async_result(timeout=timeout); except Exception as err:; for fut in futures:; fut.cancel(); raise err; if chunksize > 1:; return (val; for future in futures; for val in await async_result_or_cancel_all(future)); return (await async_result_or_cancel_all(future); for future in futures); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10761#issuecomment-894501440:87,failure,failure,87,https://hail.is,https://github.com/hail-is/hail/pull/10761#issuecomment-894501440,1,['failure'],['failure']
Availability,"With Hail 0.2-721af83bc30a, it is now getting a java heap space error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:64,error,error,64,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,2,"['avail', 'error']","['available', 'error']"
Availability,"With [this code](https://github.com/hail-is/hail/blob/81b0b20ea96278ca74c17b7618d53c330a5ee15f/hail/src/main/scala/is/hail/backend/service/Worker.scala#L182) commented out, error looks like [this](https://gist.github.com/iris-garden/33e7b0508f06f7e6aa4c17bd8c4d92b7)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12468#issuecomment-1834392857:173,error,error,173,https://hail.is,https://github.com/hail-is/hail/pull/12468#issuecomment-1834392857,1,['error'],['error']
Availability,"With google, we're only seeing 429s, our transient error code has some of the cases:. https://github.com/hail-is/hail/blob/2e9ca86bfdca9cb29ae18cf5cecbe04828bb4bd3/hail/python/hailtop/utils/utils.py#L642-L645",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14595#issuecomment-2201432395:51,error,error,51,https://hail.is,https://github.com/hail-is/hail/issues/14595#issuecomment-2201432395,1,['error'],['error']
Availability,"Working backwards, we need to not return a 500 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:50,error,error,50,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,10,['error'],['error']
Availability,"Working on failure in takeByAnnotationAnnotation. Error is segmentation fault, triggered by presence of null values, in either key or value position of Row(). Reverting recent RVB and SRVB changes did not fix, so issue does not appear to be there (also no uses of .pType in these files)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8142#issuecomment-590029353:11,failure,failure,11,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-590029353,3,"['Error', 'failure', 'fault']","['Error', 'failure', 'fault']"
Availability,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:116,error,error,116,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536,1,['error'],['error']
Availability,"Wow `deploy_auth` failed with this:. ```; /controller.sh: line 35: 7 Segmentation fault (core dumped) ""${COMMAND[@]}""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14382#issuecomment-1971696481:82,fault,fault,82,https://hail.is,https://github.com/hail-is/hail/pull/14382#issuecomment-1971696481,1,['fault'],['fault']
Availability,Wow this is supremely annoying. https://github.com/jupyter/notebook/issues/3397 Jupyter is basically known to be broken for the normal use case of most Asyncio libraries. The recommended fix is to use a third party monkey patch. I'll revisit this if 1kg download continue to be a frequent issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8218#issuecomment-599765193:254,down,download,254,https://hail.is,https://github.com/hail-is/hail/pull/8218#issuecomment-599765193,1,['down'],['download']
Availability,"Wow, talk about a *tour de force* of debugging, well done!!. ---. OK, so this kinda makes sense. We are importing our own copies of the GCS libraries and renaming them all to `is.hail.relocated....`. We do this so that we're not stuck with whatever version Dataproc is including. We pin our dataproc image version to `2.1.2-debian11` (see [here](https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/start.py#L147)) which [was released in January 2023](https://cloud.google.com/dataproc/docs/release-notes#January_23_2023). The latest available version of [Dataproc's Debian images](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) is 2.1.25-debian11 which depends on GoogleCloudDataproc hadoop connector version [2.2.15](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/2.2.15) which relies on Google Cloud Storage client library version [2.22.3](https://github.com/GoogleCloudDataproc/hadoop-connectors/commit/8b79f025ef5e8231de827f4c620cd23e230c3489). I have [a PR](https://github.com/hail-is/hail/pull/13732) to upgrade us to 2.27.1 because the library broke retries in versions [2.25.0, 2.27.0). AFAICT, Google's image version page only shows the most recent five. There's no way to go back further in time. Luckily, the way back machine has [a March 2023 capture](https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) which includes our version. 2.1.2-debian11 used Google Cloud Dataproc hadoop connector version [2.2.9](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.9) This version of the hadoop connector was [using some alpha version of a gRPC version of the cloud storage library](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/18f6e9f1c745e1854d76bea9362e2332898d8895/pom.xml#L96C1-L97C1). I'm not sure what's up with that. OK, here's my proposal: let's change that IMAGE_VERSION to the latest one ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645:560,avail,available,560,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645,1,['avail'],['available']
Availability,"Ya I was wondering too which is why I tried this out. It was helpful in observing how much the number of files is the bottleneck for the copy tool. Takes between 15-20 seconds to copy down all 5k files in the repo while jobs that copy more MB but in fewer files are [much better](https://ci.hail.is/batches/6235246/jobs/52). I don't exactly prefer how, but it feels silly to spend 25% of the time of `auth_image` downloading the entire repo when you need just the auth directory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12371#issuecomment-1289877495:184,down,down,184,https://hail.is,https://github.com/hail-is/hail/pull/12371#issuecomment-1289877495,2,['down'],"['down', 'downloading']"
Availability,"Yea, binary incompatibility is as good a guess as any. I'm sure it's this commit though. What's the best way to break down the commit into likely culprits that I could test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-429132148:118,down,down,118,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429132148,1,['down'],['down']
Availability,Yeah I think this doesn't much matter once we've eliminated uses of netlib. Hail will get a link error instead of falling back on hopelessly slow non-native libraries.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5040#issuecomment-570319184:97,error,error,97,https://hail.is,https://github.com/hail-is/hail/issues/5040#issuecomment-570319184,1,['error'],['error']
Availability,"Yeah but it would really mess with debugging. I think probably still worth keeping the `unify_exprs` and `TypeError` lines (it'll be a little redundant, but it's just type checking so shouldn't be slow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7088#issuecomment-533099406:142,redundant,redundant,142,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-533099406,1,['redundant'],['redundant']
Availability,"Yeah, I think I agree that we have a unique UUID. What I'm more skeptical of is: what if we throw an exception in a `write`? Do we clean up all the open resources? If not, that could totally leave a writer open that will conflict when we retry (even if we retried on a different VM!). ---. `retryTransientErrors` expects partition code to be safe to execute twice, but otherwise it's quite simple:. ```scala; def retryTransientErrors[T](f: => T): T = {; var delay = 0.1; var errors = 0; while (true) {; try {; return f; } catch {; case e: Exception =>; errors += 1; if (errors == 1 && isRetryOnceError(e)); return f; if (!isTransientError(e)); throw e; if (errors % 10 == 0); log.warn(s""encountered $errors transient errors, most recent one was $e""); }; delay = sleepAndBackoff(delay); }. throw new AssertionError(""unreachable""); }; ```; and this is the call site:; ```scala; val htc = new ServiceTaskContext(i); var result: Array[Byte] = null; var userError: HailException = null; try {; retryTransientErrors {; result = f(context, htc, theHailClassLoader, fs); }; } catch {; case err: HailException => userError = err; }; htc.close(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331:475,error,errors,475,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331,12,['error'],['errors']
Availability,"Yeah, I'm working on smaller images in down time between other meetings",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9481#issuecomment-700750562:39,down,down,39,https://hail.is,https://github.com/hail-is/hail/pull/9481#issuecomment-700750562,1,['down'],['down']
Availability,"Yeah, agreed. I've never taken inventory of the full test suite. This was the first bug that caused an error message though, previous ones have just been about things being too slow. So we'll need benchmarks to catch that stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939:103,error,error,103,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939,1,['error'],['error']
Availability,"Yeah, it says no such method found. One of many bad expr error messages",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/846#issuecomment-249639159:57,error,error,57,https://hail.is,https://github.com/hail-is/hail/issues/846#issuecomment-249639159,1,['error'],['error']
Availability,"Yeah, no trailing bin. I think we can sanity-check the SPARK_HOME setting in the HailContext constructor to give a more informative error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319707620:132,error,error,132,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319707620,1,['error'],['error']
Availability,"Yeah, one test fails.`test_annotate_col_agg_lowering`. This error is encountered on the worker. It gets raised there, then the driver encounters it and re-raises it. That's how we ultimately see it on the client-side.; ```; E HailException: Premature end of file: expected 4 bytes, found 0; E is.hail.utils.HailException: Premature end of file: expected 4 bytes, found 0; E 	at is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:17); E 	at is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:17); E 	at is.hail.utils.package$.fatal(package.scala:78); E 	at is.hail.utils.richUtils.RichInputStream$.readFully$extension1(RichInputStream.scala:13); E 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:546); E 	at is.hail.io.LZ4InputBlockBuffer.readBlock(InputBuffers.scala:584); E 	at is.hail.io.BlockingInputBuffer.readBlock(InputBuffers.scala:382); E 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:388); E 	at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:405); E 	at is.hail.io.LEB128InputBuffer.readByte(InputBuffers.scala:217); E 	at is.hail.io.LEB128InputBuffer.readInt(InputBuffers.scala:223); E 	at __C3100collect_distributed_array.__m3103INPLACE_DECODE_r_int32_TO_r_int32(Unknown Source); E 	at __C3100collect_distributed_array.__m3147INPLACE_DECODE_r_struct_of_r_int32ANDr_int64END_TO_r_struct_of_r_int32ANDr_int64END(Unknown Source); E 	at __C3100collect_distributed_array.__m3146INPLACE_DECODE_r_array_of_r_struct_of_r_int32ANDr_int64END_TO_r_dict_of_r_int32ANDr_int64(Unknown Source); E 	at __C3100collect_distributed_array.__m3141DECODE_r_struct_of_r_int32ANDr_array_of_r_int32ANDr_float64ANDr_array_of_r_float64ANDr_int64ANDr_array_of_r_struct_of_r_int32ANDr_int64ENDEND_TO_SBaseStructPointer(Unknown Source); E 	at __C3100collect_distributed_array.__m3128split_StreamFor_region24_75(Unknown Source); E 	at __C3100collect_distributed_array.__m3128split_StreamFor(Unknown Source); E 	at __C3100collect_distributed_array.__m3125begin_group_0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11681#issuecomment-1079785317:60,error,error,60,https://hail.is,https://github.com/hail-is/hail/pull/11681#issuecomment-1079785317,5,"['Error', 'error']","['ErrorHandling', 'error']"
Availability,"Yeah, sorry about this. I've been working on de novo stuff on the side and am really unhappy about the current trio abstraction (pasting a bunch of code in from Mendel errors). I'll review as it is now, though, so we can get this in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-257355964:168,error,errors,168,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-257355964,1,['error'],['errors']
Availability,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615,1,['error'],['error']
Availability,"Yeah, that was a weird failure. I wonder if a node got preempted and createDatabase2 is not idempotent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7863#issuecomment-573896406:23,failure,failure,23,https://hail.is,https://github.com/hail-is/hail/pull/7863#issuecomment-573896406,1,['failure'],['failure']
Availability,"Yeah, there is some bug in `import_vcf -> _same` checkpointing before filter also fixes it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11450#issuecomment-1061869461:49,checkpoint,checkpointing,49,https://hail.is,https://github.com/hail-is/hail/pull/11450#issuecomment-1061869461,1,['checkpoint'],['checkpointing']
Availability,"Yeah, this PR really shouldn't have introduced the kinds of failures we're seeing. That seems to be the refrain of all of my lowering refactoring PRs though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7567#issuecomment-557024561:60,failure,failures,60,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-557024561,1,['failure'],['failures']
Availability,"Yeah. Really odd that prometheus doesn't somehow signal that it needs more memory, or exits as failure. It just sits and spins.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6773#issuecomment-517021923:95,failure,failure,95,https://hail.is,https://github.com/hail-is/hail/issues/6773#issuecomment-517021923,1,['failure'],['failure']
Availability,Yep! Loaded it up and it worked without error. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3015#issuecomment-373027311:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/issues/3015#issuecomment-373027311,1,['error'],['error']
Availability,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:130,down,downstream,130,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406,1,['down'],['downstream']
Availability,"Yep, just waiting on final (unrelated) fixes to go through on the v3.1.1; nuclear variant release files before pinging them about copying over all; our changed files. On Wed, Mar 10, 2021 at 3:29 PM Dan King ***@***.***> wrote:. > I'll wait for Grace to chime in about the desired public location.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/10169#issuecomment-796049879>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABNSGHRVFLHDPK26N3MKTXLTC7CBTANCNFSM4Y42HNKA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-797031640:111,ping,pinging,111,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-797031640,1,['ping'],['pinging']
Availability,"Yes the errors were fatal, but using non-preemtible instances fixed it, so will close this issue. Thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6635#issuecomment-513887926:8,error,errors,8,https://hail.is,https://github.com/hail-is/hail/issues/6635#issuecomment-513887926,1,['error'],['errors']
Availability,Yes with the caveat that I was going to look at the worker logs in the PR test namespace just to make sure there were no hidden errors that would be problematic. I broke the logging query generator -- fixed in #13813 -- and didn't think this was urgent. I'll take a look at the logs now without that fix in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769106759:128,error,errors,128,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769106759,1,['error'],['errors']
Availability,Yes! I definitely want to assert no errors in the logs. I think we're very close to being able to assert that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956533915:36,error,errors,36,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956533915,1,['error'],['errors']
Availability,"Yes, I looked into this, it is a transient error that should be retried. I will change batch2 to use the retry infrastructure after this PR goes in: https://github.com/hail-is/hail/pull/7284",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7287#issuecomment-542232729:43,error,error,43,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-542232729,1,['error'],['error']
Availability,"Yes, I receive the same bug today, when I follow the instruction: https://hail.is/docs/0.2/install/other-cluster.html ; I believe this is a bug caused by pip or setuptools. If you downgrade the pip to 8.1.1, you will get another bug. . > cd build/deploy; python3 setup.py -q sdist bdist_wheel; sed '/^pyspark/d' python/requirements.txt | xargs python3 -m pip install -U; Exception:; Traceback (most recent call last):; File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/basecommand.py"", line 209, in main; status = self.run(options, args); File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/commands/install.py"", line 287, in run; wheel_cache; File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/basecommand.py"", line 270, in populate_requirement_set; wheel_cache=wheel_cache; File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/req/req_install.py"", line 230, in from_line; wheel_cache=wheel_cache, constraint=constraint); File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/req/req_install.py"", line 77, in __init__; req = pkg_resources.Requirement.parse(req); File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 3036, in parse; req, = parse_requirements(s); ValueError: not enough values to unpack (expected 1, got 0)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-826316427:180,down,downgrade,180,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-826316427,1,['down'],['downgrade']
Availability,"Yes, PR #137 treats the hemizygous Y case as well. As with Plink, I do not consider hets at homozygous sites to be Mendel errors, but rather sequencing errors. Users should deal with them separately as they see fit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/91#issuecomment-235397652:122,error,errors,122,https://hail.is,https://github.com/hail-is/hail/issues/91#issuecomment-235397652,2,['error'],['errors']
Availability,"Yes, it failed on master. It was failing in OrderedRVDType, because the key `idx` didn't match the partition key `[idx, a]`. The error only happens if `idx` is in both the old and new key.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3749#issuecomment-396753521:129,error,error,129,https://hail.is,https://github.com/hail-is/hail/pull/3749#issuecomment-396753521,1,['error'],['error']
Availability,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:39,failure,failure,39,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575,4,"['error', 'failure']","['errors', 'failure']"
Availability,"Yes. > seems odd to put null as position. I agree. That's why we need your new IR with no positions!. > Will this change slow things down unnecessarily if drop_samples is not used immediately after a read?. Yes, possibly a little, since FilterSamples needs to handle the case where not all samples are filtered. Options: specialize the case in FilterSamples when the condition is false (or true -- no-op) or add a DropSamples node. I prefer the former. FilterSamples is somewhat better in the coming OrderedRDD2/RegionValueBuilder stuff. The implementation has almost no overhead for when the condition is false. I say leave this as is and we'll pick that up when my later PRs go in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2285#issuecomment-336261462:133,down,down,133,https://hail.is,https://github.com/hail-is/hail/pull/2285#issuecomment-336261462,1,['down'],['down']
Availability,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:529,avail,available,529,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989,2,['avail'],['available']
Availability,"Yes. it runs fine on our regular cluster locally. As a workaround, we; are qsubing our hail jobs for each chromosome with --master local[16]; reading the matrix tables stored on our gpfs system. Those are running; fine on the UK Biobank data. When we try to running the same job with; master=yarn on the hadoop cluster that error occurs. An earlier version of; Hail (before the GLIBC error) was running fine on the Hadoop cluster. John. On Wed, Dec 19, 2018 at 6:28 PM Tim Poterba <notifications@github.com>; wrote:. > sorry to lose this! I think we're not as good at keeping on top of issue; > comments as forum posts/zulip/etc.; >; > I have no idea what this is coming from, never seen something like this; > before.; >; > Can you run pyspark stuff normally?; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/4733#issuecomment-448783924>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AB3rDZkqSvk3zcRuS41UEh_xHLQ5ccabks5u6sk1gaJpZM4YPdeC>; > .; >. -- ; John Farrell, Ph.D.; Biomedical Genetics-Evans 218; Boston University Medical School; 72 East Concord Street; Boston, MA. ph: 617-358-3562 (New Number)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-448845809:324,error,error,324,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-448845809,2,['error'],['error']
Availability,"You have a few failures, rvb.addAnnotation(t, a) needs to be rvb.addAnnotation(t.virtualType, a)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6711#issuecomment-514763348:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-514763348,1,['failure'],['failures']
Availability,You have a file naming error in build.yaml.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7916#issuecomment-575914242:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/pull/7916#issuecomment-575914242,1,['error'],['error']
Availability,You have a pylint error with trailing whitespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9822#issuecomment-764957523:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764957523,1,['error'],['error']
Availability,"You have an error based on where the maven code is being run (not the directory with the pom file).; ```; [[1;31mERROR[m] Failed to execute goal [32morg.apache.maven.plugins:maven-dependency-plugin:2.8:resolve[m [1m(default-cli)[m: [1;31mGoal requires a project to execute but there is no POM in this directory (/). Please verify you invoked Maven from the correct directory.[m -> [1m[Help 1][m; [[1;31mERROR[m] ; [[1;31mERROR[m] To see the full stack trace of the errors, re-run Maven with the [1m-e[m switch.; [[1;31mERROR[m] Re-run Maven using the [1m-X[m switch to enable full debug logging.; [[1;31mERROR[m] ; [[1;31mERROR[m] For more information about the errors and possible solutions, please read the following articles:; [[1;31mERROR[m] [1m[Help 1][m http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException; The command '/bin/sh -c mvn dependency:resolve && rm pom.xml test-jar-with-dependencies.xml' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-489712270,3,['error'],"['error', 'errors']"
Availability,You have an import error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7886#issuecomment-574836135:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574836135,1,['error'],['error']
Availability,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:14,error,errors,14,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393,1,['error'],['errors']
Availability,"You removed the ""wasSplit"" requirement so error is not thrown in BiallelicMethodSuite:. ```; interceptRequire {; multi.variantQC(); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2422#issuecomment-343779308:42,error,error,42,https://hail.is,https://github.com/hail-is/hail/pull/2422#issuecomment-343779308,1,['error'],['error']
Availability,You still have an error in there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5467#issuecomment-468314466:18,error,error,18,https://hail.is,https://github.com/hail-is/hail/pull/5467#issuecomment-468314466,1,['error'],['error']
Availability,You still have test failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9035#issuecomment-658331383:20,failure,failures,20,https://hail.is,https://github.com/hail-is/hail/pull/9035#issuecomment-658331383,1,['failure'],['failures']
Availability,"You'll get an error [if the character set is incompatible with the collation](https://dev.mysql.com/doc/refman/8.0/en/charset-collation-compatibility.html), so as long as this passes CI it should be fine on that account. As to why that particular collation, I found it on the [case sensitivity in string searches page](https://dev.mysql.com/doc/refman/8.0/en/case-sensitivity.html). The default is `utf8mb4_0900_ai_ci` and the first case sensitive alternative mentioned is this one. The change log indicates that ""as"" means accent sensitive and ""cs"" means case sensitive. The 0900 seems to refer to Unicode 9.0.0. Anyway, it seems sensible to use a Unicode collation rather than a binary one because Unicode encodings have more meaning than their bytes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12276#issuecomment-1268732639:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/12276#issuecomment-1268732639,1,['error'],['error']
Availability,"You'll have to provide me with the full error to help, I don't know where that's coming from. Perhaps I edited another function elsewhere and didn't include it up there. > that are each in the same 2 genes. The same two genes? How can a gene appear twice when you've grouped by the gene id?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1830286991:40,error,error,40,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830286991,1,['error'],['error']
Availability,"You'll note that `check_hail_37` is failing because of an infinite recursion bug. I believe it's pylint's fault through its `astroid` depenency, though I'm not sure what the change in pandas 1.1.5 is that causes this. Mentioned here: https://github.com/hail-is/hail/pull/9804",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9819#issuecomment-744092288:106,fault,fault,106,https://hail.is,https://github.com/hail-is/hail/pull/9819#issuecomment-744092288,1,['fault'],['fault']
Availability,You're getting a doctest failure @tpoterba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7404#issuecomment-548046645:25,failure,failure,25,https://hail.is,https://github.com/hail-is/hail/pull/7404#issuecomment-548046645,1,['failure'],['failure']
Availability,You're getting an assertion error in MatrixIR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3857#issuecomment-400791168:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/3857#issuecomment-400791168,1,['error'],['error']
Availability,"You've essentially done the same though saying that there will never be more than 32 regions in GCP (the `VARCHAR(32)`). The corresponding JSON would be O(32 * length-of-region-str), which would be just as error-prone, though possibly considerably longer. This is why I thought it was a space-saving issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1274914725:206,error,error-prone,206,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1274914725,1,['error'],['error-prone']
Availability,"Your application master container failed with exit code 11. There was no human interpretable error message, but googling turned up this:. http://stackoverflow.com/questions/31284799/spark-streaming-job-exited-with-code-11. Hitting ""spark.yarn.max.executor.failures"" would be totally consistent with the observed behavior, although I'm not sure why we didn't get the same error message. http://spark.apache.org/docs/latest/running-on-yarn.html. Again, this is related to the 1.5 bug I mentioned before: executors killed to give resources to other jobs shouldn't be counted as killed. Let's try again with two changes:; 1. I increased max executor failures to 500 in `hail-new-vep`.; 2. Instead of using repartition, use `importvcf -n 1000 /path/to/my.vcf.bgz splitmulti ...`. I think this will fix all the problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-211046143:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-211046143,4,"['error', 'failure']","['error', 'failures']"
Availability,"Your diff includes the Genotype class (which changed in master yesterday). Try fetching the current master, rebasing, and force pushing (you might want to copy your branch first to ease recovery in case you have trouble with rebasing)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1288#issuecomment-274950856:186,recover,recovery,186,https://hail.is,https://github.com/hail-is/hail/pull/1288#issuecomment-274950856,1,['recover'],['recovery']
Availability,"Your errors are a result of -0.0, as https://github.com/hail-is/hail/issues/6086",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6050#issuecomment-490955457:5,error,errors,5,https://hail.is,https://github.com/hail-is/hail/pull/6050#issuecomment-490955457,1,['error'],['errors']
Availability,"Yup, it looks like that was the problem. That was totally my fault too. Thanks Milo!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6663#issuecomment-514793080:61,fault,fault,61,https://hail.is,https://github.com/hail-is/hail/issues/6663#issuecomment-514793080,1,['fault'],['fault']
Availability,"[this comment](https://github.com/hail-is/hail/pull/8265#issuecomment-601360575) describes the test failure -- it looks like something was a numpy array, but is now a nested list.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8265#issuecomment-601665796:100,failure,failure,100,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601665796,1,['failure'],['failure']
Availability,\; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1868,echo,echo,1868,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,\; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:11567,echo,echo,11567,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2907,failure,failure,2907,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['failure'],['failure']
Availability,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14642,toler,tolerations,14642,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['toler'],['tolerations']
Availability,"___/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: from hail import *; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-3181c5d8fca5> in <module>(); ----> 1 from hail import *. /opt/Software/hail/python/hail/__init__.py in <module>(); ----> 1 import hail.expr; 2 from hail.representation import *; 3 from hail.context import HailContext; 4 from hail.dataset import VariantDataset; 5 from hail.expr import *. /opt/Software/hail/python/hail/expr.py in <module>(); 1 import abc; 2 from hail.java import scala_object, Env, jset; ----> 3 from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; 4 ; 5 . /opt/Software/hail/python/hail/representation/__init__.py in <module>(); ----> 1 from hail.representation.variant import Variant, Locus, AltAllele; 2 from hail.representation.interval import Interval; 3 from hail.representation.genotype import Genotype, Call; 4 from hail.representation.annotations import Struct; 5 from hail.representation.pedigree import Trio, Pedigree. /opt/Software/hail/python/hail/representation/variant.py in <module>(); 1 from hail.java import scala_object, Env, handle_py4j; ----> 2 from hail.typecheck import *; 3 ; 4 class Variant(object):; 5 """""". /opt/Software/hail/python/hail/typecheck/__init__.py in <module>(); ----> 1 from check import *; 2 ; 3 __all__ = ['typecheck',; 4 'typecheck_method',; 5 'none',. ImportError: No module named 'check'. In [2]: hc = HailContext(sc); ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-2-2e980fcce31d> in <module>(); ----> 1 hc = HailContext(sc). NameError: name 'HailContext' is not defined. In [3]: ; ```; There are still some errors, is there something wrong with my configurations?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:3939,error,errors,3939,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['error'],['errors']
Availability,"___________ test_callback _________________________________. client = <hailtop.batch_client.client.BatchClient object at 0x7f9cbc23b610>. async def test_callback(client):; import nest_asyncio # pylint: disable=import-outside-toplevel; ; nest_asyncio.apply(); ; app = web.Application(); callback_bodies = []; callback_event = asyncio.Event(); ; def url_for(uri):; host = os.environ['HAIL_BATCH_WORKER_IP']; port = os.environ['HAIL_BATCH_WORKER_PORT']; return f'http://{host}:{port}{uri}'; ; async def callback(request):; body = await request.json(); callback_bodies.append(body); callback_event.set(); return web.Response(); ; app.add_routes([web.post('/test', callback)]); runner = web.AppRunner(app); await runner.setup(); site = web.TCPSite(runner, '0.0.0.0', 5000); await site.start(); ; try:; token = secrets.token_urlsafe(32); b = create_batch(; client, callback=url_for('/test'), attributes={'foo': 'bar', 'name': 'test_callback'}, token=token; ); head = b.create_job('alpine:3.8', command=['echo', 'head']); b.create_job('alpine:3.8', command=['echo', 'tail'], parents=[head]); b.submit(); await asyncio.wait_for(callback_event.wait(), 5 * 60); callback_body = callback_bodies[0]; ; # verify required fields present; callback_body.pop('cost'); callback_body.pop('msec_mcpu'); callback_body.pop('time_created'); callback_body.pop('time_closed'); callback_body.pop('time_completed'); callback_body.pop('duration'); callback_body.pop('duration_ms'); callback_body.pop('cost_breakdown'); > assert callback_body == {; 'id': b.id,; 'user': 'test',; 'billing_project': 'test',; 'token': token,; 'state': 'success',; 'complete': True,; 'closed': True,; 'n_jobs': 2,; 'n_completed': 2,; 'n_succeeded': 2,; 'n_failed': 0,; 'n_cancelled': 0,; 'attributes': {'foo': 'bar', 'name': 'test_callback'},; }, callback_body; E AssertionError: {'attributes': {'client_job': '8051758-182', 'foo': 'bar', 'name': 'test_callback'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}; E assert {'id': 26",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427:1124,echo,echo,1124,https://hail.is,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427,1,['echo'],['echo']
Availability,"_fetchall; > async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; > await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; > await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; > await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; > await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; > await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; > first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; > packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; > err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; > raise errorclass(errno, errval); > pymysql.err.OperationalError: (1054, ""Unknown column 'cancelled.id' in 'on clause'""); > ```. This error strikes me as odd because `cancelled.id` has been updated to `cancelled.batch_id` in `delete_prev_cancelled_job_group_cancellable_resources_records`:. [batch/driver/main.py](https://github.com/hail-is/hail/blob/c6e3c660035379e6fe3f96fb4385f8b3c7e8d436/batch/batch/driver/main.py#L1474). Based on the error, it looks like the `main.py` being executed at `/usr/local/lib/python3.9/dist-packages/batch/driver/main.py` is still using the old version of the code, the changes from the PR were not correctly reflected in the environment. Is it possible that we might be missing a `pip install` step to ensure the latest code is deployed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053:1969,error,errorclass,1969,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053,3,['error'],"['error', 'errorclass']"
Availability,"_message_path': '/dev/termination-log',; 'termination_message_policy': 'File',; 'tty': None,; 'volume_devices': None,; 'volume_mounts': [{'mount_path': '/gsa-key',; 'mount_propagation': None,; 'name': 'gsa-key',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/io',; 'mount_propagation': None,; 'name': 'batch-2554-job-4-8vvgl',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',; 'mount_propagation': None,; 'name': 'default-token-8h99c',; 'read_only': True,; 'sub_path': None}],; 'working_dir': None}],; 'dns_config': None,; 'dns_policy': 'ClusterFirst',; 'enable_service_links': True,; 'host_aliases': None,; 'host_ipc': None,; 'host_network': None,; 'host_pid': None,; 'hostname': None,; 'image_pull_secrets': None,; 'init_containers': None,; 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-4gq2',; 'node_selector': None,; 'priority': 500000,; 'priority_class_name': 'user',; 'readiness_gates': None,; 'restart_policy': 'Never',; 'runtime_class_name': None,; 'scheduler_name': 'default-scheduler',; 'security_context': {'fs_group': None,; 'run_as_group': None,; 'run_as_non_root': None,; 'run_as_user': None,; 'se_linux_options': None,; 'supplemental_groups': None,; 'sysctls': None},; 'service_account': 'default',; 'service_account_name': 'default',; 'share_process_namespace': None,; 'subdomain': None,; 'termination_grace_period_seconds': 30,; 'tolerations': [{'effect': None,; 'key': 'preemptible',; 'operator': None,; 'toleration_seconds': None,; 'value': 'true'},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/not-ready',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/unreachable',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None}],; 'volumes': [{'aws_elastic_block_store': None,; 'azure_disk': None,; 'azure_file': None,; 'cephfs': None,; 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:5313,toler,tolerations,5313,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['toler'],['tolerations']
Availability,`./gradlew -Dspark.version=3.1.1 shadowJar archiveZip`; failed for the main branch. ```; FAILURE: Build failed with an exception. * What went wrong:; Task 'archiveZip' not found in root project 'hail'.; ```. any suggestion?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-908358179:89,FAILURE,FAILURE,89,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-908358179,1,['FAILURE'],['FAILURE']
Availability,"`/usr/lib/spark` I see reference of scala 2.12.15 same as in the hail logs; ```sh; $ cat /usr/lib/spark/RELEASE ; Spark 3.3.2-amzn-0.1 built for Hadoop 3.3.3-amzn-3.1; Build flags: -Divy.home=/home/release/.ivy2 -Dsbt.ivy.home=/home/release/.ivy2 -Duser.home=/home/release -Drepo.maven.org= -Dreactor.repo=file:///home/release/.m2/repository -Dhadoop.version=3.3.3-amzn-3.1 -Dyarn.version=3.3.3-amzn-3.1 -Dhive.version=2.3.9-amzn-3 -Dparquet.version=1.12.2-amzn-3 -Dprotobuf.version=2.5.0 -Dfasterxml.jackson.version=2.13.4 -Dfasterxml.jackson.databind.version=2.13.4 -Dcommons.httpclient.version=4.5.9 -Dcommons.httpcore.version=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:1477,down,downgraded,1477,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['down'],['downgraded']
Availability,"```; # xsltproc -o fuckyou.html --html fuckyou.xslt; compilation error: file fuckyou.xslt line 2 element stylesheet; xsltParseStylesheetProcess : document is not a stylesheet; # vim fuckyou.xslt ; # cat fuckyou.xslt ; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">; </xsl:stylesheet>; # xsltproc -o fuckyou.html --html fuckyou.xslt; ```. www.w3.org definitely speaks TLS based on `curl`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8511#issuecomment-611277147:65,error,error,65,https://hail.is,https://github.com/hail-is/hail/pull/8511#issuecomment-611277147,1,['error'],['error']
Availability,"```; * installing *source* package ‘ncdf4’ ...; ** package ‘ncdf4’ successfully unpacked and MD5 sums checked; configure.ac: starting; checking for nc-config... no; -----------------------------------------------------------------------------------; Error, nc-config not found or not executable. This is a script that comes with the; netcdf library, version 4.1-beta2 or later, and must be present for configuration; to succeed. If you installed the netcdf library (and nc-config) in a standard location, nc-config; should be found automatically. Otherwise, you can specify the full path and name of; the nc-config script by passing the --with-nc-config=/full/path/nc-config argument; flag to the configure script. For example:. ./configure --with-nc-config=/sw/dist/netcdf4/bin/nc-config. Special note for R users:; -------------------------; To pass the configure flag to R, use something like this:. R CMD INSTALL --configure-args=""--with-nc-config=/home/joe/bin/nc-config"" ncdf4. where you should replace /home/joe/bin etc. with the location where you have; installed the nc-config script that came with the netcdf 4 distribution.; -----------------------------------------------------------------------------------; ERROR: configuration failed for package ‘ncdf4’; * removing ‘/usr/local/lib/R/3.3/site-library/ncdf4’; ERROR: dependency ‘ncdf4’ is not available for package ‘GWASTools’; * removing ‘/usr/local/lib/R/3.3/site-library/GWASTools’; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057:250,Error,Error,250,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057,4,"['ERROR', 'Error', 'avail']","['ERROR', 'Error', 'available']"
Availability,"```; + make -k check-services; PYTHONPATH=""hail/python:auth:batch:ci:memory:notebook:monitoring:website:gear:web_common"" python3 -m flake8 --config setup.cfg auth; auth/auth/auth.py:515:86: W291 trailing whitespace; make: *** [Makefile:42: check-auth] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12889#issuecomment-1515270732:252,Error,Error,252,https://hail.is,https://github.com/hail-is/hail/pull/12889#issuecomment-1515270732,1,['Error'],['Error']
Availability,"```; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2272, in run; await self.jvm.execute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.Goo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:5,Error,Error,5,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['Error'],['Error']
Availability,"```; echo $SPARK_CLASSPATH; /Users/ih/languages/hail.is/hail/build/libs/hail-all-spark.jar; ```; SPARK_CLASSPATH is set correctly, however, I still get the error message; ```; >>> hc = HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 83, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```. What do I need to do about the driverClassPath?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319712395:5,echo,echo,5,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319712395,2,"['echo', 'error']","['echo', 'error']"
Availability,"```; hail: info: SparkUI: http://10.131.101.159:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-f69b497; >>> print sc; <SparkContext master=yarn appName=PySparkShell>; >>> print hc; <hail.context.HailContext object at 0x1f15350>; >>> hc.import_vcf(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; TypeError: import_vcf() takes at least 2 arguments (1 given); >>> hc.import_vcf('/hail/sample.vcf'); [Stage 0:> (0 + 1) / 2]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-313>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, com2, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:691,Error,Error,691,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,3,"['Error', 'failure']","['Error', 'failure']"
Availability,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:543,error,error,543,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508,2,"['avail', 'error']","['available', 'error']"
Availability,`check-services` error: `on-shutdown` needs a ` # pylint: disable=unused-argument`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106#issuecomment-801560644:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/pull/10106#issuecomment-801560644,1,['error'],['error']
Availability,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-220667612:181,down,down,181,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612,2,['down'],['down']
Availability,`kubectl describe pod POD_NAME` will tell you there reasons the pod could not be scheduled. I often see this issue when we run out of available CPU.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461105202:134,avail,available,134,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461105202,1,['avail'],['available']
Availability,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:256,down,down,256,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228,1,['down'],['down']
Availability,"`pip install -e .`; Defaulting to user installation because normal site-packages is not writeable; Obtaining file:///home/skr/hail2/hail; Installing build dependencies ... done; Checking if build backend supports build_editable ... done; Getting requirements to build editable ... error; error: subprocess-exited-with-error; ; × Getting requirements to build editable did not run successfully.; │ exit code: 1; ╰─> [14 lines of output]; error: Multiple top-level packages discovered in a flat-layout: ['tls', 'gear', 'hail', 'auth', 'blog', 'infra', 'batch', 'query', 'docker', 'memory', 'devbin', 'gateway', 'website', 'grafana', 'notebook', 'graphics', 'datasets', 'monitoring', 'web_common', 'prometheus', 'letsencrypt'].; ; To avoid accidental inclusion of unwanted files or directories,; setuptools will not proceed with this build.; ; If you are trying to create a single distribution with multiple packages; on purpose, you should not rely on automatic discovery.; Instead, consider the following options:; ; 1. set up custom discovery (`find` directive with `include` or `exclude`); 2. use a `src-layout`; 3. explicitly set `py_modules` or `packages` with a list of names; ; To find more information, look for ""package discovery"" on setuptools docs.; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: subprocess-exited-with-error. × Getting requirements to build editable did not run successfully.; │ exit code: 1; ╰─> See above for output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290:281,error,error,281,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290,7,['error'],['error']
Availability,"`read()` with no argument is `read(-1)`. Python's [`read`](https://docs.python.org/3/library/io.html#io.RawIOBase.read) and aiohttp's [`read`](https://docs.aiohttp.org/en/stable/streams.html#aiohttp.StreamReader.read) will return all bytes until EOF if their argument is `-1`. Python:; > Read up to size bytes from the object and return them. As a convenience, if size is unspecified or -1, all bytes until EOF are returned. Otherwise, only one system call is ever made. Fewer than size bytes may be returned if the operating system call returns fewer than size bytes.; > ; > If 0 bytes are returned, and size was not 0, this indicates end of file. If the object is in non-blocking mode and no bytes are available, None is returned.; >; > The default implementation defers to readall() and readinto(). Aiohttp:; > Read up to n bytes. If n is not provided, or set to -1, read until EOF and return all read bytes.; >; > If the EOF was received and the internal buffer is empty, return an empty bytes object. We should document that our `read` follows these same rules in `hailtop.aiotools.stream.ReadableStream.read`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10771#issuecomment-904688934:704,avail,available,704,https://hail.is,https://github.com/hail-is/hail/pull/10771#issuecomment-904688934,1,['avail'],['available']
Availability,"`test_summarize_run` is segfaulting on the CI server, but it runs fine for me locally (in fact, I can run all of `test_expr.py` with no failures). can somebody else try cloning this branch and running tests locally?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6742#issuecomment-520512516:136,failure,failures,136,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-520512516,1,['failure'],['failures']
Availability,`time ../Downloads/plink_mac/plink --bfile src/test/resources/svip_3_2013 --missing`; real 0m0.383s; user 0m0.293s; sys 0m0.072s. Hail reading bed to vds -- 11s (intelliJ test). `time ../Downloads/plink_mac/plink --bgen src/test/resources/bigger.bgen --sample src/test/resources/bigger.sample --make-bed --out testplink`. real 0m9.164s; user 0m8.429s; sys 0m0.520s. Hail reading bgen to vds -- 40s (IntelliJ test),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/72#issuecomment-155571444:9,Down,Downloads,9,https://hail.is,https://github.com/hail-is/hail/pull/72#issuecomment-155571444,2,['Down'],['Downloads']
Availability,a downloadable link to the distributions can be found here: ; https://hail.is/docs/stable/getting_started.html. The current link for Spark 2.0.2 is https://storage.googleapis.com/hail-common/distributions/0.1/Hail-0.1-4238176-Spark-2.0.2.zip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320243052:2,down,downloadable,2,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320243052,1,['down'],['downloadable']
Availability,"a.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at py4j.commands.CallCommand.execute(CallCommand.java:79); [13:46:55]	[:makeHailDocs] 	at py4j.GatewayConnection.run(GatewayConnection.java:214); [13:46:55]	[:makeHailDocs] 	at java.lang.Thread.run(Thread.java:745); [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1692,error,error,1692,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,1,['error'],['error']
Availability,a.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:50); 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:462); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:498); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); 	at is.hail.utils.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:350); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:495); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:494); 	at jdk.internal.reflect.GeneratedMethodAccessor109.invoke(Unknown Source); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); 	at java.base/java.lang.Thread.run(Thread.java:829). Hail version: 0.2.122-be9d88a80695; Error summary: ArrayIndexOutOfBoundsException: Index 177860 out of bounds for length 177860; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:11591,Error,Error,11591,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['Error'],['Error']
Availability,able.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217388,recover,recover,217388,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Availability,"ache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:6",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206577,down,down,206577,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['down'],['down']
Availability,"actually that error isn't even so bad, it says ""drop"" in it. I'm closing this! ⚔️",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3673#issuecomment-392915045:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/issues/3673#issuecomment-392915045,1,['error'],['error']
Availability,actually will reopen when the downsample PR goes in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7131#issuecomment-539046228:30,down,downsample,30,https://hail.is,https://github.com/hail-is/hail/pull/7131#issuecomment-539046228,1,['down'],['downsample']
Availability,"actually, given the error output, it looks like `apply` isn't converting to `fApply` at all -- we wouldn't see this error if it were converting correctly",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/147#issuecomment-172043157:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/147#issuecomment-172043157,2,['error'],['error']
Availability,"added some logic to allow `CallCC` code to be emitted more than once in the same function (see `testDuplicateCallCC`, which used to cause bytecode verification errors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7055#issuecomment-531960975:160,error,errors,160,https://hail.is,https://github.com/hail-is/hail/pull/7055#issuecomment-531960975,1,['error'],['errors']
Availability,addresses poor error message in https://github.com/hail-is/hail/issues/4033,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4036#issuecomment-408955855:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/pull/4036#issuecomment-408955855,1,['error'],['error']
Availability,"after a few more minutes of thought, a warning is probably not sufficient here. We have to assume that losing parallelism means that the computation will never finish. There aren't really any good options:. - error; - unkey the cols too; - shuffle",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4646#issuecomment-433472683:209,error,error,209,https://hail.is,https://github.com/hail-is/hail/issues/4646#issuecomment-433472683,1,['error'],['error']
Availability,"ah, good point. yes, it caused an error on range_matrix_table(1, 1). _force_count_rows()",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3387#issuecomment-381986471:34,error,error,34,https://hail.is,https://github.com/hail-is/hail/pull/3387#issuecomment-381986471,1,['error'],['error']
Availability,"ail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:889); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:3466,Error,Error,3466,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['Error'],['Error']
Availability,"ail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7410,error,error,7410,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['error'],['error']
Availability,"ailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:4174,echo,echo,4174,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"al: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry from our local scientific community is that many of them have transitioned to Query-on-Batch where we have exact and total control over the memory available to the driver and the workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:3538,avail,available,3538,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['avail'],['available']
Availability,"alMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2222,Error,ErrorHandling,2222,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Error'],['ErrorHandling']
Availability,all builds are failing because the repository the hosts the shadow jar plugin is down,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10985#issuecomment-1011304877:81,down,down,81,https://hail.is,https://github.com/hail-is/hail/pull/10985#issuecomment-1011304877,1,['down'],['down']
Availability,also have rebase conflict and test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7134#issuecomment-535987529:35,failure,failures,35,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-535987529,1,['failure'],['failures']
Availability,also see test failure,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3019#issuecomment-369657687:14,failure,failure,14,https://hail.is,https://github.com/hail-is/hail/pull/3019#issuecomment-369657687,1,['failure'],['failure']
Availability,"also, NB: I got this error after trying the script one last time; ```; ERROR: (gcloud.iam.service-accounts.keys.create) RESOURCE_EXHAUSTED: Maximum number of keys on account reached.; - '@type': type.googleapis.com/google.rpc.RetryInfo; retryDelay: 86401s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4502#issuecomment-427220196:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/4502#issuecomment-427220196,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"also, this error message is WAY better and fully debuggable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1623#issuecomment-290762181:11,error,error,11,https://hail.is,https://github.com/hail-is/hail/issues/1623#issuecomment-290762181,1,['error'],['error']
Availability,"amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:4055,Down,Downloading,4055,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"another possible error - I think prune dead fields might be broken if you have a same-name row field on the left and right, and only use the `_1` version from the right table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4311#issuecomment-420261417:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/4311#issuecomment-420261417,1,['error'],['error']
Availability,"aproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternati",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3930,echo,echo,3930,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"ar/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1202,down,down,1202,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927,2,['down'],['down']
Availability,"are you sure it's a dataproc problem?. If scalding is using java unsafe in a not-guaranteed-to-work way, then a core dump s totally possible. Example - the JVM will sometimes tolerate misaligned floats/ints, and sometimes will crash.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419972098:175,toler,tolerate,175,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419972098,1,['toler'],['tolerate']
Availability,"aring metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6151,Down,Downloading,6151,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"ary. It has happened twice. The failing partition is different in each run. 1. 49340 https://batch.hail.is/batches/8069235/jobs/51280; 2. 25997 https://batch.hail.is/batches/8083195/jobs/27937. The pipeline runs two table collects to get sample information, then converts the matrix table to a table of ndarrays of the value `hl.int(hl.is_defined(mt.GT))`. The entries are getting subsetted, so there is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""B",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:995,error,error,995,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,2,['error'],"['error', 'errors']"
Availability,ase; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123ab,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:14539,echo,echo,14539,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"ataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has happened in 2/8 test_dataproc steps that I have run myself or seen run. The more workers you have, the higher the chance at least one worker fails. As @bpblanken suggested [here](https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412), restarting docker on a failed worker works. Docker starts fine. However, I missed a subtlety: we must restart *after* installation but *before* we try to pull our VEP docker image. I also added a sleep in hopes that gives various things a chance to die off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13711,down,download,13711,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,3,"['down', 'error']","['download', 'error']"
Availability,"atingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:4656,error,error,4656,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['error'],['error']
Availability,"atus 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.0rc1 Requires-Python >=3.8; 1.22.0rc2 Requires-Python >=3.8; 1.22.0rc3 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.4.0 Requires-Py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:7415,Down,Downloading,7415,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,ava:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:5094,recover,recover,5094,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Availability,ava:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4901,recover,recover,4901,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Availability,"ava:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748). During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:4776,error,error,4776,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['error'],['error']
Availability,"ay/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/et",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:4245,echo,echo,4245,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:3924,Down,Downloading,3924,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"b/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in write(self, output, overwrite, stage_locally, _codec_spec); 1375 hl.current_backend().validate_file_scheme(output); 1376 ; -> 1377 Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:4350,checkpoint,checkpoint,4350,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['checkpoint'],['checkpoint']
Availability,"bad news, though -- after playing with the thing a bit longer and trying to bump the partition number up, it's really not reliable. At all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7087#issuecomment-532883549:122,reliab,reliable,122,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532883549,1,['reliab'],['reliable']
Availability,"base64 can contain some symbols, can the cluster names handle it? I normally use `tr`:. ```; $ echo `LC_CTYPE=C tr -dc 'a-z0-9' < /dev/urandom | head -c 8`; k1awyx7o; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-417796578:95,echo,echo,95,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417796578,1,['echo'],['echo']
Availability,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:268,echo,echo,268,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,3,"['Error', 'echo', 'error']","['Error', 'echo', 'error']"
Availability,"bs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2004,Down,Downloading,2004,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"bump for approval, I fixed the lint error",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9985#issuecomment-773392574:36,error,error,36,https://hail.is,https://github.com/hail-is/hail/pull/9985#issuecomment-773392574,1,['error'],['error']
Availability,bump. Test failure was unrelated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13812#issuecomment-1798877987:11,failure,failure,11,https://hail.is,https://github.com/hail-is/hail/pull/13812#issuecomment-1798877987,1,['failure'],['failure']
Availability,bump. can we get this in? I want to PR the stacked changes so that Dan can see the batch failure modes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6908#issuecomment-525915270:89,failure,failure,89,https://hail.is,https://github.com/hail-is/hail/pull/6908#issuecomment-525915270,1,['failure'],['failure']
Availability,bump... can we come to an agreement on what error we're catching?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-837286361:44,error,error,44,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-837286361,1,['error'],['error']
Availability,"burden.vds'); [13:46:55]	[:makeHailDocs] .linreg_burden(key_name='gene',; [13:46:55]	[:makeHailDocs] variant_keys='va.genes',; [13:46:55]	[:makeHailDocs] single_key='false',; [13:46:55]	[:makeHailDocs] agg_expr='gs.map(g => g.gt).max()',; [13:46:55]	[:makeHailDocs] y='sa.burden.pheno',; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] Exception raised:; [13:46:55]	[:makeHailDocs] Traceback (most recent call last):; [13:46:55]	[:makeHailDocs] File ""/usr/lib64/python2.7/doctest.py"", line 1315, in __run; [13:46:55]	[:makeHailDocs] compileflags, 1) in test.globs; [13:46:55]	[:makeHailDocs] File ""<doctest default[0]>"", line 7, in <module>; [13:46:55]	[:makeHailDocs] covariates=['sa.burden.cov1', 'sa.burden.cov2'])); [13:46:55]	[:makeHailDocs] File ""<decorator-gen-233>"", line 2, in linreg_burden; [13:46:55]	[:makeHailDocs] File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 119, in handle_py4j; [13:46:55]	[:makeHailDocs] 'Error summary: %s' % (msg, e.message, Env.hc().version, msg)); [13:46:55]	[:makeHailDocs] FatalError: An error occurred while calling into JVM, probably due to invalid parameter types.; [13:46:55]	[:makeHailDocs] ; [13:46:55]	[:makeHailDocs] Java stack trace:; [13:46:55]	[:makeHailDocs] An error occurred while calling o3918.linregBurden. Trace:; [13:46:55]	[:makeHailDocs] py4j.Py4JException: Method linregBurden([class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class java.lang.String, class [Ljava.lang.String;]) does not exist; [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); [13:46:55]	[:makeHailDocs] 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); [13:46:55]	[:makeHailDocs] 	at py4j.Gateway.invoke(Gateway.java:272); [13:46:55]	[:makeHailDocs] 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); [13:46:55]	[:makeHailDocs] 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203:1401,Error,Error,1401,https://hail.is,https://github.com/hail-is/hail/pull/1708#issuecomment-297039203,1,['Error'],['Error']
Availability,"c/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:2670,echo,echo,2670,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['echo'],['echo']
Availability,"cala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3867,ERROR,ERROR,3867,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"can we dig into just one of these errors? Can you get the IR before and after streamify, so we know what it's doing wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586599441:34,error,errors,34,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586599441,1,['error'],['errors']
Availability,"can you try running with the latest version? I fixed this specific issue recently, but it's possible you'll get another error (it's not tested as part of our CI)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5657#issuecomment-475096140:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/issues/5657#issuecomment-475096140,1,['error'],['error']
Availability,"cc: @cseed @rcownie @catoverdrive . Amanda saw this error with these changes:. ```; amwang: fyi, this is the error the executor is seeing on a cluster with that jar:. ERROR: dlopen(""/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so""): /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); FATAL: caught exception java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:52,error,error,52,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"cc: @daniel-goldstein is it possible to make the error more clear? The CI test job is a bit hard to read. Maybe the last line of the output should be something like ""XXXX file is out of date, run YYYY""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1325464620:49,error,error,49,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1325464620,2,['error'],['error']
Availability,ccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6726,Error,ErrorHandling,6726,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Error'],['ErrorHandling']
Availability,"ce for Amanda:; ```; Traceback (most recent call last):; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 157, in <module>; main(args); File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 98, in main; pca_mt = hl.ld_prune(pca_mt, r2=0.1, n_cores=args.num_cores); File ""<decorator-gen-788>"", line 2, in ld_prune; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/typecheck/check.py"", line 490, in _typecheck; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/methods/statgen.py"", line 2918, in ld_prune; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 530 in stage 9.0 failed 20 times, most recent failure: Lost task 530.19 in stage 9.0 (TID 19101, gt1-w-78.c.broad-mpg-gnomad.internal, executor 199): java.lang.ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:643); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9$$anonfun$apply$10.apply(RVD.scala:222); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9$$anonfun$apply$10.apply(RVD.scala:221); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:1024,failure,failure,1024,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['failure'],['failure']
Availability,"che.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3258,failure,failure,3258,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['failure'],['failure']
Availability,"ci failure:; ```; hail version: 0.2.109-c163bcb21073; Error summary: AssertionError: assertion failed; self = <test.hail.linalg.test_linalg.Tests testMethod=test_tree_matmul>. @fails_service_backend(); @fails_local_backend(); def test_tree_matmul(self):; nm = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]); m = BlockMatrix.from_numpy(nm, block_size=2); nrow = np.array([[7.0, 8.0, 9.0]]); row = BlockMatrix.from_numpy(nrow, block_size=2); ; > with BatchedAsserts() as b:. test/hail/linalg/test_linalg.py:612: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/hail/linalg/test_linalg.py:96: in __exit__; vals.extend(list(hl.eval(tuple([all_bms[k] for k in bm_keys[batch_start:batch_start + batch_size]])))); <decorator-gen-692>:2: in eval; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:191: in eval; return eval_timed(expression)[0]; <decorator-gen-690>:2: in eval_timed; ???; hail/typecheck/check.py:577: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:161: in eval_timed; return Env.backend().execute(MakeTuple([ir]), timed=True)[0]; hail/backend/py4j_backend.py:82: in execute; raise e.maybe_user_error(ir) from None; hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); ../../.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro549', <py4j.clientserver.JavaClient object at 0x7fd0d58f6fb0>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/home/edmund/.local/src/hail/.venv/lib/python3.10/site-packages/pyspark/__init__.py'>; s = 'java.lang.AssertionError: assertion failed', tpl = JavaObject id=o550; deepest = 'AssertionError: assertion failed'; full = 'java.lang.AssertionError: asserti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:3,failure,failure,3,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"col.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1847,Error,Error,1847,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['Error'],['Error']
Availability,compile errors,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8603#issuecomment-618375915:8,error,errors,8,https://hail.is,https://github.com/hail-is/hail/pull/8603#issuecomment-618375915,1,['error'],['errors']
Availability,"computeLoadings': compute_loadings; --> 211 })).persist()); 212 ; 213 g = t.index_globals(). <decorator-gen-1340> in persist(self, storage_level). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:3658,checkpoint,checkpoint,3658,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['checkpoint'],['checkpoint']
Availability,"conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper. ```. This is an error we are getting when submitted to the hadoop cluster. This runs fine locally. ```; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.4-d602a3d7472d; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-0019-0.2.4-d602a3d7472d.log; 2019-01-22 00:19:54 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 00:19:54 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:6031,error,error,6031,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['error'],['error']
Availability,"create_vm; await self.compute_client.post(f'/zones/{location}/instances', params=params, json=vm_config); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/base_client.py"", line 30, in post; async with await self._session.post(url, **kwargs) as resp:; File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 21, in post; return await self.request('POST', url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiocloud/common/session.py"", line 103, in request; return await request_retry_transient_errors(self._http_session, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 770, in request_retry_transient_errors; return await retry_transient_errors(session.request, method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 147, in request_and_raise_for_status; body=body; hailtop.httpx.ClientResponseError: 400, message='Bad Request', url=URL('https://compute.googleapis.com/compute/v1/projects/hail-vdc/zones/us-central1-a/instances?requestId=e2555a38-1583-47e2-ab15-c3d7ad84e700') body='{\n ""error"": {\n ""code"": 400,\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""errors"": [\n {\n ""message"": ""Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default). To use instance termination action, the VM instance must use the Spot provisioning model."",\n ""domain"": ""global"",\n ""reason"": ""invalid""\n }\n ]\n }\n}\n'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728:1449,error,error,1449,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144087728,2,['error'],"['error', 'errors']"
Availability,"crets.token_urlsafe(64); b1 = self.client.create_batch(attributes={'tag': tag, 'name': 'b1'}); b1.create_job('alpine', ['sleep', '30']); b1.close(); ; b2 = self.client.create_batch(attributes={'tag': tag, 'name': 'b2'}); b2.create_job('alpine', ['echo', 'test']); b2.close(); ; def assert_batch_ids(expected, complete=None, success=None, attributes=None):; batches = self.client.list_batches(complete=complete, success=success, attributes=attributes); # list_batches returns all batches for all prev run tests; actual = set([batch.id for batch in batches]).intersection({b1.id, b2.id}); self.assertEqual(actual, expected); ; assert_batch_ids({b1.id, b2.id}, attributes={'tag': tag}); ; b2.wait(); ; > assert_batch_ids({b1.id}, complete=False, attributes={'tag': tag}). test/test_batch.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; test/test_batch.py:87: in assert_batch_ids; self.assertEqual(actual, expected); E AssertionError: Items in the second set but not the first:; E 19; ________________________________ test_callback _________________________________. client = <batch.client.BatchClient object at 0x7f0d1363ee80>. def test_callback(client):; from flask import Flask, request; app = Flask('test-client'); output = []; ; @app.route('/test', methods=['POST']); def test():; output.append(request.get_json()); return Response(status=200); ; try:; server = ServerThread(app); server.start(); batch = client.create_batch(callback=server.url_for('/test')); head = batch.create_job('alpine:3.8', command=['echo', 'head']); left = batch.create_job('alpine:3.8', command=['echo', 'left'], parents=[head]); right = batch.create_job('alpine:3.8', command=['echo', 'right'], parents=[head]); tail = batch.create_job('alpine:3.8', command=['echo', 'tail'], parents=[left, right]); batch.close(); batch.wait(); i = 0; while len(output) != 4:; time.sleep(0.100 * (3/2) ** i); i += 1; if i > 14:; break; > assert len(output) == 4; E assert 5 == 4; E -5; E +4. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506:1889,echo,echo,1889,https://hail.is,https://github.com/hail-is/hail/pull/6260#issuecomment-498852506,4,['echo'],['echo']
Availability,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:452,failure,failure,452,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,4,"['down', 'failure']","['down', 'failure']"
Availability,"ct(self._jvdf.count(genotypes).toJavaMap()); 1130; 1131 def deduplicate(self):. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1440,error,error,1440,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['error'],['error']
Availability,ctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29302,Down,Downloading,29302,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,cutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:306); 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 		... 27 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:6953,recover,recover,6953,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Availability,cutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:326); 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 		... 27 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:7144,recover,recover,7144,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Availability,"d Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1406,error,error,1406,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,2,"['error', 'failure']","['error', 'failure']"
Availability,"d.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:159); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:408); 	at java.base/java.lang.Thread.run(Thread.java:834). Hail version: 0.2.128-ce3ca9c77507; Error summary: SocketTimeoutException: connect timed out; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 223, in _rpc; raise fatal_error_from_java_error_triplet(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/test.py"", line 6, in <module>; ht.write('gs://ehigham-hail-tmp/test_hail_in_notebook.ht'); hail.utils.java.FatalError: SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/com",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:10697,Error,Error,10697,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['Error'],['Error']
Availability,"dTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2604,echo,echo,2604,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,4,['echo'],['echo']
Availability,"deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2841,downtime,downtime,2841,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['downtime'],['downtime']
Availability,"der: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: from hail import *; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-3181c5d8fca5> in <module>(); ----> 1 from hail import *. /opt/Software/hail/python/hail/__init__.py in <module>(); ----> 1 import hail.expr; 2 from hail.representation import *; 3 from hail.context import HailContext; 4 from hail.dataset import VariantDataset; 5 from hail.expr import *. /opt/Software/hail/python/hail/expr.py in <module>(); 1 import abc; 2 from hail.java import scala_object, Env, jset; ----> 3 from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; 4 ; 5 . /opt/Software/hail/python/hail/representation/__init__.py in <module>(); ----> 1 from hail.representation.variant import Variant, Locus, AltAllele; 2 from hail.representation.interval import Interval; 3 from hail.representation.genotype import Genotype, Call; 4 from hail.representa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:2148,avail,available,2148,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['avail'],['available']
Availability,do I have test failures? I thought it passed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8067#issuecomment-585439143:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/8067#issuecomment-585439143,1,['failure'],['failures']
Availability,do we have tests that verify we get errors if we try to rekey with annotate_rows?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4242#issuecomment-417758456:36,error,errors,36,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417758456,1,['error'],['errors']
Availability,docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=doc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4167,echo,echo,4167,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9643,echo,echo,9643,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"doctest failure from show() changing the width, it seems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5666#issuecomment-475778217:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/5666#issuecomment-475778217,1,['failure'],['failure']
Availability,doctest failures related to `ld_score_regression.sample.mt` -- either we need to mark those as SKIP or check that file in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6510#issuecomment-506860319:8,failure,failures,8,https://hail.is,https://github.com/hail-is/hail/pull/6510#issuecomment-506860319,1,['failure'],['failures']
Availability,"down to 38 minutes, with the cleaning up GCP instances now a significant delay. <img width=""2031"" alt=""Screen Shot 2023-05-17 at 23 02 20"" src=""https://github.com/hail-is/hail/assets/106194/6b14d74e-877c-4259-bc26-fd4fed91fe26"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1552339379:0,down,down,0,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1552339379,1,['down'],['down']
Availability,"duh you're totally right, I don't know why I thought retrying was happening higher up and not further down in the http client.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255528060:102,down,down,102,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255528060,1,['down'],['down']
Availability,"e a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, good point!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:3391,error,errors,3391,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,"['error', 'redundant']","['errors', 'redundant']"
Availability,"e broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBufferSpec""}}}}; ```; Error for run 1.; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:655) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:385) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:403) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac0",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:2157,Error,Error,2157,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['Error'],['Error']
Availability,"e may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to catch and re-connect to Kubernetes; https://github.com/datawire/ambassador/pull/724",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2097,error,errors,2097,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,6,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'errors']"
Availability,"e reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## W",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2212,error,error,2212,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['error'],['error']
Availability,"e {worker_machine_type}; --region {region}; --project {gcs_project}; --service-account {account}; --num-master-local-ssds 1; --num-worker-local-ssds 1 ; --max-idle=60m; --max-age=1440m; --subnet=projects/{gcs_project}/regions/{region}/subnetworks/subnetwork; {cluster_name}; ```. </details>. I have the driver node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: low memory! at or below SIGTERM limits: mem 0.12%, swap 1.00%; Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM to process 8421 uid 0 ""java"": badness 1852, VmRSS 40578 MiB; ```. Indeed, the VmRSS is the memory in use from the kernel's perspective so it includes any off-heap memory created by Hail. The Hail log indicates the region pools ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:1512,avail,avail,1512,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,1,['avail'],['avail']
Availability,"e(mt_ld_fn); print(mt.count()); print(""running omit filter""); mt=mt.annotate_cols(pheno = table[mt.s]); mt = mt.filter_cols(mt.pheno.omit == 0); print(mt.count()); print(""running pc_relate""); pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); pairs = pc_rel.filter(pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:========================================",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4103,avail,available,4103,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['avail'],['available']
Availability,"e):; """"""Converts an answer received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protoc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:3982,error,error,3982,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['error'],['error']
Availability,"e.g.; ```; # cat Makefile; foo:; 	echo '[ \; hello \; '; # make foo; echo '[ \; hello \; '; [ hello ; ```. If you create such a Makefile and run Make do you see `[ hello ` echo'd or do you see something else, possibly including newlines?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1889782238:34,echo,echo,34,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1889782238,3,['echo'],['echo']
Availability,"eAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.coll",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2157,failure,failure,2157,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,2,['failure'],['failure']
Availability,"eContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9585,failure,failure,9585,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['failure'],['failure']
Availability,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19932,Error,Error,19932,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Error'],['Error']
Availability,"ebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""fiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:2548,Failure,Failure,2548,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,1,['Failure'],['Failure']
Availability,echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' '],MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3455,echo,echo,3455,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']',MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:8931,echo,echo,8931,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"ect id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:889); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:3690,failure,failure,3690,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['failure'],['failure']
Availability,"edit:. Issue seems to be something else; may be having issue connecting to sql host on cluster, at least when executed by ci. @jigold I'll fix it, then ping you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-477827672:152,ping,ping,152,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-477827672,1,['ping'],['ping']
Availability,"eepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:6",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:4451,failure,failure,4451,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['failure'],['failure']
Availability,eflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:6700,Error,ErrorHandling,6700,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Error'],['ErrorHandling']
Availability,"eles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFaile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8006,failure,failure,8006,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['failure'],['failure']
Availability,"eption); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2248,Error,ErrorHandling,2248,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Error'],['ErrorHandling']
Availability,"equirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: R",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:4635,Down,Downloading,4635,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"er file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505))). In the Kudu logs, I'm seeing tons of:. W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS; a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet; 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable:; CreateTablet request on kudu.tserver.TabletServerAdminService from; 69.173.65.227:42904 dropped due to backpressure. The service queue is; full; it has 50 items. Suggestions on how to proceed? Should I increase the service queue size?. —; You are receiving this because you authored the thread.; Reply to this email directly or view it on GitHub; https://github.com/broadinstitute/hail/pull/242#issuecomment-208516279",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:2490,error,error,2490,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['error'],['error']
Availability,er of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. -------------,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1360,Down,Download,1360,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,erExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.u,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115987,ERROR,ERROR,115987,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"erba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1169,error,error,1169,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787,2,['error'],['error']
Availability,"error in test java:; ```; [[TestNGClassFinder]] Unable to read methods on class is.hail.relocated.com.indeed.util.varexport.servlet.ViewExportedVariablesServlet - unable to resolve class reference freemarker/template/ObjectWrapper; Exception in thread ""main"" java.lang.NoClassDefFoundError: freemarker/template/ObjectWrapper; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8576#issuecomment-615459260:0,error,error,0,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615459260,1,['error'],['error']
Availability,ers/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$argum,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:12027,echo,echo,12027,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"et.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: An error occurred while calling into JVM, probably due to invalid parameter types```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:1436,error,error,1436,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,example failure: https://ci.hail.is/batches/179008/jobs/30,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10017#issuecomment-775554336:8,failure,failure,8,https://hail.is,https://github.com/hail-is/hail/pull/10017#issuecomment-775554336,1,['failure'],['failure']
Availability,"file:///home/release/.m2/repository -Dhadoop.version=3.3.3-amzn-3.1 -Dyarn.version=3.3.3-amzn-3.1 -Dhive.version=2.3.9-amzn-3 -Dparquet.version=1.12.2-amzn-3 -Dprotobuf.version=2.5.0 -Dfasterxml.jackson.version=2.13.4 -Dfasterxml.jackson.databind.version=2.13.4 -Dcommons.httpclient.version=4.5.9 -Dcommons.httpcore.version=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:1763,echo,echo,1763,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"fmtlib looks promising, but. a) I'm not keen to mix in non-standard third-party source code into our; codebase; unless it's a significant win over the functionality that is; available everywhere; and familiar to everyone. printf has been getting the job done for; decades. b) the speed test shows it still slower than printf (iirc 1.56s vs; 1.35s), just not a; *lot* slower (whereas C++ iostreams are very slow). On Fri, Aug 3, 2018 at 10:45 AM Patrick Schultz <notifications@github.com>; wrote:. > *@patrick-schultz* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207566438>:; >; > > +#include <sys/stat.h>; > +#include <sys/time.h>; > +#include <unistd.h>; > +#include <atomic>; > +#include <memory>; > +#include <mutex>; > +#include <iostream>; > +#include <sstream>; > +#include <string>; > +#include <vector>; > +; > +#if 0; > +#define D(fmt, ...) { \; > + char buf[1024]; \; > + sprintf(buf, fmt, ##__VA_ARGS__); \; > + fprintf(stderr, ""DEBUG: %s,%d: %s"", __FILE__, __LINE__, buf); \; >; > @cseed <https://github.com/cseed> suggested using the fmt library (; > https://github.com/fmtlib/fmt), which I believe is being proposed for; > standardization. That is my preference also. It appears to be as safe as; > IOstreams and as fast as printf. It supports both Python style and C; > style formatting, e.g.; >; > fmt::print(""Hello, {}!"", ""world""); // uses Python-like format string syntaxfmt::printf(""Hello, %s!"", ""world""); // uses printf format string syntax; >; > Format strings are checked at compile time, as are argument types.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/3973#discussion_r207566438>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AJzExrljAAlcA6ksqZiNPyOw8wgC0hheks5uNGH7gaJpZM4VbZpP>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410319504:174,avail,available,174,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410319504,1,['avail'],['available']
Availability,"for some reason I guess I tagged this issue wrong from the PR?. We traced this down to a Spark problem with the iterator that comes out of the shuffle, and so #4228 should fix the memory leak issue for this specific case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4202#issuecomment-416762038:79,down,down,79,https://hail.is,https://github.com/hail-is/hail/issues/4202#issuecomment-416762038,1,['down'],['down']
Availability,"fwiw, `j.env(VAR, VAL)` is also available though that would set it for all the commands in the job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12840#issuecomment-1495034877:32,avail,available,32,https://hail.is,https://github.com/hail-is/hail/pull/12840#issuecomment-1495034877,1,['avail'],['available']
Availability,"g,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2254,Down,Downloading,2254,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"g.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockManagerId(14, scc-q05.scc.bu.edu, 42935, None); 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removed 14 successfully in removeExecutor; 2019-01-22 13:12:06 DAGScheduler: INFO: Shuffle files lost for executor: 14 (epoch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:190210,ERROR,ERROR,190210,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"gWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://ww",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2516,echo,echo,2516,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,2,['echo'],['echo']
Availability,"gatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Interrupting monitor thread; 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Shutting down all executors; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asking each executor to shut down; 2019-01-22 13:12:06 SchedulerExtensionServices: INFO: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-22 13:12:06 YarnClientSchedulerBackend: INFO: Stopped; 2019-01-22 13:12:06 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!; 2019-01-22 13:12:06 MemoryStore: INFO: MemoryStore cleared; 2019-01-22 13:12:06 BlockManager: INFO: BlockManager stopped; 2019-01-22 13:12:06 BlockManagerMaster: INFO: BlockManagerMaster stopped; 2019-01-22 13:12:06 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!; 2019-01-22 13:12:06 TransportResponseHandler: ERROR: Still have 1 requests outstanding when connection from /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(Transpo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:209487,down,down,209487,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,5,"['ERROR', 'down']","['ERROR', 'down']"
Availability,ge$.using(package.scala:657); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); E 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$1(LocalBackend.scala:109); E 	at is.hail.backend.local.LocalBackend.executeToEncoded(LocalBackend.scala:208); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeEncode$2(LocalBackend.scala:237); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); E 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$1(LocalBackend.scala:109); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeEncode$1(LocalBackend.scala:236); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.backend.local.LocalBackend.executeEncode(LocalBackend.scala:234); E 	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E Hail version: 0.2.124-b31f1dcea5d2; E Error summary: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:15240,Error,Error,15240,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Error'],['Error']
Availability,"ge(1, 10)}. # import bgen(s); mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; ['dosage'],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # load scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/hail-0.1/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # merge sumstats on bgen matrixtable; mt = mt.annotate_rows(ss=sumstats[mt.locus]). # handle strand/sign flips -- score in terms of alt allele; mt = mt.annotate_rows(beta = hl.case(); .when(((mt.alleles[0] == mt.ss.nt1) &; (mt.alleles[1] == mt.ss.nt2)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt1) &; (flip_text(mt.alleles[1]) == mt.ss.nt2)),; (-1*mt.ss.ldpred_inf_beta)); .when(((mt.alleles[0] == mt.ss.nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArrayS",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2031,down,down,2031,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['down'],['down']
Availability,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:36,ERROR,ERROR,36,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019,1,['ERROR'],['ERROR']
Availability,"got a doctest error:; ```; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 035 trait 3 & trait 4 :math:`r_g` = 0.6; 036 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 037 ordered according to user specification and :math:`r_g` values are ordered by the ; 038 order in which the corresponding genetic covariance terms will appear in the ; 039 covariance matrix, reading lines in the upper triangular matrix from left to; 040 right, top to bottom (read first row left to right, read second row left to ; 041 right, etc.), exluding the diagonal.; 042 ; 043 >>> cov_matrix, rg = get_cov_matrix(h2=[0.1, 0.3, 0.2, 0.6], rg=[0.4, 0.3, 0.1, 0.2, 0.15, 0.6]); 044 >>> print(cov_matrix); Differences (unified diff with -expected +actual):; @@ -1,4 +1,4 @@; -array([[0.1 , 0.06928203, 0.04242641, 0.0244949 ],; - [0.06928203, 0.3 , 0.04898979, 0.06363961],; - [0.04242641, 0.04898979, 0.2 , 0.2078461 ],; - [0.0244949 , 0.06363961, 0.2078461 , 0.6 ]]; +[[0.1 0.06928203 0.04242641 0.0244949 ]; + [0.06928203 0.3 0.04898979 0.06363961]; + [0.04242641 0.04898979 0.2 0.2078461 ]; + [0.0244949 0.06363961 0.2078461 0.6 ]]. /hail/python/hail/experimental/ldscsim.py:44: DocTestFailure. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8265#issuecomment-601360575:14,error,error,14,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601360575,1,['error'],['error']
Availability,got test failure: https://ci.hail.is/batches/13670/jobs/33/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7092#issuecomment-533282019:9,failure,failure,9,https://hail.is,https://github.com/hail-is/hail/pull/7092#issuecomment-533282019,1,['failure'],['failure']
Availability,"haha, you are an Awk Queen! This looks great to me, I'm glad we were able to nail down the security.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9076#issuecomment-662526023:82,down,down,82,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662526023,1,['down'],['down']
Availability,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:1585,error,error,1585,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617,1,['error'],['error']
Availability,"hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT bu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3798,echo,echo,3798,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,hand deploying the last working master that is:; ```; * | | | | 49ec05df2 - (7 hours ago) [query] Throw a validation error for queries that read/write same path (#8327) - Tim Poterba (HEAD); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434:117,error,error,117,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434,1,['error'],['error']
Availability,"happens:). ```; >>> def serialize_test(n):; ... t = [i for i in range(n)]; ... t1 = datetime.now(); ... print(len(t)); ... print(hl.eval(hl.len(hl.array(t)))); ... t2 = datetime.now(); ... print(t2 - t1); ... ; >>> serialize_test(40000000); 40000000; Exception in thread ""Thread-2"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649); 	at java.lang.StringBuilder.append(StringBuilder.java:202); 	at py4j.StringUtil.unescape(StringUtil.java:57); 	at py4j.Protocol.getString(Protocol.java:475); 	at py4j.Protocol.getObject(Protocol.java:302); 	at py4j.commands.AbstractCommand.getArguments(AbstractCommand.java:82); 	at py4j.commands.CallCommand.execute(CallCommand.java:77); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:1116,ERROR,ERROR,1116,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['ERROR'],['ERROR']
Availability,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3741,error,error,3741,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['error'],['error']
Availability,"he result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2401,error,error,2401,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['error'],['error']
Availability,hedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionCon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216037,recover,recover,216037,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Availability,hedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215966,recover,recover,215966,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Availability,"heh, cc: @daniel-goldstein I think we both hit the same transient error",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11943#issuecomment-1163390424:66,error,error,66,https://hail.is,https://github.com/hail-is/hail/pull/11943#issuecomment-1163390424,1,['error'],['error']
Availability,"heh, wow, the situation wrt kube-dns is pretty bad eh? https://github.com/kubernetes/kubernetes/issues/57659. My understanding is that the other kube-system pods tolerate every taint (by their special kube-system nature), so I think this argument only applies to kube-dns, is that your understanding?. We should stay alert to DNS issues, even with this change, because all our DNS queries will be routed to non-preemptible nodes (see: https://github.com/kubernetes/kubernetes/issues/57659#issuecomment-477009356).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7784#issuecomment-570278981:162,toler,tolerate,162,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-570278981,1,['toler'],['tolerate']
Availability,"here are some example error msgs:; ```; In [2]: vds.linreg([]); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-b8bbc41a5ebd> in <module>(); ----> 1 vds.linreg([]). /Users/tpoterba/hail/python/hail/dataset.py in linreg(self, y, covariates, root, min_ac, min_af); 2216 """"""; 2217; -> 2218 jvds = self._jvdf.linreg(y, jarray(env.jvm.java.lang.String, covariates), root, min_ac, min_af); 2219 return VariantDataset(self.hc, jvds); 2220. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 113 if e.args[0].startswith('An error occurred while calling'):; 114 msg = 'An error occurred while calling into JVM, probably due to invalid parameter types'; --> 115 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (msg, e.message, msg)); 116; 117 return deco. FatalError: An error occurred while calling into JVM, probably due to invalid parameter types. Java stack trace:; An error occurred while calling o29.linreg. Trace:; py4j.Py4JException: Method linreg([class java.util.ArrayList, class [Ljava.lang.String;, class java.lang.String, class java.lang.Integer, class java.lang.Double]) does not exist; 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326); 	at py4j.Gateway.invoke(Gateway.java:272); 	at py4j.commands.AbstractCommand.invokeMe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190787,1,['error'],['error']
Availability,"here is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBufferSpec""}}}}; ```; Error for run 1.; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:1413,error,error,1413,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['error'],['error']
Availability,"hink for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I thought we decided we; > preferred that. I thought it would take less time to get a subdirectory working than figure out; how to add a new domain and a cert and deal with DNS. Long term a subdomain makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:2225,down,down,2225,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,2,['down'],['down']
Availability,"his pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3069,error,errors,3069,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['error'],['errors']
Availability,hmm; ```; $ make publish-python-dill && make mirror-dockerhub-images; DOCKER_PREFIX=us-docker.pkg.dev/hail-vdc/hail bash python-dill/push.sh; + for version in 3.8 3.8-slim 3.9 3.9-slim 3.10 3.10-slim 3.11 3.11-slim; + public=hailgenetics/python-dill:3.8; + DOCKER_BUILDKIT=1; + docker build --build-arg PYTHON_VERSION=3.8 --file Dockerfile.out --build-arg BUILDKIT_INLINE_CACHE=1 --tag hailgenetics/python-dill:3.8 .; [+] Building 0.0s (2/2) FINISHED docker:desktop-linux; => [internal] load build definition from Dockerfile.out 0.0s; => => transferring dockerfile: 2B 0.0s; => [internal] load .dockerignore 0.0s; => => transferring context: 2B 0.0s; ERROR: failed to solve: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount969738803/Dockerfile.out: no such file or directory; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13310#issuecomment-1652511856:651,ERROR,ERROR,651,https://hail.is,https://github.com/hail-is/hail/pull/13310#issuecomment-1652511856,1,['ERROR'],['ERROR']
Availability,"hread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:3120,error,error,3120,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['error'],['error']
Availability,http://discuss.hail.is/t/sample-qc-error/556,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3900#issuecomment-403211589:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/issues/3900#issuecomment-403211589,1,['error'],['error']
Availability,https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20writing.20vcf,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4011#issuecomment-408512050:76,Error,Error,76,https://hail.is,https://github.com/hail-is/hail/issues/4011#issuecomment-408512050,1,['Error'],['Error']
Availability,https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/Error.20summary.3A.20ClassTooLargeException.3A.20Class.20too.20large/near/419319423,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14232#issuecomment-1922106113:80,Error,Error,80,https://hail.is,https://github.com/hail-is/hail/pull/14232#issuecomment-1922106113,1,['Error'],['Error']
Availability,https://hail.zulipchat.com/#narrow/stream/379853-Hail.2FVariants/topic/Echo.20VDS/near/396801065,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13823#issuecomment-1764943591:71,Echo,Echo,71,https://hail.is,https://github.com/hail-is/hail/issues/13823#issuecomment-1764943591,1,['Echo'],['Echo']
Availability,i think the better solution is to check that the ordered key inside the partitioner is the same as the implicit ordered key supplied. this also could have caught the errors,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1864#issuecomment-302978822:166,error,errors,166,https://hail.is,https://github.com/hail-is/hail/pull/1864#issuecomment-302978822,1,['error'],['errors']
Availability,"if the code snippet that I wrote above works, I think we should either cache the results of `classAsBytes` and reuse them in `result`, or honestly just throw an error if you try to call `result` multiple times on the same function builder. (what would be the use case? If we want to use the same function multiple times we generally call `val f = fb.result()` once and use `f` as many times as necessary to get instances of the desired function.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7384#issuecomment-546392520:161,error,error,161,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546392520,1,['error'],['error']
Availability,il.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-f69b497; Error summary: SparkException: Failed to get broadcast_4_piece0 of broadcast_4; >>> ; ```; @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:8612,Error,Error,8612,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['Error'],['Error']
Availability,"ils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, cli",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29523,Down,Downloading,29523,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"imits; await _handle_api_error(_edit_billing_limit, db, billing_project, limit); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 212, in _handle_api_error; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2227, in _edit_billing_limit; await insert() # pylint: disable=no-value-for-parameter; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 34, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 64, in wrapper; return await fun(tx, *args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2212, in insert; (billing_project,),; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 209, in execute_and_fetchone; await cursor.execute(sql, args); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 672, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 1153, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 641, in _read_packet; packet.raise_for_error(); File ""/usr/local/lib/python3.7/dist-packages/pymysql/protocol.py"", line 221, in raise_for_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.7/dist-packages/pymysql/err.py"", line 143, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1205, 'Lock wait timeout exceeded; try restarting transaction'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586:2262,error,errorclass,2262,https://hail.is,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586,1,['error'],['errorclass']
Availability,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1442,down,downstream,1442,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927,2,['down'],['downstream']
Availability,"in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:2646,failure,failure,2646,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['failure'],['failure']
Availability,"ing too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:1167,Error,ErrorHandling,1167,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,2,['Error'],['ErrorHandling']
Availability,"ins.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1452,Error,ErrorLog,1452,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,2,"['Error', 'error']","['ErrorLog', 'error']"
Availability,introduced dummy failure to test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4448#issuecomment-424758306:17,failure,failure,17,https://hail.is,https://github.com/hail-is/hail/pull/4448#issuecomment-424758306,1,['failure'],['failure']
Availability,ion: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). ERROR SUMMARY: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:8236,ERROR,ERROR,8236,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['ERROR'],['ERROR']
Availability,"ion=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 maskName=Mask missingValuesInExpressionsShouldEvaluateAsFailing=false invalidatePreviousFilters=false filter_mismatching_base_and_quals=false""; ##contig=<ID=1,length=248956422>; ##contig=<ID=2,length=242193529>; ##contig=<ID=3,length=198295559>; ##contig=<ID=4,length=190214555>; ##contig=<ID=5,length=181538259>; ##contig=<ID=6,length=170805979>; ##contig=<ID=7,length=159345973>; ##contig=<ID=8,length=145138636>; ##contig=<ID=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:20364,mask,mask,20364,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['mask'],['mask']
Availability,"ionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1449,error,error,1449,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,2,['error'],['error']
Availability,"irectory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 211, in <module>; asyncio.run(main()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/asyncio/runners.py"", line 44, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 182, in main; files = json.loads(args.files); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/__init__.py"", line 346, in loads; return _default_decoder.decode(s); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```; What am I missing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163:2051,Error,Error,2051,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163,1,['Error'],['Error']
Availability,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1236,down,down,1236,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,2,['down'],['down']
Availability,"it is stock hail, but I'm running with a new GRCh38 VEP config + files from; gs://hail-common/vep/vep/GRCh38/vep85-GRCh38-init.sh. though I've run it on another VCF and didn't get that error. ; I'll rerun to make sure it wasn't a misconfiguration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372:185,error,error,185,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301756372,1,['error'],['error']
Availability,"it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the imperative style (yet), I wrote routines to convert them back and forth: `EmitCode.fromI { cb => ... }` provides a CodeBuilder and converts a resulting IEmitCode back to an EmitCode, and EmitCode.toI(cb) does the opposite. There is also `(Emit)CodeBuilder.scoped` that will run a code builder function and collect the code as a Code[Unit]. A second idea introduced in this PR is that some PType operations may only be available on a PSettable/PValue, rather than on PCode. The motivation is the canonical array ref implementation, which involves a lot of duplicate code generation. In the current setup, we can have a compound PValue, and this introduces a PCanonicalIndexableValue that has three fields: the base array address, the length and the elements addresss. FYI @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:2418,avail,available,2418,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413,2,['avail'],['available']
Availability,"it must be some system- or file-system- specific issue -- is there any possibility you could try to use 0.2.8 to read that file, and see if you get the error (or another one)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5744#issuecomment-479056057:152,error,error,152,https://hail.is,https://github.com/hail-is/hail/issues/5744#issuecomment-479056057,1,['error'],['error']
Availability,it's a bit further down but definitely there,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7644#issuecomment-560944851:19,down,down,19,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-560944851,1,['down'],['down']
Availability,"ite-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30226,Down,Downloading,30226,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"k.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:2172,Error,Error,2172,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['Error'],['Error']
Availability,"kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30400,Down,Downloading,30400,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"kUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'foo.py', '--cluster=dk', '--files=', '--py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip', '--properties=']' ret",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1475,avail,available,1475,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['avail'],['available']
Availability,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1680,Error,Error,1680,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,3,"['Error', 'error']","['Error', 'error']"
Availability,"l last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 5, in serialize_test; File ""<decorator-gen-466>"", line 2, in eval; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 158, in eval; return eval_typed(expression)[0]; File ""<decorator-gen-468>"", line 2, in eval_typed; File ""/Users/wang/code/hail/hail/python/hail/typecheck/check.py"", line 561, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/wang/code/hail/hail/python/hail/expr/expressions/expression_utils.py"", line 199, in eval_typed; return (Env.backend().execute(expression._ir), expression.dtype); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 94, in execute; self._to_java_ir(ir))); File ""/Users/wang/code/hail/hail/python/hail/backend/backend.py"", line 86, in _to_java_ir; code = r(ir); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 511, in render; return '(ArrayLen {})'.format(r(self.a)); File ""/Users/wang/code/hail/hail/python/hail/ir/renderer.py"", line 29, in __call__; return x.render(self); File ""/Users/wang/code/hail/hail/python/hail/ir/ir.py"", line 2003, in render; return f'(Literal {self._typ._parsable_string()} ' \; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 159, in escape_str; return Env.jutils().escapePyString(s); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/Users/wang/code/hail/hail/python/hail/utils/java.py"", line 215, in deco; return f(*args, **kwargs); File ""/Users/wang/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 327, in get_return_value; py4j.protocol.Py4JError: An error occurred while calling o33.escapePyString; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184:3928,error,error,3928,https://hail.is,https://github.com/hail-is/hail/issues/5407#issuecomment-474983184,1,['error'],['error']
Availability,"l/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has happened in 2/8 test_dataproc steps that I have run myself or seen run. The more workers you have, the higher the chance at least one worker fails. As @bpblanken suggested [here](https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412), restarting docker on a failed worker works. Docker starts fine. However, I missed a subtlety: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13347,failure,failure,13347,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['failure'],['failure']
Availability,"l` module. It's a collection of, well, experimental tools that we're working on but which are not yet ready for broad use. A couple years ago, the functionality which you seek was moved to [`hail.vds`](https://hail.is/docs/0.2/vds/index.html):; - `hl.vds.lgt_to_get`; - Densify has become `hl.vds.to_dense_mt`. You're not the first person to report some surprise at the disappearance of `hail.experimental` items. I've made a note to clarify the description of the `hail.experimental` module. If you do need the docs for the old version of Hail you can find them [in the code](https://github.com/hail-is/hail/blob/0.2.119/hail/python/hail/experimental/vcf_combiner/densify.py) though I understand that's a more frustrating viewing experience. Since you're referencing the old GVCF combiner, it sounds like you might be using what we call the old ""merged sparse representation"". In this representation, the reference and variant data is stored together in one Hail Matrix Table. We have found this representation error prone. Analysts often accidentally forget to properly handle the reference blocks. This representation is also slightly large than our new split representation: VDS. You can convert back and forth between VDS and the ""merged sparse representation"":; 1. VDS -> merged sparse representation: [`hl.vds.to_merged_sparse_representation(vds)`](https://hail.is/docs/0.2/vds/hail.vds.to_merged_sparse_mt.html#hail.vds.to_merged_sparse_mt) (but please note that the VDS does not necessarily store a reference allele for each reference block, so you might need to provide one [see details here](https://discuss.hail.is/t/vds-to-merged-sparse-matrix/3478/3)); 2. [`hl.vds.VariantDataset.from_merged_representation(sparse_mt)`](https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html#hail.vds.VariantDataset.from_merged_representation). We recommend that new projects use the VDS. It is a high-quality, durable solution which we've used to combine and analyze as many as 250k genomes and also ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13377#issuecomment-1666132086:1206,error,error,1206,https://hail.is,https://github.com/hail-is/hail/issues/13377#issuecomment-1666132086,1,['error'],['error']
Availability,"ld 1: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[+PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[+PInt32]}. Child 2: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[PInt32]}. No problems doing:. ```python; mt1 = hl.import_vcf(resource('sample.vcf'), array_elements_required=True); mt2 = hl.import_vcf(resource('sample_missing_pl2.vcf'), array_elements_required=False); mt1.entries().union(mt2.entries()).count(); ```. Also shows me that array_elements_required=True really does force requiredeness to be passed on the virtual type; by setting the array_elements_required=True on the vcf file with missing PL values, I can also trigger an error LoweringPipeline, suggesting that requiredeness (and other type information) is not overridden by reading over the data first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-581048722:2447,error,error,2447,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581048722,1,['error'],['error']
Availability,"ler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:13513,AVAIL,AVAILABLE,13513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,"ler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.comp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6721,AVAIL,AVAILABLE,6721,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,let's meet tomorrow either before lab meeting (~9:40) or after lunch to make a plan? There's definitely a problem with the way something is getting rebuilt. The extra failures should be things we can resolve as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-434121212:167,failure,failures,167,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-434121212,1,['failure'],['failures']
Availability,"libs/hail-all-spark.jar --conf=spark.executor.extraClassPath=./hail-all-spark.jar; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Aug 4 2017, 00:39:18) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1176,ERROR,ERROR,1176,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['ERROR'],['ERROR']
Availability,"lient = <hailtop.batch_client.client.BatchClient object at 0x7f9cbc23b610>. async def test_callback(client):; import nest_asyncio # pylint: disable=import-outside-toplevel; ; nest_asyncio.apply(); ; app = web.Application(); callback_bodies = []; callback_event = asyncio.Event(); ; def url_for(uri):; host = os.environ['HAIL_BATCH_WORKER_IP']; port = os.environ['HAIL_BATCH_WORKER_PORT']; return f'http://{host}:{port}{uri}'; ; async def callback(request):; body = await request.json(); callback_bodies.append(body); callback_event.set(); return web.Response(); ; app.add_routes([web.post('/test', callback)]); runner = web.AppRunner(app); await runner.setup(); site = web.TCPSite(runner, '0.0.0.0', 5000); await site.start(); ; try:; token = secrets.token_urlsafe(32); b = create_batch(; client, callback=url_for('/test'), attributes={'foo': 'bar', 'name': 'test_callback'}, token=token; ); head = b.create_job('alpine:3.8', command=['echo', 'head']); b.create_job('alpine:3.8', command=['echo', 'tail'], parents=[head]); b.submit(); await asyncio.wait_for(callback_event.wait(), 5 * 60); callback_body = callback_bodies[0]; ; # verify required fields present; callback_body.pop('cost'); callback_body.pop('msec_mcpu'); callback_body.pop('time_created'); callback_body.pop('time_closed'); callback_body.pop('time_completed'); callback_body.pop('duration'); callback_body.pop('duration_ms'); callback_body.pop('cost_breakdown'); > assert callback_body == {; 'id': b.id,; 'user': 'test',; 'billing_project': 'test',; 'token': token,; 'state': 'success',; 'complete': True,; 'closed': True,; 'n_jobs': 2,; 'n_completed': 2,; 'n_succeeded': 2,; 'n_failed': 0,; 'n_cancelled': 0,; 'attributes': {'foo': 'bar', 'name': 'test_callback'},; }, callback_body; E AssertionError: {'attributes': {'client_job': '8051758-182', 'foo': 'bar', 'name': 'test_callback'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}; E assert {'id': 260, 'user': 'test', 'billing_project': 'test', 'token': 'dL_z32",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427:1178,echo,echo,1178,https://hail.is,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427,1,['echo'],['echo']
Availability,"local notebook works fine for me as well, looks to be just dataproc that's not working as expected. submitting that test command as a script finished in 36.2s. notebook is currently still hanging with this output (it's been 11 minutes):; ```; BokehJS 3.2.2 successfully loaded.; Initializing Hail with default parameters...; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; SPARKMONITOR_LISTENER: Started SparkListener for Jupyter Notebook; SPARKMONITOR_LISTENER: Port obtained from environment: 55989; SPARKMONITOR_LISTENER: Application Started: application_1695402030462_0001 ...Start Time: 1695402594764; Running on Apache Spark version 3.3.0; SparkUI available at http://notebook-slowdown-repro-m.c.broad-ctsa.internal:43055/; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-ee7fef6fc40d; LOGGING: writing to /home/hail/hail-20230922-1709-0.2.124-ee7fef6fc40d.log; [Stage 0:> (0 + 2) / 2]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1731791216:738,avail,available,738,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1731791216,1,['avail'],['available']
Availability,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:65,down,down,65,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727,2,"['down', 'failure']","['down', 'failures']"
Availability,looking at the failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3414#issuecomment-383174303:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/3414#issuecomment-383174303,1,['failure'],['failures']
Availability,looking into test failure,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2531#issuecomment-350003512:18,failure,failure,18,https://hail.is,https://github.com/hail-is/hail/pull/2531#issuecomment-350003512,1,['failure'],['failure']
Availability,"looks good to me, I downloaded the docs and poked around. Much improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7047#issuecomment-530825854:20,down,downloaded,20,https://hail.is,https://github.com/hail-is/hail/pull/7047#issuecomment-530825854,1,['down'],['downloaded']
Availability,"looks like a compile error in TestUtils.scala. The rest looks good, will approve when tests pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059:21,error,error,21,https://hail.is,https://github.com/hail-is/hail/pull/5077#issuecomment-452945059,1,['error'],['error']
Availability,looks like a random failure in test_batch. I retried the build,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10058#issuecomment-781729645:20,failure,failure,20,https://hail.is,https://github.com/hail-is/hail/pull/10058#issuecomment-781729645,1,['failure'],['failure']
Availability,looks like a rebase error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7134#issuecomment-558211252:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-558211252,1,['error'],['error']
Availability,"ls.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); 	at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.107-2387bb00ceee; Error summary: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. ```; [hail-20230803-1955-0.2.107-2387bb00ceee.log](https://github.com/hail-is/hail/files/12254716/hail-20230803-1955-0.2.107-2387bb00ceee.log); Log file attached.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:10846,Error,Error,10846,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,3,"['Error', 'failure']","['Error', 'failure']"
Availability,ly$20.apply(ContextRDD.scala:280); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-76c42fe; Error summary: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:9561,Error,Error,9561,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['Error'],['Error']
Availability,match error on ReadMatrix,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840#issuecomment-401624633:6,error,error,6,https://hail.is,https://github.com/hail-is/hail/pull/3840#issuecomment-401624633,1,['error'],['error']
Availability,maybe we can raise an error on array iteration instead of unsupported operation exception,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4081#issuecomment-410359446:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/issues/4081#issuecomment-410359446,1,['error'],['error']
Availability,"maybe we should check the result type in anything that could localize an unrealizable python data structure, and error? Places like collect/take/aggregate?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1694#issuecomment-365623725:113,error,error,113,https://hail.is,https://github.com/hail-is/hail/issues/1694#issuecomment-365623725,1,['error'],['error']
Availability,maybe we should throw a better error here?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7797#issuecomment-570617965:31,error,error,31,https://hail.is,https://github.com/hail-is/hail/issues/7797#issuecomment-570617965,1,['error'],['error']
Availability,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1580,avail,available,1580,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['avail'],['available']
Availability,"mem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:2431,avail,available,2431,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['avail'],['available']
Availability,"mespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instan",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11043,error,error,11043,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['error'],['error']
Availability,moved to https://discuss.hail.is/t/error-building-jar-for-hail-0-2-11/912,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5659#issuecomment-475210838:35,error,error-building-jar-for-hail-,35,https://hail.is,https://github.com/hail-is/hail/issues/5659#issuecomment-475210838,1,['error'],['error-building-jar-for-hail-']
Availability,"n n1-highmem-8 driver. The cluster is created by hailctl with no custom driver settings; <details><summary>template for hailctl dataproc start</summary>. [Source](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/extract/run_in_hail_cluster.py#L36C1-L48C1). ```; hailctl dataproc start ; --autoscaling-policy={autoscaling_policy}; --worker-machine-type {worker_machine_type}; --region {region}; --project {gcs_project}; --service-account {account}; --num-master-local-ssds 1; --num-worker-local-ssds 1 ; --max-idle=60m; --max-age=1440m; --subnet=projects/{gcs_project}/regions/{region}/subnetworks/subnetwork; {cluster_name}; ```. </details>. I have the driver node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:1192,avail,avail,1192,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,1,['avail'],['avail']
Availability,"n-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs: Collecting google-auth==1.27.0; 931 | amazon-ebs: Downloading google_auth-1.27.0-py2.py3-none-any.whl (135 kB); 932 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 30.6 MB/s eta 0:00:00; 933 | amazon-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Req",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:4401,Down,Downloading,4401,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238901220:1436,error,error,1436,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220,2,['error'],['error']
Availability,"nagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4174,AVAIL,AVAILABLE,4174,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"ndComputeWithIndex/s_yyHm37RY7YTSWH29gP5SM0RwKxgs9EXbg9_YMf7ho= with scratch directory '/batch/1c00c7157d4d41bcbf508f12d75329b1'; 2023-09-13 16:37:36.617 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-13 16:37:36.821 services: WARN: A limited retry error has occured. We will automatically retry 4 more times. Do not be alarmed. (next delay: 1938). The most recent error was javax.net.ssl.SSLException: Connection reset.; 2023-09-13 16:37:38.893 WorkerTimer$: INFO: readInputs took 2278.496020 ms.; 2023-09-13 16:37:38.893 : INFO: RegionPool: initialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Cau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2337,ERROR,ERROR,2337,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,1,['ERROR'],['ERROR']
Availability,"nder.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648); 	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514); 	at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440); 	at is.hail.HailContext$.configureLogging(HailContext.scala:132); 	at is.hail.HailContext$.apply(HailContext.scala:159); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-289>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); File ""/opt/Software/hail/python/hail/java.py"", line 42, in hc; raise EnvironmentError('no Hail context initialized, create one first'); EnvironmentError: no Hail context initialized, create one first; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:3372,Error,Error,3372,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['Error'],['Error']
Availability,ndering if the problem is actually workload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 G,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1284,Down,Download,1284,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,"ne-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.0rc1 Requires-Python >=3.8; 1.22.0rc2 Requires-Python >=3.8; 1.22.0rc3 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Require",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:7852,ERROR,ERROR,7852,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['ERROR'],['ERROR']
Availability,"nect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg St",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5068,error,errors,5068,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['error'],['errors']
Availability,needs a bump. There's also a weird failure in the tutorial docs - seems like something is generating an `inf` that Python doesn't expect,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6742#issuecomment-519546529:35,failure,failure,35,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-519546529,1,['failure'],['failure']
Availability,"needs bump, looks like random failure (shuffler was last test run)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9223#issuecomment-670168023:30,failure,failure,30,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670168023,1,['failure'],['failure']
Availability,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1357,down,downward,1357,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,2,['down'],['downward']
Availability,"nfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs://hail-common/builds/devel/jars/hail-devel-38dbf156b630-Spark-2.2.0.jar', '--py-files=gs://hail-common/builds/devel/python/hail-devel-38dbf156b630.zip,/var/folders/rn/t2xcx1ps4h96txll46qkkfsj2q8bnl/T/pyscripts_fYVAte.zip', '--p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7465,Error,Error,7465,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['Error'],['Error']
Availability,"ng Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:2493,Error,ErrorHandling,2493,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,2,['Error'],"['Error', 'ErrorHandling']"
Availability,"ng pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30317,Down,Downloading,30317,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"no need to apologize for this! there was extra code hanging around that just added noise, so more work was definitely required. It's also good for my own development to slow down and be more attentive to that kind of thing sometimes :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2901#issuecomment-367128376:174,down,down,174,https://hail.is,https://github.com/hail-is/hail/pull/2901#issuecomment-367128376,1,['down'],['down']
Availability,nobody's complained about this in a long time. our errors are pretty good here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/258#issuecomment-301789487:51,error,errors,51,https://hail.is,https://github.com/hail-is/hail/issues/258#issuecomment-301789487,1,['error'],['errors']
Availability,"note this doesn't add the errors everywhere, just the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11641#issuecomment-1075436071:26,error,errors,26,https://hail.is,https://github.com/hail-is/hail/pull/11641#issuecomment-1075436071,1,['error'],['errors']
Availability,"nt = <py4j.java_gateway.GatewayClient object at 0x11098bc50>, target_id = 'z:is.hail.HailContext', name = 'apply'. def get_return_value(answer, gateway_client, target_id=None, name=None):; """"""Converts an answer received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCom",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:3870,error,error,3870,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['error'],['error']
Availability,"ny.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3866,echo,echo,3866,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"nylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6828,Down,Downloading,6828,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"o give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Not existing user-specified remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:4063,ERROR,ERROR,4063,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['ERROR'],['ERROR']
Availability,"o pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getFileStatus(GoogleHadoopFileSystem.java:724); E 	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115); E 	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349); E 	at org.apache.hadoop.fs.Globber.glob(Globber.java:202); E 	at org.apache.hadoop.fs.FileSystem.globStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1561,Error,Error,1561,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['Error'],['Error']
Availability,"ob:; ```python; utils.py	retry_long_running:923	in delete_prev_cancelled_job_group_cancellable_resources_records	; Traceback (most recent call last):; File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 915, in retry_long_running; return await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/hailtop/utils/utils.py"", line 959, in loop; await f(*args, **kwargs)\n File ""/usr/local/lib/python3.9/dist-packages/batch/driver/main.py"", line 1485, in delete_prev_cancelled_job_group_cancellable_resources_records; async for target in targets:\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 334, in execute_and_fetchall; async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1054, ""Unknown column 'cancelled.id' in 'on clause'""); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340:1929,error,errorclass,1929,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2349752340,1,['error'],['errorclass']
Availability,"odified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1246,Avail,Available,1246,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['Avail'],['Available']
Availability,"oh, crap - the lowering rule for MatrixEntriesTable has the same problem:; ```; E Current key: [1:10000,[A,G],sample_001]; E Previous key: [1:10000,[A,G],sample_500]; E This error can occur after a split_multi if the dataset; E contains both multiallelic variants and duplicated loci.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6267#issuecomment-499296452:174,error,error,174,https://hail.is,https://github.com/hail-is/hail/pull/6267#issuecomment-499296452,1,['error'],['error']
Availability,"oh, man, this is super exciting. 3x on the combiner? yes please!. We can probably make incremental performance improvements to the LIR method splitting code to bring the compile and execute back down, and that one I consider a little less critical anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044:195,down,down,195,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044,1,['down'],['down']
Availability,"oh, that's it. You can't do that. This is still a bad error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4081#issuecomment-410356589:54,error,error,54,https://hail.is,https://github.com/hail-is/hail/issues/4081#issuecomment-410356589,1,['error'],['error']
Availability,"ok, so back to the same error. I think your Spark cluster must be configured incorrectly, which is something we can't really help with. You should be able to do:; ```; pyspark; ```; and then:; ```python; sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); ```; without error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321224012:24,error,error,24,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321224012,2,['error'],['error']
Availability,"ok. Figured it out. The issue is that the build of the new image finally succeeded after 18 minutes, but the test timed out and batch must have been shut down already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-614882682:154,down,down,154,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-614882682,1,['down'],['down']
Availability,"okay, now ready. In addition to the bugfix and expr array features, fixed a lingering stupid problem in our error traversal method",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/574#issuecomment-239036784:108,error,error,108,https://hail.is,https://github.com/hail-is/hail/pull/574#issuecomment-239036784,1,['error'],['error']
Availability,"okay, so this makes no sense to me, and i don't understand gradle at all really, but i tried reproducing the issue with each recent release until i found the one where it started presenting (0.2.123), then tried it on every commit in between the previous release and that one, and found that the issue started presenting after https://github.com/hail-is/hail/pull/13551 merged. i tried reverting that commit on the current `main` and confirmed the issue stopped showing up. i also tried downgrading just the `google-cloud-storage` version back to 2.17.1, since that was bumped in that commit, but the issue still presented.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1737808187:487,down,downgrading,487,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1737808187,1,['down'],['downgrading']
Availability,"okay, so this must be some problem with the code, not the file format. Can you reproduce the error in the current version from the new files?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743#issuecomment-358467186:93,error,error,93,https://hail.is,https://github.com/hail-is/hail/issues/2743#issuecomment-358467186,1,['error'],['error']
Availability,"on-autosomes for `concordance`. Importing two VCFs, splitting multi, and `describe()`, `count()`, and `_force_count_rows()` all behave as expected. ```; import hail as hl. giab_gs_path = 'gs://xxxxxxxx'; giab_ds = hl.import_vcf(giab_gs_path, reference_genome='GRCh38'); giab_sm = hl.SplitMulti(giab_ds); giab_ds = giab_sm.result(); giab_ds.describe(); gaib_ds.count(); gaib_ds._force_count_rows(); # all good. sent_gs_path = 'gs://xxxxxxxxxxx'; sent_ds = hl.import_vcf(sent_gs_path, reference_genome='GRCh38'); sent_sm = hl.SplitMulti(sent_ds); sent_ds = sent_sm.result(); sent_ds.describe(); sent_ds.count(); sent_ds._force_count_rows(); # all good. # then this gives the stack trace below. giab_22_ds = hl.filter_intervals(giab_ds, [hl.parse_locus_interval('chr22', reference_genome='GRCh38')]); ```. Stack trace:. ```; FatalError: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 20 times, most recent failure: Lost task 0.19 in stage 19.0 (TID 312, gilson-validation-test-2-w-4.c.perfect-atrium-179917.internal, executor 2): java.lang.ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1641); 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1637); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun$apply$20.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.Con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:1086,failure,failure,1086,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['failure'],['failure']
Availability,"on-ebs: Collecting google-cloud-storage==1.25.*; 934 | amazon-ebs: Downloading google_cloud_storage-1.25.0-py2.py3-none-any.whl (73 kB); 935 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00; 936 | amazon-ebs: Collecting humanize==1.0.0; 937 | amazon-ebs: Downloading humanize-1.0.0-py2.py3-none-any.whl (51 kB); 938 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.9/51.9 kB 14.6 MB/s eta 0:00:00; 939 | amazon-ebs: Collecting hurry.filesize==0.9; 940 | amazon-ebs: Downloading hurry.filesize-0.9.tar.gz (2.8 kB); 941 | amazon-ebs: Preparing metadata (setup.py): started; 942 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 943 | amazon-ebs: Collecting janus<1.1,>=0.6; 944 | amazon-ebs: Downloading janus-1.0.0-py3-none-any.whl (6.9 kB); 945 | amazon-ebs: Requirement already satisfied: Jinja2==3.0.3 in /usr/local/lib/python3.7/site-packages (3.0.3); 946 | amazon-ebs: Collecting nest_asyncio==1.5.4; 947 | amazon-ebs: Downloading nest_asyncio-1.5.4-py3-none-any.whl (5.1 kB); 948 | amazon-ebs: Requirement already satisfied: numpy<2 in /usr/local/lib64/python3.7/site-packages (1.21.6); 949 | amazon-ebs: Collecting orjson==3.6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:5565,Down,Downloading,5565,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"on3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in persist(self, storage_level); 2110 Persisted table.; 2111 """"""; -> 2112 return Env.backend().persist(self); 2113 ; 2114 def unpersist(self) -> 'Table':. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/backend/backend.py in persist(self, dataset); 167 from hail.context import TemporaryFilename; 168 tempfile = TemporaryFilename(prefix=f'persist_{type(dataset).__name__}'); --> 169 persisted = dataset.checkpoint(tempfile.__enter__()); 170 self._persisted_locations[persisted] = (tempfile, dataset); 171 return persisted. <decorator-gen-1330> in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals). /opt/conda/miniconda3/lib/python3.10/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 585 def wrapper(__original_func: Callable[..., T], *args, **kwargs) -> T:; 586 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 587 return __original_func(*args_, **kwargs_); 588 ; 589 return wrapper. /opt/conda/miniconda3/lib/python3.10/site-packages/hail/table.py in checkpoint(self, output, overwrite, stage_locally, _codec_spec, _read_if_exists, _intervals, _filter_intervals); 1329 ; 1330 if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; -> 1331 self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); 1332 _assert_type = self._type; 1333 _load_refs = False. <decorator-gen-1332> in write(self, output, overwrite, stage_locally, _codec_spec). /opt/conda/miniconda3/lib/python3.10/site-package",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124:3802,checkpoint,checkpoint,3802,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1734196124,1,['checkpoint'],['checkpoint']
Availability,"on=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_note",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2076,echo,echo,2076,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/658#issuecomment-242136648:28,error,error,28,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648,1,['error'],['error']
Availability,"onents/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has hap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13049,down,download,13049,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['down'],['download']
Availability,"ook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1445,toler,tolerate,1445,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,2,['toler'],['tolerate']
Availability,oops my fault.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4394#issuecomment-423657075:8,fault,fault,8,https://hail.is,https://github.com/hail-is/hail/issues/4394#issuecomment-423657075,1,['fault'],['fault']
Availability,oops. thanks for preempting the test failure!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4380#issuecomment-423268427:37,failure,failure,37,https://hail.is,https://github.com/hail-is/hail/pull/4380#issuecomment-423268427,1,['failure'],['failure']
Availability,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:75536,ERROR,ERROR,75536,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:98443,ERROR,ERROR,98443,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,op.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.Future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:126080,ERROR,ERROR,126080,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=Dels,Number=1,Type=Float,Description=""Fraction of Reads Containing Spanning Deletions"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=HaplotypeScore,Number=1,Type=Float,Description=""Consistency of the site with at most two segregating haplotypes"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=OriginalContig,Number=1,Type=String,Description=""The name of the source contig/chromosome prior to liftover."">; ##INFO=<ID=OriginalStart,Number=1,Type=String,Description=""The p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:6991,down,downsampled,6991,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['down'],['downsampled']
Availability,other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; adding 'hail/experimental/codec.py'; adding 'hail/experimental/compile.py'; adding 'hail/experimental/datasets.json'; adding 'hail/experimental/datasets.py'; a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:17006,avail,available,17006,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['avail'],['available']
Availability,"ow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df17cef; Error summary: IndexOutOfBoundsException: 3; ```; (NB: a custom VEP/LOFTEE, but that shouldn't matter - ran same thing on `devel-cd48e11` and it worked fine)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:7097,Error,Error,7097,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Error'],['Error']
Availability,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52482,ERROR,ERROR,52482,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60953,ERROR,ERROR,60953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,pache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPool,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:68097,ERROR,ERROR,68097,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-lim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1368,error,error,1368,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['error'],['error']
Availability,ping,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2322#issuecomment-339350297:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/2322#issuecomment-339350297,1,['ping'],['ping']
Availability,"ping @catoverdrive , should be addressed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6912#issuecomment-524057804:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/6912#issuecomment-524057804,1,['ping'],['ping']
Availability,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,3,"['failure', 'ping']","['failure', 'ping']"
Availability,ping @cseed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4427#issuecomment-424465040:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/4427#issuecomment-424465040,1,['ping'],['ping']
Availability,ping @danking,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5369#issuecomment-466130520:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-466130520,2,['ping'],['ping']
Availability,"ping @danking , should be set",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5523#issuecomment-469831308:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/5523#issuecomment-469831308,1,['ping'],['ping']
Availability,ping @jbloom22,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2505#issuecomment-348619118:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/2505#issuecomment-348619118,1,['ping'],['ping']
Availability,ping @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4924#issuecomment-446410064:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-446410064,1,['ping'],['ping']
Availability,ping @jigold would be stellar if you can look at it before end of day today :3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3698#issuecomment-393986893:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/3698#issuecomment-393986893,1,['ping'],['ping']
Availability,ping @rcownie,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3546#issuecomment-390226011:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/3546#issuecomment-390226011,1,['ping'],['ping']
Availability,ping https://discuss.hail.is/t/issue-with-hl-experimental-densify/2748/3 when this merges.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12121#issuecomment-1222859271:0,ping,ping,0,https://hail.is,https://github.com/hail-is/hail/pull/12121#issuecomment-1222859271,1,['ping'],['ping']
Availability,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12320,Error,Error,12320,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,1,['Error'],['Error']
Availability,"plyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:6744,failure,failures,6744,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,1,['failure'],['failures']
Availability,"port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5067b2fc{/storage/rdd/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5058985f{/environment,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@57318cba{/environment/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:5054,AVAIL,AVAILABLE,5054,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"pper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2100,failure,failure,2100,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,2,['failure'],['failure']
Availability,"ppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:1114,down,down,1114,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987,2,['down'],['down']
Availability,"precated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloadin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2134,Down,Downloading,2134,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,presumably related to: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/mysterious.20behavior/near/182390817 (log posted next message down),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746:161,down,down,161,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746,1,['down'],['down']
Availability,"priate init method. Edit: re-running ./gradlew shadowJar fixes this. Tests are not passing, but seemingly not because of any of these changes. If I checkout last commit in master, they also fail. Furthermore, I can restore all changes from last FS PR manually, and no benefit. ```; HEAD is now at 117c365c3 [ci] also handle batch Ready state (#5909); (hail) alex:~/projects/hail/hail/python:$ pytest test/ -x; ======================================================================================= test session starts ========================================================================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1028,ERROR,ERRORS,1028,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,2,['ERROR'],"['ERROR', 'ERRORS']"
Availability,"proc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:4096,echo,echo,4096,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"putCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3520067{/storage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@ca57ac0{/storage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@24ce0621{/storage/rdd,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4669,AVAIL,AVAILABLE,4669,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"qxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1195,error,error,1195,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['error'],['error']
Availability,"r getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cdf8858{/stages/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3418c91b{/stages/stage,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6e2585c5{/stages/stage/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2063dbf5{/stages/pool,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4035fb2e{/stages/pool/json,null",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:4298,AVAIL,AVAILABLE,4298,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['AVAIL'],['AVAILABLE']
Availability,"r node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43 of 52223 MiB ( 0.08%), swap free: 0 of 0 MiB ( 0.00%); Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: low memory! at or below SIGTERM limits: mem 0.12%, swap 1.00%; Nov 22 15:31:09 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM to process 8421 uid 0 ""java"": badness 1852, VmRSS 40578 MiB; ```. Indeed, the VmRSS is the memory in use from the kernel's perspective so it includes any off-heap memory created by Hail. The Hail log indicates the region pools are tiny, ~10s of MiB. Not a concern. After the JVM is killed, memory jumps back up to 40683MiB (which checks out, that's roughly what the killed process was using).; ```; Nov 22 15:31:10 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 40683 of 52223 MiB (77.90%), swap free: 0 of 0 MiB ( 0.00%); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:2001,avail,avail,2001,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,2,['avail'],['avail']
Availability,"r received from the Java gateway into a Python object.; ; For example, string representation of integers are converted to Python; integer, string representation of objects are converted to JavaObject; instances, etc.; ; :param answer: the string returned by the Java gateway; :param gateway_client: the gateway client used to communicate with the Java; Gateway. Only necessary if the answer is a reference (e.g., object,; list, map); :param target_id: the name of the object from which the answer comes from; (e.g., *object1* in `object1.hello()`). Optional.; :param name: the name of the member from which the answer comes from; (e.g., *hello* in `object1.hello()`). Optional.; """"""; if is_error(answer)[0]:; if len(answer) > 1:; type = answer[1]; value = OUTPUT_CONVERTER[type](answer[2:], gateway_client); if answer[1] == REFERENCE_TYPE:; raise Py4JJavaError(; ""An error occurred while calling {0}{1}{2}.\n"".; format(target_id, ""."", name), value); else:; raise Py4JError(; ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; > format(target_id, ""."", name, value)); E py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; E py4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist; E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318); E 	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339); E 	at py4j.Gateway.invoke(Gateway.java:276); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748). /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/protocol.py:332: Py4JError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:4109,error,error,4109,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['error'],['error']
Availability,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2058,error,error,2058,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['error'],['error']
Availability,random test failure...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6349#issuecomment-504085816:12,failure,failure,12,https://hail.is,https://github.com/hail-is/hail/pull/6349#issuecomment-504085816,1,['failure'],['failure']
Availability,"rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 42.509 secs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:3027,Error,Error,3027,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,3,"['Error', 'FAILURE', 'error']","['Error', 'FAILURE', 'errors']"
Availability,"re: test failures, that policy makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3322#issuecomment-379580976:9,failure,failures,9,https://hail.is,https://github.com/hail-is/hail/pull/3322#issuecomment-379580976,1,['failure'],['failures']
Availability,ready to approve once tests passing (test_hail_java has current failures),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8243#issuecomment-594854003:64,failure,failures,64,https://hail.is,https://github.com/hail-is/hail/pull/8243#issuecomment-594854003,1,['failure'],['failures']
Availability,"rebased, and yet another transient error added (EAI_EAGAIN, temporary failure in name resolution).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10390#issuecomment-832198390:35,error,error,35,https://hail.is,https://github.com/hail-is/hail/pull/10390#issuecomment-832198390,2,"['error', 'failure']","['error', 'failure']"
Availability,"ref: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20importing.20dbsnp.20VCF/near/177209550. I have confirmed that the test I added fails with the same error laurent saw in the linked conversation, and passes with the change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919:81,Error,Error,81,https://hail.is,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919,2,"['Error', 'error']","['Error', 'error']"
Availability,"regarding doc test failure, you need `g.DP`, not `g.dp`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2497#issuecomment-348701713:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/2497#issuecomment-348701713,1,['failure'],['failure']
Availability,release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/p,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5560,echo,echo,5560,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,report/accumulators have been removed: https://github.com/hail-is/hail/pull/2024. No longer relevant. .... although better input integrity checking and error reporting would still be nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-319497230:152,error,error,152,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-319497230,1,['error'],['error']
Availability,"return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_server_config_namespace,; - 'name': 'database-server-config',; - 'mount_path': '/sql-config',; - }; - ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; - parents=parents,; - always_run=True,; - network='private',; - regions=[REGION],; - ); + pass; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index e6f67bb486..662c873590 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -190,7 +190,7 @@ steps:; to: /io/pyproject.toml; dependsOn:; - hello_image; - - kind: createDatabase; + - kind: createDatabase2; name: hello_database; databaseName: hello; image:; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:3199,echo,echo,3199,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,1,['echo'],['echo']
Availability,"rg.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.lau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3201,failure,failure,3201,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['failure'],['failure']
Availability,"rg.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.serv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194331,ERROR,ERROR,194331,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"rk-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:2399,Error,ErrorHandling,2399,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['Error'],['ErrorHandling']
Availability,rkload-dependent and is based on the number of jobs / number of files. The GCS best practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1320,Down,Download,1320,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,"rs $HAIL_HOME/build/libs/hail-all-spark.jar --conf=spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar --conf=spark.executor.extraClassPath=./hail-all-spark.jar; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Aug 4 2017, 00:39:18) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768); 	at org.ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:1101,avail,available,1101,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['avail'],['available']
Availability,"rs or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillment rate was, what our error rate was, what the conversion rate is ( users that visited and created / users that visited our service). Cons against using a separate data store: non-atomic operations at the boundary between sql and k8. This is isn't a problem if we choose one master view (our db), and frankly seems unavoidable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2567,outage,outages,2567,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,2,"['error', 'outage']","['error', 'outages']"
Availability,"s Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1873,avail,available,1873,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['avail'],['available']
Availability,"s) = (result_tuple._1(), result_tuple._2()); 100 value = ir.typ._from_encoding(result); 102 return (value, timings) if timed else value. File ~/mambaforge/lib/python3.9/site-packages/py4j/java_gateway.py:1304, in JavaMember.__call__(self, *args); 1298 command = proto.CALL_COMMAND_NAME +\; 1299 self.command_header +\; 1300 args_command +\; 1301 proto.END_COMMAND_PART; 1303 answer = self.gateway_client.send_command(command); -> 1304 return_value = get_return_value(; 1305 answer, self.gateway_client, self.target_id, self.name); 1307 for temp_arg in temp_args:; 1308 temp_arg._detach(). File ~/mambaforge/lib/python3.9/site-packages/hail/backend/py4j_backend.py:21, in handle_java_exception.<locals>.deco(*args, **kwargs); 19 import pyspark; 20 try:; ---> 21 return f(*args, **kwargs); 22 except py4j.protocol.Py4JJavaError as e:; 23 s = e.java_exception.toString(). File ~/mambaforge/lib/python3.9/site-packages/py4j/protocol.py:330, in get_return_value(answer, gateway_client, target_id, name); 326 raise Py4JJavaError(; 327 ""An error occurred while calling {0}{1}{2}.\n"".; 328 format(target_id, ""."", name), value); 329 else:; --> 330 raise Py4JError(; 331 ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".; 332 format(target_id, ""."", name, value)); 333 else:; 334 raise Py4JError(; 335 ""An error occurred while calling {0}{1}{2}"".; 336 format(target_id, ""."", name)). Py4JError: An error occurred while calling o83._1. Trace:; java.lang.NegativeArraySizeException: -1966455376; 	at py4j.Base64.encodeToChar(Base64.java:681); 	at py4j.Base64.encodeToString(Base64.java:734); 	at py4j.Protocol.encodeBytes(Protocol.java:154); 	at py4j.ReturnObject.getPrimitiveReturnObject(ReturnObject.java:150); 	at py4j.Gateway.getReturnObject(Gateway.java:188); 	at py4j.Gateway.invoke(Gateway.java:283); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691:3059,error,error,3059,https://hail.is,https://github.com/hail-is/hail/issues/12035#issuecomment-1186014691,1,['error'],['error']
Availability,s.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$2(LocalBackend.scala:144); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:130); 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:159); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:408); 	at java.base/java.lang.Thread.run(Thread.java:834). Hail version: 0.2.128-ce3ca9c77507; Error summary: SocketTimeoutException: connect timed out; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:22240,Error,Error,22240,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['Error'],['Error']
Availability,"s.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2130,echo,echo,2130,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,s.package$.using(package.scala:664); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); E 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$1(Backend.scala:185); E 	at is.hail.backend.Backend.jsonToBytes(Backend.scala:175); E 	at is.hail.backend.Backend.matrixTableType(Backend.scala:185); E 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:104); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); E 	at java.base/java.lang.Thread.run(Thread.java:829); E; E; E; E Hail version: 0.2.127-e81ad92151ab; E Error summary: ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:5953,Error,Error,5953,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,2,"['Error', 'error']","['Error', 'error']"
Availability,"s_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can differ between ""v2"" and ""v3"", so we just check the overall job adds up to the same usage after deduplicating the resource IDs on both tables. I recommend looking at the main function towards the bottom of the script and then working your way through it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2979,error,errors,2979,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,1,['error'],['errors']
Availability,scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202830,ERROR,ERROR,202830,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,se$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217459,recover,recover,217459,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Availability,see gitter. Possible different Python semantics on windows throw a name error at `__all__.extend(genetics.__all__)`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3286#issuecomment-378015179:72,error,error,72,https://hail.is,https://github.com/hail-is/hail/pull/3286#issuecomment-378015179,1,['error'],['error']
Availability,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:111,failure,failure,111,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521,1,['failure'],['failure']
Availability,"seqr hail search code is here: https://github.com/broadinstitute/seqr/tree/master/hail_search. Running the aiohttp service is just `python -m hail_search`. Runs on hail 0.2.126. The data you need is `gs://seqr-datasets/v03/GRCh38/SNV_INDEL/runs/manual__2023-11-07T23-31-23.149902+00-00`; For running the service, you need a `DATASETS_DIR` env variable defined, and that data should be available at `$DATASETS_DIR/GRCh38/SNV_INDEL`. Post body for a relatively quick search:; ```; {; ""genome_version"": ""GRCh38"",; ""num_results"": 100,; ""annotations"": {; ""in_frame"": [; ""inframe_insertion"",; ""inframe_deletion""; ],; ""missense"": [; ""stop_lost"",; ""initiator_codon_variant"",; ""start_lost"",; ""protein_altering_variant"",; ""missense_variant""; ],; ""nonsense"": [; ""stop_gained""; ],; ""splice_ai"": ""0.2"",; ""frameshift"": [; ""frameshift_variant""; ],; ""structural"": [],; ""extended_splice_site"": [],; ""essential_splice_site"": [; ""splice_donor_variant"",; ""splice_acceptor_variant""; ],; ""structural_consequence"": [; ""LOF"",; ""DUP_LOF"",; ""INV_SPAN"",; ""COPY_GAIN""; ]; },; ""datasetType"": ""VARIANTS"",; ""pathogenicity"": {; ""hgmd"": [; ""disease_causing""; ],; ""clinvar"": [; ""pathogenic"",; ""likely_pathogenic""; ]; },; ""dataset_type"": ""ALL"",; ""secondary_dataset_type"": null,; ""inheritance_mode"": ""de_novo"",; ""inheritance_filter"": {; ""A"": ""has_alt"",; ""N"": ""ref_ref""; },; ""sample_data"": {; ""SNV_INDEL"": [; {; ""sample_id"": ""RGP_2436_2_D1"",; ""individual_guid"": ""I0097169_rgp_2436_2"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""N""; },; {; ""sample_id"": ""RGP_2436_3_D1"",; ""individual_guid"": ""I0097170_rgp_2436_3"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""A""; },; {; ""sample_id"": ""RGP_2436_1_D1"",; ""individual_guid"": ""I0097168_rgp_2436_1"",; ""family_guid"": ""F041731_rgp_2436"",; ""project_guid"": ""R0594_rare_genomes_project_gen"",; ""affected"": ""N""; }; ]; },; ""sort"": ""xpos"",; ""sort_metadata"": null,; ""frequencies"": {; ""g1k"": {;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1810939501:385,avail,available,385,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1810939501,1,['avail'],['available']
Availability,should we error or return NaN here? I'm not sure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4819#issuecomment-440772136:10,error,error,10,https://hail.is,https://github.com/hail-is/hail/pull/4819#issuecomment-440772136,1,['error'],['error']
Availability,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:543,error,error,543,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397,2,['error'],['error']
Availability,so this fixes a match error in that case?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4000#issuecomment-408235922:22,error,error,22,https://hail.is,https://github.com/hail-is/hail/pull/4000#issuecomment-408235922,1,['error'],['error']
Availability,some inference assertion failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8013#issuecomment-580526563:25,failure,failures,25,https://hail.is,https://github.com/hail-is/hail/pull/8013#issuecomment-580526563,1,['failure'],['failures']
Availability,"sorry, wasn't clear. I don't think it's trivial to figure out what a no-args checkpoint should do, but it IS trivial to make that change back-compatibly when we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966:77,checkpoint,checkpoint,77,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469468966,2,['checkpoint'],['checkpoint']
Availability,"sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206386,down,down,206386,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,5,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"sspath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sessionInfo(); sparklyr::invoke(ht, ""count""); ```. it generates this output:; ```; (foo) # Rscript /tmp/failure.R; R version 3.5.1 (2018-07-02); Platform: x86_64-apple-darwin17.6.0 (64-bit); Running under: macOS High Sierra 10.13.6. Matrix products: default; BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib; LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib. locale:; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8. attached base packages:; [1] stats graphics grDevices utils datasets methods base . loaded via a namespace (and not attached):; [1] Rcpp_0.12.19 dbplyr_1.2.2 compiler_3.5.1 pillar_1.3.0 ; [5] later_0.7.5 bindr_0.1.1 r2d3_0.2.2 base64enc_0.1-3 ; [9] tools_3.5.1 digest_0.6.18 jsonlite_1.5 tibble_1.4.2 ; [13] nlme_3.1-137 lattice_0.20-35 pkgconfig_2.0.2 rlang_0.2.2 ; [17] shiny_1.1.0 DBI_1.0.0 rstudioapi_0.8 bindrcpp_0.2.2 ; [21] withr_2.1.2 dplyr_0.7.7 httr_1.3.1 sparklyr_0.9.2 ; [25] rappdirs_0.3.1 h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977:1281,failure,failure,1281,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-430702977,1,['failure'],['failure']
Availability,st practices states the initial capacity is 5000 read requests / second per bucket including list operations until the bucket has time to scale up its capacity. https://cloud.google.com/storage/docs/request-rate#best-practices. ```. ==============================================================================; DIAGNOSTIC RESULTS ; ==============================================================================. ------------------------------------------------------------------------------; Latency ; ------------------------------------------------------------------------------; Operation Size Trials Mean (ms) Std Dev (ms) Median (ms) 90th % (ms); ========= ========= ====== ========= ============ =========== ===========; Delete 0 B 5 43.1 6.4 40.9 50.9 ; Delete 1 KiB 5 44.2 12.7 42.5 58.1 ; Delete 100 KiB 5 44.7 10.4 42.8 56.3 ; Delete 1 MiB 5 41.5 3.7 40.2 45.7 ; Download 0 B 5 74.6 7.9 73.2 84.0 ; Download 1 KiB 5 84.3 15.9 80.6 103.4 ; Download 100 KiB 5 81.9 16.0 82.7 99.6 ; Download 1 MiB 5 90.6 6.5 94.5 96.8 ; Metadata 0 B 5 23.6 2.7 23.6 26.3 ; Metadata 1 KiB 5 25.5 2.1 26.9 27.4 ; Metadata 100 KiB 5 26.2 3.6 27.3 29.9 ; Metadata 1 MiB 5 24.0 3.7 23.3 28.4 ; Upload 0 B 5 98.1 16.6 95.5 117.9 ; Upload 1 KiB 5 116.7 21.8 115.5 142.1 ; Upload 100 KiB 5 116.5 17.8 115.1 135.1 ; Upload 1 MiB 5 168.2 18.5 179.6 185.6 . ------------------------------------------------------------------------------; Write Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Write throughput: 977.7 Mibit/s.; Parallelism strategy: both. ------------------------------------------------------------------------------; Read Throughput ; ------------------------------------------------------------------------------; Copied 5 512 MiB file(s) for a total transfer size of 2.5 GiB.; Read throughput: 1.11 Gibit/s.; Parallelism strategy: both. -----------------------------------------------------,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:1401,Down,Download,1401,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['Down'],['Download']
Availability,"started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44817.; 2019-01-22 13:11:37 NettyBlockTransferService: INFO: Server created on 10.48.225.55:44817; 2019-01-22 13:11:37 BlockManager: INFO: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registering BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMasterEndpoint: INFO: Registering block manager 10.48.225.55:44817 with 2.5 GB RAM, BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManagerMaster: INFO: Registered BlockManager BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 BlockManager: INFO: Initialized BlockManager: BlockManagerId(driver, 10.48.225.55, 44817, None); 2019-01-22 13:11:37 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@77390398{/metrics/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:38 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.203:44870) with ID 8; 2019-01-22 13:11:38 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q19.scc.bu.edu:44319 with 21.2 GB RAM, BlockManagerId(8, scc-q19.scc.bu.edu, 44319, None); 2019-01-22 13:11:40 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.229:36354) with ID 7; 2019-01-22 13:11:40 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.202:50198) with ID 6; 2019-01-22 13:11:40 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q21.scc.bu.edu:33025 with 21.2 GB RAM, BlockManagerId(7, scc-q21.scc.bu.edu, 33025, None); 2019-01-22 13:11:40 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q18.scc.bu.edu:39123 with 21.2 GB RAM, BlockManagerId(6, scc-q18.scc.bu.edu, 39123, None); 2019-01-22 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:20172,AVAIL,AVAILABLE,20172,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['AVAIL'],['AVAILABLE']
Availability,still a bad error message. almost done with a fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084:12,error,error,12,https://hail.is,https://github.com/hail-is/hail/issues/4078#issuecomment-410711084,1,['error'],['error']
Availability,still chasing down ndarray regressions...; ```; linear_regression_rows_nd 246.9% 50.654 125.085 100.0% 8 8; ndarray_matmul_float64_benchmark 165.8% 4.999 8.290 100.0% 1 1; hwe_normalized_pca_blanczos_small_data_10_iterations 128.7% 54.708 70.395 100.0% 8 8; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10539#issuecomment-869629782:14,down,down,14,https://hail.is,https://github.com/hail-is/hail/pull/10539#issuecomment-869629782,1,['down'],['down']
Availability,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:901,Error,Error,901,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708,1,['Error'],['Error']
Availability,still got test failures: https://ci.hail.is/batches/48405/jobs/45,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8778#issuecomment-631468022:15,failure,failures,15,https://hail.is,https://github.com/hail-is/hail/pull/8778#issuecomment-631468022,1,['failure'],['failures']
Availability,"t :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; adding 'hail/experimental/codec.py'; adding 'hail/experimental/compile.py'; adding 'hail/experimental/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16960,avail,available,16960,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['avail'],['available']
Availability,"t = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants with at least one non-ref GT""); mt = mt.filter_rows(hl.agg.any(mt.GT.is_non_ref())); ; - #logger.info(f""Saving checkpoint""); - #mt = mt.checkpoint(os.path.join(args.temp_bucket, ""readviz_select_samples_checkpoint1.vds""),; - # overwrite=True, _read_if_exists=True); + logger.info(f""Saving checkpoint""); + mt = mt.checkpoint(""readviz_select_samples_checkpoint1.vds"",; + overwrite=True, _read_if_exists=True); ; def sample_ordering_expr(mt):; """"""For variants that are present in more than 10 samples (or whatever number args.num_samples is set to),; ```. And tried running the bad step:. ```bash; python3 step1__select_samples.py; ```. I was able to get past the checkpoint: ; ```; INFO (Readviz_prep 73): Saving checkpoint; [Stage 0:> (0 + 1) / 1]. 2023-09-01 18:10:29.262 Hail: INFO: wrote matrix table with 11450 rows and 955359 columns in 1 partition to readviz_select_samples_checkpoint1.vds; ```. @bw2 are you still encountering this issue? Did my diff oversimplify it? Do you suspect the issue after the checkpoint?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:2110,checkpoint,checkpoint,2110,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,7,['checkpoint'],['checkpoint']
Availability,"t look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, ti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4083,error,errors,4083,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['error'],['errors']
Availability,t org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199159,ERROR,ERROR,199159,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,3,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"t py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:3000,error,error,3000,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['error'],['error']
Availability,"t to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2960,echo,echo,2960,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,t variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ di,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5568,echo,echo,5568,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"t/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:12973,down,download,12973,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['down'],['download']
Availability,"t: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.48721539559123606, sa.cov.Cov1 -> 0.5924758486080468, sa.cov.Cov2 -> -0.23132255249379874); 2017-08-28 21:47:50 Ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5741,redundant,redundant,5741,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,1,['redundant'],['redundant']
Availability,"td=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/FS.d -MT build/FS.o -c FS.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/Na; tiveCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:15405,Down,Downloading,15405,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:3079,toler,tolerations,3079,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,6,['toler'],"['tolerationSeconds', 'tolerations']"
Availability,"tension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:204); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:35); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); 	at is.hail.utils.richUtils.RichIterable.foreachBetween(RichIterable.scala:12); 	at is.hail.expr.ir.Pretty$.is$hail$expr$ir$Pretty$$pretty$1(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.expr.ir.Pretty$$anonfun$is$hail$expr$ir$Pretty$$pretty$1$6.apply(Pretty.scala:405); 	at is.hail.utils.richUtils.RichIterator$.foreachBetween$extension(RichIterator.scala:32); ```; and keeps going for a while. So maybe this Python thing is fixed, and we need a new ticket for this Java error?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024:2491,error,error,2491,https://hail.is,https://github.com/hail-is/hail/issues/5262#issuecomment-555090024,1,['error'],['error']
Availability,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8231#issuecomment-594308654:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/8231#issuecomment-594308654,3,['failure'],['failures']
Availability,"test failures related to locals, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8394#issuecomment-606100914:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/8394#issuecomment-606100914,1,['failure'],['failures']
Availability,test failures were caused by PType inference bugs that are now fixed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8297#issuecomment-600361269:5,failure,failures,5,https://hail.is,https://github.com/hail-is/hail/pull/8297#issuecomment-600361269,1,['failure'],['failures']
Availability,"test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5087,down,downloads,5087,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['down'],['downloads']
Availability,test/hail/methods/test_impex.py::BGENTests::test_import_bgen_no_entries FAILED. a deserialization error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8282#issuecomment-598486137:98,error,error,98,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-598486137,1,['error'],['error']
Availability,"test_ld_score_regression is failing now, caused by . ```scala; case GetTupleElement(o, idx) =>; infer(o); val t = coerce[PTuple](o.pType2); assert(idx >= 0 && idx < t.size); ```. in inferPType throwing. Will track down. I've seen this before, once. edit:. t.size 1, idx: 1; GetTupleElement(Ref(__iruid_28388,tuple(1:float64))),1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586753124:214,down,down,214,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586753124,1,['down'],['down']
Availability,"text_mt_path).drop('a_index', 'was_split'); context_mt = context_mt.annotate_rows(vep=context_mt.vep.drop('colocated_variants')); context_mt = hl.filter_intervals(context_mt, [hl.parse_locus_interval('1-22')]); ht.annotate(**context_mt[ht.key, :]).write(output_ht_path, overwrite); ```; gives:; ```; Java stack trace:; java.lang.ArrayIndexOutOfBoundsException: 1; at is.hail.annotations.Annotation$$anonfun$copy$1.apply(Annotation.scala:46); at is.hail.annotations.Annotation$$anonfun$copy$1.apply(Annotation.scala:46); at scala.Array$.tabulate(Array.scala:331); at is.hail.annotations.Annotation$.copy(Annotation.scala:46); at is.hail.rvd.OrderedRVDPartitioner.enlargeToRange(OrderedRVDPartitioner.scala:133); at is.hail.rvd.KeyedOrderedRVD.orderedJoin(KeyedOrderedRVD.scala:35); at is.hail.rvd.OrderedRVD.orderedJoin(OrderedRVD.scala:119); at is.hail.expr.TableJoin.execute(Relational.scala:1828); at is.hail.expr.TableMapRows.execute(Relational.scala:1859); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:465); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); at is.hail.table.Table.write(Table.scala:899); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-ce257b532e2c; Error summary: ArrayIndexOutOfBoundsException: 1; ```; (this doesn't happen on master)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3637#issuecomment-392147738:2025,Error,Error,2025,https://hail.is,https://github.com/hail-is/hail/pull/3637#issuecomment-392147738,1,['Error'],['Error']
Availability,thank goodness for random batch failures!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6312#issuecomment-501000473:32,failure,failures,32,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-501000473,1,['failure'],['failures']
Availability,"thank you very much for the reply,; when I upgrade the decorator from 3.4.0 to 4.1.2 , this error disappears：. ```; Installing collected packages: decorator; Found existing installation: decorator 3.4.0; Uninstalling decorator-3.4.0:; Successfully uninstalled decorator-3.4.0; Successfully installed decorator-4.1.2. ```. But there is another error, as follows：. ```; bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = hail.Context(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = hail.HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-470>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 245, in _typecheck; return f(*args, **kwargs); File ""/opt/Software/hail/python/hail/context.py"", line 88, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```; My Java version; ```; [root@mg opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (bui",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579:92,error,error,92,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579,2,['error'],['error']
Availability,thanks. I had an initial version of the artifacts stuff using `.log` extensions but that wasn't enough! I hypothesized that .log was auto downloaded and never changed it back from no-ext. :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072:138,down,downloaded,138,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429328072,1,['down'],['downloaded']
Availability,that is a crazy error though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4052#issuecomment-409700517:16,error,error,16,https://hail.is,https://github.com/hail-is/hail/pull/4052#issuecomment-409700517,1,['error'],['error']
Availability,that's what ~I suggested~ I almost suggested until I was shot down 😄,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469475986:62,down,down,62,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469475986,1,['down'],['down']
Availability,"the error in #4529 was:; ```; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,the error message? or what?. Can you write a docstring for that function?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1854#issuecomment-302792319:4,error,error,4,https://hail.is,https://github.com/hail-is/hail/pull/1854#issuecomment-302792319,1,['error'],['error']
Availability,the real error is in there at the top: ; ```; ExpressionException: Hail cannot impute the type of 'None'; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5700#issuecomment-478923407:9,error,error,9,https://hail.is,https://github.com/hail-is/hail/issues/5700#issuecomment-478923407,1,['error'],['error']
Availability,then make sure the error says number of eigenvectors and not ``n_eigs``,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1929#issuecomment-313825166:19,error,error,19,https://hail.is,https://github.com/hail-is/hail/pull/1929#issuecomment-313825166,1,['error'],['error']
Availability,"this error disappears：. ```; Installing collected packages: decorator; Found existing installation: decorator 3.4.0; Uninstalling decorator-3.4.0:; Successfully uninstalled decorator-3.4.0; Successfully installed decorator-4.1.2. ```. But there is another error, as follows：. ```; bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = hail.Context(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = hail.HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-470>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 245, in _typecheck; return f(*args, **kwargs); File ""/opt/Software/hail/python/hail/context.py"", line 88, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```; My Java version; ```; [root@mg opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (build 1.8.0_91-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579:1116,avail,available,1116,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579,1,['avail'],['available']
Availability,"this is a python error, with analyze probably",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4110#issuecomment-411901041:17,error,error,17,https://hail.is,https://github.com/hail-is/hail/issues/4110#issuecomment-411901041,1,['error'],['error']
Availability,"this is not a great error message. The problem is you're missing the lz4 dependency, see here:; https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122:20,error,error,20,https://hail.is,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122,1,['error'],['error']
Availability,"this is now failing (previously passed, no issues on output) due to subsetTo assertion in master. edit: Strange the subsetTo commit was made 19 days ago. The assertion failure originates from subsetTo, but must have been caused by something else. Ah, my local master branch was old.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545:168,failure,failure,168,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545,1,['failure'],['failure']
Availability,this is ready for review. the tests failed spectacularly for reasons I don't understand with py4j errors. https://ci.hail.is/batches/7644244,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1638415522:98,error,errors,98,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1638415522,1,['error'],['errors']
Availability,tils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29452,Down,Downloading,29452,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Down'],['Downloading']
Availability,"timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:3744,failure,failure,3744,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['failure'],['failure']
Availability,tive_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166); 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.service.Worker$.main(Worker.scala:164); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more. Logs; Main; Log ; 2023-09-24 17:23:30.055 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:388) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:4453,ERROR,ERROR,4453,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['ERROR'],['ERROR']
Availability,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:210,down,down,210,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090,2,"['down', 'fault']","['down', 'faults']"
Availability,"to elaborate, I'm seeing this test case hit the same error:; ```; t = hl.utils.range_table(10).annotate(**{f'f{i}': 0 for i in range(1300)}); mt = hl.utils.range_matrix_table(10, 10). mt.annotate_cols(foo=t[mt.col_idx])._force_count_rows(); ```. with the following stack trace: ; ```; 2019-10-28 17:15:08 root: ERROR: Verify Output 1 for is/hail/codegen/generated/C_etypeDecode_9:; 2019-10-28 17:15:08 root: ERROR: RuntimeException: Method code too large!; From java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:333); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:372); 	at is.hail.expr.types.encoded.EType$.buildDecoder(EType.scala:199); 	at is.hail.io.TypedCodecSpec.buildDecoder(RowStore.scala:36); 	at is.hail.rvd.RVD.collect(RVD.scala:694); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:750); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:90); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:1125); 	at is.hail.expr.ir.Interpr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723:53,error,error,53,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"to reproduce:. ```; l = hl.Locus('1', 1); rows = [; 	hl.Struct(locus=l, alleles=[""A"", ""T""]),; 	hl.Struct(locus=l, alleles=[""T"", ""T""]),; 	hl.Struct(locus=l, alleles=[""A"", ""CC"", ""TT""])]. t = hl.Table.parallelize(rows, hl.tstruct(locus=hl.tlocus(), alleles=hl.tarray(hl.tstr)), ['locus', 'alleles']); split = hl.split_multi(t); print(split.collect()); ```; should error, but does not. Writing/reading/showing `split` should also error, but instead prints:; ```; 2019-06-04 18:55:11 Hail: INFO: wrote table with 4 rows in 1 partition to foo; +---------------+------------+---------+-----------+---------------+-----------------+; | locus | alleles | a_index | was_split | old_locus | old_alleles |; +---------------+------------+---------+-----------+---------------+-----------------+; | locus<GRCh37> | array<str> | int32 | bool | locus<GRCh37> | array<str> |; +---------------+------------+---------+-----------+---------------+-----------------+; | 1:1 | [""A"",""T""] | 1 | false | 1:1 | [""A"",""T""] |; | 1:1 | [""T"",""T""] | 1 | false | 1:1 | [""T"",""T""] |; | 1:1 | [""A"",""CC""] | 1 | true | 1:1 | [""A"",""CC"",""TT""] |; | 1:1 | [""A"",""TT""] | 2 | true | 1:1 | [""A"",""CC"",""TT""] |; +---------------+------------+---------+-----------+---------------+-----------------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6223#issuecomment-498872887:361,error,error,361,https://hail.is,https://github.com/hail-is/hail/issues/6223#issuecomment-498872887,2,['error'],['error']
Availability,to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3113,echo,echo,3113,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['echo'],['echo']
Availability,"top_level references are references to things that should be in the EvalContext. top_level=False are references intermediate variables. Inside the view_join_x stuff, we constructed top_level=False references to things like `row` and `va`, so these threw errors if used inside aggregators. I'll add a test for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3055#issuecomment-369985720:254,error,errors,254,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369985720,1,['error'],['errors']
Availability,"ue, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:3802,failure,failure,3802,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['failure'],['failure']
Availability,"ugh so this is really annoying. Shading libraries with native libraries doesn't work out of the box. The SO file that indeed ships has a symbol like this:; ```; 0000000000001440 T _Java_com_indeed_util_mmap_MMapBuffer_mmap; ```; But after relocation, the java counterpart's package doesn't match the package you see above. Ergo, link error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8576#issuecomment-616207938:334,error,error,334,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616207938,1,['error'],['error']
Availability,"ugh, I'm still exhausted from shooting Konrad down.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5528#issuecomment-469477749:46,down,down,46,https://hail.is,https://github.com/hail-is/hail/pull/5528#issuecomment-469477749,1,['down'],['down']
Availability,"ugh, test failure in local backend. Will debug in a bit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9618#issuecomment-713077513:10,failure,failure,10,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-713077513,1,['failure'],['failure']
Availability,"unately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertis",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:1914,echo,echo,1914,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,4,"['avail', 'echo']","['avail', 'echo']"
Availability,updated error:; ```; is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Key should be in partition 1: ([bar]-[foo]]; Invalid key: [quam]; ```; 🤔,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793:8,error,error,8,https://hail.is,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793,2,['error'],['error']
Availability,us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgen,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:7218,echo,echo,7218,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['echo'],['echo']
Availability,"use-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fuse-overlayfs: unknown argument ignored: lazytime; fuse: device not found, try 'modprobe fuse' first; fuse-overlayfs: cannot mount: No such file or directory; : exit status 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:1214,error,error,1214,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,2,"['Error', 'error']","['Error', 'error']"
Availability,"uting eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.48721539559123606, sa.cov.Cov1 -> 0.5924758486080468, sa.cov.Cov2 -> -0.23132255249379874); 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: sigmaG2 = 0.25961182888248313; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.02099926061895032; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: delta = 0.08088714874566029; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: h2 = 0.9251659631261897; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: seH2 = 0.11525741506951385; 2017-08-28 21:47:50 Hail: INFO: baldingnichols: generating genotypes for 10 populations, 100 samples, and 300 variants...; 2017-08-28 21:47:50 Hail: INFO: Coerced sorted dataset; bash: line 12: 5047 Segmentation fault spark-submit --class org.testng.TestNG --jars ./hail-all-spark-test.jar --conf ""spark.driver.extraClassPath=./hail-all-spark-test.jar"" --conf 'spark.executor.extraClassPath=./hail-all-spark-test.jar' ./hail-all-spark-test.jar ./testng.xml; + TEST_EXIT_CODE=139; + set -e; + gcloud --project broad-ctsa compute scp --recurse cluster-ci-d1n0nk3vmzf4-m:test-output test-output; scp: test-output: No such file or directory; ERROR: (gcloud.compute.scp) [/usr/bin/scp] exited with return code [1].; + cleanup; + gcloud --project broad-ctsa -q dataproc clusters delete --async cluster-ci-d1n0nk3vmzf4; Deleting [https://dataproc.googleapis.com/v1/projects/broad-ctsa/regions/global/clusters/cluster-ci-d1n0nk3vmzf4] with operation [projects/broad-ctsa/regions/global/operations/3a35ffb5-aac5-4329-9762-67a8f4fadf66].; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:7392,fault,fault,7392,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,2,"['ERROR', 'fault']","['ERROR', 'fault']"
Availability,"v.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:2822,failure,failure,2822,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930,1,['failure'],['failure']
Availability,"ve(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; match argument types (Int,shuffle: Boolean); Error occurred in an application involving default arguments.; start.copy(rdd = start.rdd.coalesce(k, shuffle = shuffle)(null).asOrderedRDD); ^; 5 errors found; :compileScala FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':compileScala'.; > Compilation failed. *",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:2407,error,error,2407,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['error'],['error']
Availability,warnings are errors is the bees knees,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12322#issuecomment-1279306858:13,error,errors,13,https://hail.is,https://github.com/hail-is/hail/pull/12322#issuecomment-1279306858,1,['error'],['errors']
Availability,we added a base case with nicer error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2232#issuecomment-422371999:32,error,error,32,https://hail.is,https://github.com/hail-is/hail/issues/2232#issuecomment-422371999,1,['error'],['error']
Availability,we have better error messages now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531:15,error,error,15,https://hail.is,https://github.com/hail-is/hail/issues/1202#issuecomment-317747531,1,['error'],['error']
Availability,"we have some random failures of tests for some of our services related to resource contention, it's a big problem :(. If you push an empty commit, I can rerun the tests and hope they succeed this time...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301:20,failure,failures,20,https://hail.is,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301,1,['failure'],['failures']
Availability,"we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObjec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1588,Down,Down,1588,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['Down'],['Down']
Availability,"we recently locked down our CI to run PRs only from the development team -- we intend to add a feature to our CI to let us run the test suite on a specific PR and SHA, but don't have that feature yet... 😦 . @cseed 	@danking this isn't hard, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6262#issuecomment-498911743:19,down,down,19,https://hail.is,https://github.com/hail-is/hail/pull/6262#issuecomment-498911743,1,['down'],['down']
Availability,we should put this on top level expression. Someone could do something like `'NA12878' in mt.s` and that should have an error as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9164#issuecomment-665108669:120,error,error,120,https://hail.is,https://github.com/hail-is/hail/pull/9164#issuecomment-665108669,1,['error'],['error']
Availability,"we've moved in the opposite direction -- be tolerant as possible on input, let people minrep if they want",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1371#issuecomment-422369573:44,toler,tolerant,44,https://hail.is,https://github.com/hail-is/hail/issues/1371#issuecomment-422369573,1,['toler'],['tolerant']
Availability,what error do you get now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8866#issuecomment-635292105:5,error,error,5,https://hail.is,https://github.com/hail-is/hail/pull/8866#issuecomment-635292105,1,['error'],['error']
Availability,"what the hell, bytecode verify errors from using locals? i saw another code-function creating locals so i assumed it was safe to do so, but apparently it isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768:31,error,errors,31,https://hail.is,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768,1,['error'],['errors']
Availability,what was the error you saw?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5207#issuecomment-457740960:13,error,error,13,https://hail.is,https://github.com/hail-is/hail/issues/5207#issuecomment-457740960,1,['error'],['error']
Availability,"while the 1kg vcf that generated the assertion error looks like ; ```; ##fileformat=VCFv4.2; ##ApplyRecalibration=""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:47,error,error,47,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['error'],['error']
Availability,xecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:108548,ERROR,ERROR,108548,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,xecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:145676,ERROR,ERROR,145676,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['ERROR'],['ERROR']
Availability,"y curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, tim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4548,error,errors,4548,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['error'],['errors']
Availability,"y4j_backend.py"", line 94, in execute; jir = self._to_java_value_ir(ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 280, in _to_java_value_ir; return self._to_java_ir(ir, self._parse_value_ir); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 276, in _to_java_ir; ir._jir = parse(r(finalize_randomness(ir)), ir_map=r.jirs); File ""/home/edmund/.local/src/hail/hail/python/hail/backend/spark_backend.py"", line 245, in _parse_value_ir; return self._jbackend.parse_value_ir(; File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1304, in __call__; return_value = get_return_value(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 21, in deco; return f(*args, **kwargs); File ""/home/edmund/.local/src/hail/.venv/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value; raise Py4JJavaError(; py4j.protocol.Py4JJavaError: An error occurred while calling o1.parse_value_ir.; : java.util.NoSuchElementException: key not found: __uid_4; at scala.collection.immutable.Map$Map1.apply(Map.scala:114); at is.hail.expr.ir.Env.apply(Env.scala:128); at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:890); at is.hail.expr.ir.IRParser$.$anonfun$ir_value_expr$1(Parser.scala:820); at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); at is.hail.utils.StackSafe$.run(StackSafe.scala:16); at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); at is.hail.expr.ir.IRParser$.$anonfun$parse_value_ir$1(Parser.scala:2072); at is.hail.expr.ir.IRParser$.parse(Parser.scala:2068); at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:2072); at is.hail.backend.spark.SparkBackend.$anonfun$parse_value_ir$2(SparkBackend.scala:710); at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:70); at is.hail.utils.package$.using(package.scala:635); at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteCont",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:9899,error,error,9899,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['error'],['error']
Availability,"yeah this was a simple error, forgot to do one upstream processing step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11413#issuecomment-1050268863:23,error,error,23,https://hail.is,https://github.com/hail-is/hail/issues/11413#issuecomment-1050268863,2,['error'],['error']
Availability,"yeah, there's a non-deterministic failure, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664632674:34,failure,failure,34,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664632674,1,['failure'],['failure']
Availability,"yep, run `./gradlew makeDocs`. It could be somehow related to another random failure (no space left on device)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6742#issuecomment-519554156:77,failure,failure,77,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-519554156,1,['failure'],['failure']
Availability,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:19,failure,failure,19,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328,2,"['FAILURE', 'failure']","['FAILURES', 'failure']"
Availability,you've got a compilation error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3840#issuecomment-401560032:25,error,error,25,https://hail.is,https://github.com/hail-is/hail/pull/3840#issuecomment-401560032,1,['error'],['error']
Availability,"your intuition is exactly what we're doing. We look for bind and lambda AST nodes, add those to the declared scope. If we find a `top_level=False` reference not in that scope, we error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3055#issuecomment-369986005:179,error,error,179,https://hail.is,https://github.com/hail-is/hail/pull/3055#issuecomment-369986005,2,['error'],['error']
Availability,"ython/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3080,echo,echo,3080,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['echo'],['echo']
Availability,"~Actually, this does appear to be a gap in our test coverage. I put in a failing assertion, and both `BlockMatrixIRSuite.scala` and `test_linalg.py` passed.~ Never mind, just forgot to run in local mode. I didn't notice any obvious errors in your code, but I don't think this is the right way to do it, so I tried writing it myself. I'll try to write a test that exercises this path and test both implementations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13807#issuecomment-1766662634:232,error,errors,232,https://hail.is,https://github.com/hail-is/hail/pull/13807#issuecomment-1766662634,1,['error'],['errors']
Availability,"~I verified this scales all the way to 50,000 partitions but the batch-driver can't schedule these fast enough to make the test fast. They take less than a second but more than 300ms. We'd need like 64 8-core (512 cores) nodes to bring test time down to a reasonable amount. We should strive to get there but batch would need to schedule at 512 jobs per second for that to make sense.~ Hmm, something went wrong. OK, we'll need to revisit 50k partition tables. But let's get this in, the current code is obviously wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13065#issuecomment-1550142533:246,down,down,246,https://hail.is,https://github.com/hail-is/hail/pull/13065#issuecomment-1550142533,1,['down'],['down']
Availability,"━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; 920 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 921 | amazon-ebs: Requirement already satisfied: boto3<2.0,>=1.17 in /usr/local/lib/python3.7/site-packages (1.24.78); 922 | amazon-ebs: Requirement already satisfied: botocore<2.0,>=1.20 in /usr/local/lib/python3.7/site-packages (1.27.78); 923 | amazon-ebs: Collecting decorator<5; 924 | amazon-ebs: Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); 925 | amazon-ebs: Collecting Deprecated<1.3,>=1.2.10; 926 | amazon-ebs: Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB); 927 | amazon-ebs: Collecting dill<0.4,>=0.3.1.1; 928 | amazon-ebs: Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB); 929 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.8/95.8 kB 15.3 MB/s eta 0:00:00; 930 | amazon-ebs:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:3350,Down,Downloading,3350,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['Down'],['Downloading']
Availability,"━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 113.0 MB/s eta 0:00:00; 976 | ==> amazon-ebs: ERROR: Ignored the following versions that require a different python version: 1.22.0 Requires-Python >=3.8; 1.22.0rc1 Requires-Python >=3.8; 1.22.0rc2 Requires-Python >=3.8; 1.22.0rc3 Requires-Python >=3.8; 1.22.1 Requires-Python >=3.8; 1.22.2 Requires-Python >=3.8; 1.22.3 Requires-Python >=3.8; 1.22.4 Requires-Python >=3.8; 1.23.0 Requires-Python >=3.8; 1.23.0rc1 Requires-Python >=3.8; 1.23.0rc2 Requires-Python >=3.8; 1.23.0rc3 Requires-Python >=3.8; 1.23.1 Requires-Python >=3.8; 1.23.2 Requires-Python >=3.8; 1.23.3 Requires-Python >=3.8; 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12; 3.0.0.dev10 Requires-Python >=3.8; 3.0.0.dev11 Requires-Python >=3.8; 3.0.0.dev13 Requires-Python >=3.8; 3.0.0.dev14 Requires-Python >=3.8; 3.0.0.dev15 Requires-Python >=3.8; 3.0.0.dev16 Requires-Python >=3.8; 3.0.0.dev17 Requires-Python >=3.8; 3.0.0.dev18 Requires-Python >=3.8; 3.0.0.dev3 Requires-Python >=3.8; 3.0.0.dev4 Requires-Python >=3.8; 3.0.0.dev5 Requires-Python >=3.8; 3.0.0.dev6 Requires-Python >=3.8; 3.0.0.dev7 Requires-Python >=3.8; 3.0.0.dev8 Requires-Python >=3.8; 3.0.0.dev9 Requires-Python >=3.8; 977 | ==> amazon-ebs: ERROR: Could not find a version that satisfies the requirement sys_platform!=win32 (from versions: none); 978 | ==> amazon-ebs: ERROR: No matching distribution found for sys_platform!=win32. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:9583,ERROR,ERROR,9583,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,2,['ERROR'],['ERROR']
Availability,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:583,Avail,Available,583,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['Avail'],['Available']
Availability,"👍. Either way seems fine to me. Slight preference for `echo` because it's a shell built-in, so no external executables are invoked. AFAICT, [a POSIX-compliant shell](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html) will do what we want. If there is more than one py4j in that directory things will definitely break. AFAIK, more than one py4j in that directory is an error, correct? If not, we could do something like `$(ls ... | tr '\n' ':')` to list all of them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1573#issuecomment-287766096:55,echo,echo,55,https://hail.is,https://github.com/hail-is/hail/pull/1573#issuecomment-287766096,2,"['echo', 'error']","['echo', 'error']"
Deployability," ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/datap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1440,deploy,deploy,1440,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability," $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(); ```. ```bash; #! /usr/bash; set -ex. # define tmp directory; __TMP_DIR__=/tmp//pipeline.yG41vqpS/. # __TASK__0 write_input; cp gs://hail-jigold/random_file.txt ${__TMP_DIR__}/rsfKylng. # __TASK__1 write_input; cp gs://hail-jigold/input.bed ${__TMP_DIR__}/xJONBVn7.bed. # __TASK__2 write_input; cp gs://hail-jigold/input.bim ${__TMP_DIR__}/xJONBVn7.bim. # __TASK__3 write_input; cp gs://hail-jigold/input.fam ${__TMP_DIR__}/xJONBVn7.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=${__TMP_DIR__}/xJONBVn7; __RESOURCE_GROUP__1=${__TMP_DIR__}/TB7ZUbj8; __RESOURCE__6=${__TMP_DIR__}/TB7ZUbj8.fam; __RESOURCE__10=${__TMP_DIR__}/EVeRHf7V; __RESOURCE__1=${__TMP_DIR__}/xJONBVn7.bed; __RESOURCE__2=${__TMP_DIR__}/xJONBVn7.bim; __RESOURCE__3=${__TMP_DIR__}/xJONBVn7.fam; __RESOURCE_GROUP__2=${__TMP_DIR__}/MXBQugBx; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:2203,pipeline,pipeline,2203,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741,1,['pipeline'],['pipeline']
Deployability," --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5772,deploy,deployments,5772,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['deploy'],['deployments']
Deployability," 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-spark.jar; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/pyspark.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/pyspark.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15787,install,install,15787,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['install'],['install']
Deployability," 3.6. The `hail-ubuntu` image explicitly installs Python 3.7. I'm happy to drop linting for Python 3.6 from build.yaml if compilers team is OK with that (ask Tim). We are already using Ubuntu 20.04 for our services and tests. See below for details, but `hail-ubuntu` is based on 20.04. We explicitly install JDK 8 in the `base` image. ---. This PR doesn't change the hail-ubuntu image which is the basis for nearly all our images. `DOCKER_ROOT_IMAGE` seems to have been introduced [here](https://github.com/hail-is/hail/pull/9660). That's my bad for not flagging in the review that this isn't actually a ""root image"". `DOCKER_ROOT_IMAGE` is just some Linux image with the standard utils. It was introduced as a distinct global concept when Docker Hub began enforcing rate limits (so we needed all our test images to live in GCR). If you look at the occurrences of `DOCKER_ROOT_IMAGE` or `docker_root_image` you'll find it's almost exclusively used in the tests except for one occurrence in `build-batch-worker-image-startup-gcp.sh`. We could just remove that line. That line is an attempt to keep a relatively recent version of the ubuntu image cached on the worker VM so that we can save some time when pulling the worker Docker image. In practice, the ubuntu image is extraordinarily tiny and quickly becomes out of date (because we rarely rebuild the VM). hail-ubuntu uses a timestamped ubuntu 20.04 tag: `ubuntu:focal-20201106`. I did this because we kept getting screwed by new ubuntu images getting released which were incompatible with us. We would only find out later when we changed the hail-ubuntu Dockerfile and triggered a refetch of the latest image at the `ubuntu:18.04` tag which included the breaking changes. You're correct that this is technical debt of ours. DOCKER_ROOT_IMAGE should really be SOME_LINUX_IMAGE and we shouldn't bother trying to fetch it in the worker image startup script. You're fighting the good fight, but, man, keeping a growing software project clean is hard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805:1715,release,released,1715,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805,1,['release'],['released']
Deployability," Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLxOwBss; cat ${__RESOURCE__11} ${__RESOURCE__13} ${__RESOURCE__15} >> ${__RESOURCE__17}'. # __TASK__9 write_output; __RESOURCE__17=GLxOwBss; cp ${__RESOURCE__17} gs://jigold/final_output.txt; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1437,pipeline,pipeline,1437,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,9,['pipeline'],['pipeline']
Deployability," Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E Call(alleles=[0, 0], phased=True),; E Call(alleles=[1, 0], phased=True),; E - Call(alleles=[1, 1], phased=True),; E - Call(alleles=[0, 1], phased=True),; E - Call(alleles=[1, 1], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[0, 1], phased=True),; E Call(alleles=[1, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[1, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 0], phased=True),; E Call(alleles=[0, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E ]. test/hail/methods/test_statgen.py:1642: AssertionError; ------------------------------ Captured log call -------------------------------; WARNING backend.service_backend:java.py:186 To ensure reproducible randomness across Hail sessions, you must set the ""global_seed"" parameter in hl.init(), in addition to the local seed in each random function.; INFO backend.service_backend:java.py:190 balding_nichols_model: generating genotypes for 1 populations, 5 samples, and 5 variants...; INFO batch_client.aioclient:aioclient.py:741 created batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397330495:4663,update,updated,4663,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397330495,3,['update'],['updated']
Deployability," __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1154,pipeline,pipeline,1154,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,1,['pipeline'],['pipeline']
Deployability, argument validation and removed the test-dataproc and wheel dependencies in the Makefile to demonstrate the functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + ',MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1033,deploy,deploy-,1033,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability, at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$3(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:926); at org.apache.spark.repl.SparkILoop.$anonfun$process$1(SparkILoop.scala:138); at scala.Option.fold(Option.scala:251); at org.apache.spark.repl.SparkILoop.newReader$1(SparkILoop.scala:138); at org.apache.spark.repl.SparkILoop.preLoop$1(SparkILoop.scala:142); at org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:203); at org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189); at org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201); at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236); at org.apache.spark.repl.Main$.doMain(Main.scala:78); at org.apache.spark.repl.Main$.main(Main.scala:58); at org.apache.spark.repl.Main.main(Main.scala); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.base/java.lang.reflect.Method.invoke(Method.java:566); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958); at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203); at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90); at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. Nothing strike my attention apart the fact that `$SPARK_SCALA_VERSION` is null and that `scala` in not found by `which` in both scenario (hail installed or not),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:11229,deploy,deploy,11229,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,10,"['deploy', 'install']","['deploy', 'installed']"
Deployability," batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16980,pipeline,pipeline,16980,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability," determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:1830,update,update,1830,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['update'],['update']
Deployability," error.... ipython vcf2mt.py 22; ```; Running on Apache Spark version 2.2.1; SparkUI available at http://10.48.225.55:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2-721af83bc30a; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/hail-20181113-2115-0.2-721af83bc30a.log; Converting vcf /project/ukbiobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1057,install,install,1057,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['install'],['install']
Deployability," exact trigger is . ```sql; DROP TRIGGER IF EXISTS aggregated_job_resources_v2_after_update $$; CREATE TRIGGER aggregated_job_resources_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can differ between ""v2"" and ""v3"", so we just check the overall job adds up to the same usage after deduplicating the resourc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2700,update,update,2700,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,1,['update'],['update']
Deployability," fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:994,upgrade,upgraded,994,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['upgrade'],['upgraded']
Deployability," for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:2431,CONFIGURAT,CONFIGURATION,2431,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIGURAT'],['CONFIGURATION']
Deployability, for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z buil,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10985,release,release,10985,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability," image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1706,update,update,1706,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,2,"['install', 'update']","['install', 'update']"
Deployability," is special here vs the rest of the standard library? What assumption >are you working under?. Yes. The C++11 final standard introduced some new constraints on the definition of std::string; and std::list (essentially outlawing a copy-on-write implementation of std::string, and requiring; that std::list::size() must be O(1)). Consequently libstdc++ had to be redesigned with incompatible implementations of ; std::string and std::list to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My und",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:978,release,release,978,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612,1,['release'],['release']
Deployability," is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Hail version: 0.2-721af83bc30a; Error summary: OutOfMemoryError: Java heap space; ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 883, in send_command; response = connection.send_command(command); File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1040, in send_command; ""Error while receiving"", e, proto.ERROR_ON_RECEIVE); py4j.protocol.Py4JNetworkError: Error while receiving. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:10834,install,install,10834,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,3,['install'],['install']
Deployability," reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1147,install,installed-,1147,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['install'],['installed-']
